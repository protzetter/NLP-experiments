{"cells":[{"cell_type":"markdown","source":"Created on December 20th 2020 by Patrick Rotzetter\n\nhttps://www.linkedin.com/in/rotzetter/\n\n# Small experiment of document mining with various techniques Part 1\n\nIn the first part, we will be using spacy for tokenization, parsing and named entity recognition. We will then look at some simple statistics based on spacy results.","metadata":{"cell_id":"00000-7079e1ca-a2f2-48e5-b5c0-c4daef44134e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00001-4ee23b8c-5b4b-4341-b555-0f5cdfb7f019","deepnote_to_be_reexecuted":false,"source_hash":"359195b9","execution_millis":18,"execution_start":1609408903481,"deepnote_cell_type":"code"},"source":"#check python version and path\nimport sys\nsys.executable","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"'/usr/local/bin/python'"},"metadata":{}}]},{"cell_type":"code","metadata":{"cell_id":"00002-5a3f2a00-7b81-4e3f-ba51-828ec90a4fb3","deepnote_to_be_reexecuted":false,"source_hash":"a807d41a","execution_millis":7825,"execution_start":1609408903502,"deepnote_cell_type":"code"},"source":"#import main libraries\nimport numpy as np\nimport PyPDF2\nimport spacy\nimport pandas as pd\nimport re\nfrom pptx import Presentation\nimport pdftotext\nimport texthero as hero","execution_count":null,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the model via spacy.load('en_core_web_sm')\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00003-c681b3d0-7d28-4ac8-b5e5-1c6644a54282","deepnote_to_be_reexecuted":false,"source_hash":"ab8c3b25","execution_millis":1714,"execution_start":1609408911331,"deepnote_cell_type":"code"},"source":"#  validate spacy language models just in case, this command does not work on Mac ARM systems unless you have installed the brew workaround and reinstalled python\n!python -m spacy validate","execution_count":null,"outputs":[{"name":"stdout","text":"\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n\u001b[1m\n====================== Installed models (spaCy v2.3.5) ======================\u001b[0m\n\u001b[38;5;4mℹ spaCy installation: /opt/venv/lib/python3.7/site-packages/spacy\u001b[0m\n\nTYPE      NAME             MODEL            VERSION                            \npackage   en-core-web-lg   en_core_web_lg   \u001b[38;5;2m2.3.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00004-d91201ad-7d3d-4903-a370-5602331274d9","deepnote_to_be_reexecuted":false,"source_hash":"c04b7596","execution_millis":3,"execution_start":1609408913052,"deepnote_cell_type":"code"},"source":"# function to read PDF files using PYPdf2\ndef readPdfFilePY(filename):\n    text=\"\"\n    read_pdf = PyPDF2.PdfFileReader(filename,'rb')   \n    for i in range(read_pdf.getNumPages()):\n        page = read_pdf.getPage(i)\n        txt=page.extractText()\n        text=text+txt\n    return text\n","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00005-a1901dd3-c023-404a-ad5e-54bb9e243052","deepnote_to_be_reexecuted":false,"source_hash":"528c1789","execution_millis":6,"execution_start":1609408913062,"deepnote_cell_type":"code"},"source":"# function to read PDF files using pdftotext\ndef readPdfFile(filename):\n    text=\"\"\n    with open(filename, \"rb\") as f:\n        pdf = pdftotext.PDF(f)\n        for page in pdf:\n            text=text+page\n    return text","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00006-7ffcd334-575e-4630-9cdc-b490391638ea","deepnote_to_be_reexecuted":false,"source_hash":"c5e1e201","execution_millis":7,"execution_start":1609408913073,"deepnote_cell_type":"code"},"source":"# function to read PPT files\ndef readPPTFile(filename):\n    text=\"\"  \n    prs = Presentation(filename)\n    for slide in prs.slides:\n        for shape in slide.shapes:\n            if hasattr(shape, \"text\"):\n                text=text+shape.text\n    text=remove_special_characters(text)\n    return text","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00007-ec1a18d4-c491-4d83-bd0b-89d80d023486","output_cleared":false,"source_hash":"bb2fc269","execution_millis":0,"deepnote_to_be_reexecuted":false,"execution_start":1609409219029,"deepnote_cell_type":"code"},"source":"#path of first input test file\npath='./sampledocs/'\ndocFile = path+'Technology-and-innovation-in-the-insurance-sector.pdf' ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00008-edb5c9c2-54e2-45ed-96da-d747f9899901","deepnote_to_be_reexecuted":false,"source_hash":"8b32020b","execution_millis":156,"execution_start":1609409220492,"deepnote_cell_type":"code"},"source":"#read the PDF File\n \ntextFromPdf=readPdfFile(docFile)\nlen(textFromPdf)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"126362"},"metadata":{}}]},{"cell_type":"code","metadata":{"cell_id":"00009-fc0448e8-7ba3-4e60-8959-895843d4f751","deepnote_to_be_reexecuted":false,"source_hash":"148ad8dd","execution_start":1609409222189,"execution_millis":7,"deepnote_cell_type":"code"},"source":"# print the start of file to check we have read it correctly\nprint(textFromPdf[0:100])","execution_count":null,"outputs":[{"name":"stdout","text":"Technology and\ninnovation in the\ninsurance sector\nTECHNOLOGY AND INNOVATION\n  IN THE INSURANCE SECTO\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00010-11ccde72-7d82-46c4-a80a-3632f634dbaa","deepnote_to_be_reexecuted":false,"source_hash":"3aab7773","execution_start":1609409223044,"execution_millis":2,"deepnote_cell_type":"code"},"source":"#path of second input file\ndocFile = path+'Digital-disruption-in-Insurance.pdf' ","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00011-d1f41ce8-cbfe-4078-b3d4-ac372ae82f1f","deepnote_to_be_reexecuted":false,"source_hash":"a3228439","execution_start":1609409223786,"execution_millis":799,"deepnote_cell_type":"code"},"source":"#read the second PDF file   \ntextFromPdf2=readPdfFile(docFile)\nlen(textFromPdf2)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":19,"data":{"text/plain":"449846"},"metadata":{}}]},{"cell_type":"code","metadata":{"cell_id":"00012-a10dadf0-e73c-4dcc-a7de-e724cc0fe202","deepnote_to_be_reexecuted":false,"source_hash":"2120c43f","execution_start":1609409224588,"execution_millis":9,"deepnote_cell_type":"code"},"source":"# print the file to check we have read it correctly\nprint(textFromPdf2[0:100])","execution_count":null,"outputs":[{"name":"stdout","text":"Digital disruption\nin insurance:\nCutting through\nthe noise\n Contents\n Preface\t\t\t\t\t\t\t\t               \n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Process files with spacy and calculate their similarity","metadata":{"cell_id":"00013-ef2036de-3984-4291-bfa2-f1ee6019c9c3","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00014-507e8032-a56f-4a2a-8d6c-a4f7951ec65a","deepnote_to_be_reexecuted":false,"source_hash":"e0cff2","execution_start":1609409226373,"execution_millis":6076,"deepnote_cell_type":"code"},"source":"# load spacy with large English model\n# nlp = spacy.load(\"en-core-web-lg\") -- was working on Windows, but now on a Mac\nimport en_core_web_lg\nnlp = en_core_web_lg.load()","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00015-5a9a6acb-dc7c-4335-9fec-43b61059b819","deepnote_to_be_reexecuted":false,"source_hash":"ac8654df","execution_start":1609409232449,"execution_millis":14113,"deepnote_cell_type":"code"},"source":"# let us process the 2 files using spacy pipeline\ndocpdf1=nlp(textFromPdf)\ndocpdf2=nlp(textFromPdf2)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00016-cf670244-9dcf-438e-ac68-11a7e12d2f20","deepnote_to_be_reexecuted":false,"source_hash":"c607b422","execution_start":1609409246569,"execution_millis":269,"deepnote_cell_type":"code"},"source":"# let us check the document vector shape\ndocpdf1.vector.shape","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":23,"data":{"text/plain":"(300,)"},"metadata":{}}]},{"cell_type":"code","metadata":{"cell_id":"00017-742a4d24-042e-4f45-87a8-50dba5f3353f","deepnote_to_be_reexecuted":false,"source_hash":"9df9cb86","execution_start":1609409246842,"execution_millis":4451,"deepnote_cell_type":"code"},"source":"# calculate document similarity with spacy function\ndocpdf1.similarity(docpdf2)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":24,"data":{"text/plain":"0.9921302281086746"},"metadata":{}}]},{"cell_type":"code","metadata":{"cell_id":"00018-88ece158-678f-4f1d-a28d-d51d8b00a274","deepnote_to_be_reexecuted":false,"source_hash":"a95fdbac","execution_start":1609409251302,"execution_millis":5,"deepnote_cell_type":"code"},"source":"# document similarity can also be calculated by multiplying the document vectors. we will use this later on\nnp.dot(docpdf1.vector,docpdf2.vector)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":25,"data":{"text/plain":"6.81796"},"metadata":{}}]},{"cell_type":"markdown","source":"## Process all directory files with spacy","metadata":{"cell_id":"00019-0d4e055e-df16-462b-b102-d6b6ce8e0d03","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00020-e077d573-9d1d-4d08-96ac-e94be911fcb4","deepnote_to_be_reexecuted":false,"source_hash":"31b9d7f8","execution_start":1609409251311,"execution_millis":7,"deepnote_cell_type":"code"},"source":"# helper function to process documents in an apply function and return the nlp object\ndef processDoc(doc):\n    return nlp(doc)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00021-4abcc142-8b60-4c8d-8eb7-6dfc54ede2cf","deepnote_to_be_reexecuted":false,"source_hash":"4ff71c02","execution_start":1609409251322,"execution_millis":25755,"deepnote_cell_type":"code"},"source":"# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n\ndocName=[]\ndocType=[]\ndocText=[]\ndocNLP=[]\nimport glob\nlist_of_files = glob.glob(path+'*.pdf')           # create the list of file\nfileNames=[]\nfor file_name in list_of_files:\n    fileText=readPdfFile(file_name)\n    docName.append(file_name)\n    docType.append('pdf')\n    docText.append(fileText)\nlist_of_files = glob.glob(path+'*.pptx')           # create the list of file\nfor file_name in list_of_files:\n    fileText=readPPTFile(file_name)\n    docName.append(file_name)\n    docType.append('ppt')\n    docText.append(fileText)\nfullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\nfullDocs['cleanText']=hero.clean(fullDocs['Text'])\nfullDocs['NLP']=fullDocs['cleanText'].apply(processDoc)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00022-f30704b2-24ac-4575-a46b-c496aba2c153","deepnote_to_be_reexecuted":false,"source_hash":"2775c8a4","execution_start":1609409277078,"execution_millis":0,"deepnote_cell_type":"code"},"source":" print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))","execution_count":null,"outputs":[{"name":"stdout","text":"Average length of text:193429.625\nMin length of text:17792\nMax length of text:449846\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00023-477dc6c1-2d27-40f1-8aa6-41969f133484","deepnote_to_be_reexecuted":false,"source_hash":"aa02f66","execution_start":1609409277796,"execution_millis":581,"deepnote_cell_type":"code"},"source":"fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\nfullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\nfullDocs.head()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":30,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":5,"column_count":7,"columns":[{"name":"Name","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"./sampledocs/Module-1-Lecture-Slides.pdf","count":1},{"name":"./sampledocs/fra-2020-artificial-intelligence_en.pdf","count":1},{"name":"3 others","count":3}]}},{"name":"Type","dtype":"object","stats":{"unique_count":1,"nan_count":0,"categories":[{"name":"pdf","count":5}]}},{"name":"Text","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"Application of AI, Insurtech and Real Estate\nTechnology\nIntroduction to Insurtech\nProfessor Christopher Geczy, PhD\n  Introduction to Insurtech\n  • The insurance industry is global and diversified across applications and\n    subsegments\n  • Gross Written Premium (GWP) was $4.8 trillion in 2017*\n      • The amount of insurance written, not including commissions and costs\n  • Net income worldwide grew 23% last year, vs. actual growth of 14% in\n    2017*\n                                                       * Ernst & Young, “Global Insurance Trends Analysis 2018,” June 2018.\n5\n   Introduction to Insurtech\n   • The insurance industry needs to respond to technological change & disruption\n       • Wearables\n       • Driverless vehicles\n       • Internet of Things\n       • “Big Data”\n       • Natural language processing\n       • Blockchain\n       • Distributed ledger technologies\n       • Climate change\n       • Etc.\n   • New technologies offer opportunities to increase efficiency in the industry and\n     serve new markets\n                                                       * Ernst & Young, “Global Insurance Trends Analysis 2018,” June 2018.\n16\n   Introduction to Insurtech\n   • There is no standardized definition of insurtech\n   • It is said to be revolutionizing the insurance industry and changing the way\n     insurers do business\n                                             * Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.\n18\n“ InsurTech can be described as “an insurance company,\n  intermediary, or insurance value chain segment specialist that\n  utilizes technology to either compete or provide valued-added\n  benefits to the insurance industry.”\n                                               - SIA PARTNERS, 2016\n   Key Emerging Technologies Leveraged by Insurtech Companies\n                                                 Source: Capgemini World InsurTech Report 2018\n20\nApplication of AI, Insurtech and Real Estate\nTechnology\nEmerging Technologies: AI & Machine Learning\nProfessor Christopher Geczy, PhD\n   Key Emerging Technologies Leveraged by Insurtech Companies\n   • Artificial Intelligence\n      • A descriptor for software which can perform functions ordinarily\n         associated with human reasoning\n          • Iterative learning\n          • Self-awareness & emotions\n      • Insurers hope to exploit AI for chatbots\n      • Allstate’s Allstate Business Insurance\n         Expert (ABIE)\n          • Provides answers in real time to\n             customer owners' questions\n          • Other insurers have followed\n                                                  Source: “What Is Insurtech and How Are Insurers Using It?”\n                                                  https://www.thebalancesmb.com/what-is-insurtech-4584490.\n                                                  Graphic: Accenture presentation: “Accenture’s 2017 Technology Vision for Insurance.”\n37\n   Key Emerging Technologies Leveraged by Insurtech Companies\n   Machine Learning\n   • Enables computers to \"learn\" over time\n   • Using algorithms & mathematical models to simulate neural networks in the\n     human brain\n   • Allows computers to extract patterns from raw data rather than following\n     specific instructions\n   • Gives the appearance of being closer to the activities of the human brain\n   • Some insurance companies amass large amounts of data\n      • Yet, according to the National Association of Insurance Commissioners,\n         most insurers use only 10 to 15% of the data they collect\n                                                               Source: “What Is Insurtech and How Are Insurers Using It?”\n                                                               https://www.thebalancesmb.com/what-is-insurtech-4584490.\n44\n   Key Emerging Technologies Leveraged by Insurtech Companies\n   • Machine learning could allow insurers to mine their data more effectively\n     and extract valuable information\n      • Risk modeling: Analyze claims data to predict the risk of future losses\n      • Demand modeling: Predict demand for their products in the future and\n        to estimate premiums\n      • Detecting fraud: Identify patterns of behavior that aren't obvious to\n        human adjusters\n      • Processing claims: Automate claim reporting and processing\n      • Underwriting: Help underwriters analyze data collected from applicants.\n         • Computers can aid in the decision making process, flag risks or\n            inconsistencies in data that underwriters might not be able to see\n         • Can also check external sources such as social media to verify the\n            accuracy of the data                              Source: “What Is Insurtech and How Are Insurers Using It?”\n                                                              https://www.thebalancesmb.com/what-is-insurtech-4584490.\n52\nApplication of AI, Insurtech and Real Estate\nTechnology\nRedefining the Insurance Industry\nProfessor Christopher Geczy, PhD\n   Redefining the Insurance Industry\n   1. Product Design\n   2. Selling & Marketing / Front Office\n   3. Underwriting\n   4. Policy Administration\n   5. Claims Management\n58\n   Insurtech Are Redefining the Insurance Industry\n                                                                                 Policy                       Claims\n      Product Design           Front Office           Underwriting\n                                                                            Administration                Management\n                                Marketing,\n     Actuarial Models and                             Underwriting         Policy Acquisition            Claims Servicing\n                              Distribution and\n       Product Design                                 New Policies           and Servicing                  and Payout\n                          Channel Management\n    • 360° view of        • Extended multi-      • Real-time             • Segment the market         • Streamlined claims\n      customer’ needs       device & mobility      information             based on servicing           process with low\n    • More personalized     offering               capturing               desires                      waiting time\n      product designs     • Integrated           • Advanced risk         • Automated systems          • Instant notification of\n    • Design new            omnichannel            analytics enabling      with Straight                the claim &\n      products              offerings & touch      risk-based pricing      Through Processing           proactive status\n    • Adjust products in    points               • Automated workflow      (STP) capabilities           updates\n      real time           • Real-time updates &    management & rules    • Automated ,                • Real-time claims\n    • Disaggregate          interactions with      engines                 premium reminders            status monitoring\n      product mix           clients              • Customer value-led      and renewal notice         • Advanced analytics-\n      seamlessly          • Quick identification   promotions &          • Anytime access to            based fraud\n    • Design and deliver    of cross-selling &     discounts               policy details/view          detection\n      products to the end   up-selling           • Digitized systems\n      customer they want    opportunities          with less reliance on\n                          • Detecting client       data that a customer\n                            satisfaction           needs to provide\n                          • Information\n                            availability & price\n                            transparency\n                                                                                            Source: Capgemini World InsurTech Report 2018\n63\nApplication of AI, Insurtech and Real Estate\nTechnology\nClassification of Insurtech Companies\nProfessor Christopher Geczy, PhD\n   Segmentation of Insurtech Firms\n   • There are different classification methods for InsurTech firms and\n     initiatives, but they rotate to similar concepts:\n      • A “traditional” view: Full-stack / Agents / Brokers\n      • A nuanced view of sub-segments: Carriers / Enablers / Distributors\n67\n   Segmentation of Insurtech Firms\n   • Milken Institute has three main classifications:\n     • Full-stack Insurers: Platforms that underwrite policies, assume the risk,\n        and, in most cases, manage the process from beginning to end\n     • Agents: Platforms that act on behalf of a carrier, essentially acting as an\n        extension of an incumbent carrier\n     • Brokers: Platforms that provide customers with a variety of policies\n        offered by both incumbent carriers and insurgent InsurTech platforms\n         • May or may not be paid commission based on the policies sold\n            through their platform\n         • May require customers to scroll through policies offered or\n            automatically connect customers to a preferred policy through\n            algorithms employed & based on a user’s response to a set of\n            questions\n                                         Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.\n68\n   Segmentation of Insurtech Firms\n   • To make the classification:\n                                   Yes: It is an insurer\n         Is it a full insurance                                                   One: It is an agent\n               company?\n                                 No: Is it partnered with\n                                     one or multiple\n                                 insurance companies?\n                                                                               Multiple: It is a broker\n                                             Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.\n69\n   Classification of Insurtech Firms: Capgemnini\n   • Second classification system, used by Capgemini\n      • It categorizes InsurTech providers by their role in the distribution chain:\n         • Enablers\n         • Distributors\n         • Full Carriers\n                                                               Source: Capgemini World InsurTech Report 2018\n70\n   Classification of Insurtech Firms: Capgemnini\n                                                 Source: Capgemini World InsurTech Report 2018\n71\n     Segmentation of Insurtech Firms by Model\n                                                   InsurTech Platform Models\n                                     35\n                                     30\n                                                                                                 31\n              Number of InsurTechs\n                                     25\n                                     20\n                                     15\n                                                                 18\n                                     10       12\n                                     5\n                                     0\n                                          FUL L STACK          AGENT                          BROKER\n                                                    InsurTech Platform Models\n                                                              Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.\n72\n73\n                                                                                                          A                                      No. of product offerings\n                                                                           H                                  ut\n                                                                                                                    o\n                                                                               ea\n                                                                                                                                                                 4                   10                      18\n                                                                                     lth\n                                                                                                                         (I\n                                                                                                                              nc                             0   2           6        8          12   14     16\n                                                                                                (I                                   l.\n                                   Li                                                                nd                                   bi\n                                                     fe                                                                                        ke                            4\n                                                                                                          iv\n                                                                       (u                                      /f                                    )               2\n                                                                            ni                                      am\n                                                                                 ve                                         ily                                                                        13\n                                                                                      rs\n                                                                                            al                                       he\n                                     H                                                           ,                                         al\n                                                     om                                               te                                        th                           4\n                                                                                                           rm                                        ;…\n                                                                       e/\n                                                                                                                    ,                                                2\n                                                                            R                                           U\n                                                                                 en                                         ni                                                                                     16\n                                                                                      t                                          t\n                                                                                           (I                                        Li\n                                                                                                nc                                        nk\n                                                                                                      l.                                        ed               1\n                                                                                                                                                                                     6\n                                                                                                           tit                                           …\n                                                                           D                                     le\n                                                                               is                                                                                                            9\n                                                                                    ab                                   in\n                                                                                          ili                                 su\n                                                                                                ty                                   ra\n                                                          B                                          /P                                    nc\n                                                                      us                                  er                                    e)                           4\n                                                                           in                                  so                                                    2\n                                                                                es                                      na                                                               8\n                                                                                     s                                        lL\n                              E                                                            (I                                    ia\n                                             nd                                                 nc                                        bi\n                                                                      ow                              l.                                       lit                           4\n                                                                                                           sh                                        y…\n                                                                           m                                        ip                                                           5\n                                                                                en                                       pi\n                                                                                      t,                                      ng\n                                                                                            R                                         re\n                                                                                                 et                                        tu\n                                                                      Tr                              ir\n                                                                                                           em                                   rn                       3\n                                                                           av                                       en                                   …                   4\n                                                                                el\n     Product offering categories\n                                                                                     (i                                  t,\n                                                                                          nc                                     P\n                                                                                                 l.                                  en\n                                                         S                                            in                                   si\n                                                                                                                                                on\n                                                                      pe                                   -t\n                                                                                                                ri                                                               5\n                                                                           ci\n                                                                                al                                   p\n                                                                                     ty                                  in\n                                                                                                                              su\n                                                                                           (I                                         ra\n                                                                                                nc                                         nc\n                                                                                                      l.                                        e;                   2\n                                                                                                                                                                     2\n                                                                                                           bi                                            …\n                                                                                                                 cy\n                                                                                                                         cl                                                              8\n                                                                                                                                                                                                                                Type of Insurtech Platform, by Product Offering\n                                                                                                                              e,\n                                                                                                                                     m\n                                                                                                                                          ob\n                                                                                                                                                ile                  2\n                                                                                                                                                         …                           6\n                                                                                                                                                                                                                                                                                  Segmentation of Insurtech Firms by Model and Product\n                                                                                                                                                                                     6\n              Milken Institute, “InsurTech Rising: A Profile of the\n                                                                                                                                                                                                  Broker   Agent\n                                                                                                                                                                                                                   Full Stack\n              InsurTech Landscape,” December 2018.\n   Segmentation of Insurtech Firms\n   • A significant portion (approximately 40%) of Insurtech companies could\n     better be described as technology solution providers\n      • Human Resources and Earned Benefits Solution Providers\n      • Data Solution Providers\n      • Infrastructure Solution Providers\n                                                             Milken Institute, “InsurTech Rising: A Profile of the\n                                                             InsurTech Landscape,” December 2018.\n81\n   Segmentation of Insurtech Firms\n   • Human Resources and Earned Benefits Solution Providers\n      • Platforms using or deploying technology to help firms manage their\n         human capital more efficiently and cost effectively\n   • Data Solution Providers\n      • Platforms that specialize in collecting, aggregating, and/or analyzing\n         vast quantities of data to support (re)insurance carriers, startups, and\n         other stakeholders\n   • Infrastructure Solution Providers\n      • Platforms that focus on making back-end processes more efficient\n         through the use of application programming interfaces (APIs) or that\n         provide the means by which platforms can integrate and/or build\n         customizable insurance products and services          Milken Institute, “InsurTech Rising: A Profile of the\n                                                               InsurTech Landscape,” December 2018.\n87\n     Number of Technology Solution Providers in Insurtech Market\n                                                          Insurtech Platform Models\n                                15.5\n                                 15\n                                                 15                                                   15\n                                14.5\n          Number of Providers\n                                 14\n                                13.5\n                                 13\n                                                                            13\n                                12.5\n                                 12\n                                        Human Resources and       Data Solution Provider    Infrastructure Solution\n                                       Earned Benefits Solution                                     Provider\n                                        Provider - Small, Mild,\n                                          Large Businesses\n                                                                                                     Milken Institute, “InsurTech Rising: A Profile of the\n                                                          Type of Tech-Solution Providers            InsurTech Landscape,” December 2018.\n88\n     Examples of Full-Carrier Insurtech Firms\n                     Insurtech Firm\n                                         Products/Services Offered                                   Example\n                          Type\n                                                                                 ZhongAn is a Chinese property insurer that uses\n                                         Traditional insurance model\n                     Digital Carrier                                             the online channel to sell its products and handle\n                                         conducted online or on mobile\n                                                                                 claims\n                                         A risk-sharing network where a\n                                         group of associated individuals pools   insPeer, a France-based community insurance\n                     P2P Insurer         their premiums to insure against risk   platform, allows members to pool in money within\n     Full Carriers\n                                         and generally stands to benefit         a group for covering a group member’s deductible\n                                         regarding premium returns\n                                         Smaller insurance packages with         Leveraging mobile technology, BIMA offers\n                     Micro Insurer       lower premiums and typically lower      affordable insurance products to low-income\n                                         coverage                                populations in emerging markets\n                                         On-demand insurance coverage that       New York-based Sure offers on-demand personal\n                     On-Demand Insurer   can be purchased online as well as      or episodic policies that a user can buy either via\n                                         via mobile apps                         website or an app\n                                                                                 US-based Metromile offers auto insurance with\n                     Usage-Based         Premiums prices per usage or risky\n                                                                                 fees based on the number of miles the insured’s\n                     Insurer             behavior displayed by the customer\n                                                                                 car logs\n                                                                                                    Source: Capgemini World InsurTech Report 2018.\n99\n      Examples of Full-Distributor Insurtech Firms\n                                                                                  PolicyBazaar specializes in comparative analysis of\n                                          Online site enables individuals to\n                     Marketplace                                                  products from various insurers based on price,\n                                          compare plans from different insurers\n                                                                                  quality and key benefits\n                                                                                  Artificially intelligent insurance advisory application\n                                          One-stop app(s) allows customers to\n                                                                                  Brolly delivers contextually relevant insights\n                     Personal Financial   manage all their policies, obtain\n                                                                                  through web and mobile applications, so customers\n                     Assistant            coverage recommendations and\n                                                                                  can manage policies in one place and know where\n                                          compare and purchase plans\n      Distributors\n                                                                                  coverage may be duplicated or missing.\n                                          Online platform allows customers to     Licensed broker Coverfox offers insurance products\n                     Digital Broker\n                                          compare and purchase policies           for vehicles, home, health services and travel.\n                                          Online site enables commercial          CoverHound, a US-based InsurTech firm that offers\n                     B2B Digital\n                                          customers to compare plans from         a comparison platform for personal and small\n                     Distributor\n                                          different insurers                      commercial insurance products.\n                                                                                  London-based Bought By Many uses social media\n                                          Customized or flexible front-office     data to connect people with similar insurance needs\n                     Value-Adding\n                                          solutions via partnership with an       and then uses the group’s collective buying power to\n                     Intermediary\n                                          insurer/reinsurer for risk management   negotiate with insurers for deals that aren’t available\n                                                                                  for individuals.\n                                                                                                       Source: Capgemini World InsurTech Report 2018.\n109\n      Examples of Full-Enabled Insurtech Firms\n                                                                                   PremFina’s white label solution for brokers allows\n                 Front-Office Solution   Process-improvement solutions for the\n                                                                                   them to extend premium financing options to their\n                 Providers               front office\n                                                                                   customers and manage insurance policies.\n                                                                                   RiskGenius applies artificial intelligence to streamline\n                 Policy/Plan             Process-improvement solutions for         the work of insurance professionals by retrieving\n                 Management              underwriting and policy/plan              details on a specific coverage or exclusion, analyzing\n                 Solution Provider       administration                            policies and extracting relevant information such as a\n                                                                                   premium limit or deductible.\n      Enablers\n                                                                                   RightIndem has a white-label self-service insurance\n                 Claims Management       Process-improvement solutions\n                                                                                   claims platform for insurers that allows customers to\n                 Solution Provider       specifically for claims management\n                                                                                   interact with their claim in their own time.\n                                                                                   Carpe Data leverages fata from various channels\n                                                                                   such as online content, social media, connected\n                                         Data capture or analytics solutions for\n                 Data Specialist                                                   devices and offers predictive scoring and data\n                                         use cases or across the value chain\n                                                                                   products for the insurers, enabling them to predict risk\n                                                                                   better.\n                                         Solutions based on a specific\n                 Technology                                                        Betterview,     a drone-technology specialist, allows\n                                         technology such as blockchain or                Source: Capgemini World InsurTech Report 2018.\n                 Specialist                                                        drone-based inspection or property assessment.\n                                         drones\n                                                                                                         Source: Capgemini World InsurTech Report 2018.\n110\nApplication of AI, Insurtech and Real Estate\nTechnology\nInvestment & Market Size of the Insurtech Industry\nProfessor Christopher Geczy, PhD\n    Insurtech: Market Size\n    • What’s clear is that it’s large and growing\n    • Global Insurtech market revenue $532.7MM as of 2018*\n    • Market revenue expected to reach $1.2 billion by 2023 (+16% CAGR, 2018-\n      2023) *\n    • AsiaPacific will have highest regional CAGR, growing in financial hubs in\n      Hong Kong/Singapore/India\n       • Health insurance segment is expected to have the higher segment CAGR*\n    • Total Insurtech investments, 2017: $3.2 billion **\n    • Total deals, 2017: 202 deals for $2.2 billion\n    • 83% involved an insurer/reinsurer as investor **\n    • Estimated 5-year CAGR, 2012-2017: +45% **          *Orbis Research, “Global (Insurance Technology) InsurTech\n                                                         Market Size 2019,” Dec. 2018. **Ernst & Young, “Global\n                                                         Insurance Trends Analysis 2018,” June 2018.\n120\n    Size of Insurtech Market\n                                                                    258\n    Global private investment                         $12.3                               242\n    (VC, PE and M&A)\n                                                                 $10.3\n    in insurtech\n    2013-2018                                164       165\n                           114\n                   89                                                                   $5.7\n                                           $4.0\n                          $2.4\n                 $1.6\n                  2013     2014             2015       2016        2017                  2018\n                                Capital invested ($B)       Deal count\n                                                               Source: KPMG International: Pulse of Fintech 2018,\n                                                               Global Analysis of Investment in Fintech, January 4, 2019\n121\n    Insurtech: Startup Count\n            Number of Insurtech Startups Exploded Over Just 2 Years\n                  (Source: SMA – Strategy Meets Action, 2017)\n122\n    Insurtech: Disruptors\n                          The Top Insurtech Disruptors\n123\n    Insurtech: Key Techs\n    • More and more insurers are moving key business functions to the cloud*\n    • Tech research firm Ovum’s annual survey of penetration of Software as a\n      Service (SaaS) grew from 13% in 2016 to 26% last year\n                                                * Source: Ovum survey data cited in Deloitte, “2019 Insurance Industry Outlook”\n125\n    Insurtech: Key Techs\n                         * Source: Ovum survey data cited in Deloitte, “2019 Insurance Industry Outlook”\n126\n    Insurtech: Key Techs\n    • AIA Hong Kong has launched a blockchain app to share life policy data\n      with its bank distributors\n    • AXA Europe is offering flight delay insurance on a blockchain platform\n      featuring smart contracts\n    • Ovum’s annual survey of penetration of Software as a Service (SaaS) grew\n      from 13% in 2016 to 26% last year.\n    • Carriers and consortiums are expected to launch more impactful\n      blockchain initiatives due to concerns around data technology\n130\n    Profile of Insurtech Market\n    • In the U.S., 63 Insurtech deals, with a total value of $1.59 billion, were\n      announced in Q4 2018\n       • Compared with Q4 of 2017, deal count in Q4 increased by 24%, while\n          funding volume also increased by 155%\n       • 63 transactions in Q4 2018 – higher than Q3, but lower than Q1 & Q2\n    • Globally\n       • UK investment down 9% from last quarter\n       • China the second largest investor for Q4 after the U.S.\n       • UK has been responsible for 8% of total investment since 2012\n       • Investment from international markets remains strong; transactions\n          outside of the U.S. account for 43% of total transactions since 2012 and\n          57% in the 4th quarter of 2018\n                                                               *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n129\n    Profile of Insurtech Market\n    • While early-stage investments remain strong\n       • Seed and Series A account for 64% of total transactions since 2012 and\n          62% this quarter (up 4% from last quarter)\n    • Insurtech funding is maturing to mid – and later-stages – 45% of financings\n      in 2018 took place at the Series A, B, or C stages\n       • Could lead to consolidation further up\n                                                             *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n133\n    Profile of Insurtech Market\n    • Property and Casualty (P&C) funding volume increased by 57% from Q3\n      2018 and increased by 89% from Q4 2017\n       • 41 P&C transactions in the quarter was only marginally higher than the\n          40 transactions in Q3 but marks a 52%\n    • Life and Health (L&H) funding volume increased by 1% from Q3 2018 but\n      marked a 362% increase from Q4 2017\n       • 2018 hits record level of Insurtech investment, driven by large\n          investments; deal count increased by 10% from Q3 2018 and funding\n          volume increased by 26%\n                                                             *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n137\n    Size of Insurtech Market\n             Quarterly Insurtech funding volume – all stages\n             ($ in millions)\n         $2,000\n                                                                                                                    Property &\n         $1,800\n                                                                                                                    Casualty\n         $1,600\n         $1,400\n                                                                                                                     Life & Health\n         $1,200\n         $1,000\n           $800\n           $600\n           $400\n           $200\n              $0\n                   Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4\n                     2012         2013         2014        2015        2016          2017          2018\n           Deal Count\n            P&C   5  3   4   4  5  4   12 9 10    7 16  8 16  14 16 20  44  18 33 29   23   33 27 42  43  44   40   41\n            L&H   8  6   7   9 15  8    9 4  9   15 14 15 10  19 14 20  15  16  6 13   16   32 21  9  23  27   17   22\n                                                                                          *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n138\n      Size of Insurtech Market\n            Quarterly Insurtech transaction by target country\n                                          18%                        United States                                                 United States\n                                                                     United Kingdom                   24%                          China\n                   2012 – Q4 2018\n                                     2%                              China                                                         United Kingdom\n                                                                                      Q4 2018\n                                    4%                               Germany                                              43%      Germany\n                                    5%                               India                       5%                                South Africa\n                                                           57%\n                                    6%                               France                                                        Other\n                                                                                                  6%\n                                                                     Other\n                                          8%                                                           9%\n                                                                                                                13%\n                                                 2013-Q4 2018 Transactions: 972                               Q4 2018 Transactions: 63\n            Quarterly InsurTech transaction by investments stage\n                                           13%                       Seed/Angel                         16%\n                                                                                                                                     Seed/Angel\n                  2012 – Q4 2018\n                                      3%                             Series A\n                                     2%                                                                                 32%          Series A\n                                                                                                 0%3%\n                                                                                       Q4 2018\n                                    5%                    41%        Series B                                                        Series B\n                                                                                                  5%\n                                                                     Series C                                                        Series C\n                                    13%                              Series D                                                        Series D\n                                                                     Series E+                    14%                                Series E+\n                                                                     Other                                                           Other\n                                                23%\n                                                                                                                 30%\n                                           2013-Q4 2018 Transactions: 972                                     Q4 2018 Transactions: 63\n                                                                                                                       *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n139\n    Size of Private Technology Investment by Insurers/Reinsurers\n     Private technology investments by (re)insurers\n       140\n                                                                    118                118\n       120                                                105\n       100                                                            33                 31\n                                                            26\n        80                                       66                                      26\n                                                                      28\n                                                            28\n        60\n                                                  22\n        40                           29                     27\n                                                                      31                 34\n                                                  19\n        20      1          4                      14\n                                                            24        26                 27\n                                                  11\n         0\n               2012       2013       2014        2015      2016      2017               2018\n                                        Q1    Q2      Q3 Q4\n                                                                CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n144\nApplication of AI, Insurtech and Real Estate\nTechnology\nInsurtech, Fintech, & Financial Inclusion\nProfessor Christopher Geczy, PhD\n    Microinsurance\n    • Investment firm: Omidyar Network\n       • Started by eBay founder Pierre Omidyar in 2004\n       • Model: Combines for-profit LLC with grantmaking\n         501(c)(3)\n       • $1.5B committed since inception:                          Pierre & Pam Omidyar\n          • $676MM for-profit investments, $782MM grants\n                                                         Total Commitments by\n                                                          Year, 2004-YTD 2019\n                                                                          https://www.omidyar.com/financials\n150\n    Microinsurance\n    • Common feature of microinsurance products is the low-income, low-net-\n      worth population served which otherwise has very limited access to\n      insurance\n    • Microinsurance products by type (can be group or individual coverage)\n       • Term life\n       • Health / accident / disability\n       • Casualty (crop insurance, livestock, theft, fire, natural disaster)\n       • Certain forms of retirement savings plans\n       • Microinsurance by underwriter / delivery channel type\n          • Large multinational insurance companies\n          • Credit unions or mutual associations\n          • Government or NGOs\n          • Small community organizations\n161\n    Microinsurance\n    • Benefits of microinsurance\n       • Financial protection through risk pooling\n       • Insureds can assume more risk\n          • Crop insurance vs. drought enables small farmers to plant crops which\n             have higher yields in “good” years & poorer yields in drought years\n          • Safeguard vs. families falling back into poverty due to illness, death of\n             breadwinner, housing destroyed, etc.\n          • Indian ministry of health found one-quarter of all hospitalizations\n             pushed individual or family into poverty due to cost of treatment*\n                                              * Tina Rosenberg, “The Microinsurance Revolution,” New York Times Opinionator blog, June 6, 2012 at\n                                                http://opinionator.blogs.nytimes.com/2012/06/06/the-microinsurance-revolution/\n167\n    Microinsurance\n    • Benefits of microinsurance\n       • Can target specific at-risk populations\n          • HIV-positive, living in flood zone, microentrepreneurs\n       • Can complement social welfare programs, bolster other microfinance\n         initiatives\n          • Can assign term life policy to secure business, education or mortgage\n              loan\n                                              * Tina Rosenberg, “The Microinsurance Revolution,” New York Times Opinionator blog, June 6, 2012 at\n                                                http://opinionator.blogs.nytimes.com/2012/06/06/the-microinsurance-revolution/\n172\n    Microinsurance\n    • BIMA – insurance business disruptor,\n      “Insurtech” leader\n    • Swedish-founded mobile insurance & health\n      company that provides accident, life,\n      & health insurance products to 26 million low-income consumers in 15\n      countries (Africa, Asia, LatAm)\n       • Largest markets: Ghana, Sri Lanka, Bangladesh, Pakistan.\n    • Mobile technology lowered prices and brought affordable insurance to\n      the world’s poorest\n                                  TechCrunch https://techcrunch.com/2017/12/19/bima-raises-97m-from-allianz-for-microinsurance-aimed-at-emerging-markets/\n                                  BIMA website: http://www.bimamobile.com/about-bima/about-us-new/\n176\n    Microinsurance\n    • Business model: BIMA provides microinsurance subscription via basic\n      mobile phone service for as little as 60c/month on a “pay-as-you-go” rolling\n      monthly cover\n       • Offers payouts of up to $1,000 to the family if the insured person dies\n       • Sign-up takes only 3 minutes and payments are collected through basic\n          mobile phone service\n    • Customer growth: over 550 thousand new customers/month (BIMA, 2018)\n    • 93% of customers live on less than $10/day (BIMA, 2018)\n    • 75% of subscribers never had insurance before\n    • Educating customers is the top priority\n    • $300MM valuation at Dec-2017 sale of LeapFrog Investments’ stake to\n      Allianz for $97MM\n                                 TechCrunch https://techcrunch.com/2017/12/19/bima-raises-97m-from-allianz-for-microinsurance-aimed-at-emerging-markets/\n                                 BIMA website: http://www.bimamobile.com/about-bima/about-us-new/\n184\n      Size of Private Technology Investment by Insurers/Reinsurers\n            Private technology investments by (re)insurers by target country\n                                          16%                        United States                                                United States\n                                                                                                       20%\n                                                                     France\n                   2012 – Q4 2018\n                                                                                                                                  China\n                                     3%                              China                                               33%\n                                                                                      Q4 2018\n                                    5%                               United Kingdom                                               Germany\n                                    6%                      54%      Germany                     13%                              United Kingdom\n                                                                     Canada                                                       Other\n                                     8%                              Other\n                                           8%                                                          13%\n                                                                                                                   21%\n                                                 2013-Q4 2018 Transactions: 972                              Q4 2018 Transactions: 63\n            Private technology investments by (re)insurers by investment stage\n                                                                                                         12%      11%\n                                           11%       16%             Seed/Angel                                                         Seed\n                                      3%\n                                     4%                              Series A                     12%                                   Series A\n                  2012 – Q4 2018\n                                                                     Series B                                             19%           Series B\n                                                                                       Q4 2018\n                                    13%                              Series C                                                           Series C\n                                                                     Series D                                                           Series E+\n                                                           29%                                    11%\n                                                                     Series E+                                                          Other\n                                                                     Other\n                                          24%                                                                   35%\n                                           2013-Q4 2018 Transactions: 972                                    Q4 2018 Transactions: 63\n                                                                                                                    *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n185\n","count":1},{"name":"GETTING THE\nFUTURE RIGHT\n―\nARTIFICIAL\nINTELLIGENCE AND\nFUNDAMENTAL\nRIGHTS\n                   REPORT\n© European Union Agency for Fundamental Rights, 2020\nReproduction is authorised provided the source is acknowledged.\nFor any use or reproduction of photos or other material that is not under the European Union Agency for Fundamental Rights\ncopyright, permission must be sought directly from the copyright holders.\nNeither the European Union Agency for Fundamental Rights nor any person acting on behalf of the Agency is responsible\nfor the use that might be made of the following information.\nLuxembourg: Publications Office of the European Union, 2020\nPrint       ISBN 978-92-9474-861-4        doi:10.2811/58563        TK-03-20-119-EN-C\nPDF         ISBN 978-92-9474-860-7        doi:10.2811/774118       TK-03-20-119-EN-N\n© Photo credits:\nCover: HQUALITY/Adobe Stock                                            Page 61: Copyright © 2020 CODED BIAS - All Rights Reserved\nPage 5: Mimi Potter/Adobe Stock                                        Page 63: Siberian Art/Adobe Stock\nPage 8: monsitj/Adobe Stock                                            Page 68: Good Studio/Adobe Stock\nPage 14: Monsitj/Adobe Stock                                           Page 75: Sikov/Adobe Stock\nPage 16: Mykola Mazuryk/Adobe Stock                                    Page 79: robsonphoto/Adobe Stock\nPage 20: metamorworks/Adobe Stock                                      Page 82: thodonal/Adobe Stock\nPage 25: Gorodenkoff/Adobe Stock                                       Page 86: blackboard/Adobe Stock\nPage 28: Dimco/Adobe Stock                                             Page 88: blackboard/Adobe Stock\nPage 32: VideoFlow/Adobe Stock                                         Page 92: Monopoly919/Adobe Stock\nPage 37: zapp2photo/Adobe Stock                                        Page 95: Gorodenkoff/Adobe Stock\nPage 41: bestforbest/Adobe Stock                                       Page 96: Freedomz/Adobe Stock\nPage 44: zapp2photo/Adobe Stock                                        Page 100: Copyright © 2020 CODED BIAS - All Rights Reserved\nPage 47: European Communities                                          Page 103: Copyright © 2020 CODED BIAS - All Rights Reserved\nPage 52: blacksalmon/Adobe Stock\nForeword\n         Did you know that artificial intelligence already plays a role in deciding\n         what unemployment benefits someone gets, where a burglary is likely to\n         take place, whether someone is at risk of cancer, or who sees that catchy\n         advertisement for low mortgage rates?\n         We speak of artificial intelligence (AI) when machines do the kind of things\n         that only people used to be able to do. Today, AI is more present in our lives\n         than we realise – and its use keeps growing. The possibilities seem endless.\n         But how can we fully uphold fundamental rights standards when using AI?\n         This report presents concrete examples of how companies and public\n         administrations in the EU are using, or trying to use, AI. It discusses the\n         potential implications for fundamental rights and shows whether and how\n         those using AI are taking rights into account.\n         FRA interviewed just over a hundred public administration officials, private\n         company staff, as well as diverse experts – including from supervisory and\n         oversight authorities, non-governmental organisations and lawyers – who\n         variously work in the AI field.\n         Based on these interviews, the report analyses how fundamental rights are\n         taken into consideration when using or developing AI applications. It focuses\n         on four core areas – social benefits, predictive policing, health services and\n         targeted advertising. The AI uses differ in terms of how complex they are,\n         how much automation is involved, their potential impact on people, and how\n         widely they are being applied.\n         The findings underscore that a lot of work lies ahead – for everyone.\n         One way to foster rights protection is to ensure that people can seek remedies\n         when something goes awry. To do so, they need to know that AI is being\n         used. It also means that organisations using AI need to be able to explain\n         their AI systems and how they deliver decisions based on them.\n         Yet the systems at issue can be truly complex. Both those using AI systems,\n         and those responsible for regulating their use, acknowledge that they do not\n         always fully understand them. Hiring staff with technical expertise is key.\n         Awareness of potential rights implications is also lacking. Most know that\n         data protection can be a concern, and some refer to non-discrimination. They\n         are less aware that other rights – such as human dignity, access to justice and\n         consumer protection, among others – can also be at risk. Not surprisingly,\n         when developers review the potential impact of AI systems, they tend to\n         focus on technical aspects.\n         To tackle these challenges, let’s encourage those working on human rights\n         protection and those working on AI to cooperate and share much-needed\n         knowledge – about tech and about rights.\n                                                                                         1\n  Those who develop and use AI also need to have the right tools to assess\n  comprehensively its fundamental rights implications, many of which may not\n  be immediately obvious. Accessible fundamental rights impact assessments\n  can encourage such reflection and help ensure that AI uses comply with\n  legal standards.\n  The interviews suggest that AI use in the EU, while growing, is still in its\n  infancy. But technology moves quicker than the law. We need to seize the\n  chance now to ensure that the future EU regulatory framework for AI is firmly\n  grounded in respect for human and fundamental rights.\n  We hope the empirical evidence and analysis presented in this report spurs\n  policymakers to embrace that challenge.\n                                                          Michael O’Flaherty\n                                                                      Director\n2\nContents\n  Foreword \b��������������������������������������������������������������������������������������������������������������������������������������������������������������� 1\n  Key findings and FRA opinions \b��������������������������������������������������������������������������������������������������������������������������� 5\n  1     AI AND FUNDAMENTAL RIGHTS – WHY IT IS RELEVANT FOR POLICYMAKING \b������������������������������� 15\n        1.1.   WHY THIS REPORT? \b����������������������������������������������������������������������������������������������������������������������� 17\n        1.2.   WHAT DO WE MEAN BY ARTIFICIAL INTELLIGENCE? \b���������������������������������������������������������������� 19\n        1.3.\t\u0007 AI AND FUNDAMENTAL RIGHTS IN THE EU POLICY FRAMEWORK: MOVING\n               TOWARDS REGULATION \b��������������������������������������������������������������������������������������������������������������� 21\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 24\n  2     PUTTING FUNDAMENTAL RIGHTS IN CONTEXT – SELECTED USE CASES OF AI IN THE EU \b������������� 25\n        2.1.   EXAMPLES OF AI USE IN PUBLIC ADMINISTRATION \b���������������������������������������������������������������� 30\n        2.2. EXAMPLES OF AI USE IN THE PRIVATE SECTOR \b������������������������������������������������������������������������ 37\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 45\n  3     FUNDAMENTAL RIGHTS FRAMEWORK APPLICABLE TO AI \b������������������������������������������������������������ 47\n        3.1.   FUNDAMENTAL RIGHTS FRAMEWORK GOVERNING THE USE OF AI \b�������������������������������������� 47\n        3.2.   ‘USE CASE’ EXAMPLES \b���������������������������������������������������������������������������������������������������������������� 50\n        3.3.   REQUIREMENTS FOR JUSTIFIED INTERFERENCES WITH FUNDAMENTAL RIGHTS \b������������������ 52\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 54\n  4     IMPACT OF CURRENT USE OF AI ON SELECTED FUNDAMENTAL RIGHTS \b���������������������������������������� 57\n        4.1.   PERCEIVED RISKS \b�������������������������������������������������������������������������������������������������������������������������� 57\n        4.2.   GENERAL AWARENESS OF FUNDAMENTAL RIGHTS AND LEGAL FRAMEWORKS IN\n               THE AI CONTEXT \b�������������������������������������������������������������������������������������������������������������������������� 58\n        4.3.   HUMAN DIGNITY \b�������������������������������������������������������������������������������������������������������������������������� 60\n        4.4.   RIGHT TO PRIVACY AND DATA PROTECTION – SELECTED CHALLENGES \b��������������������������������� 61\n        4.5.   EQUALITY AND NON-DISCRIMINATION \b�������������������������������������������������������������������������������������� 68\n        4.6.   ACCESS TO JUSTICE \b���������������������������������������������������������������������������������������������������������������������� 75\n        4.7.   RIGHT TO SOCIAL SECURITY AND SOCIAL ASSISTANCE \b������������������������������������������������������������ 79\n        4.8.   CONSUMER PROTECTION \b������������������������������������������������������������������������������������������������������������ 79\n        4.9.   RIGHT TO GOOD ADMINISTRATION \b��������������������������������������������������������������������������������������������� 81\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 83\n  5     FUNDAMENTAL RIGHTS IMPACT ASSESSMENT – A PRACTICAL TOOL FOR PROTECTING\n        FUNDAMENTAL RIGHTS \b�������������������������������������������������������������������������������������������������������������������� 87\n        5.1.   CALLING FOR A FUNDAMENTAL RIGHTS IMPACT ASSESSMENT – AVAILABLE\n               GUIDANCE AND TOOLS \b���������������������������������������������������������������������������������������������������������������� 87\n        5.2.   IMPACT ASSESSMENTS AND TESTING IN PRACTICE \b���������������������������������������������������������������� 91\n        5.3.   FUNDAMENTAL RIGHTS IMPACT ASSESSMENT IN PRACTICE \b�������������������������������������������������� 96\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 99\n  6     MOVING FORWARD: CHALLENGES AND OPPORTUNITIES \b������������������������������������������������������������� 101\n                                                                                                                                                                              3\n  Figures\n  Figure 1: Companies using AI in 2020, by Member State (%) \b������������������������������������������������������������������������������������������������������������� 26\n  Figure 2: Examples of different automation and complexity levels in use cases covered \b��������������������������������������������������������������� 27\n  Figure 3: Words interviewees most often used to describe the AI ‘use cases’ \b��������������������������������������������������������������������������������� 29\n  Figure 4: Awareness of GDPR right to opt out from direct marketing, in the EU and United Kingdom, by country and\n            region (%) \b��������������������������������������������������������������������������������������������������������������������������������������������������������������������������������� 65\n  Figure 5: Awareness of right to have a say when decisions are automated, by age, gender and difficulty in paying bills (%) \b������� 67\n  Figure 6: Awareness about the risks of discrimination when using AI, by country (%) \b������������������������������������������������������������������� 73\n  Figure 7: Correlations of words respondents most often mention when discussing future plans to use AI \b������������������������������� 102\n4\nKey findings and FRA opinions\nNew technologies have profoundly changed how we organise and live\nour lives. In particular, new data-driven technologies have spurred the\ndevelopment of artificial intelligence (AI), including increased automation of\ntasks usually carried out by humans. The COVID-19 health crisis has boosted\nAI adoption and data sharing – creating new opportunities, but also challenges\nand threats to human and fundamental rights.\nDevelopments in AI have received wide attention by the media, civil\nsociety, academia, human rights bodies and policymakers. Much of that\nattention focuses on its potential to support economic growth. How different\ntechnologies can affect fundamental rights has received less attention. To\ndate, we do not yet have a large body of empirical evidence about the wide\nrange of rights AI implicates, or about the safeguards needed to ensure that\nthe use of AI complies with fundamental rights in practice.\nOn 19 February 2020, the European Commission published a White Paper on\nArtificial Intelligence – A European approach to excellence and trust. It outlines\nthe main principles of a future EU regulatory framework for AI in Europe.\nThe White Paper notes that it is vital that such a framework is grounded in\nthe EU’s fundamental values, including respect for human rights – Article 2\nof the Treaty on European Union (TEU).\nThis report supports that goal by analysing fundamental rights implications\nwhen using artificial intelligence. Based on concrete ‘use cases’ of AI\nin selected areas, it focuses on the situation on the ground in terms of\nfundamental rights challenges and opportunities when using AI.\n                                                                                   5\n                   The overarching fundamental rights framework* that applies to the use of AI in the\n  Legal            EU consists of the Charter of Fundamental Rights of the EU (the Charter) as well as the\n  framework        European Convention on Human Rights.\n                   Multiple other Council of Europe and international human rights instruments are relevant.\n                   These include the 1948 Universal Declaration of Human Rights and the major UN human\n                   rights conventions.**\n                   In addition, sector-specific secondary EU law, notably the EU data protection acquis and\n                   EU non-discrimination legislation, helps safeguard fundamental rights in the context of AI.\n                   Finally, the national laws of EU Member States also apply.\n                   *   For more, see FRA (2012), Bringing rights to life: The fundamental rights landscape of the\n                       European Union, Luxembourg, Publications Office of the European Union.\n                   ** These major conventions include: the 1966 International Covenant on Civil and Political Rights;\n                       the 1966 International Covenant on Economic, Social and Cultural Rights; the 1965 International\n                       Convention on the Elimination of All Forms of Racial Discrimination; the 1979 Convention on the\n                       Elimination of All Forms of Discrimination against Women; the 1984 Convention against Torture;\n                       the 1989 Convention on the Rights of the Child; the 2006 Convention on the Rights of Persons with\n                       Disabilities; and the 2006 International Convention for the Protection of All Persons from Enforced\n                       Disappearance.\n                       For more on the universal international human rights law framework, including their enforcement\n                       mechanisms, see e.g. De Schutter, O. (2015), International Human Rights Law: Cases, Materials,\n                       Commentary, Cambridge, Cambridge University Press, 2nd edition.\n            The report is based on 91 interviews with officials in public administration\n            and staff in private companies, in selected EU Member States. They were\n            asked about their use of AI, their awareness of fundamental rights issues\n            involved, and practices in terms of assessing and mitigating risks linked to\n            the use of AI.\n            Moreover, 10 interviews were conducted with experts who deal, in various\n            ways, with the potential fundamental rights challenges of AI. This group\n            included public bodies (such as supervisory and oversight authorities), non-\n            governmental organisations and lawyers.\n6\n                     SAFEGUARDING FUNDAMENTAL RIGHTS – SCOPE,\n                     IMPACT ASSESSMENTS AND ACCOUNTABILITY\n                     Considering the full scope of fundamental rights\n                     with respect to AI\n                                                                                    FRA OPINION 1\nUsing AI systems engages a wide range of fundamental\n                                                                                    When introducing new policies and\n      rights, regardless of the field of application. These\n                                                                                    adopting new legislation on AI, the\n include – but also go beyond – privacy, data protection,                           EU legislator and the Member States,\n                 non-discrimination and access to justice.                          acting within the scope of EU law,\n                                                                                    must ensure that respect for the full\n                     The EU Charter of Fundamental Rights (the Charter)             spectrum of fundamental rights, as\n                     became legally binding in December 2009 and has the            enshrined in the Charter and the EU\n                     same legal value as the EU treaties. It brings together        Treaties, is taken into account. Specific\n                     civil, political, economic and social rights in a single text. fundamental rights safeguards need to\n                     Pursuant to Article 51 (1) of the Charter, the institutions,   accompany relevant policies and laws.\n                     bodies, offices and agencies of the Union have to respect\n                                                                                    In doing so, the EU and its Member\n                     all the rights as embodied in the Charter. EU Member\n                                                                                    States should rely on robust evidence\n                     States have to do so when they are implementing Union\n                                                                                    concerning AI’s impact on fundamental\n                     law. This applies equally to AI as to any other field.\n                                                                                    rights to ensure that any restrictions\n                     The fieldwork of this research shows that a large              of certain fundamental rights respect\n                     variety of systems are used under the heading of AI.           the principles of necessity and\n                     The technologies analysed entail different levels of           proportionality.\n                     automation and complexity. They also vary in terms of\n                                                                                    Relevant safeguards need to be\n                     the scale and potential impact on people.\n                                                                                    provided for by law to effectively\n                     FRA’s findings show that using AI systems implicate a          protect against arbitrary interference\n                     wide spectrum of fundamental rights, regardless of the         with fundamental rights and to give\n                     field of application. These include, but also go beyond,       legal certainty to both AI developers\n                     privacy and data protection, non-discrimination and            and users. Voluntary schemes\n                     access to justice. Yet, when addressing the impact of AI       for observing and safeguarding\n                     with respect to fundamental rights, the interviews show,       fundamental rights in the development\n                     the scope is often delimited to specific rights.               and use of AI can further help mitigate\n                                                                                    rights violations. In line with the\n                     A wider range of rights need to be considered when\n                                                                                    minimum requirements of legal clarity\n                     using AI, depending on the technology and area of use. In\n                                                                                    – as a basic principle of the rule of\n                     addition to rights concerning privacy and data protection,\n                                                                                    law and a prerequisite for securing\n                     equality and non-discrimination, and access to justice,\n                                                                                    fundamental rights – the legislator has\n                     other rights could be considered. These include, for\n                                                                                    to take due care when defining the\n                     example, human dignity, the right to social security and\n                                                                                    scope of any such AI law.\n                     social assistance, the right to good administration (mostly\n                     relevant for the public sector) and consumer protection        Given the variety of technology\n                     (particularly important for businesses). Depending on          subsumed under the term AI and\n                     the context of the AI use, any other right protected in        the lack of knowledge about the full\n                     the Charter needs consideration.                               scope of its potential fundamental\n                                                                                    rights impact, the legal definition of\n                                                                                    AI-related terms might need to be\n                                                                                    assessed on a regular basis.\n                                                                                                                              7\n                                                 Using effective impact assessments to prevent\n                                                 negative effects\n  FRA OPINION 2\n                                                   Prior impact assessments mainly focus on technical\n  The EU legislator should consider making         issues. They rarely address potential effects on\n  mandatory impact assessments that\n                                                   fundamental rights. This is because knowledge on how\n  cover the full spectrum of fundamental\n  rights. These should cover the private\n                                                   AI affects such rights is lacking.\n  and public sectors, and be applied\n                                                 Deploying AI systems engages a wide spectrum of\n  before any AI-system is used. The\n                                                 fundamental rights, regardless of the field of application.\n  impact assessments should take into\n                                                 Pursuant to Article 51 (1) of the Charter, EU Member States\n  account the varying nature and scope\n                                                 must respect all rights embodied in the Charter when\n  of AI technologies, including the level of\n                                                 they are implementing Union law. In line with existing\n  automation and complexity, as well as\n                                                 international standards – notably the United National\n  the potential harm. They should include\n                                                 Guiding Principles on Business and Human Rights (UNGPs)\n  basic screening requirements that can\n                                                 – businesses should have in place “a human rights due\n  also serve to raise awareness of potential\n                                                 diligence process to identify, prevent, mitigate and account\n  fundamental rights implications.\n                                                 for how they address their impacts on human rights”\n  Impact assessments should draw on              (Principles 15 and 17). This is irrespective of their size and\n  established good practice from other           sector, and encompasses businesses working with AI.\n  fields and be regularly repeated during\n                                                 While pursuing its commitments to the UNGPs, the EU\n  deployment, where appropriate. These\n                                                 has adopted several legislative acts addressing sector-\n  assessments should be conducted in a\n                                                 specific instruments, in particular in the context of due\n  transparent manner. Their outcomes\n                                                 diligence-related obligations for human rights. Discussions\n  and recommendations should be in the\n                                                 are currently underway on proposing new EU secondary\n  public domain, to the extent possible.\n                                                 law. Such law would require businesses to carry out due\n  To aid the impact assessment process,\n                                                 diligence of the potential human rights and environmental\n  companies and public administration\n                                                 impacts of their operations and supply chains. Such law\n  should be required to collect the\n                                                 would likely be cross-sectoral and provide for sanctions\n  information needed for thoroughly\n                                                 for non-compliance – which should encompass the use of\n  assessing the potential fundamental\n                                                 AI. See FRA’s recent report on Business and Human rights\n  rights impact.\n                                                 – access to remedy, which calls for improved horizontal\n  The EU and Member States should                human rights diligence rules for EU-based companies.\n  consider targeted actions to support\n                                                 Impact assessments are an important tool for businesses\n  those developing, using or planning\n                                                 and public administration alike to mitigate the potential\n  to use AI systems, to ensure effective\n                                                 negative impact of their activities on fundamental rights.\n  compliance with their fundamental\n                                                 EU law in specific sectors requires some forms of impact\n  rights impact assessment obligations.\n                                                 assessments, such as Data Protection Impact Assessments\n  Such actions could include funding,\n                                                 under the General Data Protection Regulation (GDPR).\n  guidelines, training or awareness\n                                                 Many interviewees reported that a data protection impact\n  raising. They should particularly – but\n                                                 assessment, as required by law, was conducted. However,\n  not exclusively – target the private\n                                                 these took different forms. Moreover, prior assessments,\n  sector.\n                                                 when conducted, focus mainly on technical aspects. They\n  The EU and Member States should                rarely address potential impacts on fundamental rights.\n  consider using existing tools, such as         According to some interviewees, fundamental rights impact\n  checklists or self-evaluation tools,           assessments are not carried out when an AI system does\n  developed at European and international        not, or appears not to, affect fundamental rights negatively.\n  level. These include those developed by\n                                                 The research shows that the interviewees’ knowledge on\n  the EU High-Level Group on Artificial\n                                                 fundamental rights – other than data protection and, to\n  Intelligence.\n                                                 some extent, non-discrimination – is limited. The majority\n                                                 acknowledge, however, that the use of AI has an impact\n                            on fundamental rights. Some interviewees indicate that their systems do not\n                            affect fundamental rights, which is to some extent linked to the tasks the AI\n                            systems are used for.\n                            All respondents are aware of data protection issues. Most respondents also\n                            realise that discrimination could – generally – be a problem when AI is used.\n8\n                      However, the exact meaning and applicability of rights related to data protection\n                      and non-discrimination remains unclear to many respondents.\n                      The research findings show differences between the private and public sector.\n                      Interviewees from the private sector are often less aware of the wider range of\n                      fundamental rights that could be affected. Data protection issues are known to\n                      the private sector. However, other rights, such as non-discrimination or access\n                      to justice-related rights, are less well known among business representatives\n                      who work with AI. Some were fully aware of potential problems. But others\n                      said that the responsibility for checking fundamental rights issues lies with\n                      their clients.\n                      Ensuring effective oversight and overall\n                      accountability\n                                                                                     FRA OPINION 3\n        Businesses and public administrations that are\n    developing and using AI are in contact with various                              The EU and Member States should ensure that\n  bodies that are responsible for overseeing AI-related                              effective accountability systems are in place\nsystems within their respective mandates and sectors.                                to monitor and, where needed, effectively\n                                                                                     address any negative impact of AI systems\n  These bodies include data protection authorities. But\n                                                                                     on fundamental rights. They should consider,\n   those using AI are not always sure which bodies are                               in addition to fundamental rights impact\n                responsible for overseeing AI systems.                               assessments (see FRA opinion 2), introducing\n                                                                                     specific safeguards to ensure that the\n                      In line with well-established international human rights       accountability regime is effective. This could\n                      standards – for example, Article 1 of the European             include a legal requirement to make available\n                      Convention on Human Rights (ECHR) and Article 51 of the        enough information to allow for an assessment\n                      Charter – states are obliged to secure people’s rights and     of the fundamental rights impact of AI systems.\n                      freedoms. To effectively comply, states have to – among        This would enable external monitoring and\n                      others – put in place effective monitoring and enforcement     human rights oversight by competent bodies.\n                      mechanisms. This applies equally with respect to AI.\n                                                                                     The EU and Member States should also\n                      At the level of monitoring, the findings point to the          make better use of existing oversight expert\n                      important role of specialised bodies established in specific   structures to protect fundamental rights\n                      sectors that are also responsible for AI oversight within      when using AI. These include data protection\n                      their mandates. These include, for example, oversight          authorities, equality bodies, national human\n                      in the area of banking, or data protection authorities.        rights institutions, ombuds institutions and\n                      A variety of such bodies are potentially relevant to the       consumer protection bodies.\n                      oversight of AI from a fundamental rights perspective.\n                                                                                     Additional resources should be earmarked to\n                      However, the responsibilities of bodies concerning\n                                                                                     establish effective accountability systems by\n                      the oversight of AI remains unclear to many of those\n                                                                                     ‘upskilling’ and diversifying staff working for\n                      interviewed from the private and the public sector.\n                                                                                     oversight bodies. This would allow them to\n                      Public administrations’ use of AI is sometimes audited,        deal with complex issues linked to developing\n                      as part of their regular audits. Private companies in          and using AI.\n                      specific sectors also have specialised oversight bodies,\n                                                                                     Similarly, the appropriate bodies should be\n                      for example in the area of health or financial services.\n                                                                                     equipped with sufficient resources, powers\n                      These also check the use of AI and related technologies,\n                                                                                     and – importantly – expertise to prevent and\n                      for example as part of their certification schemes. Private\n                                                                                     assess fundamental rights violations and\n                      sector interviewees expressed a wish for bodies that could\n                                                                                     effectively support those whose fundamental\n                      provide expert advice on the possibilities and legality of\n                                                                                     rights are affected by AI.\n                      potential AI uses.\n                                                                                     Facilitating cooperation between appropriate\n                      In the EU, there is a well-developed set of independent\n                                                                                     bodies at national and European level can help\n                      bodies with a mandate to protect and promote fundamental\n                                                                                     share expertise and experience. Engaging with\n                      rights. These include data protection authorities, equality\n                                                                                     other actors with relevant expertise – such\n                      bodies, national human rights institutions and ombuds\n                                                                                     as specialist civil society organisations – can\n                      institutions. The research shows that those using or\n                                                                                     also help. When implementing such actions at\n                      planning to use AI often contacted different bodies about\n                                                                                     national level, Member States should consider\n                      their use of AI, such as consumer protection bodies.\n                                                                                     using available EU funding mechanisms.\n                                                                                                                                     9\n                              Most often, users of AI contacted data protection authorities to seek guidance, input\n                              or approval where personal data processing was involved. Interviewed experts\n                              highlight the relevance of data protection authorities for overseeing AI systems with\n                              respect to the use of personal data. However, they also note that data protection\n                              authorities are under-resourced for this task and lack specific expertise on AI issues.\n                              Experts, including those working for oversight bodies such as equality bodies and\n                              data protection authorities, agree that the expertise of existing oversight bodies\n                              needs to be strengthened to allow them to provide effective oversight of AI related\n                              issues. According to the experts, this can be challenging given that these bodies’\n                              resources are already stretched. They also highlighted the important role of relevant\n                              civil society organisations specialised in the fields of technology, digital rights and\n                              algorithms. They can enhance accountability in the use of AI systems.\n                              NON-DISCRIMINATION, DATA PROTECTION AND ACCESS TO\n                              JUSTICE: THREE HORIZONTAL THEMES\n                              The research shows that the use of AI affects various fundamental rights.\n                              Apart from context-related specific aspects that affect different rights to a\n                              varying extent, the fundamental rights topics which emerged in the research to\n                              repeatedly apply to most AI cases include: the need to ensure non-discriminatory\n                              use of AI (right not to be discriminated); the requirement to process data legally\n                              (right to personal data protection); and the possibility to complain about AI-based\n                              decisions and seek redress (right to an effective remedy and to a fair trial).\n                              The two main fundamental rights highlighted in the interviews are data\n                              protection and non-discrimination. In addition, effective ways to complain\n                              about the use of AI came up repeatedly, linked to the right to a fair trial\n                              and effective remedy. The following three FRA opinions, which reflect these\n                              findings, should be read alongside the other opinions, which call for a more\n                                                     comprehensive recognition of, and response to, the full\n                                                     range of fundamental rights affected by AI.\n                                                     Specific safeguards to ensure non-discrimination\n                                                     when using AI\n   FRA OPINION 4                                        Interviewees rarely mentioned carrying out detailed\n                                                        assessments of potential discrimination when using AI.\n   EU Member States should consider\n   encouraging companies and public\n                                                        This suggests a lack of in-depth assessments of such\n   administration to assess any potentially\n                                                        discrimination in automated decision making.\n   discriminatory outcomes when using AI\n   systems.\n                                                     The obligation to respect the principle of non-\n   The European Commission and Member                discrimination is enshrined in Article 2 of the TEU,\n   States should consider providing funding          Article 10 of the TFEU (requiring the Union to combat\n   for targeted research on potentially              discrimination on a number of grounds), and Articles 20\n   discriminatory impacts of the use of AI           and 21 of the Charter (equality before the law and non-\n   and algorithms. Such research would               discrimination on a range of grounds). More specific and\n   benefit from the adaptation of established        detailed provisions in several EU directives also enshrine\n   research methodologies, from the social           this principle, with varying scopes of application.\n   sciences, that are employed to identify\n                                                     Automation and the use of AI can greatly increase\n   potential discrimination in different areas\n                                                     the efficiency of services and can scale up tasks that\n   – ranging from recruitment to customer\n                                                     humans would not be able to undertake. However, it is\n   profiling.\n                                                     necessary to ensure that services and decisions based on\n   Building on the results of such research,         AI are not discriminatory. Recognising this, the European\n   guidance and tools to support those               Commission recently highlighted the need for additional\n   using AI to detect possible discriminatory\n   outcomes should be developed.\n10\nlegislation to safeguard non-discrimination when using AI in the EU anti-\nracism action plan 2020-2025.\nMost interviewees are in principle aware that discrimination might happen.\nYet, they rarely raised this issue themselves. Only few believe their systems\ncould actually discriminate.\nInterviewees also rarely mentioned detailed assessments of potential\ndiscrimination, meaning that there is a lack of in-depth assessment of potential\ndiscrimination.\nA common perception is that omitting information about protected attributes,\nsuch as gender, age or ethnic origin, can guarantee that an AI system does\nnot discriminate. This is not necessarily true, however. Information potentially\nindicating protected characteristics (proxies), which can often be found in\ndatasets, could lead to discrimination.\nIn certain cases, AI systems can also be used to test for and detect discriminatory\nbehaviour, which can be encoded in datasets. However, very few interviewees\nmentioned the possibility of collecting such information about disadvantaged\ngroups to detect potential discrimination. In the absence of in-depth analysis\nof potential discrimination in the actual use of AI systems, there is also almost\nno discussion and analysis of the potential positive effect of using algorithms\nto make decisions fairer. Moreover, none of the interviewees working on AI\nmentioned using AI to detect possible discrimination as a positive outcome, in\nthe sense that discrimination can be better detected when data are analysed\nfor potential bias.\nSince detecting potential discrimination through the use of AI and algorithms\nremains challenging, and interviewees only briefly addressed the issue, different\nmeasures are needed to address this. These include the requirement to consider\nissues linked to discrimination when assessing the use of AI, and investment\ninto further studies of potential discrimination that use a diverse range of\nmethodologies.\nThis could involve, for example, discrimination testing. This could build on similar\nestablished methodologies for testing bias in everyday life, such as with respect\nto job applications, where the applicant’s name is changed to (indirectly) identify\nethnicity. In relation to AI applications, such tests could involve the possible\ncreation of fake profiles for online tools, which only differ with respect to\nprotected attributes. In this way, the outcomes can be checked with respect to\npotential discrimination. Research could also benefit from advanced statistical\nanalysis to detect differences in datasets concerning protected groups, and\ntherefore can be used as a basis for exploring potential discrimination.\nFinally, some research interviews underscored that results from complex\nmachine learning algorithms are often very difficult to understand and explain.\nThus, further research to better understand and explain such results (so-called\n‘explainable AI’) can also help to better detect discrimination when using AI.\n                                                                                     11\n                                                   More guidance on data protection\n                                                       More clarity is needed on the scope and meaning of\n   FRA OPINION 5                                       legal provisions regarding automated decision making.\n   The European Data Protection Board\n                                                   Data protection is critical in the development and use of\n   (EDPB) and the European Data\n                                                   AI. Article 8 (1) of the Charter and Article 16 (1) of the TFEU\n   Protection Supervisor (EDPS) should\n                                                   provide that everyone has the right to the protection of\n   consider providing further guidance\n                                                   their personal data. The GDPR and the Law Enforcement\n   and support to effectively implement\n                                                   Directive (Directive (EU) 2018/680) further elaborate\n   GDPR provisions that directly apply\n                                                   on this right, and include many provisions applicable to\n   to the use of AI for safeguarding\n                                                   the use of AI.\n   fundamental rights, in particular as\n   regards the meaning of personal data            The interviewees indicated that most of the AI systems\n   and its use in AI, including in AI training     they employ use personal data, meaning data protection\n   datasets.                                       is affected in many different ways. However, a few\n                                                   applications – according to the interviewees – do not\n   There is a high level of uncertainty\n                                                   use personal data, or only use anonymised data, and\n   concerning the meaning of automated\n                                                   hence data protection law would not apply. If personal\n   decision making and the right to\n                                                   data are used, all data protection related principles and\n   human review linked to the use of AI\n                                                   provisions apply.\n   and automated decision making. Thus,\n   the EDPB and the EDPS should also               This report highlights an important issue linked to data\n   consider further clarifying the concepts        protection, which is also relevant for other fundamental\n   of ‘automated decision making’ and              rights with respect to automated decision making.\n   ‘human review’, where they are                  According to a Eurobarometer survey, only 40 % of\n   mentioned in EU law.                            Europeans know that they can have a say when decisions\n                                                   are automated. Knowledge about this right is considerably\n   In addition, national data protection\n                                                   higher among those working with AI – the majority of\n   bodies should provide practical\n                                                   interviewees raised this issue. However, many of the\n   guidance on how data protection\n                                                   interviewees, including experts, argued that more clarity\n   provisions apply to the use of\n                                                   is needed on the scope and meaning of legal provisions\n   AI. Such guidance could include\n                                                   on automated decision making.\n   recommendations and checklists,\n   based on concrete use cases of AI,              In the area of social benefits, interviewees mentioned only\n   to support compliance with data                 one example of fully automated, rule-based decisions.\n   protection provisions.                          All other applications they mentioned are reviewed by\n                                                   humans. Interviewees in public administration stressed\n                                                   the importance of human review of any decisions.\n                                                   However, they rarely described what such human review\n                                                   actually involves and how other information was used\n                                                   when reviewing output from AI systems.\n                              While interviewees disagree as to whether or not the existing legislation is\n                              sufficient, many called for more concrete interpretation of the existing data\n                              protection rules with respect to automated decision making, as enshrined\n                              in Article 22 of the GDPR.\n12\n                      Effective access to justice in cases involving\n                      AI-based decisions\n                                                                                      FRA OPINION 6\n  To effectively contest decisions based on the use of AI,\npeople need to know that AI is used, and how and where                                The EU legislator and Member States\n   to complain. Organisations using AI need to be able to                             should ensure effective access to\n                                                                                      justice for individuals in cases involving\n       explain their AI system and decisions based on AI.\n                                                                                      AI-based decisions.\n                      Access to justice is both a process and a goal, and is crucial  To ensure that available remedies are\n                      for individuals seeking to benefit from other procedural        accessible in practice, the EU legislator\n                      and substantive rights. It encompasses a number of core         and Member States could consider\n                      human rights. These include the right to a fair trial and to    introducing a legal duty for public\n                      an effective remedy under Article 6 and 13 of the ECHR          administration and private companies\n                      and Article 47 of the EU Charter of Fundamental Rights.         using AI systems to provide those\n                      Accordingly, the notion of access to justice obliges states     seeking redress information about\n                      to guarantee each individual’s right to go to court – or,       the operation of their AI systems.\n                      in some circumstances, an alternative dispute resolution        This includes information on how\n                      body – to obtain a remedy if it is found that the individual’s  these AI systems arrive at automated\n                      rights have been violated.                                      decisions. This obligation would help\n                                                                                      achieve equality of arms in cases of\n                      In accordance with these standards, a victim of a human\n                                                                                      individuals seeking justice. It would\n                      rights violation arising from the development or use of an\n                                                                                      also support the effectiveness of\n                      AI system by a public or private entity has to be provided\n                                                                                      external monitoring and human\n                      with access to remedy before a national authority. In line\n                                                                                      rights oversight of AI systems (see\n                      with relevant case law under Article 47 of the Charter and\n                                                                                      FRA opinion 3).\n                      Article 13 of the ECHR, the remedy must be “effective in\n                      practice as well as in law”.                                    In view of the difficulty of explaining\n                                                                                      complex AI systems, the EU, jointly\n                      The research findings identify the following preconditions\n                                                                                      with the Member States, should\n                      for the remedy to be effective in practice in cases\n                                                                                      consider developing guidelines to\n                      involving AI systems and their impact on fundamental\n                                                                                      support transparency efforts in this\n                      rights: everyone needs to be aware when AI is used and\n                                                                                      area. In so doing, they should draw on\n                      informed of how and where to complain. Organisations\n                                                                                      the expertise of national human rights\n                      using AI must ensure that the public is informed about\n                                                                                      bodies and civil society organisations\n                      their AI system and the decisions based on them.\n                                                                                      active in this field.\n                      The findings show that explaining AI systems and how\n                      they make decisions in layman terms can be challenging.\n                      Intellectual property rights can hamper the provision of detailed information\n                      about how an algorithm works. In addition, certain AI systems are complex.\n                      This makes it difficult to provide meaningful information about the way a\n                      system works, and on related decisions.\n                      To tackle this problem, some companies interviewed avoid using complex\n                      methods for certain decision making altogether, because they would not be\n                      able to explain the decisions. Alternatively, they use simpler data analysis\n                      methods for the same problem to obtain some understanding about the main\n                      factors influencing certain outcomes. Some of the private sector interviewees\n                      pointed to efforts made to gradually improve their understanding of AI\n                      technology.\n                                                                                                                                 13\n141\nAI AND FUNDAMENTAL RIGHTS – WHY\nIT IS RELEVANT FOR POLICYMAKING\n               Artificial intelligence (AI) is increasingly used in the private and public sectors,\n               affecting daily life. Some see AI as the end of human control over machines.\n               Others view it as the technology that will help humanity address some of its\n               most pressing challenges. While neither portrayal may be accurate, concerns\n               about AI’s fundamental rights impact are clearly mounting, meriting scrutiny\n               of its use by human rights actors.\n               Examples of potential problems with using AI-related technologies in relation\n               to fundamental rights have increasingly emerged. These include:\n               ――an algorithm used to recruit human resources was found to generally\n                    prefer men over women;1\n               ――an online chatbot2 became ‘racist’ within a couple of hours;3\n               ――machine translations showed gender bias;4\n               ――facial recognition systems detect gender well for white men, but not for\n                    black women;5\n               ――a public administration’s use of algorithms to categorise unemployed\n                    people did not comply with the law;6\n               ――and a court stopped an algorithmic system supporting social benefit\n                    decisions for breaching data protection laws.7\n               These examples raise profound questions about whether modern AI systems\n               are fit for purpose and how fundamental rights standards can be upheld\n               when using or considering using AI systems.\n               This report addresses these questions by providing a snapshot of the current\n               use of AI-related technologies in the EU – based on selected use cases – and\n               its implications on fundamental rights.\n                                                                                                    15\n               This report is the main publication stemming from FRA’s project on Artificial intelligence,\n   FRA’s work  big data and fundamental rights. The project aims to assess the positive and negative\n   on AI, big  fundamental rights implications of new technologies, including AI and big data.\n   data and\n   fundamental The current report builds on the findings of a number of earlier papers:\n   rights      •    Facial recognition technology: fundamental rights considerations in the context of law\n                    enforcement (2019): this paper outlines and analyses fundamental rights challenges\n                    triggered when public authorities deploy live FRT for law enforcement purposes. It also\n                    briefly presents steps to take to help avoid rights violations.\n               •    Data quality and artificial intelligence – mitigating bias and error to protect\n                    fundamental rights (2019): this paper highlights the importance of awareness and\n                    avoidance of poor data quality.\n               •    #BigData: Discrimination in data-supported decision making (2018): this focus paper\n                    discusses how such discrimination can occur and suggests possible solutions.\n               As part of the project, FRA is also exploring the feasibility of studying concrete examples of\n               fundamental rights challenges when using algorithms for decision making through either\n               online experiments or simulation studies.\n               Several other FRA publications address relevant issues:\n               •    The Guide on Preventing unlawful profiling today and in the future (2018) illustrates\n                    what profiling is, the legal frameworks that regulate it, and why conducting profiling\n                    lawfully is both necessary to comply with fundamental rights and crucial for effective\n                    policing and border management.\n               •    The Handbook on European data protection law (2018 edition) is designed to\n                    familiarise legal practitioners not specialised in data protection with this area of law.\n               •    Data from FRA’s Fundamental Rights Survey. It surveyed a random sample of 35,000\n                    people across the EU, including findings on people’s opinions and experiences linked to\n                    data protection and technology (2020) and security (2020).\n               •    FRA’s report on Business and human rights – access to remedy analyses obstacles and\n                    promising practices in relation to access to remedies for victims of business-related\n                    human rights abuses. By analysing complaints mechanisms in EU Member States, the\n                    research maps what hinders and what facilitates access to remedies.\n16\n1.1.\t WHY THIS REPORT?\nThe growing attention to AI and its potential to drive economic growth has not\nbeen matched by a body of evidence about how different technologies can\naffect fundamental rights – positively or negatively. Only concrete examples\nallow for a thorough examination of whether, and to what extent, applying\na technology interferes with various fundamental rights – and whether any\nsuch interference can be justified, in line with the principles of necessity\nand proportionality.\nThis report provides a fundamental rights-based analysis of concrete ‘use\ncases’ – or case studies. ‘Use case’ is a term in software engineering. This\nreport loosely defines it as the specific application of a technology for a\ncertain goal used by a specified actor.\nThe report illustrates some of the ways that companies and the public sector\nin the EU are looking to use AI to support their work, and whether – and\nhow – they are taking fundamental rights considerations into account. In\nthis way, it contributes empirical evidence, analysed from a fundamental\nrights perspective, that can inform EU and national policymaking efforts to\nregulate the use of AI tools.\nWhat did the research cover?\nFRA conducted fieldwork research in five EU Member States: Estonia, Finland,\nFrance, the Netherlands and Spain. It collected information from those involved\nin designing and using AI systems in key private and public sectors on how\nthey address relevant fundamental rights issues.\nThe research – based on 91 personal interviews – gathered information on:\n――the purpose and practical application of AI technologies;\n――the assessments conducted when using AI and the applicable legal\n    framework and oversight mechanisms;\n――the awareness of fundamental rights issues and potential safeguards\n    in place; and\n――future plans.\nIn addition, 10 experts involved in monitoring or observing potential\nfundamental rights violations concerning the use of AI, including civil society,\nlawyers and oversight bodies, were interviewed.\nPresenting the main findings\nThis report presents the main findings of the fieldwork. In particular, the\nreport includes:\n――An overview of the use of AI in the EU across a range of sectors, with a\n    focus on: (1) social benefits, (2) predictive policing, (3) healthcare, and\n    (4) targeted advertising.\n――An analysis of the awareness of fundamental rights and further implications\n    on selected rights, with a focus on the four use cases.\n――A discussion of measures to assess and mitigate the impact of AI-related\n    technologies on people’s fundamental rights.\nTwo annexes, available on FRA’s website, supplement the report:\n――Annex 1 gives a detailed description of the research methodology and\n    the questions asked in the interviews.\n――Annex 2 provides examples of potential errors when using AI in selected\n    areas.\n                                                                                 17\n   In addition, country-specific information on each of the five Member States\n   covered complements the fieldwork. This research, delivered by the contractor,\n   is also available on FRA’s website. It maps policy developments on AI and\n   the legal framework governing its use in different sectors.\n   Supporting rights-compliant policymaking\n   This report provides evidence on the extent to which fundamental rights\n   considerations are brought into discussions and activities to develop, test, employ\n   and monitor AI systems in the EU. It also highlights how different technologies\n   can affect some of the rights set out in the Charter, and reflects on how to protect\n   these rights as AI becomes both more widespread and more sophisticated.\n   The analysis of selected fundamental rights challenges can help the EU and\n   its Member States, as well as other stakeholders, assess the fundamental\n   rights compatibility of AI systems in different contexts. The findings in the\n   report about current views and practices among those using AI supports\n   policymakers in identifying where further actions are needed.\n   The report does not aim to provide a comprehensive mapping of the use of\n   different AI systems in the five EU Member States covered by the research,\n   or to provide in-depth technical information about how the different systems\n   mentioned by the interviewees work.\n   Conducting\n   the                               Who?\n   interviews                        This report is based on 91 semi-structured interviews with representatives from public\n                                     administration and private companies who are involved in the use of AI for their services and\n                                     businesses. FRA intentionally provided a very general definition of AI to those interviewed as\n                                     part of the research, based on existing definitions.\n                                     The organisations interviewed were active in public administration in general, with some\n                                     working in law enforcement.\n                                     The private companies include those working in health, retail, pricing and marketing,\n                                     financial services, insurance, employment, transport and energy. Importantly, except for two\n                                     interviewees, the research did not include companies that sell AI to other companies. Instead,\n                                     the entities use AI to support their own operations.\n                                     In addition, ten interviews were conducted with experts dealing with potential challenges of\n                                     AI in public administration (e.g. supervisory authorities), in non-governmental organisations\n                                     or as lawyers working in this field.\n                                     Where?\n                                     Interviews were carried out in five EU Member States (Estonia, Finland, France, the\n                                     Netherlands and Spain). These countries were selected based on their different levels\n                                     of uptake of AI technology and of policy development in the area of AI, as well as to\n                                     incorporate experience from across different parts of the EU.\n                                     How?\n                                     FRA outsourced the fieldwork to Ecorys. FRA staff supervised the work, and developed\n                                     the research questions and methodology. Interviewers received dedicated training before\n                                     conducting the fieldwork.\n                                     Interviews were carried out anonymously. As a consequence, no information identifying the\n                                     organisation concerned is provided in the report. In addition, certain details of the applications\n                                     described – most notably the country – are omitted to protect respondents’ anonymity. This\n                                     was communicated to interviewees, increasing their level of trust and allowing them to speak\n                                     more freely about their work. It also proved useful for recruiting respondents.\n18\n                                                           1.2.\t WHAT DO WE MEAN BY ARTIFICIAL INTELLIGENCE?\n                                                           There is no universally accepted definition of AI. Rather than referring to\n                                                           concrete applications, it reflects recent technological developments that\n                                                           encompass a variety of technologies. Although AI is usually defined very\n                                                           widely, a survey conducted in 2020 on behalf of the European Commission\n                                                           among companies in the EU showed that eight in ten people working at\n                                                           companies in the EU say they know what AI is. Slightly more than two in\n                                                           10 respondents from companies in the EU-27 do not know (7 %) or are not\n                                                           sure about (14 %) what AI is.8\n                                                                           FRA’s research did not apply a strict definition of AI on the use\nHigh-level                                                                 cases it presents. For the interviews, AI was defined broadly,\n                                                                           with reference to the definition provided by the High-Level\nexpert group                                                               Expert Group on Artificial Intelligence (AI HLEG ).\non artificial                                                              The interviewees also expressed a variety of ways to think\n                                                                           about AI. When identifying use cases to explore in the research,\nintelligence                                                               the project focused on applications that support decision\n                                                                           making based on data and machine learning, and applications\n      “Artificial intelligence (AI) refers to systems that\n                                                                           and systems that contribute to automating tasks that are usually\n      display intelligent behaviour by analysing their\n      environment and taking actions – with some degree                    undertaken by humans or which cannot be undertaken by\n      of autonomy – to achieve specific goals. AI-based                    humans due to their large scale. As such, the use cases in this\n      systems can be purely software-based, acting                         report provide insight into the different technologies that are\n      in the virtual world (e.g. voice assistants, image                   used and discussed in selected areas under the broad heading\n      analysis software, search engines, speech and                        of AI. As there may be some contention concerning whether\n      face recognition systems) or AI can be embedded                      certain use cases constitute AI at the current level of use, the\n      in hardware devices (e.g. advanced robots,                           report often refers to ‘AI and related technologies’.\n      autonomous cars, drones or Internet of Things\n      applications).”                                                      The past years have seen an enormous increase in computing\n      This initial definition of AI HLEG was subject to                    power, increased availability of data and the development of\n      further discussion in the groups. See AI HLEG (2019), A              new technologies for analysing data. The increased amount and\n      definition of AI: Main capabilities and disciplines.                 variety of data, sometimes available almost in real time over\n                                                                           the internet, is often referred to as big data. Machine learning\n                                                                           technologies and related algorithms, including deep learning,\n                                                                           benefit enormously from this increased computing power and\n                                                                           data availability, and their development and use is flourishing.\n                                                           The use of these terms is, however, of limited use. It can even prove\n                                                           counterproductive, as it triggers ideas linked to science fiction rather than\n                                                           any real application of AI. A variety of myths exist about what AI is and\n                                                           can do,9 often spread via (social) media. For example, some claim that AI\n                                                           can act on its own, being some form of entity. This distracts from the fact\n                                                           that all AI systems are made by humans and that computers only follow\n                                                           instructions made and given by humans. For a human-centric approach to\n                                                           AI, it is important to note that AI can never do anything on its own – it is\n                                                           human beings who use technology to achieve certain goals. However, the\n                                                           human work and decision making behind the AI systems is often not visible\n                                                           or the centre of attention.\n“Currently, there is no lawyer\n                                                           Entire studies and many discussions have explored possible AI definitions.\nwho can tell the definition of AI\n                                                           The European Commission’s Joint Research Centre analysed AI definitions.\nand we’ve asked around pretty\n                                                           It highlights that they often refer to issues linked to the perception of the\nthoroughly. No one can tell.”\n                                                           environment (i.e. the way a system receives input/data from its environment,\n(Public administration, Netherlands)\n                                                           e.g. through sensors), information processing, decision making and the\n                                                           achievement of specific goals. Definitions frequently refer to machines\n                                                           behaving like humans or taking over tasks associated with human intelligence.\n                                                           Given the difficulty of defining intelligence, many definitions remain vague.\n                                                           This makes the use of AI hard to measure in practice10 and, equally, challenging\n                                                           to define in law.11\n                                                                                                                                             19\n   This report discusses the use of AI based on concrete applications. These\n   differ in terms of their complexity, level of automation, potential impact on\n   individuals, and the scale of application.\n   Most of the discussion around, and the actual use of AI, involves deploying\n   machine learning technologies. These can be seen as a sub-domain of AI.\n   There is also some confusion around the term “learning”, which implies that\n   machines learn like humans. In reality, much of current machine learning\n   is based on statistical learning methodologies.12 Machine learning uses\n   statistical methods to find rules in the form of correlations that can help to\n   predict certain outcomes.\n   This is different from traditional statistical analysis, because it does not involve\n   detailed checks of how these predictions were produced (often referred to as\n   ‘black boxes’13). Traditional statistical analysis is based on specific theoretical\n   assumptions about the data generation processes and the correlations used.14\n   Machine learning is geared towards producing accurate outcomes, and can\n   be used for automating workflows or decisions, if an acceptable level of\n   accuracy can be obtained.\n   The usual example is an email spam filter, which uses statistical methods to\n   predict if an email is spam. As it is not important to know why a certain email\n   was blocked and because spam can be predicted with very high accuracy,\n   we do not really need to understand how the algorithm works (i.e. based on\n   what rules emails get blocked). However, depending on the complexity of\n   the task, prediction is not always possible with high accuracy. Moreover, as\n   this report highlights, not understanding why certain outcomes are predicted\n   is not acceptable for certain tasks.\n   The area of machine learning incorporates several approaches. Most often,\n   machine learning refers to finding rules that link data to a certain outcome\n   based on a dataset that includes outcomes (supervised learning). For example,\n   a dataset of emails, which are labelled as spam or not (‘ham’), is used to find\n   correlations and rules that are associated with spam emails in this dataset.\n   These rules are then used to ‘predict’ with some degree of likelihood if any\n   future email is spam or not.\n   Sometimes, machine learning is used to find hidden groups in datasets\n   without defining a certain outcome (unsupervised learning) – for example,\n   segmenting people into groups based on similarities in their demographics.\n20\nFinally, rules and correlations can be found through trial and error\n(reinforcement learning). These systems try to optimise a certain goal\nthrough experimentation, and update their rules automatically to have the\nbest possible output. Such systems need enormous amounts of data and\ncan hardly be used on humans, as it involves experimentation. They were\nmainly responsible for the success of winning board games against humans,\nwhich were often sensationalised by media.\n1.3.\t\u0007AI AND FUNDAMENTAL RIGHTS IN THE EU POLICY\n           FRAMEWORK: MOVING TOWARDS REGULATION\nPolicymakers have for some time highlighted the potential for AI and related\ntechnologies to improve efficiency and drive economic growth. Yet public\nauthorities and international organisations have only recently reflected on\nthe fundamental rights challenges associated with such technologies. Coupled\nwith the growing use and accuracy of AI systems, this has turned attention\nto whether and how to regulate their use.\nA 2017 European Parliament resolution marked a milestone in the EU’s\nrecognition of the fundamental rights implications of AI. The resolution\nstressed that “prospects and opportunities of big data can only be fully tapped\ninto by citizens, the public and private sectors, academia and the scientific\ncommunity when public trust in these technologies is ensured by a strong\nenforcement of fundamental rights”.15 It called on the European Commission,\nthe Member States, and data protection authorities “to develop a strong and\ncommon ethical framework for the transparent processing of personal data\nand automated decision-making that may guide data usage and the ongoing\nenforcement of Union law”. 16\nLater that year, the European Council called for a “sense of urgency to address\nemerging trends” including “issues such as artificial intelligence […], while\nat the same time ensuring a high level of data protection, digital rights and\nethical standards”.17 The European Council invited the European Commission\nto put forward a European approach to AI.\nResponding to these calls, the European Commission published in 2018 its\nCommunication on AI for Europe18 and set up a High Level Expert Group on\nAI.19 Both initiatives include a strong reference to fundamental rights.\nThe Commission-facilitated High Level Expert Group was made up of 52\nindependent experts from academia, civil society and industry (including a\nrepresentative from FRA). It published ‘Ethics Guidelines for Trustworthy AI’\nand ‘Policy and investment recommendations for trustworthy AI’ in 2019.\nThese were developed further in 2020.20 Its work triggered further discussion\non the importance of framing AI in human rights terms, alongside ethical\nconsiderations. This led to the development of Ethics Guidelines that refer\nto the Charter and place fundamental rights consideration with respect to AI.\nThe Ethics Guidelines include an assessment list for trustworthy AI, which has\nbeen translated into a checklist to guide those who develop and deploy AI.21\nIndicating political support at the highest level, the European Council calls\nin its Strategic Guidelines for 2019-2024 to “ensure that Europe is digitally\nsovereign” and for policy to be “shaped in a way that embodies our societal\nvalues”.22 Similarly, Commission President Von der Leyen committed to “put\nforward legislation for a coordinated European approach on the human and\nethical implications of [AI]”.23 This prompted significant moves towards setting\nout an EU legal framework to govern the development and use of AI and\nrelated technologies, including with respect to their impact on fundamental\nrights.\n                                                                                 21\n   In February 2020, the European Commission published a White Paper on\n   artificial intelligence. It sets out policy options for meeting the twin objectives\n   of “promoting the uptake of AI and addressing the risks associated with certain\n   uses of this new technology”. The paper promotes a common European\n   approach to AI. It deems this necessary “to reach sufficient scale and avoid\n   the fragmentation of the single market”. As it notes, “[t]he introduction of\n   national initiatives risks to endanger legal certainty, to weaken citizens’ trust\n   and to prevent the emergence of a dynamic European industry”.24 Legal\n   uncertainty is also a concern of companies planning to use AI.\n   The Commission White Paper on AI highlights risks to fundamental rights as\n   one of the main concerns associated with AI. It acknowledges that “the use\n   of AI can affect the values on which the EU is founded and lead to breaches\n   of fundamental rights, be it as a result from flaws in the overall design of\n   AI systems, or from the use of data without correcting possible bias”. It also\n   lists some of the wide range of rights that can be affected.25\n   The White Paper on AI indicates the Commission’s preference for the\n   possible new regulatory framework to follow a risk-based approach, in\n   which mandatory requirements would, in principle, only apply to high-risk\n   applications. These would be determined on the basis of two cumulative\n   criteria: if it is employed in a sector, such as healthcare, transport or parts\n   of the public sector, where significant risks can be expected to occur; and if\n   it is used in a manner where significant risks are likely to arise. This latter\n   risk could be assessed based on the impact on the affected parties, adding\n   a harm-based element.\n   The White Paper also highlights some instances where AI use for certain\n   purposes should be considered high-risk, irrespective of the sector. These\n   include the use of AI applications in recruitment processes or for remote\n   biometric identification, including facial recognition technologies.\n   Following a public consultation, which ran from February to June 2020,26\n   the Commission is expected to propose legislation on AI in the first quarter\n   of 2021.27\n   Ahead of the proposal, the EU’s co-legislators have considered various aspects\n   of the potential legal framework. In October 2020, the European Parliament\n   adopted resolutions with recommendations to the European Commission on\n   a framework of ethical aspects of AI, robotics and related technologies,28\n   and a civil liability regime for AI.29 It also adopted a resolution on intellectual\n   property rights for the development of artificial intelligence technologies,30\n   and continues to work on resolutions on AI in criminal law and its use by\n   the police and judicial authorities in criminal matters,31 and AI in education,\n   culture and the audio-visual sector.32 It also established a special committee\n   on artificial intelligence in the digital age.33\n   Following their meeting on 1-2 October 2020, the heads of state and\n   government of the EU Member States declared that the “EU needs to be a\n   global leader in the development of secure, trustworthy and ethical Artificial\n   Intelligence” and invited the Commission to “provide a clear, objective definition\n   of high-risk Artificial Intelligence systems.34 In addition, the Council of the EU\n   adopted Conclusions on shaping Europe’s digital future35 and on seizing the\n   opportunities of digitalisation for access to justice, which included a dedicated\n   section on deploying AI systems in the justice sector.36 The German Presidency\n   of the Council of the EU published conclusions on the Charter of Fundamental\n   Rights in the context of artificial intelligence and digital change; the text was\n   supported, or not objected to, by 26 Member States.37\n22\nThe growing reference to fundamental rights in these discussions indicates\nthat a fundamental rights framework alongside other legal frameworks38 is\nnecessary for an effective and human rights compliant evaluation of the many\nopportunities and challenges brought by new technologies. Many existing AI\ninitiatives are guided by ethical frameworks, which are typically voluntary.\nA fundamental rights-centred approach to AI is underpinned by legal regulation,\nwhere the responsibility for respecting, protecting and fulfilling rights rests\nwith the State. This should guarantee a high level of legal protection against\npossible misuse of new technologies. It also provides a clear legal basis\nfrom which to develop AI, where reference to fundamental rights – and their\napplication in practice – is fully embedded.39\nIn addition to steps towards legal regulation, the EU is taking significant\npolicy and financial actions to support the development of AI and related\ntechnologies. Alongside the White Paper, the Commission published the\nEuropean Data Strategy.40 It aims to set up a single market for data, including\nnine common European data spaces, covering areas such as health data\nand financial data. The proposal for the 2021-2027 Multiannual Financial\nFramework would create a Digital Europe Programme worth € 6.8 billion\nto invest in the EU’s “strategic digital capacities”, including AI, in addition to\nfunding through Horizon Europe and the Connecting Europe Facility.41\nOther international actors are also considering steps to regulate AI. Most\nnotably, the Council of Europe is an active player in the field of AI and related\ntechnologies. In September 2019, the Committee of Ministers of the Council\nof Europe set up the Ad Hoc Committee on Artificial Intelligence (CAHAI). It\naims to examine “the feasibility and potential elements of a legal framework\nfor the development, design and application of AI, based on the Council of\nEurope’s standards on human rights, democracy and the rule of law”.42 In\nApril 2020, the Committee of Ministers of the Council of Europe adopted\nrecommendations on the human rights impact of algorithmic systems.43\nIn addition, the Organisation for Economic Cooperation and Development\n(OECD) adopted AI principles and created an AI policy observatory.44 At global\nlevel, UNESCO is starting to develop a global standard setting instrument\non AI.45 These are selected examples of the wide range of legal and policy\ninitiatives aiming to contribute to standard setting in the area of AI. This\nincludes, amongst others, actual (draft) legislation, soft-law, guidelines and\nrecommendations on the use of AI, or reports with recommendations for\nlaw and policy.\nFRA put together a (non-exhaustive) list of initiatives linked to AI policymaking.46\nWhile these also include legislative initiatives in EU Member States, many\norganisations and businesses launched initiatives to tackle ethical concerns\nof AI. However, while useful to tackle potential problems with AI, ethical\napproaches often rely on voluntary action. This does not sufficiently address\nthe obligation to respect fundamental rights.\nAs FRA pointed out in its Fundamental Rights Report 2019: “only a rights-based\napproach guarantees a high level of protection against possible misuse of new\ntechnologies and wrongdoings using them.”47 The European Commission’s\ninitiative on regulating AI helps to avoid disjointed responses to AI across\nMember States, which can undermine businesses across the EU and with\nentities outside the EU.\n                                                                                     23\n   Endnotes\n   1  Reuters (2018), ‘Amazon scraps secret AI recruiting tool that showed bias against women’, 10 October 2018.\n   2  Chatbot or chatterbot is a common AI feature embedded in messaging applications to simulate human conversation through voice or text.\n   3  Independent (2017), ‘AI robots learning racism, sexism and other prejudices from humans, study finds’, 17 April 2017.\n   4  Prates, M., Avelar, P. and Lamb, L. (2019) ‘Assessing Gender Bias in Machine Translation – A Case Study with Google Translate’, 11 March\n      2019.\n   5  The Gender Shades project evaluating the accuracy of AI powered gender classification products.\n   6  See for example: Der Standard (2020), Datenschutzbehörde kippt umstrittenen AMS-Algorithmus, or AlgorithmWatch (2019), Poland:\n      Government to scrap controversial unemployment scoring system.\n   7  Privacy First (2020), Dutch risk profiling system SyRI banned following court decision.\n   8  European Commission (2020), European enterprise survey on the use of technologies based on artificial intelligence, Luxembourg, July\n      2020.\n   9  See, for example, the website “AI myths”.\n   10 Samoili, S., López Cobo, M., Gómez, E., De Prato, G., Martínez-Plumed, F., and Delipetrev, B. (2020), AI Watch. Defining Artificial\n      Intelligence. Towards an operational definition and taxonomy of artificial intelligence, Luxembourg.\n   11 Schuett, J. (2019), A legal definition of AI, arXiv: 1909.01095\n   12 Hastie, T., Tibshirani R., and Friedman, J. (2009), The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.\n   13 See, for example: Pasquale, F. (2015), The Black Box Society. The Secret Algorithms That Control Money and Information, Harvard\n      University Press, Cambrigde and London; and Rai, A. (2020), ‘Explainable AI: from black box to glass box’, Journal of the Academy of\n      Marketing Science, Vol. 48, pp. 137-141.\n   14 A seminal paper describing this difference is: Breiman, L. (2001), ‘Statistical Modeling: The Two Cultures’, Statistical Science, 2001, Vol. 16,\n      No. 3, pp. 199-231.\n   15 European Parliament resolution of 14 March 2017 on fundamental rights implications of big data: privacy, data protection, non-\n      discrimination, security and law-enforcement (2016/2225(INI)), para. 1.\n   16 Ibid., para. 20.\n   17 European Council (2017), European Council meeting (19 October 2017) – Conclusions, EUCO 14/17, Brussels, 19 October 2017, p. 8.\n   18 European Commission (2018), Communication from the Commission to the European Parliament, the European Council, the Council, the\n      European Economic and Social Committee and the Committee of the Regions on Artificial Intelligence for Europe, COM(2018) 237 final,\n      25 April 2018.\n   19 More information is available on the webpage of the High level expert group.\n   20 High-Level Expert Group on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy Artificial Intelligence; Policy and investment\n      recommendations for trustworthy AI.\n   21 High-Level Expert Group on Artificial Intelligence (2020), Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-\n      assessment.\n   22 European Council, A New Strategic Agenda 2019-2014, p. 4.\n   23 Vonder Leyen, Ursula, A Union that strives for more: My agenda for Europe, p. 13.\n   24 European Commission, White Paper On Artificial Intelligence – A European approach to excellence and trust, COM(2020) 65 final, Brussels,\n      19 February 2020, p. 2.\n   25 Ibid., p. 12.\n   26 European Commission (2020), White paper on Artificial Intelligence: Public consultation towards a European approach for excellence and\n      trust,17 July 2020.\n   27 European Commission (2020), Adjusted Commission Work Programme 2020, Annex I: New initiatives, 27 May 2020.\n   28 European Parliament, Legislative Observatory, Framework of ethical aspects of artificial intelligence, robotics and related technologies,\n      2020/2012 (INL).\n   29 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a civil liability regime for artificial\n      intelligence, 2020/2014 (INL).\n   30 European Parliament resolution of 20 October 2020 on intellectual property rights for the development of artificial intelligence\n      technologies, 2020/2015 (INI).\n   31 European Parliament, Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters,\n      2020/2016 (INI).\n   32 European Parliament, Legislative Observatory, Artificial intelligence in education, culture and the audiovisual sector, 2020/2017 (INI).\n   33 European Parliament decision of 18 June 2020 on setting up a special committee on artificial intelligence in a digital age, and defining\n      its responsibilities, numerical strength and term of office, 2020/2684 (RSO).\n   34 European Council (2020), Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13/20, 2 October 2020.\n   35 Council of the European Union (2020), Shaping Europe’s Digital Future – Council Conclusions, 9 June 2020.\n   36 Council of the European Union, Council Conclusions “Access to Justice – Seizing the Opportunities of Digitalisation”, 13 October 2020.\n   37 Council of the European Union, Presidency Conclusions – the Charter of Fundamental Rights in the context of Artificial Intelligence and\n      Digital Change, 21 October 2020.\n   38 See e.g. Pagallo, U., Casanovas, P. & Madelin, R. (2019), ‘The middle-out approach: assessing models of legal governance in data\n      protection, artificial intelligence, and the Web of Data’, The Theory and Practice of Legislation 7 (1), pp. 1-25.\n   39 See FRA (2019), Fundamental Rights Report 2019, Luxembourg, Publications Office, Chapter 7.\n   40 Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the\n      Committee of the Regions, A European strategy for data, COM/2020/66 final.\n   41 European Council, Conclusions from Special meeting of the European Council (17, 18, 19, 20 and 21 July 2020), EUCO 10/20, 21 July 2020.\n   42 Council of Europe, Ad Hoc Committee on Artificial Intelligence (CAHAI), Factsheet: Governance for digital transformation.\n   43 Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights impacts of\n      algorithmic systems (adopted by the Committee of Ministers on 8 April 2020 at the 1373rd meeting of the Ministers’ Deputies).\n   44 See the dedicated OECD website.\n   45 See the dedicated UNESCO website.\n   46 See, for an overview by FRA, AI Policy Initiatives, or at the Council of Europe website.\n   47 FRA (2019), Fundamental Rights Report, Luxembourg, Publications Office, p. 166.\n24\n2\nPUTTING FUNDAMENTAL RIGHTS IN\nCONTEXT – SELECTED USE CASES OF AI\nIN THE EU\n                                 In the EU, the use of AI-related technologies is relatively wide-spread.\n                                 A recent survey shows that 42 % of companies use AI-related technologies\n                                 – and that 18 % plan to do so.\n                                                              This chapter presents selected cases of AI use\nNote on                                                       – typically referred to as ‘use cases’ in the AI\n                                                              field. FRA collected information on such cases\ninterviewees The use cases presented in this\n                                                              from five EU Member States: Estonia, France,\n                                                              Finland, the Netherlands and Spain. They involve\n             chapter are based on information\n                                                              different areas of application across public\n             obtained in interviews with\n             both public and private sector                   administration and private companies. Special\n             representatives.                                 focus is put on the use of AI in the areas of social\n                                                              benefits; predictive policing; health services;\n             The interviewed representatives                  and targeted advertising.\n             from public administration work\n             in the areas of health services,                 The chapter provides information on the current\n             infrastructure and energy, the\n                                                              use of AI, as well as basic information on EU\n             judiciary, law enforcement,\n                                                              competence, in these select areas. The use cases\n             migration and border management,\n             social benefits, tax, as well as                 provide a good sense of what kind of AI and\n             transportation and traffic control.              related technologies are currently being used.\n             Interviewees from private companies              The examples also offer context for the\n             mainly work in retail, marketing                 fundamental rights analysis. Looking at a\n             and pricing, the health sector, in               broad variety of use cases provides important\n             financial services, energy, insurance,           insights on how the actual use of AI can affect\n             employment and transport, as well\n                                                              people’s fundamental rights. Chapter 4 includes\n             as in cross-cutting areas with a focus\n                                                              a discussion of fundamental rights implications,\n             on AI development for different\n             sectors.                                         and makes reference to the cases described in\n                                                              this chapter.\n                                                                                                                   25\n                                    According to the European Enterprise Survey, at the beginning of 2020, 42 % of companies\n     Use of AI by                   in the EU said they use technologies that depend on AI. This percentage ranges from 27 %\n     companies in                   in Estonia and Cyprus to 61 % in Czechia (see Figure 1). Another 18 % of companies are\n     the EU in 2020                 planning to use AI in the future.\n                                    The survey indicates that AI is used mostly in the IT sector (63 %). The technologies used\n                                    comprise a variety of IT applications aiming at process or equipment optimisation, anomaly\n                                    detection, process automation, and forecasting, price optimisation and decision making.\n                                    FIGURE 1:       COMPANIES USING AI IN 2020, BY MEMBER STATE (%)\n                                     100\n                                     90\n                                     80\n                                      70           26\n                                                        20                               1\n                                                             18 21 27\n                                     60                                   11   12 19\n                                                                                     61      8    16 31 21 21\n                                            18                                                                23 14 25 22\n                                      50                                                                                                 9    22 19\n                                                                          54                 54                                                     16                  20\n                                                        51 50                  51                                                                      10 13\n                                                   48           46                                                                       48                  20 20 16\n                                      40                                            43\n                                                                                                  46                      44\n                                            42                       40                                     40 40                                      40\n                                      30                                                                             36             36              36\n                                                                                                                               33             34 35       35            34\n                                                                                                       31                                                          29\n                                      20                                                                                                                     27 27\n                                      10\n                                       0\n                                           EU-27\n                                                   RO\n                                                                                                                                                                        UK\n                                                   AT\n                                                   SE\n                                                    SI\n                                                   NL\n                                                   BG\n                                                   LU\n                                                   EL\n                                                   CZ\n                                                    LT\n                                                   BE\n                                                   MT\n                                                   LV\n                                                   DK\n                                                   HR\n                                                   DE\n                                                   HU\n                                                   FR\n                                                   PT\n                                                   PL\n                                                    IT\n                                                    FI\n                                                   ES\n                                                    IE\n                                                   EE\n                                                   CY\n                                                   SK\n                                                                               Currently using AI                   Planning to use AI\n                                    Notes:\t\u0007The survey asked about the use or plans for use of ten different AI related technologies, such\n                                            as speech recognition, visual diagnostics, fraud detection, analysis of emotions, forecasting\n                                            based on machine learning and more. Includes the percentage of companies using at least one\n                                            AI technologies. N = 9,640.\n                                    Source: FRA, 2020 [based on data extracted from European Commission, European enterprise\n                                            survey on the use of technologies based on artificial intelligence, Luxembourg, July\n                                            2020]\n     As noted, this report focuses on four broad AI ‘use cases’:\n     ――social benefits,\n     ――predictive policing,\n     ――health services, and\n     ――targeted advertising.\n     These areas are particularly sensitive as regards fundamental rights. Two                                                  “AI and machine learning are\n     cover mainly the public administration’s use of AI (social benefits allocation                                             different concepts. AI is an umbrella\n     and predictive policing). The other two concern private companies (health                                                  term.”\n     services and targeted advertising). These use cases provide the basis for                                                  (Private company, Estonia)\n     the report’s fundamental rights analysis by offering the necessary context.\n     Where appropriate, the report also highlights findings from interviews that\n     cover areas other than these four areas.\n     Detailed studies on the taxonomy of AI are available,1 providing further                                                   “What you see now is that\n     categorisations of the technology. As noted in the introduction, interviewees                                              everyone doing something with\n     had different views about what AI is and some also stated that there is no                                                 machine learning is labelling this\n     clear definition of AI.                                                                                                    as ‘AI’.”\n                                                                                                                                (Public administration, Netherlands)\n     This report discusses specific use cases without further classifying the\n     technology applied. Yet the use of AI in the cases examined differed: the\n26\n                                                            use of technology described by the interviewees involved both varying levels\n                                                            of complexity and varying levels of automation.\n                                                            Figure 2 provides an overview of different examples of use that interviewees\n                                                            discussed under the heading of AI. Some applications are relatively\n                                                            straightforward to understand. In rule-based decision making, algorithms\n                                                            are defined based on ‘if-then-rules’ (for example, if a person has an income\n                                                            below a certain threshold, then they will be eligible for certain benefits).\n                                                            Such algorithms were used in the area of social benefits at different levels\n                                                            of automation, with examples of full, partial or no human review involved.\n                                                            Other applications used more traditional statistical methods to inform\n                                                            decisions. These include, for example, regression analysis. This is a classical\n                                                            statistical method that analyses correlation between several pieces of\n                                                            information (‘variables’) and an outcome, which is a credit score in this\n                                                            example. Others used more complex machine learning methodologies to\n                                                            feed into the production of forecasts and statistics for government reports.\n                                                            There are also algorithms with much higher levels of complexity, such as\n                                                            deep learning for diagnosis support in the area of health. Such tools still\n                                                            include a high level of human review, and hence do not include a high level\n                                                            of automation.\n                                                            By contrast, targeted advertising is an example of potentially using highly\n                                                            complex algorithms without human review of each output and decision, also\n                                                            using highly complex algorithms including deep learning and reinforcement\n                                                            learning. (See Chapter 1 for descriptions of these terms.) Human review would\n                                                            also not be possible in this area due to the scale at which such algorithms\n                                                            operate.\nFIGURE 2:             EXAMPLES OF DIFFERENT AUTOMATION AND COMPLEXITY LEVELS IN USE CASES COVERED\nhigh          rule-based           regression analysis     deep learning and\n              automated            predictions for fully   reinforcement                            Areas of examples\n              decision making      automated credit        learning for\n              on social benefits   scoring                 advertising                               Social welfare\n              rule-based           regression analysis     facial recognition                        Marketing\nAutomation\n              decision for         predictions for         technology for\n              positive             human-reviewed          identification                            Law enforcement\n              outcomes of          credit scoring          with human\n              social benefits                              review                                    Health services\n                                                                                                     Financial services\n              rule-based           machine learning        medical images\n              human decisions      supported               analysis for\n              for social           production of           diagnosis\n              benefits             forecasts\nlow\n             low                        Complexity                                high\nSource: FRA, 2020\n                                                           AI systems also vary according to the potential harm that could result\nNotes: The examples from financial                          from an erroneous decision based on the use of AI. Depending on the area\nservices and the use of facial recognition                  of application, wrong decisions – based on erroneous outputs from the\ntechnology are not covered in the                           system – can have different impacts. When using AI for decision making,\ndetailed use case descriptions, but were                    the consequences are different if a decision is affirmative but wrong (false\nmentioned in other interviews. The\n                                                            positive) or negative but wrong (false negative).\nexamples illustrate different levels of\ncomplexity and automation, as used in\npractice.                                                   These issues are particularly important when machine learning is used, as\n                                                            it is based on statistical calculations, which always come with some degree\n                                                                                                                                              27\n   of error. While rule-based algorithms can also make mistakes (especially\n   if they grow more complex), risks are lower because of the deterministic\n   nature of the rules developed.\n   For example, when using AI to make decisions on social benefits, a false\n   positive means that a person may erroneously receive benefits. This does not\n   necessarily have a negative impact on the person concerned (unless the error\n   is found out later and the money needs to be\n   paid back). However, it negatively impacts on the\n   public administration, as money is paid not in line\n   with good administration practices. In contrast, a\n   false negative would have a negative impact on\n   the individual, because they would not receive\n   benefits to which they are entitled. Annex 2,\n   available on FRA’s website, provides further\n   hypothetical examples of effects of wrong\n   decisions based on the use cases discussed.\n   Importantly, when automating tasks, the impact\n   could also scale up, potentially exacerbating\n   the negative effect on society as a whole. The\n   severity and scale of potential harm is one aspect\n   that needs to be taken into consideration when\n   analysing potential limitations on fundamental\n   rights with respect to the use of AI.\n   For example, small error rates when using facial recognition technology\n   used by law enforcement might still lead to flagging many innocent people,\n   if the technology is used at places where many people are analysed. This\n   might apply to airports or train stations, where thousands of people could\n   be scanned on a daily basis.2 A potential bias in error rates could then lead\n   to disproportionally targeting certain groups in society.\n                                     Interviewees mostly mention ‘machine learning’, including the use of neural networks\n   Technologies                      and its extensions (see Chapter 1 for a description of machine learning). Respondents\n   used across                       either directly mentioned this, or mentioned subfields of machine learning, such as image\n   all cases                         recognition or facial recognition technology (FRT).\n   identified in\n   the research                      Most often, interviewees mentioned the use of ‘supervised machine learning’ as mainly\n                                     used to optimise for a specifically defined outcome. Yet sometimes ‘unsupervised machine\n                                     learning’ was also used to categorise or cluster data. Only one case referred to the use of\n                                     ‘reinforcement learning’, without going much into detail.\n                                     Several respondents used ‘natural language processing (NLP)’. This is a technology to\n                                     analyse text and speech, and is sometimes combined with machine learning algorithms.\n                                     Few mention examples that involve rule-based algorithms, meaning that the rules for the\n                                     algorithm to follow are directly encoded (i.e. based on ‘if-then-rules’).\n                                     In some cases, interviewees did not disclose or could not provide detailed information about\n                                     the technology used.\n   Generally, the interviewees referred to more than one use case, but were\n   asked to focus on one application during interviews.\n   ――Importantly, the fieldwork shows that companies and public administrations\n        are often still at the beginning of looking into the use of AI. Only about\n        two thirds of the use cases are actually in use and deployed in practice.\n        Many of the use cases described by interviewees are at pilot stage, under\n        development, or still in the research phase.\n   ――Two AI-driven applications were halted after tests.\n28\n                                      FIGURE 3:     WORDS INTERVIEWEES MOST OFTEN USED TO DESCRIBE THE AI\n                                                    ‘USE CASES’\n                                      Notes:\t\u0007FRA visualisation of the words most frequently used in descriptions\n                                                of use cases. The bigger the size of the word, the more often the\n                                                interviewees mentioned the terms.\n                                      Source: FRA, 2020\n                                      Figure 3 shows the most frequently used words to describe the use cases\n                                      covered in this report. It highlights the importance of data when using AI\n                                      systems as well as its relevance to supporting decision making.\n                                      FRA has previously highlighted that a thorough description of the data used by\n                                      AI applications is essential for identifying and mitigating potential fundamental\n                                      rights challenges.3 A variety of data were used for the AI systems covered\n                                      in this report. However, it was difficult to obtain detailed information about\n                                      the data used, because most respondents remained rather vague about\n                                      their data sources.\n                                      Rather generically, many respondents mentioned using ‘open data’, ‘historical\n                                      data’ or ‘metadata’. More concretely, respondents mentioned using customer\n                                      data, e.g. about purchases or browsing behaviour, or administrative records,\n“It is mostly used to save time […]\n                                      such as data on social benefits and taxes. Interviewees also mentioned\nwhen you have to go through a lot\n                                      medical records, police records, court records, as well as social media and\nof material.”\n                                      traffic data. Data included text data (e.g. e-mails), audio recordings, video,\n(Public administration, Netherlands)\n                                      and geolocation data. Data come from internal databases of companies and\n                                      public administration, but also from external sources.\n “The most important is to deal with\ncases more efficiently. It’s about    The single most important reason for using AI is increased efficiency. The\nmaking use of your workforce,         vast majority of respondents, across the public and private sector, mentioned\nthe people who handle cases, as       using AI for greater speed, fewer errors and cost reduction, as fewer human\neffectively as possible.”             resources are needed. Some interviewees from law enforcement also said\n (Public administration, Netherlands) they use AI for safety and security, as well as crime prevention.\n                                      Humans previously performed many of the use cases. Some respondents said\n                                      they use AI because it entails fewer mistakes than having humans carry out\n                                                                                                                        29\n   certain tasks. Some respondents also use AI for tasks that humans did not\n   previously carry out, as the quantity of information could not be processed by\n   humans – for example, in the area of genome analysis or traffic predictions.\n   Importantly, for about half of the respondents interviewed, the use of AI is\n   relevant for decision making. However, AI is mainly used to support decision\n   making, and the final decisions remain largely in the hands of humans.\n   Interviewees pointed out that, while enthusiastic, public administration and\n   companies are still cautious when deploying AI. Many of the use cases are\n   still in the testing phase. And some, as described below, were stopped during\n   this phase. Nevertheless, almost no interviewees were aware of any plans\n   to reduce the level of technology used. In fact, most expressed intentions to\n   invest in innovation or new ways to employ currently available AI systems.\n   2.1.\t EXAMPLES OF AI USE IN PUBLIC ADMINISTRATION\n   [Use case 1]\n   Automating social welfare systems – using algorithms in the area of\n   social benefits\n   Background and EU legal framework\n   The United Nations Special Rapporteur on extreme poverty and human rights,\n   Philip Alston, warned in his October 2019 report that introducing a ‘digital\n   welfare’ state, including the use of AI, can lead to a “digital welfare dystopia”.\n   Digitalisation of welfare systems is often accompanied with reductions of\n   overall welfare budgets, narrowing the beneficiary pool, and other measures\n   that reduce the availability of welfare. Digitalisation also increases the power\n   of states by offering opportunities to control people. This is particularly\n   worrying in countries with significant rule of law deficits.4\n   The use of algorithms by public administration in welfare raises major concerns\n   with respect to its potentially negative impact on poverty and inequality, if\n   applied erroneously in the area of social benefits.5 This includes areas such\n   as child welfare services6 and unemployment benefits.7\n   Yet public authorities are keen to use new technologies to make decision\n   making on social security and other benefits more efficient and potentially\n   fairer. Globally, new technologies are used in many ways to administer\n   welfare systems. These include identity verification, eligibility assessments,\n   benefit calculations, fraud prevention and detection, risk scoring and need\n   classification, as well as communication between authorities and beneficiaries.\n   The OECD defines social benefits as transfers made to households in need\n   after certain events or particular circumstances have arisen, including sickness,\n   unemployment, retirement, housing, education or family circumstances.8\n   However, there is no commonly agreed definition of social benefits. Social\n   benefits, in particular social insurance, systems are different from private\n   insurance schemes, as they involve compulsory contributions made by both\n   employees and employers, sometimes in the form of taxation.9\n   Social policy, including social security and social protection, is an area of shared\n   competence between the EU and the Member States (Article 4 (2) (b) of the\n   TFEU). Pursuant to Article 151 of the TFEU, the EU pursues the objectives,\n   among other things, to promote “improved living and working conditions”\n   and “proper social protection”. To this end, the EU supports and complements\n30\n                                    the activities of the Member States in a number of fields, including social\n                                    security and social protection of workers and combating social exclusion\n                                    (Article 153 (1) of the TFEU). EU actions can encourage cooperation between\n                                    Member States and adopt directives with minimum requirements. Moreover,\n                                    decisions on social security and social protection can only be adopted through\n                                    special legislative procedure by a unanimous vote in the Council.10\n                                    Against this backdrop, EU Member States are mostly free to shape their social\n                                    security and social protection policies. Since there is virtually no harmonisation,\n                                    social security systems differ significantly across the EU in terms of what\n                                    benefits are provided, conditions for eligibility, how benefits are calculated,\n                                    what contributions need to be paid and by whom, etc.\n                                    Public administrations in EU Member States are working on implementing AI\n                                    and related technologies in the area of public welfare. However, information\n                                    about its applications is limited. FRA collected information about use cases\n                                    linked to:\n                                    ――using algorithms when it comes to compensating job seekers,\n                                    ――processing social benefits applications, and\n                                    ――machine learning-supported data analysis on the use of pensions.\n           Several private insurance companies interviewed for this research use AI and related\nPrivate    technologies. This includes handling requests of customers for complementary health\ninsurance  insurance, insurance compensation decision support, evaluating the credit risk of\ncompanies’ individuals, insurance pricing, insurance claims management, and decision-making support\nuse of AI  related to management functions and credit decisions.\n           Private insurance companies generally embrace AI-related technologies, as these help make\n           their business more profitable. An OECD report highlights the importance of technology\n           for this sector. But it also argues that risk classification could lead to the exclusion of those\n           belonging to certain vulnerable groups in ways that are undesirable from a societal and\n           political perspective.*\n           * OECD (2020), The Impact of Big Data and Artificial Intelligence (AI) in the Insurance Sector.\n                                    Use in practice\n                                    The use cases outlined below exemplify some of the challenges when using\n                                    or planning to use AI in the area of social benefits, linked to algorithmic\n                                    decision making.\n                                    Experimenting with new technologies to support jobseekers\n                                    Over the course of a three-year project, a public organisation experimented\n                                    with several AI-related technologies concerning all of their work related to\n                                    processing benefits for job seekers and assisting them to return to work. The\n                                    representative interviewed states that the tested technologies can improve\n                                    and foster the relationship with job seekers and improve the advice given to\n                                    both job seekers and companies. After testing is completed, the organisation\n                                    will decide if and how it will apply these technologies in its day-to-day work.\n                                    Tests include machine learning-based detection of the attractiveness of\n                                    job offers and a system for detecting whether job seekers are still actively\n                                    looking for a job.\n                                    The tests are also looking into profiling job seekers to provide advice to them.\n                                    This would include calculating the probability of someone being offered an\n                                    available job within a given time, and identifying parameters that make job\n                                    offers relevant. This may also be reflected in advice to companies on best\n                                    practices for formulating job offers. The profiling would allow the organisation\n                                                                                                                        31\n   to determine appropriate services according to the profile and background\n   of the job seeker, rather than having an analysis and advice drawn up by\n   employees. Practically, this would be done by requiring job seekers to complete\n   a monthly diary on their job search. However, it is still under consideration\n   whether the programme should be limited to providing descriptive analyses, or\n   whether it should go further and provide recommendations. The organisation\n   is hesitant about the latter aspect.\n   Additionally, a natural language processing system is being tested for analysing\n   the content of job seekers’ e-mails. Here, e-mails are categorised, relevant\n   data is extracted, and the urgency and relevance of e-mails is identified.\n   Using a chatbot, and using automatic replies to emails, is being considered.\n   The data used for the systems come from several sources from within\n   the organisation. The data on job seekers and their background, including\n   personal tax data, as well as data on salaries and social security allowances,\n   are used under very strict conditions. This is because they are derived from\n   highly regulated data sources (e.g. salary statements cannot be accessed).\n   Other data, such as job offers from companies, are also used to generate\n   knowledge about the job market. The organisation currently does not use\n   external data, such as from (professional) social media networks, because\n   no legal provisions are in place for using such data.\n   Processing housing benefits – failure and success\n   A public body responsible for processing social benefits piloted an AI tool\n   to process applications and subsequently support their staff in making\n   decisions on housing benefits. The system selected cases from new benefit\n   applications that were relatively straightforward to calculate. These include\n   new applications for housing benefits submitted by an individual living alone\n   or with children, and by an individual who does not have any other income\n   than government benefits. Overall, these cases were deemed simple, with\n   the result always being that the individual receives the benefits.\n   The technological solution was based on a decision-tree model following the\n   rules for housing benefits. Calculating general housing benefits requires income\n   estimates in advance. The data used during the testing stemmed from an\n   internal database, which contains data on benefit application processes. The\n   data was pseudonymised as there was no need to use personal information.\n   A simple statistical model (linear regression) was used, where the input is\n   the income and the cost limits, and the output is the amount of benefit.\n   However, even in such simplified cases, they\n   found it too difficult to use AI in practice\n   because of the frequent changes in the\n   legislation. The test was terminated. According\n   to the interviewee, the lack of a legal basis for\n   using machine learning does not allow using\n   it for administrative decisions. There are no\n   further plans to use AI to support decision\n   making on social benefits.\n   While the organisation is not pursuing this\n   particular project due to the aforementioned\n   legal challenges, the interviewee noted\n   potential for further applications and solutions\n   in this area in the future. It was noted that\n   AI or related technologies that can support\n   operations without having a legal impact were\n   particularly good for the organisation.\n32\nThe SyRI case In the Netherlands, the so-called\n                                                                  At the same time, the organisation is using\n                                                                  image processing for social benefits applications.\n              ‘System Risk Indication’ (SyRI)* was                Generally, benefit applicants have to complete\n              developed as a government tool to                   several forms and attachments, which are often\n              alert the Dutch public administration               submitted in paper format. For more efficient\n              about fraud risk of citizens, by                    and time-saving handling of those documents\n              processing and linking large amounts                by the agency’s staff, the hard copies received\n              of their personal data from public\n                                                                  are scanned and then classified by an automated\n              authorities.\n                                                                  system.\n              A broad coalition of civil society\n              organisations dealing with privacy                  A first step is to turn images the right way\n              issues initiated a lawsuit, prompting               round. Algorithms re-align documents that\n              the District Court of The Hague to                  were not aligned properly when they were\n              scrutinise the algorithm-based SyRI.**              scanned, remove spots and clean up and\n                                                                  edit the colouring of the document, identify\n              The court ruled that SyRI impinges\n              disproportionately on the private                   columns, paragraphs, tables, and other elements\n              life of citizens. The court found that              as distinctive blocks, recognise the script, etc.\n              everyone whose data was analysed                    Then, the application checks if the received\n              by SyRI was exposed to this risk.                   application form and attachment are marked\n              In addition, due to the opacity of                  correctly (e.g. if a document is marked as an\n              the algorithm used, citizens could                  invoice, the system determines whether this\n              “neither anticipate the intrusion into              is correct).\n              their private life nor can they guard\n              themselves against it.” ***                         The turning and the classification of the images\n              *A good description of SyRI can be                  are done by image recognition and Optical\n              found in Ilja Braun (2018), High risk               Character Recognition (OCR) technologies. They\n              citizens, in: Algorithm Watch.                      recognise text stemming from images, including\n                                                                  from photographs and scans of documents\n              ** The ruling of 5 February 2020 (in                or handwritten notes. OCR technology then\n              Dutch) is available online.                         converts the recognised text into text data that is\n              *** Privacy First (2020), Dutch                     machine-readable. Here, in a pattern recognition\n              risk profiling system SyRI banned                   process, input from the scanned images is\n              following court decision.                           first isolated, then compared to ‘glyphs’ (i.e.\n                                                                  variations of letters) stored by the system on\n                                                                  a pixel to pixel basis.\n                                   The agency will continue processing images and further develop it, for\n                                   example, by potentially making it possible to scan bar codes from attachments.\n                                   This would help to speed up the confirmation of the correctness of documents\n                                   and attachments. There will also be more solutions related to natural language\n                                   processing.\n                                   Automating unemployment benefits\n                                   In one of the countries selected, most decisions on unemployment benefits\n                                   are fully automated. The national institution responsible for unemployment\n                                   insurance benefits updated its system in 2019 to fully automate most of the\n                                   processing of benefit applications and decisions. This was done after the\n                                   relevant legislation was adapted to allow automated decisions.\n                                   If a person registers as unemployed and lodges an application for benefits,\n                                   the system draws on information about the applicant from various other\n                                   databases. This includes, for example, the population register, and tax\n                                   authorities’ databases containing information about salaries and work\n                                   experience, etc. If all conditions for receiving unemployment benefits are\n                                   fulfilled, the system calculates the period of payments, based on the length\n                                   the person has contributed to the insurance system, and the amount of\n                                   benefits, based on the average daily salary.\n                                                                                                                      33\n   The procedure is fully automated. However, an employee of the institution\n   must intervene if necessary information cannot be extracted from the\n   databases, if there is contradictory information in the databases or if the\n   decision on a case involves a level of discretion (i.e. the decision cannot be\n   definitively determined based on the data available and a human has some\n   leeway in deciding on the case).\n   The main reason for using this system is improved efficiency. In addition, the\n   system is believed to achieve consistency in the processes. This is because\n   every application, not subject to discretion, is handled in the same way.\n   [Use case 2]\n   Predictive policing – trying to anticipate crime in advance\n                                                                                     FRA ACTIVITY\n   Background and EU legal framework\n   AI technologies are used in law enforcement, particularly in predictive policing.\n                                                                                     Preventing\n   Existing research into how such tools can affect fundamental rights has           unlawful profiling\n   highlighted particular issues concerning discrimination, among other rights.\n   One recurrent concern is the potential for predictive policing to reproduce\n                                                                                     today and in the\n   and entrench existing discriminatory practices, particularly through reliance     future: a guide\n   on historical crime data that may be biased or incomplete. This is because\n                                                                                     In developing and using algorithmic\n   many crimes – such as domestic violence or hate crime – remain largely\n                                                                                     profiling, bias may be introduced at\n   unreported and therefore are under-counted in official police statistics.11\n                                                                                     each step of the process. To avoid this\n                                                                                     and subsequent potential violations\n   A focus on certain crimes, such as violence and drug-related crime in public\n                                                                                     of fundamental rights, both IT experts\n   places – rather than on business fraud and non-payment of taxes, for example –\n                                                                                     and officers interpreting the data\n   can also make law enforcement responses less equitable.12 This is because the\n                                                                                     should have a clear understanding of\n   former are often associated with certain demographics and neighbourhoods.\n                                                                                     fundamental rights.\n   Ultimately, this can undermine police relations with particular communities.\n                                                                                     This FRA guide explains what profiling\n   Criminological research on crime ‘hotspots’ has been around for several           is, the legal frameworks that regulate\n   decades – notably in the UK and USA.13 It uses police data to map certain         it, and why conducting profiling\n   crimes and undertakes statistical tests to explore crime probabilities. Various   lawfully is both necessary to comply\n   police forces have used and developed them to address different types of          with fundamental rights and crucial\n   crime concentrations or clusters (‘hotspots’).                                    for effective policing and border\n                                                                                     management.\n   More recently, adaptations of this area of applied research have used AI as a\n                                                                                     For more information, see FRA (2018),\n   tool to enhance its effectiveness, with some suggesting that using algorithmic\n                                                                                     Preventing unlawful profiling today\n   tools could reduce the police’s reliance on subjective human judgments\n                                                                                     and in the future: a guide.\n   that may reflect biases or stereotypes.14 Some studies have also indicated\n   that predictive policing could potentially reduce unnecessary surveillance,\n   questioning, and physical checks and searches,15 reducing the humiliation\n   and harassment of individuals that may occur during these activities.\n   Predictive policing aims to forecast the probability of crime and anticipate\n   emerging trends and patterns to inform crime prevention and intervention\n   strategies.16 It may also be a part of an investigation into a crime that has\n   already taken place. While there is no authoritative definition of predictive\n   policing,17 it is typically characterised by analysing data to identify common\n   patterns and trends in crime by using algorithms to create models based\n   on the analysis. This is used to forecast criminal activity that may occur in\n   the future.\n   AI technologies in this area generally either aim to ‘predict’ crimes or to\n   ‘predict’ which individuals will either commit or be victims of crimes. Tools\n   aiming to predict crimes are generally fed with historical data – largely from\n   official sources – on the time, place and type of crimes committed. This can\n   be complemented by environmental variables, such as population density,\n34\n                                        presence of certain public places or services, and major events or holidays.\n                                        They generally do not use personal data when applied.18\nFRA ACTIVITY                            In contrast, AI systems focused on predicting potential perpetrators or victims\nFacial recognition                      of crime employ both historical and real-time personal data. This could include\n                                        criminal records data, addresses, phone numbers, location data, data extracted\ntechnology                              from social media, information about known associates and health or income\non the rise:                            data. This is then combined with other criminal and environmental data.19\nfundamental rights                      The EU and its Member States have shared competence in the area of freedom,\nconsiderations in                       security, and justice (Article 4 (2) ( j) of the TFEU). This includes judicial\n                                        cooperation in criminal matters and police cooperation (Articles 82-89 of\nlaw enforcement                         the TFEU). Already when the Treaty of Lisbon was adopted, an annexed\n                                        declaration on the protection of personal data in judicial cooperation in\nEU law recognises as ‘sensitive data’\n                                        criminal matters and police cooperation observed that “specific rules on\npeople’s facial images, which are a\n                                        the protection of personal data and the free movement of such data in the\nform of biometric data if processed by\n                                        fields of […] police cooperation based on Article 16 of the [TFEU] […] prove\nfacial recognition software. But such\n                                        necessary because of the specific nature of these fields.”20\nimages are also quite easy to capture\nin public places. Although the accuracy\n                                        Within the framework of predictive policing, the collection, storage, processing,\nof matches is improving, the risk of\n                                        analysis and exchange of information is particularly relevant. The processing\nerrors remains real – particularly for\n                                        of personal data in the context of law enforcement operations is regulated at\ncertain minority groups. People whose\n                                        EU level by the Law Enforcement Directive (Directive (EU) 2016/680). 21 It sets\nimages are captured and processed\n                                        out comprehensive standards and safeguards for such processing, including\nmight not know this is happening\n                                        the safeguarding against and the prevention of threats to public security.\n– and so cannot challenge possible\nmisuses.\n                                        Use in practice\nThe FRA paper outlines and analyses\n                                        The use cases collected by FRA signal the variety of ways in which law\nthese and other fundamental rights\n                                        enforcement authorities already use, or plan to use, AI and related technologies\nchallenges that are triggered when\n                                        to support their work.\npublic authorities deploy live FRT for\nlaw enforcement purposes. It also\n                                        Examples mentioned by interviewees range from data mining systems\nbriefly presents steps to take to help\n                                        designed to map crime patterns, detecting online hate speech and making\navoid rights violations.\n                                        risk assessments on gender-based violence, to automating certain prison\nFor more information, see FRA (2019),   guard duties. Other use cases include detecting illicit objects from satellite\nFacial recognition technology:          images, and, more generally, recognising objects in images. In addition, a tool\nfundamental rights considerations in    was mentioned in the research used in the private sector for fraud prevention\nthe context of law enforcement.         and crime detection in money transfers.\n                                        Interviewees emphasised that AI or related technology systems are used to\n                                        automate and speed up tasks previously done by humans, thus freeing up\n                                        and/or better distributing resources.\n                                        Mapping crime to support the efficient allocation of investigation capacity\n                                        A national intelligence agency and public prosecutor’s office employ a data-\n                                        driven system to help their employees make choices on how, where and\n                                        when to use the available investigation capacity. The aim is to improve the\n                                        allocation of human resources, ensuring that officers can be present at the\n                                        right time and place.\n                                        The interviewees suggest that this system could make more precise\n                                        assessments compared to humans, who often rely on their gut feeling for\n                                        decisions. Still, the system is always used in combination with human appraisal\n                                        and other non-AI systems to make operational decisions.\n                                        Based on system-generated outcomes, analysts create a ‘heat map’. This\n                                        outlines the prevalence of certain crimes in certain areas. This replicates a\n                                        long-standing manual version of this crime anticipation system, whereby\n                                                                                                                          35\n   police officers put pins on a map to indicate specific risk areas. Using AI to\n   increase the speed of this process also makes it more reliable, users believe,\n   because it can analyse more data.\n   The system is based on data mining and machine learning processes. It is\n   primarily built on unique police data contained in crime reports, witness\n   statements, and suspect declarations. Gaps are, to the extent possible,\n   addressed by using other data sources, such as criminology research, and\n   social and demographic information obtained from the national office of\n   statistics. The system also uses data from open sources.\n   The specific parameters for calculation depend on the type of crime, as\n   predictive factors vary in relevance across crime areas. For example, in the\n   case of burglaries, data on burglaries is collected and combined with data\n   on the place of residence of known criminals and their distance to burgled         FRA ACTIVITY\n   houses. The relevant criteria are preselected to allow the system to produce\n   the heat map.                                                                      Detecting hate\n   Location-based predictions are made for the next six months, and indicate the\n                                                                                      speech online\n   time and location where a burglary may occur. The result is a map of small         A public agency combatting hate\n   squares where the risk of crime occurring is indicated in different shades.        crime uses an AI-based tool to detect\n   The interviewees indicated that this visualisation helps officers to analyse       online hate speech by analysing\n   neighbourhoods and observe correlations between different locations.               patterns of speech online. On the\n                                                                                      basis of the processing, the system\n   Assessing the risk of gender-based domestic violence                               determines which social groups are\n                                                                                      targeted. This helps law enforcement\n   A national police force uses an internal system to track cases of gender-\n                                                                                      adopt measures to protect them\n   based domestic violence. The system helps police officers take decisions and\n                                                                                      before threats are realised.\n   distribute resources across domestic violence cases. The system categorises\n   cases on the basis of the assessed risk of relapse and repetition, in order to     Although the tool aims to identify\n   focus on the ‘riskiest’ cases.                                                     potential victims, rather than\n                                                                                      perpetrators, law enforcement can\n   A specialist team could complete the risk analysis without using AI. However,      use the information generated by the\n   the system is able to compute a large amount of data in a short amount of          system to ask social media providers\n   time and assist untrained or non-specialist police officers in risk analysis.      for information on users to pursue\n                                                                                      criminal investigations.\n   When a case of alleged gender-based domestic violence is reported, the police\n                                                                                      One particular challenge is\n   officer starts an initial investigation. This includes collecting evidence, taking\n                                                                                      understanding the context in which\n   witness statements and – potentially – making an arrest. Using information\n                                                                                      statements are made. For example,\n   gathered from this process, the officer fills out two detailed questionnaires\n                                                                                      journalists or academics may use\n   to assess the complaints, evaluate the probability of reoffending, examine\n                                                                                      words associated with hate speech to\n   the evolution of the case and assess the behaviour of the perpetrator and\n                                                                                      report on or analyse its occurrence.\n   the victim. Police officers also indicate the level of gravity, the nature of\n   threats faced and attitudes concerning the victim.                                 In 2021, FRA plans to initiate research\n                                                                                      on online hate present on social\n   The system then produces a risk ‘score’ on a three point scale. The police         media. This will allow FRA to provide\n   officer can raise the level of risk manually, but cannot lower the risk level      input to policy developments in the\n   below that indicated by the system. Once the level is confirmed, specific          area of online content moderation,\n   measures are applied in line with established police protocols. The system         which uses AI.\n   also informs a judge about potentially ‘severe cases’ through an automated\n   system.\n36\n2.2.\t EXAMPLES OF AI USE IN THE PRIVATE SECTOR\n[Use case 3]\nAI and health – analysing medical records to save lives\nBackground and EU legal framework\nHealthcare is particularly prominent in discussions about the use of AI. Medical\ndata and online applications have the potential to support improved health\noutcomes and – as a result – wider socio-economic benefits. The COVID-19\npandemic has further increased focus and interest in the area, particularly\nin terms of the potential for (online) data and applications to enhance the\nability of governments and health services to track the spread of disease.\nHealth is also prominent in the general population’s views on uses of AI. A\n2019 Eurobarometer survey found that every second European thinks that\nAI can be best used to improve medical diagnostics, develop personalised\nmedicine, or improve surgery.22\nThis use case covers applications of AI or related technologies by public\nand private sector stakeholders in the area of medical records and disease\nprediction. Feeding data from electronic medical records (EMR) and electronic\nhealth records (EHR) into AI systems and related technologies can support\nthe development of preventative medicine that recognises early risks of\ndisease and designs appropriate interventions. Researchers can predict\nclinical events such as mortality, hospitalisation, readmissions and length\nof stay in the hospital.\nBeyond disease prediction, medical record data can be analysed to predict\npatients’ adherence to treatment and their keeping of medical appointments.\nThese technologies have the potential to support improved health outcomes,\nas well as increase the efficiency of the healthcare system.\nUnder Article 6 of the TFEU, the EU has supporting competence in protecting\nand improving human health. Member States retain full responsibility for\ndefining their health policies, organising and managing their health systems,\nand for delivering health services (Article 168 (7) of the TFEU).\nWithin the EU competence, Union action, which has to complement national\npolicies, is directed towards improving public health, preventing physical and\nmental illness and diseases, and obviating sources of danger to physical and\n                                                                                 37\n   mental health. Such action can cover health information and education, as\n   well as monitoring, early warning of and combating serious cross-border\n   threats to health (Article 168 (1) of the TFEU). In the latter areas, the EU can\n   adopt incentive measures, excluding any harmonisation of the laws and\n   regulations of the Member States.\n   Other rules and policies adopted at the EU level aim to ensure free movement\n   of citizens, their equal treatment and non-discrimination abroad, as well as\n   availability and safety of medical products and services in the single market.\n   Considering the development of technologies and their application in health\n   care, exchange of medical records, patients’ rights in cross-border situations\n   and disease prediction as a matter of public health are particularly relevant.\n   Under the GDPR, health and genetic data are considered as a special category\n   of data (Article 9) called ‘sensitive data’.23 These require specific protection as\n   their processing could create significant risks. Data subjects’ health and genetic\n   data can only be shared in specific circumstances under Article 9 (2) of the\n   GDPR. The GDPR provides an exemption to the purpose limitation principle if\n   data are used for research purposes, in line with its Article 89 (1). Researchers\n   are required to ensure that technical and organisational safeguards – such\n   as pseudonymisation and anonymity – are in place when using patient data.\n   The EU has also taken action regarding the exchange of medical records.\n   European Commission Recommendation C(2019)800 on a European Electronic\n   Health Record exchange format24 “seeks to facilitate the cross-border\n   interoperability of EHRs in the EU by supporting Members States in their\n   efforts to ensure that citizens can securely access and exchange their health\n   data wherever they are in the EU.”25 The recommendation lays out technical\n   specifications for the exchange of such data between EU Member States.\n   The European Data Strategy (February 2020) also has a strong focus on health\n   data.26 A ‘Common European health data space’ is one of the nine common\n   European data spaces whose establishment the European Commission will\n   support.\n   The Early Warning and Response System (EWRS) is owned by the European\n   Commission and operated by the European Centre for Disease Prevention\n   and Control. It aims at “notifying at EU level on serious cross-border threats\n   to health”27 and enabling “the European Commission and EU countries to be\n   in permanent communication for the purposes of alerting, assessing public\n   health risks and determining the measures that may be required to protect\n   public health.”28\n   EMR, which is a computerised medical record created for patients of a\n   healthcare organisation,29 and EHR, which contains a patient’s medical history\n   beyond one organisation and involve sharing data across the healthcare\n   system, can include a large amount of personal data. This can encompass,\n   among others: the name and contact details of the individual and their next\n   of kin; demographic information, diagnoses and test results; and medication\n   and treatment.30 They may also include patient-generated data from wearable\n   devices.31\n   There is no uniform EMR/EHR system operating across all EU Member States.32\n   Some, such as Germany, do not have a national EMR/EHR system. Others –\n   including Belgium and Denmark – have different EMR/EHR systems at the\n   regional level. The systems differ considerably depending on what data is\n   recorded and by whom and who has access to what data.33 The European\n   Commission and other stakeholders have highlighted the diversity of country-\n38\nlevel EMR/EHR systems and their lack of interoperability as a major barrier\nto the digital single market in health.34\nStudies highlight the potential for AI or related technologies to enable earlier\ndiagnosis, widen possibilities for disease prevention and improve patient\nsafety,35 strengthening the right to access preventive healthcare and benefit\nfrom medical treatment. EMR/EHR may also help to make healthcare more\npersonalised,36 while the possibility for rapid sharing of data can facilitate\nmore coordinated and timely treatment.\nHowever, use of EMR/EHR presents significant data protection risks. The\nhealthcare sector leads in terms of personal data breaches.37 The amount of\nthe personal data stored, the highest among all industries, combined with\nthe large data-sharing network and number of access points, makes the\nhealthcare sector an attractive target for hackers.38\nThe quality of data in EMR/EHR also raises some concern. Studies where\npatients were shown their medical files and asked about their accuracy\nfound that up to 50 % of information was incomplete or erroneous.39 A lot\nof important data in EMR/EHR is unstructured in the form of free text, which\nfurther reduces data quality.40 Low levels of accuracy, completeness and\noverall data quality increases the risk of medical error.41\nUse in practice\nThe applications described in the interviews include both simple and more\nadvanced models employed in the public and private sectors. The largest\nnumber of use cases refer to image-based diagnosis tools. However,\ninterviewees also discussed tools to automate various working procedures,\nsuch as the mapping of text data, filing of medical records, and analyses and\nmeasurements of body tissues and nerve fibres.\nA smaller number of examples touched on more advanced projects, such as\nsystems to monitor remotely certain health indicators, such as heart rate. In\neach case, systems complement the expertise of health professionals. The\nnext sections present examples of diagnostic and remote monitoring tools.\nImage-based tools to help detect and diagnose disease\nThe tools used to support the detection and diagnosis of diseases described\nby interviewees work in similar ways. For example, a privately owned hospital\nuses an AI system to interpret images from CT-scans of stroke patients. After\na stroke, imaging is used to detect where damage to the brain has occurred\nand where there may be blockages in the blood supply to the brain. It can\nalso generate measures that can be compared to particular values by a\nmedical specialist.\nThe interviewee feels that the application helps to determine such\ncharacteristics in images more quickly, potentially – depending on who uses\nthe tool – improving the quality of the diagnosis. However, they highlight\nthat it is not necessarily more efficient to rely on the AI application, since a\nmedical professional must be present and they could examine the image.\nRather, the tool can offer some support – for example, if the specialist finds\nit difficult to interpret a certain image or find abnormalities.\nThe system was built, trained and validated using a dataset partially based on a\nlarge scientific study to which the hospital contributed. This was supplemented\nby purchasing foreign datasets. The algorithm will not be further trained or\nadapted in the future based on new data. No new versions will be released.\n                                                                                 39\n   The developers feel that allowing the system\n   to continue to learn would make it difficult to\n                                                       Using AI to                A public authority responsible for\n   validate its operation.\n                                                       target health              inspecting food safety standards in\n                                                                                  restaurants uses machine learning to\n   A private company developed an algorithm\n   that supports the detection of breast cancer\n                                                       inspections                process customer review data from\n                                                                                  major online platforms. This helps to\n   from mammography exams. The tool gives a                                       decide where and when to conduct\n                                                                                  inspections. Previously, this process\n   probability and degree of certainty, which can\n                                                                                  was based on complaints the\n   help radiologists to speed up their analysis\n                                                                                  authority received and on previous\n   of the results and decide whether additional                                   reports. Since the introduction of\n   tests are warranted. The algorithm detects and                                 the tool, the rate of non-compliant\n   characterises anomalies in a mammography                                       restaurants identified doubled from\n   as cancerous or not.                                                           around 18 % to 36 %.\n   While the interviewee indicates that the                                       The first step involves text mining.\n                                                                                  The algorithm identifies reviews\n   system now has a very low rate of false\n                                                                                  containing key words that may\n   negatives or false positives, they note that in\n                                                                                  indicate health and safety issues,\n   many cases it does not deliver a clear outcome.                                such as ‘sick, ‘nausea’ or ‘rodents’.\n   The system was trained on radiography and                                      For the second step, the authority\n   mammography data from Europe and the EU,                                       compared results coming from\n   with written reports and past biopsies acting                                  customer reviews with previous\n   as control data.                                                               inspection reports to improve the\n                                                                                  algorithm’s accuracy and reliability.\n   Monitoring patients’ vital statistics remotely\n   A hospital is piloting a system to support\n   early detection of potential illness. Monitoring\n   patients’ health indicators – for example, blood pressure or heart rate –\n   typically takes place manually and captures the situation at a specific moment\n   in time. Constantly monitoring such indicators has the potential to identify\n   trends that doctors may otherwise not recognise and detect health issues\n   early to prevent illness. The system uses a biosensor – a kind of plaster –\n   which gathers hemodynamic data from patients continuously by constantly\n   monitoring heart pulsation and respiration.\n   The data used by the system come from the hospital and the patient. These\n   data are anonymised before being shared with the third-party provider.\n   No other information besides that gathered through the monitoring of the\n   plaster is used to build and train the system. Data on environmental factors\n   were not incorporated in the pilot because, the interviewee pointed out,\n   they could contain biases.\n   In the future, the system will combine the information gathered by the\n   biosensor with separate information from patients’ EMR to draw conclusions\n   from trends observed in the monitoring.\n40\n[Use case 4]\nTargeted advertising – profiling consumers to boost profit\nBackground and EU legal framework\nThe internet has transformed the way we live. Many people make use of\ninternet services, often offered for free, on a daily basis. Companies offering\ntheir services for free mainly generate revenue through advertising, with\nadverts automatically targeted to individual consumers based on information\nabout them.\nThe availability of data about online individual behaviour combined with\nmachine learning technologies have considerably improved the ability of\ncommercial enterprises to target individuals. This could even go as far as\nmanipulating consumers by predicting their reactions based on irrational\naspects of psychology and not reasoned choice.42\n                                   The Cambridge Analytica scandal underscored\n                                   the particularly negative impact of such uses\n                                   for political purposes. In that case, a company\n                                   illegally obtained personal data on millions of\n                                   social media users to target political adverts\n                                   to different social groups based on certain\n                                   psychological profiles.43\n                                   A recent declaration of the Committee\n                                   of Ministers of the Council of Europe\n                                   highlights the lack of knowledge about the\n                                   manipulative power of algorithms. “The\n                                   effects of the targeted use of constantly\n                                   expanding volumes of aggregated data on\n                                   the exercise of human rights in a broader\n                                   sense, significantly beyond the current\n                                   notions of personal data protection and\n                                   privacy, remain understudied and require\n                                   serious consideration.”44 Concerns have also\nbeen raised about how online advertising, powered by AI technologies, can\naffect data protection and privacy,45 consumer protection,46 the right to non-\ndiscrimination,47 and even the way democracies work.48\nThe word ‘advertising’ is associated with messages designed to influence\nconsumer behaviour. Advertising in one form or another has always targeted\nspecific groups based on their characteristics and behaviour.49\nThe growth of social media, however, has taken targeted advertising to\nanother level, using direct access to consumer data. Micro-targeting is directed\ntowards very specific groups – and the more data that is gathered through\nonline activities, the more targeted these activities can be. As social media\nproviders and platforms like Google or Amazon gather comprehensive user\ndata by monitoring the various activities of their users, advertisers can access\nmore detailed and specific information.50\nThe area of targeted advertising and systems that recommend content (e.g.\nnews or movies) is one of the few real life examples that also involves so-\ncalled reinforcement learning. This is a technology that is based on optimising\na certain goal through experimenting and updating its rules automatically\nto have the best possible output. This means a systems tries out different\nad placements through trial and error and so finds the best way to optimise\nrevenue – including an element of self-learning.\n                                                                                   41\n   While very little knowledge on the actual use of reinforcement learning is\n   available for European countries, major companies working in the area are\n   researching the issue.51\n   Issues related to targeted advertising fall under consumer protection. This\n   falls under shared EU competence with Member States under Article 4 (2) (f)\n   of the TFEU. EU consumer protection measures seek to protect the health,\n   safety and economic interests of consumers; and promote their right to\n   information, education and to organise themselves to safeguard their interests\n   (Article 169 (1) of the TFEU). The EU can adopt minimum harmonisation\n   measures to achieve a high level of consumer protection (Article 114 (3) of\n   the TFEU), yet allowing EU Member States to introduce even more stringent\n   measures nationally.\n   In secondary EU legislation, rules on advertising are covered by\n   Directive 2006/114/EC concerning misleading and comparative advertising.52\n   This directive provides a minimum level of protection from misleading\n   advertising. It also harmonises rules on comparative advertising across the\n   EU. The provisions of Directive 2006/114/EC apply to both consumer-to-\n   business and business-to-business relations. However, they are practically\n   only applied to the latter53 after Directive 2005/29/EC on unfair business-to-\n   consumer commercial practices in the internal market practices54 took effect.\n   Further, Directive 2006/123/EC on services in the internal market55 covers\n   services that include advertising. Additionally, Directive 2000/31/EC on\n   certain legal aspects of information society services, in particular electronic\n   commerce, in the internal market (E-Commerce Directive) also applies. This\n   directive forms part of the legal framework for digital services in the EU.\n   To meet significant developments in the area of new online services and\n   practices, the E-Commerce Directive is currently being revised as part of the\n   Digital Services Act package. That package aims to “strengthen the Single\n   Market for digital services and foster innovation and competitiveness of the\n   European online environment”.56\n   FRA collected information about actual use cases from six European companies\n   engaged in placing online ads, content recommendation, and personalised\n   marketing.\n   Use in practice\n   The examples covered include:\n   ――placing ads online based on click predictions (i.e. learning about the\n        likelihood that online users click on certain links or adverts) and automated\n        bidding at auctions for online advertisement space\n   ――personalised and targeted marketing and communication via email.\n   Most tasks were fully automated. The examples concern analyses of user\n   preferences and activity and calculations of probabilities of clicks and\n   purchases, including a measurement of the effectiveness of previously made\n   recommendations. This also includes methods of targeted communication\n   on the basis of identified target groups to build (long term) trust between\n   clients and service providers.\n   Targeted online ads based on click predictions\n   Business models working with click predictions and targeted advertisements\n   often follow a ‘click and buy’ policy. Companies purchase advertising space\n   on media platforms, and optimise the display of adverts by analysing the\n   interests and preferences of website users and showing them advertisements\n42\nthat interest them. The purpose is to increase the relevance of advertisements\nshown by better matching them to the interests of those who see them.\nIn the present example, the company only gets paid if people click on an\nadvertisement and buy something. Additionally, the company uses AI to\ndetect inappropriate content in advertising, such as advertisements for\nalcohol, firearms or political content.\nThe company uses a range of machine learning techniques in the field of\ncomputational advertising. To estimate the probability of a user clicking on\nan advertisement displayed in a specific context (optimising the so-called\nclick-through-rate), customers’ interests and the relevance of products are\nmeasured via a mapping of individuals’ browsing histories and transaction\npatterns. Further, information is derived from individuals’ navigation on\nmerchant websites worldwide, with whom the advertising company works.\nThis is done via anonymised third party cookies and trackers.These are placed\non these merchant websites and outline individuals’ navigation across them,\nand also list the products seen and purchased.\nThe profiles of individuals are linked to devices used by them, although\nIP addresses are anonymised. Once a product has been purchased, a\nrecommender system algorithm tries to determine other products that\nthe customer could also buy. In this case, ‘fresh’ data is valued higher than\nolder data. Browsing histories are stored for a maximum of one year, as\ninterests change and purchases older than a year are no longer necessarily\nconsidered relevant.\nAdvertisements shown to the respective person are immediately adapted\naccordingly, and they vary across websites, to also match the content of\nthe latter. Once an advertisement is posted, it is continuously analysed.\nThe combination of elements taken into account on an individual’s interest\nis confirmed when a purchase is made. Data is shared across platforms,\nwhich includes informing others once a purchase has been made, to stop\nadvertisements of that particular item. If no purchase is made, the formula\nis reviewed and the algorithm is further adapted on individuals’ continuous\nonline behaviour.\nIn the future, the company covered in this example expects to work more\non optimising its timing in terms of when it places advertisements, within\ntheir given budget for a certain time frame. It also expects to focus more on\ndisplayed ads that have an impact on consumers.\nAnother example is based on a European online market place, which links\nbuyers and sellers on a range of specialised products. Here, AI is used\nto optimise advertising campaigns, to categorise products based on the\nadvertisements that are shown on the website of the market place, to\nimprove the search engine experience by predicting complementary and\nsubstitutable products, and to detect fraud attempts.\nThe company uses machine learning to predict the value of clicks of customers\nto buy advertisement space, which is offered in real-time auctions. With\nthese examples, the company indicates that AI enables it to make decisions\nthat otherwise would not be possible without AI, or which would have to be\nsignificantly scaled down.\nTargeted communication with customers and clients\nIn the case of a retail company focusing on specialised supplies sold across\nphysical stores and online, direct marketing or personalised advertising is\n                                                                               43\n   used to increase appeal to customers, and at the same time measure the\n   efficiency of a particular instance of marketing or advertising.\n   According to the company at issue in this example, marketing emails are\n   opened at an average of 20-30 %, and particularly so when customers\n   recognise relevant and favourite products being offered. Marketing emails\n   are sent to around 250,000 registered individuals, and a system is used to\n   establish what may be considered relevant by each of these individuals.\n   This is done by analysing purchases made by the respective individuals in\n   the previous six months. 80 % of the offers displayed are directly based on\n   previous purchases. Meanwhile, 20 % are new suggestions, i.e. alternative\n   products in the same category as the previous purchases.\n   A similar approach is used by a bank that sends emails to clients. Messages\n   offering specific services or products are sent only to certain clients. Data\n   analysts calculate the probability of clients being interested in a service or\n   product. If this probability is above a certain threshold, the client will receive\n   the message. The system used does not yet include machine learning models\n   and is not fully automated. These points will be taken on when they further\n   develop the system.\n   In a third example, a grocery retailer uses loyalty cards both to increase the\n   customers’ interaction and to personalise offers. Loyalty card systems can\n   predict how many customers are likely to engage with a product offering.\n   The system covered in this example also suggests new products to customers\n   and tracks the results of these suggestions. It groups buyers with similar\n   behavioural patterns into segments to make more personalised suggestions.\n   Every week, the company’s loyalty card owners receive personalised offers\n   by email, website or mobile application, and they can access offers through\n   in-store terminals. The AI system selects the offerings based on the individual\n   purchase history, and it recommends new items that might catch the buyer’s\n   interest and prompt a purchase.\n44\nEndnotes\n1  See, for instance, Samoili et al. (2020), AI Watch. Defining Artificial Intelligence. Towards an operational definition and taxonomy\n   of artificial intelligence, Luxembourg; Karanasiou A. and Pinotsis D. (2017), ‘A study into the layers of automated decision-making:\n   emergent normative and legal aspects of deep learning’, International Review of Law, Computers & Technology, 2017, pp. 170-187.\n2  See FRA (2019), Facial recognition technology: fundamental rights considerations in the context of law enforcement, Luxembourg,\n   Publications Office, p. 9 and 22.\n3  FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications\n   Office, June 2019.\n4  UN, Human Rights Council (2019), Report of the Special rapporteur on extreme poverty and human rights, Philip Alston, A/74/48037.\n5  Eubanks, V. (2018), Automating Inequality. How hightech tools profile, police, and punish the poor, St. Martin’s Press.\n6  Redden, Joanna, Dencik, Lina and Warne, Harry (2020), Datafied child welfare services: unpacking politics, economics and power, Policy\n   Studies.\n7  Panoptykon Foundation (2015), Profiling the unemployed in poland: Social and political implications of algorithmic decision making; see\n   also Algorithm Watch (2019), Poland: Government to scrap controversial unemployment scoring system.\n8  OECD, Glossary of Statistical Terms - Social Benefits Definition, accessed 5 August 2020.\n9  J. Henry Richardson, CHAPTER IV, SOCIAL INSURANCE, Economic and Financial Aspects of Social Security (University of Toronto Press, 1960).\n   Pieters, Social Security.\n10 For an overview of the EU competence in this domain and Regulation (EC) No. 883/2004, see Paju, J. (2017), The European Union and\n   Social Security Law, Oxford, Hart Publishing, Ch. 2.\n11 Erik Bakke (2018), ‘Predictive policing: The argument for public transparency’, New York University Annual Survey of American Law, Vol.\n   74, pp. 139-140; Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, Washington University Law Review, Vol. 94, pp. 1146-1150. For\n   example, only one in five women who experienced violence brought the most serious incident to the attention of the police. See FRA\n   (2014), Violence against women: an EU-wide survey. Main results report, Luxembourg, Publications Office, p. 61.\n12 Elizabeth E. Joh (2015), The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing, UC Davis Legal Studies Research\n   Paper No. 473, p.18.\n13 Braga A., et al (2019), Hot spots policing of small geographic areas effects on crime, Campbell Systematic Reviews, Vol. 15 (3).\n14 Elizabeth E. Joh (2015). The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing, UC Davis Legal Studies Research\n   Paper No. 473, pp. 17-18. Available at: SSRN.\n15 Erik Bakke (2018), ‘Predictive policing: The argument for public transparency’, New York University Annual Survey of American Law, Vol.\n   74, pp. 137-138.\n16 Wim Hardyns and Anneleen Rummens (2017), ‘Predictive Policing as a New Tool for Law Enforcement? Recent Developments and\n   Challenges’, Eur J Crim Policy Res, p. 3, DOI: 10.1007/s10610-017-9361-2.\n17 Albert Meijer & Martijn Wessels (2019), ‘Predictive Policing: Review of Benefits and Drawbacks’, International Journal of Public\n   Administration 42:12, p. 1032, DOI: 10.1080/01900692.2019.1575664.\n18 The Law Society Commission on the Use of Algorithm in the Justice System (2019), Algorithms in the criminal justice system, p. 36.\n19 Newbold, J. (N.D.), ‘Predictive Policing’, ‘Preventative Policing’ or ‘Intelligence Led Policing’. What is the future?\n20 Declaration No. 21 Annexed to the Final Act of the Intergovernmental Conference which adopted the Treaty of Lisbon, signed on 13\n   December 2007.\n21 Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard\n   to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution\n   of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework\n   Decision 2008/977/JHA, OJ L 119, 4.5.2016, pp. 89-131.\n22 European Commission (2019), Standard Eurobarometer 92, Report, Europeans and Artificial Intelligence, p. 10.\n23 European Patients Forum (n.d.), The new EU Regulation on the protection of personal data: what does it mean for patients? A guide for\n   patients and patients’ organisations.\n24 Commission Recommendation (EU) 2019/243 of 6 February 2019 on a European Electronic Health Record exchange format, OJ L 39,\n   11.2.2019, pp. 18-27.\n25 Digital Health Society, Exchange of electronic health records across the EU, 19 February 2020.\n26 Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the\n   Committee of the Regions, A European Strategy for data, COM(2020)66 final, Brussels, 19 February 2020.\n27 Decision No. 1082/2013/EU of the European Parliament and of the Council of 22 October 2013 on serious cross-border threats to health\n   and repealing Decision No. 2119/98/EC, OJ L 293, 5.11.2013, pp. 1-15.\n28 See Commission webpage on Communicable diseases.\n29 OECD and European Union (2018), Healthcare at a glance: Europe 2018, p. 192.\n30 Vera Ehrenstein, Hadi Kharrazi, Harold Lehmann and Casey Overby Taylor (2019), ‘Obtaining Data From Electronic Health Records’, in:\n   Gliklich RE, Leavy MB, Dreyer NA (eds.), Tools and Technologies for Registry Interoperability, Registries for Evaluating Patient Outcomes:\n   A User’s Guide, 3rd ed., Addendum 2.\n31 On the use of these data in the insurance industry currently and their potential see, for example, A. Spender, C. Bullen, L. Altmann-Richer,\n   J. Cripps, R. Duffy, C. Falkous, M. Farrell, T. Horn, J. Wigzell and W. Yeap (2019), ‘Wearables and the internet of things: considerations for\n   the life and health insurance industry’, British Actuarial Journal 24:22, pp. 1-31.\n32 See WHO visualisation.\n33 See a short overview of different EHR systems in Europe from nurses’ perspective in HealthEurope (2019), The world of cloud-based\n   services: storing health data in the cloud.\n34 College of Europe (2018), Transformation Health and Care in the Digital Single Market. Synopsis report of the public consultation.\n35 European Commission (2016), Study on Big Data in public health, telemedine and healthcare; Roberta Pastorino, Corrado De Vito,\n   Giuseppe Migliara, Katrin Glocker, Ilona Binenbaum, Walter Ricciardi, Stefania Boccia (2019), Benefits and challenges of Big Data in\n   healthcare: an overview of the European initiatives, European Journal of Public Health, Vol. 29, Issue Supplement 3, pp. 23–27.\n36 Ministry of Health, Welfare and Sport of The Netherlands (2016), Digitalization in health care and benefits for patient safety: Literature\n   and web reports (2015-2016).\n37 This is according to multiple reports by different cybersecurity companies and over time. See, for example, SC Magazine (2019),\n   Healthcare leads in cost of data breaches; Shannon Williams (2020), New report reveals ‘wall of shame’ in health care data breaches;\n   Tammy Lovell (2019), Statistics reveal healthcare is the sector most affected by personal data breaches.\n                                                                                                                                                  45\n   38 SC Magazine (2019), Healthcare leads in cost of data breaches.\n   39 Annet Sollie (2016), Reuse and Sharing of Electronic Health Record Data with a focus on Primary Care and Disease Coding, Doctoral\n      dissertation at the Vrije Univesiteit Amsterdam, pp. 28-30.\n   40 Vera Ehrenstein, Hadi Kharrazi, Harold Lehmann and Casey Overby Taylor (2019), ‘Obtaining Data From Electronic Health Records’, in:\n      Gliklich RE, Leavy MB, Dreyer NA (eds.), Tools and Technologies for Registry Interoperability, Registries for Evaluating Patient Outcomes:\n      A User’s Guide, 3rd ed., Addendum 2.\n   41 Mowafa Househ, Bakheet Aldosari, Abdullah Alanazi Show, Andre Kushniruk and Elizabeth M Borycki (2017), ‘Big Data, Big Problems: A\n      Healthcare Perspective’, Studies in health technology and informatics 238, p. 38.\n   42 Sartor, Giovanni (2020), New aspects and challenges in consumer protection, study for the committee on the Internal Market and\n      Consumer Protection, Policy Department for Economic, Scientific and Quality of Life Policies, European Parliament, Luxembourg.\n   43 Neudert, Lisa and Marchal Nahema (2019), Polarisation and the use of technology in in political campaigns and communication, study\n      at the request of the Panel for the Future of Science and Technology (STOA) and managed by the Scientific Foresight Unit, within the\n      Directorate-General for Parliamentary Research Services (EPRS) of the Secretariat of the European Parliament; Information Commissioner’s\n      Office (ICO) (2018), Investigation into the use of data analytics in political campaigns.\n   44 Council of Europe (2019), Declaration by the Committee of Ministers on the manipulative capabilities of algorithmic processes,\n      Decl(13/02/2019)1.\n   45 For example, Costello, Róisín Áine (2020), The Impacts of AdTech on Privacy Rights and the Rule of Law, Technology and Regulation,\n      11–23; EDPS (2018), Opinion 3/2018, EDPS Opinion on online manipulation and personal data.\n   46 Sartor, Giovanni (2020), New aspects and challenges in consumer protection; Jabłonowska, Agnieszka et al. (2018), ‘Consumer law and\n      artificial intelligence. Challenges to the EU consumer law and policy stemming from the business’ use of artificial intelligence’ EUI\n      Working Papers, LAW 2018/11.\n   47 Wachter, Sandra (2020), ‘Affinity Profiling and Discrimination by Association in Online Behavioural Advertising’, Berkeley Technology Law\n      Journal, Vol. 35, No. 2, 2020, (forthcoming), available at SSRN.\n   48 Zuboff, Shoshana (2018), The Age of Surveillance Capitalism, London; EDPS (2018), Opinion 3/2018, EDPS Opinion on online manipulation\n      and personal data.\n   49 Martin, Gillian (2011), The importance of marketing segmentation, American Journal of Business Education, Vol. 4, No. 6.\n   50 Kaili Lambe and Becca Ricks (2020),The basics on microtargeting and political ads on Facebook.\n   51 See for example, information on the RecSys2020 Workshop on REVEAL 2020: Bandit and Reinforcement Learning from User Interactions\n      (accessed on 7 August 2020).\n   52 Directive 2006/114/EC of the European Parliament and of the Council of 12 December 2006 concerning misleading and comparative\n      advertising, OJ L 376, 27.12.2006, pp. 21-27.\n   53 European Commission, Misleading and comparative advertising directive: Objective of the directive.\n   54 Directive 2005/29/EC of the European Parliament and of the Council of 11 May 2005 concerning unfair business-to-consumer commercial\n      practices in the internal market and amending Council Directive 84/450/EEC, Directives 97/7/EC, 98/27/EC and 2002/65/EC of the\n      European Parliament and of the Council and Regulation (EC) No 2006/2004 of the European Parliament and of the Council (‘Unfair\n      Commercial Practices Directive’), OJ L 149, 11.6.2005, pp. 22-39.\n   55 Directive 2006/114/EC of the European Parliament and of the Council of 12 December 2006 concerning misleading and comparative\n      advertising, OJ L 376, 27.12.2006.\n   56 See the European Commission’s webpage on The Digital Services Act package.\n46\n3\nFUNDAMENTAL RIGHTS FRAMEWORK\nAPPLICABLE TO AI\n              The use of AI – as presented in the four use cases discussed in Chapter 2 – can\n              affect specific fundamental rights (as outlined in Chapter 4). Full compliance\n              with fundamental rights is a prerequisite for using AI-driven technologies,\n              irrespective of the area concerned.\n              This chapter introduces the general fundamental rights framework in the\n              EU that governs the use of AI, including selected secondary EU legislation\n              and national law (Section 3.1). This fundamental rights framework provides\n              the normative basis and benchmarks for the design, development and\n              deployment of AI tools.1 It helps determine whether or not a specific use of AI\n              is fundamental rights compliant. The requirements for justified interferences\n              with fundamental rights are outlined in Section 3.3.\n              3.1.\t FUNDAMENTAL RIGHTS FRAMEWORK GOVERNING\n                      THE USE OF AI\n              The cornerstone instrument of the EU fundamental rights framework applicable\n              to the use of AI is the Charter. Together with the unwritten general principles\n              of EU law, it is the main source of fundamental rights in the EU. The Charter\n              enshrines a wide array of fundamental rights and has the same legal value\n              as the EU Treaties. All EU institutions and bodies are bound by the Charter, as\n              are Member States when they act within the scope of EU law (Article 51 (1)\n              of the Charter).2\n                                                                                              47\n   Many Charter rights are the same as those set out in the European Convention\n   on Human Rights (ECHR).3 Their meaning and scope must be the same as\n   the corresponding ECHR rights (Article 52 (3) of the Charter). However, this\n   cannot prevent Union law from providing more extensive protection.\n   Fundamental rights can also be found in provisions of the Treaties (see e.g.\n   Article 6 (2) of the TEU and Titles V and X of the TFEU), and in EU secondary\n   law.4 These rights are further safeguarded in different pieces of secondary\n   EU law.\n   A central piece of EU secondary law in the context of AI is the General\n   Data Protection Regulation (GDPR – Regulation (EU) 2016/679).5 It governs\n   automated processing of personal data in the European Economic Area and\n   processing of personal data by any other means which form part of a filing\n   system – within the scope of EU law. (As a result, the GDPR does not apply\n   to national security-related data processing.)\n   The GDPR is coupled with the Law Enforcement Directive, which applies to\n   police and judicial cooperation in criminal matters. Both EU instruments include\n   numerous provisions on the protection of personal data, determining the key\n   principles of data processing, such as lawfulness, fairness and transparency.6\n   Whether EU data protection legislation applies depends on whether personal\n   data are processed. Some AI-driven applications do not use personal data\n   (for example, traffic data). Others use anonymised data. In these cases, data\n   protection laws do not apply, or their applicability is not entirely clear.7 The line\n   between personal and non-personal data is blurred, because there is some\n   risk that anonymised data can be ‘re-identified’ – ie, the anonymisation can\n   be undone. However, re-identification is usually illegal. In addition, persons\n   re-identifying the data usually have to put in major efforts and potentially need\n   access to additional information about individuals who might be included in\n   an anonymised dataset for re-identification. Section 4.2 discusses the topic in\n   more detail, linked to the results of the interviews carried out for this report.\n   In addition to the EU data protection acquis, European non-discrimination law\n   is key for safeguarding fundamental rights in the context of the use of AI and\n   related technologies. Article 2 of the TEU provides that non-discrimination is\n   one of the fundamental values of the EU, and Article 10 of the TFEU requires\n   the Union to combat discrimination on a number of grounds. Moreover,\n   Articles 20 and 21 of the Charter provide for equality before the law and\n   non-discrimination.\n   Beyond this, several EU non-discrimination directives enshrine more specific\n   and detailed provisions. They have varying scopes of application.8 These\n   include the Employment Equality Directive (2000/78/EC),9 the Racial Equality\n   Directive (2000/43/EC),10 the Gender Goods and Services Directive (2004/113/\n   EC),11 and the recast Gender Equality Directive (2006/54/EC).12\n   EU Member States are also party to other international human rights\n   conventions (see the list of conventions in the Key Findings and FRA opinions\n   section). These contain legally binding standards and safeguards to comply\n   with when they act in areas that do not fall within the scope of EU competence.\n   The main such instrument is the ECHR, ratified by all EU Member States. It is\n   accompanied by additional protocols, to which a great majority of EU Member\n   States are parties. The ECHR has a wide reach: it also applies to areas not\n   covered by EU law.\n   In addition, the Council of Europe Convention for the Protection of Individuals\n   with regard to Automatic Processing of Personal Data13 is another source of\n48\npan-European data protection obligations binding on all EU Member States.\nIt was recently modernised.14\nSector-specific EU and national legislation also enshrines safeguards for\nthe protection of fundamental rights. An overview of such more technical\nlegislation is beyond the scope of this report. However, this chapter provides\na few examples relevant to the use cases discussed in the report. This is\ncomplemented by a couple of examples of national laws from the five EU\nMember States covered.\nNone of the five EU Member States covered currently have horizontal AI-\nspecific laws, although the countries are looking into the potential need for\nregulation. Some EU countries, such as Finland, issued recommendations\nfor self-regulation and the development of responsibility standards for the\nprivate sector.15 In Estonia, an assessment concluded that a separate AI-\nspecific law will not be required in the foreseeable future, since the current\nlegal framework is sufficient.16 According to the relevant Estonian long-\nterm strategy, however, the legal environment must be adapted to avoid\nunnecessary hindrances to implementing AI.17\nThe situation concerning sectoral legislation relevant to the use of AI in different\nsectors varies across EU Member States. However, active policymaking on AI\nhas recently emerged at the national level. National action plans on AI have\nappeared and remain the core policy development in Member States. Some\ncountries are working on growing entrepreneurship.18 Others are focused on\nenacting market-oriented policies compatible with the UN 2030 Agenda for\nSustainable Development.19 Educational activities to promote AI and increasing\npublic use of AI are often identified as AI-related strategy goals. Investment\nin research and development is also frequently outlined as a relevant goal.20\nWhile domestic AI discussions on potential legislative reforms remain attentive\nto European initiatives, national, sector-specific fundamental rights safeguards\nare also being enacted. For instance, Finland began considering an overhaul of\ndomestic human rights safeguards in the public sector by proposing a broader,\nacross-the-board legislative update as opposed to individual AI laws.\nIn specific reference to the processing of personal data under immigration\nlaw, the Finnish Constitutional Law Committee has put forward a proposal to\nstrengthen the safeguards of the Finnish Constitution, overriding constitutional\nlaw shortcomings in relation to, among others, protection under the law,\naccountability, as well the ambiguity of algorithms in automated decision\nmaking. Whenever public authorities automate their decision-making\nprocesses, these processes must adhere to the constitutional principle of rule\nof law, and may not endanger the observance of rules on good administration\nand due process.21 This proposal articulated a vision on what requirements\nthe Finnish Constitution sets for AI use and automated decision making within\npublic administration.\nThe research identified other initiatives and policies linked to AI and\nfundamental rights in the five Member States examined. For example, the\nEstonian e-State charter includes a summary of citizens’ rights for better\ncommunicating with agencies electronically. It also targets AI in relation to\nthe right to know what data is collected by public authorities.22\nSimilarly, the Ministry of the Interior of the Netherlands presented a policy\nbrief to parliament on AI, public values and fundamental rights.23 The brief\nstresses a human-centric approach, where AI-applications have a strong\ninfluence on human beings or on society as a whole. It also lists the most\nimportant risks of AI for fundamental rights, such as discrimination as a result\n                                                                                     49\n   of biased data, or reduced interpersonal relations if AI takes over certain\n   forms of interaction.\n   3.2.\t ‘USE CASE’ EXAMPLES\n   Social welfare (Use case 1)\n   When regulating social welfare, EU Member States enacted rules aiming to\n   protect fundamental rights specifically in this area in addition to existing\n   horizontal EU regulations (see Section 2.1). These mostly define rules for\n   the processing and protection of personal data for the purpose of social\n   benefits and insurance.\n   In Estonia, for example, the Insurance Activities Act, applicable to all types\n   and forms of insurance, regulates the processing and transmission of personal\n   data in this context. It states that public authorities, health care providers,\n   insurance undertakings and other third parties may transmit personal data\n   at the request of an insurance undertaking if the personal health or court\n   data are necessary for the insurance undertaking to perform an insurance\n   contract or if the right and obligation to disclose such data derives from law.\n   The scope of this Act also includes data transfers for the purpose of data\n   processing within AI systems.\n   The Social Welfare Act contains more specific provisions on data protection\n   of persons in need of social assistance. They have to be notified of the\n   processing of their data and should provide consent for further processing.\n   Any person in the established target group has the right to opt out of data\n   processing. The Social Welfare Act also allows local authorities to process\n   (including using algorithms) personal data of youth between 16 and 26 years\n   of age stored in state registries to identify the youth not in employment,\n   education or training.\n   In Finland, Act No. 552/2019 on Secondary Use of Health and Social Data\n   applies to using AI in social care and healthcare. This Act is based on the norms\n   for securing and protecting sensitive personal data as outlined in the GDPR. It\n   aims to establish conditions for the effective and secure “processing of, and\n   access to, personal health and social data for certain secondary purposes,\n   such as research and statistics, innovation and development, knowledge\n   management, teaching and authority planning.”24 The Act regulates the\n   manner in which registered health data can and cannot be processed.\n   Several other laws apply to various types of social benefits. In France, the\n   2015 Code of relations between the public and the administration applies\n   for the purpose of processing or accessing personal data related to social\n   benefits with minor amendments after the entry into force of the GDPR.\n   This code states “that algorithms used by public administrations must be\n   published” and “the person subject to automated decision making has a right\n   to be informed”.25\n   Predictive policing (Use case 2)\n   In the context of predictive policing, the EU’s Law Enforcement Directive\n   contains key fundamental rights safeguards. These stipulate how law\n   enforcement authorities should apply some of the main data protection\n   principles set out in the GDPR.26 These include the requirement for data\n   controllers (i.e. the competent law enforcement authorities) to provide data\n   subjects with information on the controller’s data processing activities, such\n   as the identity and contact details of the data controller, the purposes of the\n   processing and information about the right to lodge a complaint (Article 13).\n50\nIn specific cases, data controllers shall provide further information – for\nexample, the legal basis for processing – to enable data subjects to exercise\ntheir rights. The right of access (Article 14) requires the data controller to\nconfirm, upon request of the data subject, whether there are processing\noperations related to them. If this is the case, the data subject shall be able\nto access this data and also to request additional information, including the\npurposes and legal basis of the processing and the categories of personal\ndata processed. Both the right to information and the right to access can be\nrestricted in a number of cases, including to avoid obstructing or prejudicing\nthe prevention, detection, investigation or prosecution of criminal offences;\nor to protect public security and national security.27\nIn addition, Article 11 of the Law Enforcement Directive explicitly prohibits\nautomated decision making.28 This prohibition is limited if authorised by\nEU or national law which safeguards the data subject’s rights, including “at\nleast the right to obtain human intervention on the part of the controller”\n(for more, see Section 4.2).\nIn some cases, the scope of implementing national legislation is broader than\nthe directive. For example, the Finnish Act on the Processing of Personal Data\nin Criminal Matters and in Connection with Maintaining National Security\nstrengthens the right to information by not distinguishing between the\ninformation provided in general and in special circumstances.29\nHealthcare (Use case 3)\nAs regards EU-level fundamental rights safeguards when using AI in healthcare,\nthe GDPR empowers patients with rights to be informed, in part by granting\nthem more control of their personal health data. Such data qualifies as\n‘sensitive data’30, as found, for example, in their medical records.31 The rights\ninclude the rights to access one’s own personal (health) data; to object to\nthe processing of own personal data; rectification and erasure of data, as\nwell as rights in case of breach..32\nUnder the GDPR, administrative fines for breaches of processing data, including\nhealth data, are not allowed. However, in Estonia, for instance, domestic\nlaw allows for a maximum penalty of EUR 400,000 in application of the\nmisdemeanour procedure in such cases. The Data Protection Inspectorate\ncan also impose similar fines in the misdemeanour procedure.33\nIn France, the Data Protection Act and the Public Health Code impose stricter\nrequirements than those set out in the GDPR regarding health data processing.\nThe French Data Protection Act has been amended through the Law for the\nModernisation of the Health System, to allow for the processing of personal\nhealth data for various purposes, provided they fall within the scope of one\nof the exceptions to the general principle of prohibition of sensitive data\nprocessing under Article 9 of the GDPR.34\nTargeted advertising (Use case 4)\nWhen considering fundamental rights safeguards in relation to targeted\nadvertising and the underlying mechanisms regarding profiling in particular,\nthe EU legal framework on privacy and data protection provides the most\nrelevant fundamental rights provisions. The protection of privacy and personal\ndata holds a status that takes precedence over economic benefits. Hence,\nrules on processing of (special categories of) personal data are relevant for\ncompanies operating in the area of or applying targeted advertising in that\nthey place companies under certain obligations.\n                                                                                  51\n   The main legal provisions setting out rules on protecting personal data in the\n   EU are the GDPR and the Directive on privacy and electronic communications\n   (e-Privacy Directive), which is a lex specialis to the GDPR. The GDPR is directly\n   applicable in all EU Member States whenever a company is based in the EU\n   and processes personal data, and if a company is based outside of the EU,\n   but processes data relating to individuals in the EU.\n   The e-Privacy Directive, with a strong focus on fundamental rights, concerns\n   the processing of personal data and the protection of privacy in the electronic\n   communications sector (e.g. when individuals use their computer, smartphone\n   or tablet). In 2017, the European Commission proposed an e-Privacy Regulation,\n   which would replace the current e-Privacy Directive.35 The legislative proposal\n   would broaden the scope of the directive, and include specific provisions\n   concerning unsolicited marketing, cookies and confidentiality.\n   3.3.\t REQUIREMENTS FOR JUSTIFIED INTERFERENCES\n            WITH FUNDAMENTAL RIGHTS\n   Chapter 4 highlights selected fundamental rights – as covered by the Charter –\n   that are particularly affected by AI, taking into account the four use cases\n   discussed in Chapter 2. Most of these rights are not absolute rights, so can\n   be subject to limitations in line with Article 52 (1) of the Charter. Accordingly,\n   before analysing to what extent the different fundamental rights are impacted\n   by the use of AI, this section presents the general steps that need to be\n   followed to determine whether or not a Charter right can be limited.\n   Fundamental rights affected by AI that are not absolute can be subject to\n   limitations. Interferences with such fundamental rights can only be justified\n   if they respect the requirements of the Charter and of the ECHR, in case of\n   Charter rights corresponding to rights guaranteed in the ECHR (Article 52 (3)\n   of the Charter).36\n   Pursuant to Article 52 (1) of the Charter, any limitation on fundamental\n   rights must:\n   ――be provided for by law,\n   ――genuinely meet objectives of general interest recognised by the Union\n        or the need to protect the rights and freedoms of others,\n   ――respect the essence of the right,\n   ――be necessary, and\n   ――be proportionate.37\n   The Court of Justice of the EU (CJEU) has also emphasised that any limitation\n   on the exercise of the rights and freedoms recognised by in the Charter\n   must respect “the essence” of those rights and freedoms.38 This means that\n   fundamental rights can be limited to a certain extent, but not completely\n   disregarded.\n   Once it has been established that the inalienable, essential core of a right\n   is not violated by a measure, the next step is to conduct the necessity and\n   proportionality test outlined in the Charter in respect of non-core aspects of\n   that right.39 Any interference with a Charter right needs to be examined as\n   to whether the given legitimate aim could not be obtained by other means\n   that interfere less with the right guaranteed.40 Similar requirements are\n   also imposed by the ECHR, as interpreted by the European Court of Human\n   Rights (ECtHR).41 These include the ‘essence of a right’ concept, which can be\n   derived from the object and purpose of the ECHR as a whole.42 In respect to\n   the use of new technologies, the ECtHR observed in S. and Marper v. the UK\n   that States should “strike a right balance” between protecting fundamental\n   rights and developing new technologies.43\n52\nGiven the wide range of applications of AI systems in everyday life as\npresented in the four selected use cases, a wide range fundamental rights may\nhave to be assessed, taking into account a variety of elements, depending on\nthe context and the particular area of use. Most notably, the specific purpose\nfor which AI is used, its functionality, complexity, and the scale at which it\nis deployed, are relevant for assessing fundamental rights implications.44\n                                                                               53\n   Endnotes\n   1  See also van Veen, C. (2018), ‘Artificial Intelligence; What’s Human Rights Got to Do with It?’ Data & Society: Points – blog of Data\n      & Society Research Institute, 14 May 2018; Barfield, W. & Pagallo, U. (2020), Advanced Introduction to Law and Artificial Intelligence,\n      Cheltenham/Northhampton, MA, Edward Elgar, 2020, pp. 19-20.\n   2  See also CJEU, Åklagaren v. Hans Åkerberg Fransson [GC], 26 February 2013, paras. 17, 20.\n   3  European Convention for the Protection of Human Rights and Fundamental Freedoms, as amended by Protocols Nos. 11 and 14, 4\n      November 1950, ETS 5.\n   4  For an overview of the application of the Charter, see FRA (2018a), Applying the Charter of Fundamental Rights of the European Union in\n      law and policy making at national level, Luxembourg, Publications Office.\n   5  Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard\n      to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection\n      Regulation), OJ L 119, 4.5.2016, pp. 1-88.\n   6  For more, see FRA (2018), Handbook on European Data Protection Law. 2018 Edition, Luxembourg, Publications Office.\n   7  See for example, Hacker, P. (2020), A Legal Framework for AI Training Data. Law, Innovation and Technology (forthcoming), available at\n      SSRN.\n   8  For an overview of European non-discrimination law, see FRA (2018), Handbook on European non-discrimination law. 2018 Edition,\n      Luxembourg, Publications Office.\n   9  Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation,\n      OJ L 303, 2.12.2000, pp. 16-22.\n   10 Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or\n      ethnic origin, OJ L 180, 19.7.2000, pp. 22-26.\n   11 Council Directive 2004/113/EC of 13 December 2004 implementing the principle of equal treatment between men and women in the\n      access to and supply of goods and services, OJ L 373, 21.12.2004, pp. 37-43.\n   12 Directive 2006/54/EC of the European Parliament and of the Council of 5 July 2006 on the implementation of the principle of equal\n      opportunities and equal treatment of men and women in matters of employment and occupation (recast), OJ L 204, 26.7.2006, pp. 23-36.\n   13 Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data, Strasbourg, 28 January 1981 (ETS No.\n      108).\n   14 Protocol amending the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data, Strasbourg, 10\n      October 2018 (CETS No. 223).\n   15 The AI Finland Project’s ethics working group and the Ethics Challenge added emphasis on companies and self-regulation. AI Finland,\n      ‘Etiikkahaaste (Ethics Challenge)’, Tekoäly on uusi sähkö (in Finnish).\n   16 Republic of Estonia (2019), Report of Estonia’s AI Taskforce, p. 38.\n   17 The Estonian Government launched the preparation for a long-term strategy.\n   18 For example, see the Netherlands, Ministry of Economic Affairs and Climate Policy (2019), Strategic Action on AI Strategic Action Plan AI\n      (Strategisch Actieplan AI – SAPAI).\n   19 For an example of an effort to adapt goals to the development of a sustainable market, see Spain, Ministry of Science, Innovation and\n      Universities (2019), National AI Strategy (in Spanish).\n   20 For a more comprehensive overview, see European Commission (2019), National strategies on Artificial Intelligence; or the OECD AI\n      policy observatory.\n   21 Finnish Constitutional Law Committee (2019), ‘Committee Opinion PeVL 7/2019 Vp ─ HE 18/2019 vp: Draft Proposal to Parliament for the\n      Law on the Processing of Personal Data in the Immigration Administration and for Related Laws’.\n   22 Estonia, National Audit Office and Chancellor of Justice (2018), Everyone’s Rights in e-State: The e-State Charter.\n   23 Netherlands, Ministry of the Interior and Kingdom Relations (2019) AI, public values ​​and fundamental rights (in Dutch).\n   24 Elina Saxlin-Hautamäki and Johanna Lilja (2019), Secondary use of health data – the new Finnish Act.\n   25 de Donno, M. (2017), The French Code “Des Relations Entre Le Public Et L’Administration”. A New European Era For Administrative\n      Procedure?, Italian Journal of Public Law 2, pp. 220-260.\n   26 See FRA (2018), Preventing unlawful profiling today and in the future: a guide, Luxembourg, Publications Office, Tables 2 and 4.\n   27 Sajfert, J. and Quintel, T. (2017), Data Protection Directive (EU) 2016/680 for Police and Criminal Justice Authorities, available at SSRN.\n   28 Note that Article 11 of the Law Enforcement Directive seems to apply to automated decisions taken solely through automated processing.\n      This means that this safeguard will not apply if human agency is involved. Orla Lynskey (2019), Criminal justice profiling and EU data\n      protection law: Precarious protection from predictive policing, p. 21.\n   29 The English translation is available via the Finlex website.\n   30 GDPR, recital (10) and Art. 9 (1).\n   31 European Patients Forum (n.d.), The new EU Regulation on the protection of personal data: what does it mean for patients? A guide for\n      patients and patients’ organisations.\n   32 GDPR, Arts. 15-17, 20-21 and 34.\n   33 White&Case (2019), GDPR Guide to National Implementation: Estonia.\n   34 Merav Griguer (2019), Processing health data in France: What to look out for after GDPR?\n   35 European Commission, Proposal for a regulation of the European Parliament and of the Council concerning the respect for private life\n      and the protection of personal data in electronic communications and repealing Directive 2002/58/EC (Regulation on Privacy and\n      Electronic Communications), COM(2017) 10 final, Brussels, 10.1.2017.\n   36 Charter, Art. 52 (3): “In so far as this Charter contains rights which correspond to rights guaranteed by the Convention for the Protection\n      of Human Rights and Fundamental Freedoms, the meaning and scope of those rights shall be the same as those laid down by the said\n      Convention.”\n   37 As also reiterated and explained by the CJEU. See, for example, C-73/07, Satakunnan Markkinapörssi and Satamedia, 16 December 2008,\n      para. 56; Joined cases C-92/09 and C-93/09, Volker und Markus Schecke and Eifert GbR and Hartmut Eifert, 9 November 2010, para.\n      77; Joined cases C-293/12 and C-594/12, Digital Rights Ireland Ltd v. Minister for Communications, Marine and Natural Resources and\n      Others and Kärntner Landesregierung and Others, 8 April 2014, para. 52; C-362/14, Maximillian Schrems v. Data Protection Commissioner,\n      6 October 2015, para. 92; and C-419/14, WebMindLicenses Kft. v. Nemzeti Adó-es Vámhivatal Kiemelt Adó- és Vám Főigazgatóság,\n      17 December 2015, paras. 69 and 80-82.\n   38 See CJEU, C-362/14, Maximillian Schrems v. Data Protection Commissioner, 6 October 2015, paras. 94-95, which refer to Article 52 (3) of\n      the Charter. See also Scheinin, Martin and Sorell, Tom (2015), SURVEILLE Deliverable D4.10 – Synthesis report from WP4, merging the\n54\n   ethics and law analysis and discussing their outcomes, 7 April 2015, p. 9.\n39 See e.g. Brkan, M. (2019), ‘The Essence of the Fundamental Rights to Privacy and Data Protection: Finding the Way Through the Maze\n   of the CJEU’s Constitutional Reasoning’, German Law Journal 20 (2019), p. 867; Lenaerts, K. (2019), ‘Limits on Limitations: The Essence of\n   Fundamental Rights in the EU’, German Law Journal 20 (2019), pp. 779-794.\n40 CJEU, Joined cases C-293/12 and C-594/12, Digital Rights Ireland Ltd v. Minister for Communications, Marine and Natural Resources and\n   Others and Kärntner Landesregierung and Others, 8 April 2014.\n41 See, for instance, Khelili v. Switzerland, No. 16188/07, 18 October 2011; ECtHR, S. and Marper v. the United Kingdom [GC], Nos. 30562/04\n   and 30566/04, 4 December 2008; ECtHR, K & T v. Finland, No. 25702/94, 12 July 2001; ECtHR, Z v. Finland, No. 22009/93, 25 February\n   1997; ECtHR, Huvig v. France, No. 11105/84, 24 April 1990; ECtHR, Leander v. Sweden, No. 9248/81, 26 March 1987.\n42 Scheinin, Martin and Sorell, Tom (2015), SURVEILLE Deliverable D4.10 – Synthesis report from WP4, merging the ethics and law analysis\n   and discussing their outcomes, 7 April 2015, p. 9.\n43 ECtHR, S. and Marper v. the United Kingdom [GC], Nos. 30562/04 and 30566/04, 4 December 2008, para. 112.\n44 See also Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights\n   impacts of algorithmic systems, Appendix, para. A.8.\n                                                                                                                                              55\n564\nIMPACT OF CURRENT USE OF AI ON\nSELECTED FUNDAMENTAL RIGHTS\n             Deploying AI systems engages a wide range of fundamental rights. As\n             seen in Chapter 2, the use cases presented in this report involve a range\n             of technologies of varying levels of complexity and automation. They are\n             in different phases of development and applied in different contexts, for\n             different purposes and at different scale. While the rights affected depend\n             on these factors, a number of horizontal and sector-specific fundamental\n             issues emerge.\n             The chapter begins with a general overview of risks perceived by interviewees,\n             and their general awareness of fundamental rights implications when using\n             AI. The chapter then highlights selected fundamental rights affected by AI-\n             related technologies, with reference to the four use cases analysed.\n             The analysis takes into account and presents the views, practices and\n             awareness of these issues expressed in the interviews conducted for this\n             report. Interviewees were first asked about the general risks they see when\n             using AI. They were then asked about general fundamental rights awareness\n             when using AI and about more concrete fundamental rights implications, which\n             were mostly linked to data protection, non-discrimination and availability of\n             complaints mechanisms.\n             4.1.\t PERCEIVED RISKS\n             It is important to recognise that many issues cut across different rights. For\n             example, a potentially biased decision made by an algorithm could involve\n             the right to non-discrimination, protection of personal data, and the right to an\n             effective remedy. Similarly, a particular issue can be seen from the perspective\n             of different rights. For instance, a good explanation of a decision made by\n             an algorithm is required under the right to protection of personal data, right\n             to good administration, and the right to an effective remedy and a fair trial.\n             When asked about general risks when using AI, the interviewees did not always\n             mention fundamental rights as the main risks, although some highlighted\n             related topics. Private sector representatives most often mentioned inaccuracy\n             as a risk of using AI, followed by potential bias and the proper legal basis\n             for processing personal data. One respondent from an international retail\n             company stated that one business risk is linked to European customers being\n             extremely knowledgeable about their rights; namely, people do not hesitate to\n             ask about data storage and automated decision making. If customers are not\n             properly informed, they might complain and the company may lose a client.\n             In addition, the interviewee continued, breaching the law, and possible fines\n             linked to a breach, is another major business risk.\n             With respect to public administration, bias was most often highlighted as\n             a risk associated with using AI. In addition, public authorities often discussed\n                                                                                               57\n   inaccuracy and data re-identification as risks of using AI. For example,\n   interviewees working on social benefits algorithms stated that incorrect\n   results in general are a risk. This can occur potentially due to rare cases, which\n   are not well identified by the algorithm, or due to errors in the input data.\n   They also highlighted the difficulties associated with moving from testing to\n   deploying a system, including technical challenges, resources required and\n   potential different results when deployed.\n   Respondents working on targeted advertising also highlighted business\n   risks – for example, when offering irrelevant or inappropriate content. One\n   respondent mentioned potentially losing control over automated systems.\n   In addition, interviewees indicate challenges linked to the difficulty of\n   interpreting results and outputs from AI systems. One interviewee from\n   the consultancy sector fears that the risk related to the lack or absence of\n   sufficient AI knowledge and understanding can cause ongoing projects to\n   be halted, due to a company’s inability to explain clearly what an algorithms\n   will perform, and for what purpose.\n   Another interviewee from the law enforcement sector, looking into the\n   possible use of AI to support decisions about licence applications, explains that\n   there are inherent risks on how and why such a system proposes a certain\n   response. For example, when potentially using AI to support decisions about\n   license applications for firearms, the respondent asserts that it would not\n   only be critical to understand the reasoning behind negative decisions, but\n   also positive decisions. Several interviews showed that a major concern is\n   to assign properly trained staff with sufficient expertise to trace, explain and\n   interact with the AI system.\n   This finding is also corroborated by the results of the European Commission\n   survey among companies in the EU. In that survey, 85 % indicate as an\n   obstacle to adopting AI technologies the difficulty to hire new staff with\n   the right skills; 80 % mention the complexity of algorithms as an obstacle.1\n   With respect to the ability to explain decisions based on algorithms, an\n   interviewee working in public administration mentioned that there are no\n   alternatives to being completely transparent when making decisions. There\n   should not be any room for doubt. In a similar vein, a respondent working\n   in the area of health for the private sector mentions that ‘self-learning’\n   algorithms are forbidden in their area of work, because only fixed algorithms\n   can be traced.                                                                     “The use of AI can bring many\n                                                                                      benefits, but also risks, it is like\n   Other risks reported without providing much additional information include         nuclear energy.”\n   cyber-security, data quality, excessive monitoring of people due to the use        (Interviewee working in private sector,\n   of data and algorithms, job loss due to automation, and profiling.                 Spain)\n   4.2.\t GENERAL AWARENESS OF FUNDAMENTAL RIGHTS\n            AND LEGAL FRAMEWORKS IN THE AI CONTEXT\n   Not everyone in the EU is aware about their fundamental rights. FRA’s\n   Fundamental Rights Survey shows that slightly more than every second\n   person in the EU (aged 16 or older) has heard about the Charter. Slightly\n   more people, two out of three, have heard about the ECHR and the Universal         “[Our use of AI] does not impact\n   Declaration of Human Rights. This might be because the ECHR is older and           [human rights] in any way. In terms\n   more established in people’s common knowledge.2                                    of the decision process, it does not\n                                                                                      matter whether the decision is made\n   The majority of people interviewed for this project acknowledge that using         by machine or a human.”\n   AI can generally affect fundamental rights. Only very few mention that             (Interviewee working for public\n   their use of AI does not have a potential impact on fundamental rights or          administration, Estonia)\n58\n                                     that they were not aware of any such implications. Their responses are\n                                     influenced by the different ways they use AI, but also their understanding\n                                     of what fundamental rights are.\n                                     For example, one respondent working on the production of pension forecasts\n                                     based on machine learning says that producing statistics does not have an\n                                     impact on fundamental rights, apart from data protection issues, which need\n                                     to be addressed. Another respondent working on social benefits algorithms\n                                     argues that the impact depends on “how widely human rights are defined” –\n                                     for example, the right to receive a correct pension.\n“Once all the rights related to data\n                                     None of the interviewees working on targeted advertising believe that their\nprotection are ensured, I do not see\n                                     use of AI affects fundamental rights negatively. One respondent working\nhow human rights are of relevance\n                                     on targeted communication with customers stated that one reason for such\nhere.”\n                                     a response relates to a lack of knowledge about what exactly fundamental\n(Private company, Spain)\n                                     rights are.\n                                     Practically all interviewees showed awareness about the rights to privacy\n“We did not touch the topic because\n                                     and data protection as well as to non-discrimination. Other rights, such as\nwe assume that there are no\n                                     human dignity, the right to a fair trial and to effective remedy were also\nhuman rights issues involved: all\n                                     mentioned, albeit very briefly.\nthe activities are within the legal\nframework, all the activities are\n                                     A closer look at interviewees’ responses indicates diverging views across\ncompliant with data protection and\n                                     respondent groups. Most respondents working for private companies discuss\ngood practices, and therefore we\n                                     data protection and non-discrimination, but rarely mention other rights\nassume that there are no human\n                                     challenges. A company working on targeted advertising mentions that they\nrights issues related to these\n                                     are attentive to issues linked to freedom of speech and the right to information\nsystems.”\n                                     in the sense that their company promotes these rights. This is because\n(Public administration, Spain)\n                                     posting adverts helps news and other websites obtain funding to continue\n                                     their work, one interviewee notes.\n                                     The range of rights awareness is much broader among public sector\n                                     representatives working on AI, who referred to other rights such as human\n                                     dignity and the presumption of innocence.\n                                     Those working on AI systems in different fields of application also highlight that\n                                     the use of the systems is also covered by sector-specific laws. For example,\n                                     the system making decisions about unemployment benefits is regulated by\n                                     national legislation on unemployment insurance, on administrative procedures\n                                     and on data protection. However, some respondents are not aware of any\n                                     legal standards that apply to their use of AI or are unsure about it.\n                                     In the absence of AI-specific regulation, several respondents mention ethics\n                                     guidelines and certification schemes. Some work with existing guidelines\n                                     and standards, not necessarily specifically aimed at AI. This is the case, for\n                                     example, with the IT security system ‘ISKE’ in Estonia3 or in the area of financial\n                                     services, with the Payment Card Industry Data Security Standard.4 Respondents\n                                     also refer to the standards developed by the International Organization for\n                                     Standardization (ISO), the Institute of Electrical and Electronics Engineers\n                                     (IEEE) or the European Committee for Standardization (CEN).\n                                     A respondent working on targeted advertising argues that certification is not\n                                     needed in their field because posting ads is not the same as issues linked\n                                     to the health sector or the work of banks. Several interviewees noted that\n“I do not think that we should\n                                     their organisations are developing (internal) guidelines.\nregulate specific technology like\nAI. It is sufficient to have general\n                                     Some respondents mention the guidelines developed at the EU and\nprinciples and technology-neutral\n                                     international level, such as the guidelines from the European Commission’s\nrules.”\n(Private sector, Estonia)\n                                                                                                                         59\n   High-Level Expert Group on AI, the OECD’s guidelines, or the UNESCO standards.     “Yes, there are codes, and yes, there\n   Some are aware of ongoing developments at EU and Council of Europe level.          are procedures, but both these codes\n                                                                                      and procedures are out of date,\n   Some refer to the need to update sector-specific regulations to be able to         because we are using something we\n   innovate on AI – for example, in the area of health. Yet one interviewee           created for the analog world in the\n   states that existing standards are sufficient and that AI does not need to be      digital world.”\n   regulated separately.                                                              (Private sector, Spain)\n   4.3.\t HUMAN DIGNITY\n   Using AI-driven technologies broadly implicates the duty to respect human\n   dignity, the foundation of all fundamental rights guaranteed by the Charter.5\n   Article 1 of the Charter states that human dignity is inviolable and that it must\n   be respected and protected at all times. The CJEU has confirmed in its case\n   law that the fundamental right to dignity is part of EU law.6\n   AI-driven processing of personal data must be carried out in a manner that\n   respects human dignity. This puts the human at the centre of all discussions and\n   actions related to AI. Rather than the technology, the ‘human being’ creating\n   and affected by the new technology needs to be the focus. Taking human\n   dignity as the starting point can help to ensure that the use of AI benefits\n   everyone – for example, by supporting ageing and access to healthcare in\n   a dignified manner.\n   The use of AI also risks infringing on other closely connected Charter rights,\n   such as the right to life (Article 2) and the right to integrity of the person\n   (Article 3). In this context, it is important to consider how to avoid the harmful\n   use of AI to prevent violations of these rights, for example when it comes\n   to the use of AI by people engaging in criminal activities or when AI is used\n   for weapons.7\n   Apart from such extreme cases, preserving dignity includes avoiding subjecting\n   people to AI without their knowledge and/or informed consent, which is\n   strongly linked to privacy and data protection. For example, when people’s\n   applications for social benefits are decided upon through the use of AI, people\n   need to be made aware (and consent to the use when automated decisions\n   are taken). To give another example, a certain proportion of the population\n   does not feel comfortable being subjected to biometric identification systems.\n   Hence, using it without allowing them to opt out could potentially violate\n   their dignity.8\n   Only very few respondents from public administration referred to the right\n   to dignity when discussing fundamental rights. One respondent, considering\n   the use of AI in prisons, mentions that in this particular context it first needs\n   to be assessed whether the risk of violating fundamental rights would be\n   too high, such as the right to human dignity. Other interviewees made only\n   general references to this right, without discussing it in relation to a concrete\n   use of AI.\n60\n4.4.\t RIGHT TO PRIVACY AND DATA PROTECTION –\n        SELECTED CHALLENGES\nThe right to respect for private life and the protection of personal data\n(Articles 7 and 8 of the Charter) are at the core of fundamental rights\ndiscussions around the use of AI. While closely related, the rights to respect\nfor private life and the protection of personal data are distinct, self-standing\nrights. They have been described as the “classic” right to the protection of\nprivacy and a more “modern” right, the right to data protection.9\nBoth strive to protect similar values, i.e. the autonomy and human dignity\nof individuals, by granting them a personal sphere in which they can freely\ndevelop their personalities, think and shape their opinions. They thus form\nan essential prerequisite for the exercise of other fundamental rights, such\nas the freedom of thought, conscience and religion (Article 10 of the Charter),\nfreedom of expression and information (Article 11 of the Charter), and freedom\nof assembly and of association (Article 12 of the Charter).10\nGiven that these two rights are not absolute rights, they can be subject to\nlimitations. However, any interference needs to be adequately justified11\nand cannot compromise the essential, inalienable core of that right,12 as\nexplained in Section 3.3.\n                                      The concept of “private life” or “privacy”\n                                      is complex and broad, and not susceptible\n                                      to an exhaustive definition. It covers the\n                                      physical and psychological integrity of\n                                      a person, and can, therefore, embrace\n                                      multiple aspects of the person’s physical\n                                      and social identity.13 There is also a zone of\n                                      interaction of a person with others, even in\n                                      a public context, which may fall within the\n                                      scope of “privacy”. In other contexts, the\n                                      ECtHR has used the concept of “reasonable\n                                      expectation of privacy” – referring to the\n                                      extent to which people can expect privacy\n                                      in public spaces without being subjected\n                                      to surveillance – as one of the factors,\n                                      albeit not necessarily a conclusive one,\n                                      to decide on a violation of the right to\n                                      respect for private life. Its relevance and\n                                      scope of application, however, appears\n                                      to be limited.14 Similarly, according to the\nUN Human Rights Committee, the mere fact that participants in assemblies\nare out in public does not mean that their privacy cannot be infringed. The\nsame applies to the monitoring of social media to glean information about\nparticipation in peaceful assemblies.15\nThe widespread use of AI-technologies may, as the technologies continue\nto develop, raise unchartered issues and novel concerns about the right to\nrespect for private life. AI-driven technologies may change the way we\nthink about privacy. Algorithmic tools can predict, and reveal information\nabout, people’s behaviour in unprecedented ways – without people even\nrealizing that they are giving away such information. Personal data obtained\nfrom the internet may, for instance, then be used for targeted advertising,\nraising many fundamental rights concerns.16 Issues linked to personal data\nsharing via smart-phone apps particularly raises significant concerns, including\na variety of potential harmful effects, such as manipulation and exploitation\nof vulnerabilities, discrimination, security issues and fraud (e.g. identity theft)\nand reduced trust in the digital economy.17\n                                                                                     61\n   Using AI-driven technologies often implies computerised processing of large\n   amounts of personal data. This constitutes an interference with the right to\n   protection of personal data set out in Article 8 of the Charter (embodying\n   pre-existing EU data protection law), as well as the right to private life under\n   Article 7 of the Charter and Article 8 of the ECHR.\n   Awareness of data protection issues and use of personal data\n   In the EU, 69 % of people have heard about the GDPR.18 By contrast, virtually    “We were a little anxious when the\n   all interviewees are aware of the GDPR and discussed data protection issues.     GDPR was implemented, but in the\n   Data protection rules deriving from the GDPR and national law are clearly        end it meant managing datasets\n   the most well-known and applied rights in the area of AI. Other fundamental      and access rights […] It is a good\n   rights are less known.                                                           reminder that not everything can be\n                                                                                    or should be done.”\n   When discussing the legal framework governing the use of AI, most                (Public administration, Finland)\n   respondents only mentioned data protection rules, as well as some sectoral\n   laws. Some clearly say that there is no other legal framework apart from\n   the data protection laws. An interviewee working for a Spanish public            “Actually, I’m concerned that the\n   administration notes: “we rely on the data protection regulation and norms,      GDPR might hinder AI research. I’m\n   which is all that is available at the moment”.                                   afraid that some large databases\n                                                                                    that we have used previously cannot\n   One interviewee, reflecting on an image-based diagnostic tool, expressed         be used for our research anymore.”\n   the view that the GDPR could hinder research. The hospital using the tool        (Private company, Netherlands)\n   to support diagnosis after strokes had clear rules on data protection, the\n   interviewee indicated, although they did not know whether data protection\n   certification was requested.                                                     “There is the GDPR but it does not\n                                                                                    give you specific rules. It gives\n   Others referred to more general data protection guidelines or indicated that     principles but it comes down to\n   they were not aware of such documents.                                           ethical issues and interpretation.”\n                                                                                    (Private company, Estonia)\n   All respondents working in target advertising are aware of privacy and\n   data protection issues. Although not all are responsible for data protection\n   issues in their companies, they are all aware of efforts to protect the data\n   and privacy. One interviewee mentioned that, contrary to earlier years,\n   personal data are now stored much more securely and handled with more\n   care. Attention was given to properly handling consent for data processing.\n   As a consequence, there is a high level of awareness about data protection\n   and privacy issues linked to AI use.\n   However, data protection law only applies when personal data are processed.\n   For example, using anonymised data to develop AI tools (i.e. as training data)\n   is most likely permissible in many instances and would not trigger the GDPR.\n   Research shows that data can often be de-anonymised.19 However, such\n   efforts often require expert knowledge and potentially additional information,\n   and are illegal. While the illegality of de-anonymisation does not necessarily\n   preclude the applicability of the GDPR, it is more important to consider if re-\n   identification of anonymised data is reasonably likely.20 Anonymising data is\n   only one aspect of protecting the privacy of data subjects. When assessing\n   risks of re-identification, other aspects are also important to consider when\n   disseminating anonymised data. These include who will use the data, for\n   what purpose, and what outputs will be produced.21\n   In the interviews, respondents were not always entirely clear about their use\n   of personal data. They often only superficially described the data used, as\n   mentioned in Chapter 2. In several instances, interviewees indicated that they\n   use non-personal data or anonymised data, arguing that data protection was\n   not relevant in such cases. For example, a semi-public organisation working\n   on environmental management uses aggregated data on water consumption\n62\n                                      for machine learning-based predictions of water consumption. These data\n                                      are not available at individual level.\n                                      Other interviewees said they did not use personal data, although the data\n                                      originally stem from individuals. The tool supporting restaurant inspection\n                                      by collecting data from online sources does not use any personal data – the\n                                      interviewee indicated. However, they indicated the need to be careful when\n                                      mining data online, because, even if publicly available, it might include\n                                      personal data, such as usernames.\n“It would be great to retrieve some   In another example, an insurance company is using a chatbot to make client\ndata from another service so that     contact more effective. The data used to train the system are chat protocols\nthe client wouldn’t have to repeat    (conversation logs), which are not linked to any personal data. However, in\nit all, but where does the line go in this example, linking these data to personal data might be possible in the\nreusing data?”                        future, according to the respondent.\n(Public administration, Finland)\n                                      Companies working on targeted online advertising indicate using (pseudo-)\n                                      anonymised data. This is done, for example, after excluding names and\n                                      social security keys and encrypting data. The identity of the consumers is\n                                      not relevant to the company, an interviewee mentioned.\n                                      While some indicate that they use non-personal or anonymised data, for\n                                      others this is not possible because the data are used to make predictions\n                                      or decisions about specific individuals. For example, an interviewee from\n                                      a company working on credit rating mentioned they need to know the identity\n                                      of consumers for their assessments. In this case, this is even more important\n                                      than the right to be forgotten, according to an interviewee.\n                                      An exhaustive discussion of data protection issues is not possible in this report.\n                                      However, two aspects clearly emerged during the interviews: automated\n                                      decision making linked to the right to human review, and the right to obtain\n                                      meaningful information when decisions are automated.\n                                      Automated decision making\n                                      Article 22 of the GDPR and Article 11 of the Law Enforcement Directive\n                                      generally forbid automated decision making, meaning any “decision based\n                                      solely on automated processing, including profiling, which produces legal\n                                      effects concerning him or her or similarly significantly affects him or her.”\n                                                                         Under Article 22 of the GDPR, explicit\n                                                                         consent is needed when decisions are\n                                                                         solely automated and have a legal or\n                                                                         similarly significant effect on people and\n                                                                         if such automated decision making is\n                                                                         not authorised by law. The authorisation\n                                                                         by Union or national law is the sole\n                                                                         precondition under the Law Enforcement\n                                                                         Directive (Article 11) for such processing.\n                                                                         For a decision not to be considered fully\n                                                                         automated, both instruments require human\n                                                                         review by the controller.22\n                                                                                                                         63\n   However, the concept of ‘automated’ decision making is elusive and requires\n   further discussion and research. For example, in some cases, human\n   intervention might be limited to ‘signing-off’ on the outcomes of the AI\n   system, rendering it virtually automated.23 Importantly, human review must\n   not mean a human just signing off the recommendations or outputs from\n   an algorithm. It must be done by someone who has the “authority and\n   competence to change the decision”, considering all relevant data at hand.24\n   If humans review and potentially override outcomes of the system, this\n   must also be evaluated.\n   Research indicates that humans overrule outcomes from algorithms mainly\n   when the result from the algorithm is not in line with their stereotypes. 25\n   This behaviour threatens the possible added value of automated processing\n   in being potentially more accurate or even fairer than humans. It may also\n   put minority groups at a disadvantage, and is therefore also relevant for\n   non-discrimination issues (discussed below).\n   Overall, there is disagreement about the exact scope of these provisions of\n   the EU data protection acquis, and whether they impose a general ban on\n   certain types of automated decisions, or provide data subjects with some\n   rights in the context of certain types of AI-driven decision making.26\n   Using algorithms in the area of social benefits, health and predictive policing\n   clearly have potential legal or other significant consequences. The interviews\n   suggest that those working in these areas are well aware of the concept of\n   human review before decisions are taken with the support of AI.\n   Many interviewees indicate that no automated decisions are taken. One\n   exception is an automation of unemployment benefits, which is, based on\n   national law, fully automated for decisions that do not involve any discretion.\n   In another example, from another country, only positive decisions, based on\n   pre-defined rules, are automated for student benefits. In this case all negative\n   decisions are made by humans. Both cases refer to rule-based decisions, not\n   involving the use of statistics or machine learning.\n   Another respondent, testing the use of AI systems, including machine learning\n   in the area of social benefits, mentions that equality could be negatively\n   impacted. This is because automation makes human behaviour visible,\n   including existing biased practices. This makes precautions necessary and, as\n   a consequence, the organisation would only allow decisions made by humans.\n   Interviewees working in health highlighted risks linked to the automation\n   of decisions. An interviewee discussing the tool to support stroke diagnosis\n   feels it is important not to rely on the system to avoid the risk of automation\n   or confirmation bias. They caution that early positive experiences with\n   the application could prompt users to rely on it too easily and devote less\n   attention to their own assessment of the images. Other interviewees raised\n   similar concerns. One interviewee, discussing a tool that analyses images to\n   provide a probability for the presence of a certain type of lesion, notes that\n   the technology supports the diagnosis of simple cases, but that the expertise\n   of doctors is particularly important – and trusted – in more complex cases.\n   Targeted advertising is often considered not to have a significant effect\n   on people. However, this may be the case if, for example, an individual’s\n   vulnerabilities are used for successful advertising. Considering vulnerabilities\n   is particularly important for people from disadvantaged groups, who may\n   not be aware that they can opt out of direct marketing (see box) or of their\n   right to have a say when decisions are automated.\n64\n                                                       In the absence of case law in this area, more information and research\n                                                       is needed to identify the impact of such automated decisions (i.e. which\n                                                       advertisement will be delivered to whom, when, how and why). Answering\n                                                       these questions is challenging, as targeted advertising is based on highly\n                                                       complex technology and at scale.\n                In 2019, a Eurobarometer survey asked people in the EU if they are aware of their right to\nAwareness of    opt out from direct marketing. Overall, only 59 % of EU citizens have heard about this right\nright to opt    (with 24 % having exercised it). But people can only exercise their right if they are aware\nout from direct of it – which becomes even more important when direct marketing is made much more\nmarketing       efficient through machine learning.\namong general\n                Awareness levels strongly vary across the EU. The percentage of people who know about\npopulation      their right to opt out from direct marketing ranges from 38 % in Bulgaria to 81 % in the\n                Netherlands. Figure 4 shows the percentages. It also highlights – based on FRA’s analysis of\n                Eurobarometer data – that there is a strong variation within countries, when broken down\n                by regions.\n                In some regions, fewer than one in four have heard about their right. These are areas with\n                higher shares of people at risk of poverty. This indicates the general problem that people\n                who are more disadvantaged in society tend to be less aware of this right. The data show\n                that people who are not working, who more often struggle to pay their bills, who are living\n                in rural areas, or who are older, are less aware of this right.\n                FIGURE 4:          AWARENESS OF GDPR RIGHT TO OPT OUT FROM DIRECT MARKETING, IN THE EU\n                                   AND UNITED KINGDOM, BY COUNTRY AND REGION (%)\n                        75\n                        50\n                                                                                                                                     FI−80\n                        25\n                                                                                                               SE−65\n                                                                                                                                         EE−62\n                        Insufficient or no data available\n                                                                                                                                        LV−60\n                                                                                          DK−67\n                                                                        UK−63                                                       LT−54\n                                                                 IE−74\n                                                                                      NL−81\n                                                                                                                         PL−64\n                                                                                                  DE−69\n                                                                                    BE−59\n                                                                                            LU−62                CZ−55\n                                                                                                                       SK−61\n                                                                                                              AT−62\n                                                                                                                       HU−56\n                                                                              FR−47                            SI−57\n                                                                                                                                      RO−50\n                                                                                                            HR−50\n                                                                                                        IT−56\n                                                                                                                                      BG−38\n                                                                PT−51  ES−51\n                                                                                                                              EL−53\n                                                                                                          MT−46\n                                                                                                                                               CY−59\n                Note:\t\u0007     Map does not show non-EU countries other than the UK. Light shading = more aware of right.\n                            Dark shading = less aware of right. Results for regions within countries represented by light\n                            grey spaces were excluded because there were fewer than 20 respondents, meaning the\n                            numbers of observations were too low for reliable results. N = 26,503. Question: “The General\n                            Data Protection Regulation (GDPR) guarantees a number of rights. Have you heard of each of\n                            the following rights? […] 18.2 The right to object to receiving direct marketing.”\n                Source: FRA, 2020 [Calculations and presentation based on European Commission (2019),\n                            Eurobarometer, 91.2]\n                                                                                                                                                     65\n   Experiences from use cases\n   In general, interviewed experts highlighted that data protection law is difficult\n   to interpret and lacks clarity when it comes to the meaning of automated\n   decision making. One expert from France felt that, because automated\n   decision making is so difficult to explain, all automated decision making\n   should be banned, meaning the exceptions in the GDPR that allow for some\n   automated decision making should be removed. They pointed out that AI\n   can only be used as a decision support tool.\n   Another expert, an independent lawyer from the Netherlands, views current\n   laws and standards as sufficient, but says they need to be concretised per\n   sector. Particularly, the expert mentions that the scope of the existing rules\n   on permissible automated decision making are not clear and that it remains\n   unclear to him what a comprehensive assessment, or a ‘human in the loop’\n   means. This was also raised in relation to the SyRI case, where it remained\n   unclear to what extent the decisions were reviewed.\n   Another expert working at a supervisory authority, more generally, sees\n   no need for adapting data protection laws as “the legislation is quite\n   comprehensive. It is more about the organisation of the supervision thereof,\n   and also the political will behind it”.\n   These concerns reflect the findings of other research, which also raise serious   “There is a risk of having too much\n   issues concerning the right to human review. For example, the responsible         trust in the machine.”\n   officers questioned the results of an algorithmic system built to profile         (Public administration, France)\n   unemployed people in Poland in less than one percent of cases. This essentially\n   makes a supporting tool an automated decision making tool.27\n                                                                                     “There is a huge tension surrounding\n   Linked to the question of reviewing decisions or outputs from AI systems is       the GDPR. So we want to do well,\n   the challenge of a clear lack of knowledge about how AI works. Interviewees       but might in fact be worse off,\n   often could not explain in detail how the system they use works or which          because interpretation of the data\n   data it uses, be it due to the lack of knowledge or lack of transparency.         then turns out to be impossible.”\n   Meaningful information about the logic involved, or explaining outcomes           (Public administration, Netherlands)\n   from algorithms, is essential for several fundamental rights. It is crucial\n   not only for the processing of personal data, but also for ensuring that the\n   algorithms are fair and do not discriminate. It is also necessary to enable       “If we had to explain the model, we\n   people to properly challenge decisions and AI systems.                            wouldn’t be able to. The model is\n                                                                                     statistical and not very explainable.”\n   One interviewee working for public administration explains that the complexity    (Public administration, France)\n   differs depending on the tasks. Licence administration systems can be\n   relatively straightforward. Crime prevention analysis uses more data sources,\n   which makes it harder to understand. Another interviewee working in law           “Internally we can explain the\n   enforcement says that the current AI used by police organisations is not yet      decisions of the machine learning\n   so complex that it would make explanations difficult, but that this might be      models and we have several means\n   the case in the future.                                                           to do that.”\n                                                                                     (Private sector, Estonia)\n   A respondent working on financial data transactions indicates that traditional\n   models were straightforward to understand. However, new methodologies\n   are more difficult to explain, and the company has to invest resources into       “If the systems do not have black\n   making these models more explainable. Still, the level of explainability          boxes of information or processes,\n   required by the GDPR is not clear to the respondent.                              we already take a step forward in\n                                                                                     the defence of human rights.”\n                                                                                     (Public administration, Spain)\n                                                                                     “We are strongly attached to the\n                                                                                     idea that AI has to be explainable.”\n                                                                                     (Public administration, France)\n66\n                Most people are not aware that they have the right to have a say when decisions are\nAwareness of    automated, evidence suggests. A Eurobarometer survey showed that 40 % of Europeans\nright to have   know about their data protection rights.\na say when\ndecisions are   FRA’s analysis of the Eurobarometer survey shows that this figure drops considerably\nautomated       among people with lower socio-economic status. Only 26 % of EU citizens who report\n                that they are struggling to pay their bills most of the time know about this right. This\n                lack of rights awareness among those socially disadvantaged could contribute to further\n                social exclusion if those already disadvantaged are less aware that they can challenge\n                (automated) decisions about them (see Figure 5).\n                Gender differences are small, yet women are even less aware of this right (38 % of women\n                and 43 % of men). Older people are considerably less aware (31 % among those aged 55\n                and older).\n                FIGURE 5:                        AWARENESS OF RIGHT TO HAVE A SAY WHEN DECISIONS ARE AUTOMATED, BY\n                                                 AGE, GENDER AND DIFFICULTY IN PAYING BILLS (%)\n                                           55 years and older                                 31\n                                                 40-54 years                                                       46\n                Age\n                                                 25-39 years                                                             50\n                                                 15-24 years                                                  44\n                                                     Women                                         38\n                Gender\n                                                        Men                                               43\n                                                      Refusal                                       39\n                Difficulty paying bills\n                                          Almost never/never                                             42\n                                           From time to time                                       38\n                                             Most of the time                       26\n                Total                                                                               40\n                                                                0   10       20          30        40                   50    60\n                Notes:\t\u0007N = 26,503. Question: “The General Data Protection Regulation (GDPR) guarantees a number\n                        of rights. Have you heard of each of the following rights? […] 18.5 The right to have a say\n                        when decisions are automated (e.g. an algorithm decides if you will be granted a loan or not).”\n                Source: FRA, 2020 [calculations and presentation based on European Commission (2019),\n                        Eurobarometer, 91.2]\n                                                                                                                                   67\n   4.5.\t EQUALITY AND NON-DISCRIMINATION\n   Equality before the law and non-discrimination are enshrined in Articles 20\n   and 21 of the Charter. Discrimination is “where one person is treated less\n   favourably than another is, has been or would be, treated in a comparable\n   situation” based on a perceived or real personal characteristic28 (called\n   ‘protected grounds/characteristics’). Article 21 of the Charter prohibits any\n   discrimination based on any ground such as sex, race, colour, ethnic or social\n   origin, genetic features, language, religion or belief, political or any other\n   opinion, membership of a national minority, property, birth, disability, age\n   or sexual orientation.\n   The Charter prohibition reflects corresponding rights in the ECHR (Article 14)\n   and in Protocol No. 12 to the ECHR (Article 12), but is even broader, as it\n   establishes a non-exhaustive, open list extending protection to a wide range\n   of new grounds. Unlike Article 14 of the ECHR, the Charter right to non-\n   discrimination is a freestanding right that applies to situations that do not\n   need to be covered by any other Charter provision.29\n   Main challenges\n   Discrimination is a crucial topic when it comes to the use of AI, because the\n   very purpose of machine learning algorithms is to categorise, classify and\n   separate. As one interviewed expert points out, making differences is not\n   per se a bad thing. According to this expert, when deciding to grant a loan,\n   credit history can be used to differentiate between individuals, but not on\n   the basis of protected attributes, such as gender or religion. However, many\n   personal attributes or life experiences are often strongly correlated with\n   protected attributes. The credit history might be systematically different for\n   men and women due to differences in earnings and job histories.\n   Interviewees often mention efficiency as the main purpose for using AI-\n   related technologies. Yet it is important to note that this cannot not justify\n   unfair, differential treatment.\n   Often, protected attributes might be highly correlated with risks. For example,\n   differences in life situations among men and women might often be linked to\n68\n                                       different insurance risks. This is, however, not acceptable, as the Test-Achats\n                                       ruling, 30 shows. In that case, the CJEU put an end to gender discrimination\n                                       in insurance pricing.31\n                                       Under certain circumstances and in some areas, using algorithms could\n                                       positively contribute by reducing bias and stereotyping. Algorithmic data\n                                       analysis may produce results that could dispel prejudicial attitudes. For\n                                       example, predictive policing might, in some contexts, lead to more equitable\n                                       and non-discriminatory policing by reducing reliance on subjective human\n                                       judgments.32 Predictive techniques may be used to identify so-called ‘white-\n                                       collar crimes’, such as financial crimes that are historically under-policed.33\n                                       Nevertheless, direct or indirect discrimination34 through the use of algorithms\n                                       that involve big data is considered as one of the most pressing challenges\n                                       in the use of AI-driven technologies.35 Bias and discrimination, including\n                                       gender-based discrimination, in data-supported algorithmic decision making\n                                       can occur for several reasons and at many levels in AI systems. They are\n                                       difficult to detect and mitigate.36 Often, the quality of the data and biases\n                                       within it are the source of potential discrimination and unfair treatment.37\n                                       The discriminatory effects generated on certain groups are, in practice, very\n                                       difficult for individuals to challenge.38 So far, only a limited number of court\n                                       cases have dealt with discrimination relating to AI systems.39\n               A first instance decision of the Divisional Court of Cardiff in 2019 dismissed a claim\nUK Court of    concerning the lawfulness of the South Wales Police’s use of the “AFR Locate” face\nAppeal: police recognition system. The Court of Appeal overturned that decision.\nuse of facial\nrecognition    It found that the facial recognition programme used by the police was unlawful. The Court of\nviolates       Appeal ruled that “too much discretion is currently left to individual police officers”. It added\n               that “[i]t is not clear who can be placed on the watch list, nor is it clear that there are any\nhuman rights   criteria for determining where [the technology] can be deployed”.*\n               The court also held that the police did not sufficiently investigate if the software in use\n               exhibited race or gender bias.\n               This judgment is the first in-merit specifically on this matter in Europe. It considerably\n               narrows the scope of what is permissible and what law enforcement agencies need to do to\n               fully comply with human rights law.**\n               * UK, Court of Appeal, R (Bridges) v. CC South Wales, [2020] EWCA Civ 1058, 11 August 2020.\n               ** Ars Technica, ‘Police use of facial recognition violates human rights, UK court rules’, 11 August\n               2020.\n                                       Studies have highlighted the potential for discrimination prompted by the\n                                       use of AI-systems across the areas covered by the report.40 In the area of\n                                       predictive policing, for example, a particular risk relates to the potential\n                                       for automated decision making tools to reproduce and entrench existing\n                                       discriminatory practices that undermine equality before the law (Article 20\n                                       of the Charter). The historical crime data that underpins predictive policing\n                                       may be biased,41 reflecting inherent data gaps (e.g. chronic underreporting\n                                       for certain types of crime), alongside issues with how data is recorded (e.g.\n                                       human error, but also bias by individual officers).\n                                       Crime victimisation surveys consistently show that a large proportion of crime\n                                       is never reported to the police by the public – particularly crimes involving\n                                       physical and/or sexual violence, and hate crimes. For example, FRA’s survey\n                                       on violence against women – with 42,000 respondents – showed that only\n                                       one in five women who experienced violence, by their partner or anyone\n                                       else, brought the most serious incident to the attention of the police.42 FRA’s\n                                                                                                                        69\n   EU-MIDIS II survey of 25,500 respondents across the EU showed that only\n   three in ten reported incidents of racially motivated hate crime to the police\n   or any other organisation.43\n   Compared with violent crime and hate crime, property crime – such as\n   burglary – has a higher rate of reporting to the police, particularly in developed\n   countries. This may be because this is a requirement when claiming on an\n   insurance policy.\n   In sum – relying on official crime statistics (that are based on reported crime)\n   when looking to develop AI models in the field of predictive policing is\n   particularly problematic when it comes to specific crimes and specific groups.\n   Some variables used in AI modelling can be proxies for race, ethnicity, gender\n   and other protected categories. The complexity of the algorithms makes it\n   harder to identify and remove such biases. Instead of providing objective\n   analysis, predictive policing software may turn into an ‘echo chamber’\n   cementing existing systemic flaws and injustices with the ‘stamp’ of what\n   appears to be scientific legitimacy.44\n   The use of predictive policing may also make law enforcement responses\n   less equitable by focusing on certain crimes or areas.45 Predictive policing\n   is currently focused on property crimes such as theft and burglaries, which\n   are often associated with certain demographics and neighbourhoods. This\n   can result in certain demographics and neighbourhoods – and the individuals\n   living in them – being further stigmatised.46 Meanwhile, white-collar crime –\n   typically committed by different demographics – is less prioritised.47 These\n   patterns of policing – whereby certain neighbourhoods or communities are\n   disproportionately policed – predates the use of AI. However, the ‘promise’ that\n   AI is more ‘objective’ and can, in turn, be used to counteract discriminatory\n   policing, needs to be verified in practice.\n   Oxford University researcher Sandra Wachter highlights that discrimination may\n   occur due to information linked to protected attributes in targeted advertising.\n   Newly created profiles for the purpose of advertising might amount to indirect\n   discrimination and potentially even require new characteristics to be added\n   to non-discrimination legislation, and extend for its scope to be expanded\n   to other areas.48\n   Experiences from use cases\n                                                                                      “If you want the machine not to\n   Many interviewees noted that the use of AI, in general, can discriminate, but\n                                                                                      discriminate on the basis of sex,\n   that the systems they are working with do not. Many indicated a belief that\n                                                                                      do not put the variable of sex, as\n   excluding information on protected attributes is sufficient protection against\n                                                                                      easy as that, or make the examples\n   discrimination. However, discrimination can occur due to other information\n                                                                                      symmetrical if you notice that sex\n   contained in datasets that may indicate protected attributes. Traces of\n                                                                                      has certain relevance.”\n   protected groups are often hidden in other information.\n                                                                                      (Public administration, Spain)\n   An example from a public authority, which uses AI in tax and customs,\n   shows the challenges linked to identifying possible bias and potential\n   discrimination when using algorithms. When scrutinising their algorithms,\n   a public administration body found a higher degree of errors in tax declarations\n   among recently issued national identification numbers, which have almost\n   always been attributed to immigrants. This prompted further research into the\n   correlation. It turned out that the outputs of people with recent identification\n   numbers more often contained errors because they had never filed their\n   taxes before, and did not know how to do so (which was also the case for\n   non-migrants). This is also an example of proxy information, where parts of\n   a number could indicate immigrant status.\n70\n                                        Another interviewee working on the potential use of AI for detecting benefits\n                                        fraud mentioned in this respect: “If you want to prevent discrimination based\n                                        on ethnicity, for instance, it does not suffice to just remove the ‘ethnicity\n                                        label’, because the neighbourhood composition is often also determined by\n                                        ethnicity, or ethnicity plays a role in it. So [preventing discrimination] often\n                                        goes beyond the ‘direct’ characteristics”.\n“[I]f you do not have access            Even if most of the respondents were aware of the general potential\nto sensitive personal data, it is       for discrimination when using AI, they often ruled out that their system\nimpossible to check if you are          discriminates against people based on protected characteristics. Some\nprofiling on that basis.”               respondents also believe that their tools have a positive impact in terms of\n(Public administration, Netherlands)    non-discrimination. One respondent, testing AI for social benefits decisions,\n                                        regrets not being able to use AI for data protection reasons, even though, in\n                                        the respondent’s view, automation could process big datasets effectively and\n                                        without discrimination. While noting that protection of personal data needs\n                                        to be observed, the respondent feels it hinders prompt decision making\n                                        and non-discrimination – “if it can be automated, it should be automated”.\n                                        Some respondents were not clear or not sure about whether their use of AI\n                                        could discriminate. Respondents repeatedly stated that their system cannot\n                                        discriminate because it does not include data on protected characteristics.\n                                        For example, several interviewees working in predictive policing and law\n                                        enforcement felt that there was no potential for discrimination, as the AI\n                                        systems did not use data on, or return outcomes related to, protected grounds,\n                                        or because the system does not aim to identify people.\n                                        Others working on predictive policing felt that discrimination could occur, in\n                                        particular because of issues in the training data. In relation to the predictive\n                                        policing ‘heat map’ case, for example, one interviewee noted that – because\n                                        the dataset is never fully neutral, representative or complete – there is\n                                        a strong risk of bias and possible discrimination towards particular groups.\n                                        They identified sharing datasets to increase the amount of data available\n                                        as one way to mitigate this risk, but felt that this was impeded by data\n                                        protection regulations. They also indicated that multi-level teams with the\n                                        task to travel to different police authorities and check on the quality of the\n                                        systems used are being set up.\n                                        In the area of targeted advertising, some interviewees mentioned discrimination\n                                        as a potential problem, mainly after being asked directly about it. Overall\n                                        respondents do not think that their systems discriminate. Three respondents\n                                        mention that information on gender and age is not used and consequently\n                                        no discrimination in this respect can occur. Another interviewee is not sure\n                                        if this information is included or not.\n“For discrimination, it’s complicated   A respondent working on a breast cancer detection tool highlighted that\nbecause some diseases are more          age, gender and ethnicity are relevant factors as some population groups\npresent in certain ethnic groups.       are more likely to develop certain types of cancer. Respondents working in\nPredictions take into account the       health highlighted that the potential for discrimination is also linked to who\nsexual, ethnic, genetic character. But  uses the system, suggesting that this could become a greater challenge if\nit is not discriminatory or a violation the system were used by non-medical staff.\nof human rights.”\n(Private sector, France)                A different, but related, example comes from a respondent working on credit\n                                        rating for a private company, selling credit scores of individuals created by an\n                                        algorithm. The company uses information about gender, age and citizenship\n                                        in its credit risk models. This information has some impact on the outcome\n                                        of the credit scores. For example, younger people or non-citizens have\n                                        a higher credit risk score, but the influence of demographics is much smaller\n                                        compared to credit history data. According to the interviewee, their system\n                                        “certainly does not impact on the right to non-discrimination, because we\n                                                                                                                         71\n   do not make any decisions, we sell data and data analytics. Creditors have\n   to monitor that they do not discriminate”.\n   Another interviewee working on the data strategy for a financial institution in\n   the private sector, using AI to analyse financial transactions, clearly mentions\n   the challenges of understanding what non-discrimination constitutes for their\n   work. The interviewee mentions, for example, that it is not clear to what\n   extent it is illegal to exclude older people from receiving credit if their life\n   expectancy is expected to be lower than the mortgage repayment period\n   they asked for.\n   These findings point to uncertainty and ambiguity in the financial sector with\n   respect to how Article 21 of the Charter – on non-discrimination – translates\n   into real life situations.49\n   Vulnerable groups\n   Much of the discussion and research about discrimination when using AI is\n   linked to biased results with respect to ethnic origin, gender, and to some\n   extent, age. Although it is important to analyse potential discrimination against\n   these groups, the Charter covers several other grounds of discrimination,\n   which are less often part of discussions or research.\n   These other grounds include, for example, political opinion, sexual orientation,\n   and disability. The Charter provides particular rights to some special groups\n   (beyond Articles 20 and 21), including the rights of the child (Article 24), the\n   rights of the elderly (Article 25), and the rights of persons with disabilities\n   (Article 26).\n   The question of age – with respect to older age groups and younger adults –\n   came up during the interviews, notably when it comes to insurance and\n   credit (see above).\n   However, none of the interviewees or experts directly mentioned the rights\n   of the child. This might be linked to some extent to the nature of the use\n   cases investigated, but it clearly reflects the fact that this topic is not high\n   on the agenda of many of those working in AI.\n   Article 24 of the Charter emphasises that the best interests of the child\n   must be the primary consideration in all activities of public authorities and\n   private actors that concern children, which applies of course – equally – to\n   the field of AI.50\n   Only two respondents from public administration mentioned possible use of\n   AI in the area of child custody and the distribution of children in schools. But\n   they did not address this in consideration of the rights of the child. These\n   respondents did not wish to go into more detail concerning these use cases –\n   potentially reflecting the sensitivity of this topic.\n   Finally, issues linked to the integration of people with disabilities were not\n   raised in any of the interviews.\n72\n                 A Eurobarometer survey that included questions on AI asked respondents about the areas\nAwareness        they are mostly concerned about when it comes to the use of AI, including discrimination in\namong general    decision making, unclear responsibility, and that there is nobody to complain to.\npopulation of\npotential for    Only around 40 % of EU citizens indicated that they are concerned that using AI could\nAI to lead to    lead to discrimination in terms of age, gender, race or nationality – for example, in taking\n                 decisions on recruitment, credit worthiness, etc.\ndiscrimination\n                 Results vary across countries. Higher proportions of people are concerned about\n                 discrimination in the Netherlands (58 %), Luxembourg (48 %) and Sweden (47 %). Lower\n                 proportions expressed concern in Estonia (25 %), Hungary (24 %) and Lithuania (23 %)\n                 (see Figure 6).\n                 However, from this question it is not clear if people do not know that discrimination can\n                 happen, or if they are aware that it can happen but do not think it is a problem.\n                 FIGURE 6:     AWARENESS ABOUT THE RISKS OF DISCRIMINATION WHEN USING AI, BY COUNTRY\n                               (%)\n                 100\n                  90\n                  80\n                  70\n                  60           58\n                  50                48 47 46\n                                             45 44 43\n                       40                             41 41 40 40 40 39                                               41\n                  40                                                    38 37 36\n                                                                                 35 34 34 34\n                                                                                             32 31\n                                                                                                   29 28\n                  30                                                                                       25 24 23\n                  20\n                  10\n                   0\n                       EU-27\n                               NL\n                                                                                                                      UK\n                               LU\n                               SE\n                               FR\n                               EL\n                                SI\n                               DE\n                               ES\n                               CY\n                               BE\n                                IE\n                               HR\n                               AT\n                               DK\n                                IT\n                               CZ\n                                FI\n                               BG\n                               PT\n                               SK\n                               MT\n                               LV\n                               RO\n                               PL\n                               EE\n                               HU\n                                LT\n                 Notes:\t\u0007Includes people who indicated that they are concerned that AI could lead to discrimination\n                         among three possible issues, or all of the three issues.\n                 Source: FRA calculations based on European Commission (2019), Eurobarometer, 92.3\n                                                                                                                           73\n   Tackling\n   gender                          The Charter stipulates that equality\n   inequality in between                      women and men must\n                                   be ensured in all areas, including\n   the design and employment,                      work and pay\n                                   (Article 23). Gender discrimination\n   use of AI\n                                   is a major concern when it comes to\n                                   the design and use of AI and related\n                                   technologies.*\n                                   On the development side, the\n                                   European Economic and Social\n                                   Committee notes that the\n                                   development of AI is taking place\n                                   within a homogenous environment\n                                   principally consisting of young white\n                                   men. This results in cultural and\n                                   gender disparities, which are being\n                                   embedded in AI technologies. For\n                                   example, training data are prone to\n                                   manipulation, may be biased, reflect\n                                   cultural, gender and other prejudices\n                                   or preferences, and contain errors.**\n      * See also European          This is also reflected in this research,\n         Commission, White\n                                   where, despite efforts to achieve\n         Paper On Artificial\n         Intelligence –\n                                   gender balance, the majority of\n         A European approach       interviewees were men.\n         to excellence and trust,\n         COM(2020) 65 final,       Disparities at the design and\n         Brussels, 19 February     deployment stage are linked to the\n         2020, p. 1.               systematic disadvantages affecting\n      ** European Economic         women in the labour market and\n         and Social Committee,     the potential lack of awareness\n         Artificial intelligence – of gender biases. A recent study\n         The consequences of       showed that the increased use\n         artificial intelligence   of industrial robots could widen\n         on the (digital) single   the gender gap, despite both\n         market, production,       genders benefitting from increased\n         consumption,              automation, as the analysis indicated\n         employment and society\n                                   that men in medium- and high-\n         (own-initiative opinion),\n                                   skill occupations would benefit\n         31 May 2017, JO C 288,\n         p. 43.                    disproportionally.***\n      *** Aksoy, C., Özcan, B.     Looking ahead, using data and\n         and Philipp, J. (2020),\n                                   algorithms could help to better\n         Robots and the Gender\n                                   mainstream gender equality into\n         Pay Gap in Europe, IZA\n         Discussion Paper No.      policies and processes by paying\n         13482.                    attention to gendered datasets.\n                                   Drawing on discussions around\n      ****       See the webpage\n         on data feminism on the   gender inequalities and the use\n         datasociety’s website.    of data (‘data feminism’)****\n                                   could help to raise awareness that\n      ***** Criado Perez,\n         C. (2020), Invisible      the male point of view should\n         Women. Exposing data      not be taken as the default view,\n         bias in a world designed  which also then finds its way into\n         for men, London.          datasets.*****\n74\n4.6.\t ACCESS TO JUSTICE\nThe right to an effective remedy before a tribunal and to a fair trial (Article 47\nof the Charter) is one of the most often used Charter right in legal proceedings.\nThis highlights its importance in upholding fundamental rights and the rule\nof law. This right of horizontal character empowers individuals to challenge\na measure affecting any right conferred to them by EU law, not only in respect\nof those guaranteed in the Charter.51 The CJEU has underlined that Article 47\nof the Charter constitutes a reaffirmation of the principle of effective judicial\nprotection and that the characteristics of a remedy must be determined in\na manner that is consistent with this principle.52\nThe right to an effective remedy also covers decisions taken with the support\nof AI technologies. EU data protection law reconfirms that the right to an\neffective judicial remedy must be provided in relation to decisions by the\ncontroller or the processor53 as well as the supervisory authority.54 Data\nprocessed by AI-driven technologies is no exception.\nIt is crucial to note that the possibility to lodge an administrative complaint\nbefore a supervisory authority as provided for by the GDPR and the Law\nEnforcement Directive55 is not considered an effective judicial remedy under\nArticle 47 of the Charter. This is because no court is involved in such a review.\nJudicial review should always remain available and accessible, when internal\nand alternative dispute settlement mechanisms prove insufficient or when\nthe person concerned opts for judicial review.56\nUsing AI can challenge the right to an effective remedy in different ways.\nOne prominent concern is the lack of transparency in the use and operation\nof new technologies. Algorithmic decision making is notoriously opaque:\ndata collection, algorithm training, selection of data for modelling or profiling,\nthe situation around individual consent, effectiveness and error rates of the\nalgorithm and other aspects are often not transparently reported.57\n                                                                                   75\n   Without access to this information, individuals may not be able to defend\n   themselves, assign responsibility for the decisions affecting them,58 appeal\n   any decision negatively affecting them or have a fair trial, which includes the\n   principle of equality of arms and adversarial proceedings as established by\n   the ECtHR.59 These requirements also form part of the corresponding Charter\n   right (Article 47) in view of Article 52 (3) of the Charter.\n   Main challenges\n   These issues are reflected in the specific challenges to the right to an effective\n   remedy and a fair trial that the interviewed experts outlined. Generally, experts\n   indicate a difference in accessing remedies at private companies and public\n   administration. Public authorities are more often forced to be transparent\n   about their use of AI. Meanwhile, companies appear to be more secretive,\n   the assessment of several experts suggests. However, an expert from the\n   Netherlands said that people might more readily complain to companies, but\n   be reluctant to complain to public authorities. This is because public services\n   often concern vulnerable people, in need of social benefits, who would be\n   less inclined to complain about any decisions.\n   Opportunities to successfully complain about the use of AI and challenge\n   decisions based on AI are essential for providing access to justice. The\n   interviews emphasised the following as important in this respect:\n   ――Making people aware that AI is used\n   ――Making people aware of how and where to complain\n   ――Making sure that the AI system and decisions based on AI can be explained\n   First, everyone needs to know if they are dealing with an AI system. If\n   a taken decision affects people, e.g. on social benefits, people concerned\n   might complain in general – but they will not be able to complain about the\n   use of AI if they do not know AI is involved.\n   An expert explained that, while there is general willingness to complain,\n   the biggest problem is that people often do not know that AI is being used,\n   because organisations are not transparent about this, even though this is\n   required by the GDPR. Several interviewees indicate that informing people\n   that any decision made about them is based on (partly) automated tools is\n   the very first step for providing access to complaints.\n   Second, everyone needs to know how and where to complain. It may be\n   difficult for people to know which body deals with what type of complaints.\n   One expert pointed out that consumers often do not know how to complain –\n   for example, to a bank that might use algorithms for deciding on financial\n   matters. A public administration that issues automated decisions decided\n   to add names of employees to the decisions to provide contact persons to\n   those potentially challenging the (automated) decision. Most interviewees\n   indicated that there are ways and procedures for complaints in place, which\n   are the same procedures as those for any other complaints not linked to the\n   use of AI. Only few companies or organisations that use AI on anonymised or\n   aggregated data indicate that they do not have any complaint mechanisms\n   in place.\n   Finally, those complaining need enough information to challenge the underlying\n   decision. Only thorough information about the AI systems provides equality of\n   arms to meaningfully challenge decisions. However, this is not straightforward\n   when it comes to the use of AI, particularly because of:\n   ――potential intellectual property rights issues, and\n   ――because complex systems are difficult to explain.\n76\n                                     Intellectual property rights form one hurdle to providing enough information\n                                     about how a decision was made, or how a system works. Algorithms can be\n                                     part of an implemented software, or technical invention, that may be subject\n                                     to intellectual property rights – a right protected under Article 17 (2) of the\n                                     Charter. Actors often seek out copyright, patent and trade secret protection\n                                     to safeguard their knowledge on AI.60\n                                     One interviewee from the insurance sector claims that, due to the highly\n                                     competitive market, “one may not share too much about the workings of\n                                     a used technology” as to, for instance, why a particular price was given to\n                                     a customer. This is essentially because competitors could benefit from this\n                                     knowledge if the underlying software were subject to scrutiny. Another\n                                     respondent using AI to handle visa applications notes that using systems\n                                     developed by external providers whose algorithms are covered by intellectual\n                                     property rights can hinder the necessary transparency at a later stage.\n                                     Another challenge for successfully complaining about automated decisions\n                                     or the use of AI in general is the challenge to explain the decisions based\n                                     on complex systems. The interviewees working for public administration\n                                     suggest that there is usually clear guidance on how to complain against an\n                                     administrative decision, an area where interviewees highlight the importance\n                                     of detailed explanations. For example, for the systems that automatically\n                                     provide unemployment benefits for cases that do not involve discretion,\n                                     clients can ask for the reasoning behind automated administrative acts. An\n                                     interviewee indicates that if clients wish to see the calculations behind financial\n                                     decisions, they may do so in the self-service system on the organisation’s\n                                     website or in their publications, which contain detailed descriptions of the\n                                     calculations used.\n                                     Interviewees recognise that an open and transparent logic is essential for\n                                     providing explanations regarding AI-supported decisions, but that this is\n                                     often challenging or impossible to achieve. One interviewee working for\n                                     a bank mentions that more complex machine learning solutions cannot\n                                     be used for certain decision making, because the reasoning of the system\n                                     cannot be explained easily, and this is why such systems are only used for\n                                     other purposes. However, an interviewee working for another bank indicates\n                                     that such systems are used, but they use simpler methods in addition to\n                                     the complex ones to get an idea of the probable reasons for the decisions.\n                                     One expert raised the problem that companies internally might not have\n                                     enough information about the way algorithms work themselves. The lack of\n                                     expertise and knowledge appears to be a major hindrance in practice when\n                                     seeking access to effective remedy.61\n                                     Experiences from use cases\n“The topic of transparency is very\n                                     Respondents discussing predictive policing tools highlighted transparency\nimportant nowadays, there are\n                                     as important.\nmany procedures on how to publish\nthe information, many automatic\n                                     In the gender-based violence use case, they felt that sending both the police\nmeans that help to upload the\n                                     file and the outcome of the AI system to the judge, and informing the victim\ninformation on the portals, and\n                                     of the level of risk attributed to the case and the police measures that will\nthere has been a lot of work done in\n                                     apply as a result, enhances transparency.\nterms of transparency.”\n(Public administration, Spain)\n                                     Interviewees discussing the heat map example referred to numerous requests\n                                     to the police to explain the system’s purpose and how it works, and highlighted\n                                     transparency as a way to reduce public anxiety.\n                                     A number of interviewees pointed to the possibility for individuals\n                                     affected by the system to make complaints to the police, the courts or the\n                                                                                                                         77\n   Ombudsinstitution. With reference to the domestic violence case, however,\n   the interviewee indicated that there is no procedure in place to question\n   a system of police protocol.\n   In terms of measures to protect fundamental rights in the health services use\n   cases, several interviewees referred to ethics committees, as well as general\n   legal safeguards and data protection rules. Checks and controls were primarily\n   mentioned to take place through external actors. No specific complaints\n   procedures were in place in the organisations of those interviewees who\n   responded to this question.\n   Some interviewees highlighted that doctors ultimately take responsibility for\n   decisions, and that patients often do not know about the use of an AI tool\n   in the first place. For example, in the breast cancer detection example, the\n   interviewee indicated that there is no possibility for legal recourse against\n   the developer of the tool, as the radiologist makes the decision on diagnosis\n   and is liable for any errors.\n   The safeguards in place for the targeted advertising cases mainly follow\n   data protection requirements, such as ensuring that consent is obtained and\n   respected. One company makes sure not to have clients engaged in illicit\n   practices and rejects clients from certain sectors, such as political advertising.\n   Complaints received\n   Few of the organisations interviewed received any complaints challenging\n   their use of AI. In some cases, interviewees claim to have received complaints\n   by complainants not aware that AI was used, who noticed incorrect outputs\n   in decision making.\n   For example, individuals lodged complaints regarding traffic fines, whereby\n   a police officer stopped a car driver, and upon hearing the car driver’s\n   explanation as to why the fine was wrongfully administered, proceeded\n   to manually correct the information in the system, without being able to\n   update the system’s historical data. In these cases, such fines will remain\n   visible throughout the system, and this particular person would continue to\n   be profiled as a high risk on each occasion.\n   Even though organisations rarely received any formal complaints with respect       “The number of the complaints\n   to their use of AI, interviewees often state that this is due to the early stages  about data use is miniscule, rather\n   of their AI implementation. Nonetheless, interviewees reported repeated            people may have asked to delete\n   requests for access to or rectification of personal data, and some people          some information about them.”\n   requested their information to be removed, as well as explanations as to           (Private company, Estonia)\n   why a certain recommendation was made.\n   The majority of interviewees claim that procedures are the same as to if\n   a decision had been processed or undertaken by a human. On the other\n   hand, a few other interviewees showed interest in opening new channels\n   to analyse, explain and redress decisions involving their AI solutions.\n   Other rights linked to access to justice set out in the Charter are also impacted,\n   most notably by the use of AI in law enforcement. These include, for example,\n   the presumption of innocence (Article 48 of the Charter).\n   When identifying people who are suspected of having committed a crime,\n   the police may target their activities specifically against one person or put\n   them under suspicion based on flawed and fragmented data and algorithmic\n   profiling.62 Uncritical reliance on automated tools, without proper human review\n   that takes into account other information, might contribute to discrimination\n   in decision making.\n78\n4.7.\t RIGHT TO SOCIAL SECURITY AND SOCIAL\n         ASSISTANCE\n                                 The right to social security and assistance\n                                 enshrined in Article 34 of the Charter is a classic\n                                 social right,63 inspired by various international\n                                 and European legal standards.64 This provision,\n                                 combining both elements of a right and of\n                                 a principle,65 has a great significance in the\n                                 EU in view of the free movement of people\n                                 within the Union.\n                                 Instead of tying issues of social protection\n                                 to the labour market, this Charter right takes\n                                 a new, communitarian approach when broadly\n                                 referring to “providing [social] protection in\n                                 cases such as maternity, illness, industrial\n                                 accidents, dependency or old age, and in the\n                                 case of loss of employment” (Article 34 (1)).66\nIt is, however, a primarily programmatic statement that does not prescribe\nany minimum standard of protection. It is in principle up to EU Member States\nto determine the conditions of entitlement and access to social benefits, with\nfurther clarification needed from the CJEU.67 Yet, Article 34 (1) of the Charter\nprovides protection against measures restricting or abolishing existing social\nsecurity rights.68\nIn addition, access to social rights is guaranteed to all individuals legally\nresiding within the EU who exercise their right to free movement, regardless\nof their nationality, subject to EU and national laws (Article 34 (2)). This thus\ncreates justiciable rights before national courts and the CJEU.69\nIt is becoming increasingly apparent that the impact of AI technologies on\nsocial protection systems and the lives of the many individuals who rely upon\nthem can be far-reaching and – potentially – very problematic. Introducing\nAI-driven technologies in social welfare systems risks creating barriers to\naccess to this right.70\nFor example, using AI in social security needs to account for potential\nnegative – and discriminatory – effects on non-nationals (both EU citizens\nand third-country nationals) exercising their right to freedom of movement\nin the EU. They could be negatively affected, for example, if a system relies\non data about job histories, which are not available for those moving from\nother EU Member States.\nOnly one respondent addressed the ‘right to receive a correct pension’ as\nan aspect of a wider definition of human rights. Meanwhile, none of those\ninterviewed referred to the fundamental right to social security and social\nassistance. This could partly reflect the nature of the use cases. However,\nthe lack of references to social rights among public sector interviewees was\nnotable.\n4.8.\t CONSUMER PROTECTION\nThe Charter stipulates that EU policies must ensure a high level of consumer\nprotection, which is based on Article 169 of the TFEU. EU institutions and\nother bodies needs to observe this principle, as do Member State authorities\nwhen implementing EU law.71\n                                                                                     79\n   This Charter principle provides only for the guarantee of a particular goal (“a\n   high level of consumer protection”). Article 169 of the TFEU is more concrete,\n   as it also determines the means of how to achieve the stated aim – for\n   example, protecting the health, safety and economic interest of consumers,\n   as well as promoting their right to information and education. 72\n   Among the use cases, the use of AI for targeted advertising, and the use of\n   medical records by companies, are of particular importance.\n   When it comes to targeted advertising, consumers need to be aware that\n   they can opt out from being targeted. If they are not aware, they might be\n   subjected to advertising they do not want. This is particularly problematic in\n   combination with highly sophisticated AI systems for advertising, which can\n   amount to some sort of manipulation of consumer preferences.73\n   Consumer protection is also of major relevance for the use of health data\n   (EHRs). The European Consumer Organisation (BEUC) noted that AI in the\n   area of health brings challenges for consumers. It recommends that AI\n   technologies must fully respect data protection rules, be transparent to\n   the consumer, and avoid discrimination. BEUC has also called for updated\n   regulation and legislative measures for market surveillance, law enforcement,\n   and efficient redress concerning digital health products and services to fully\n   protect EU consumers.74\n   BEUC carried out a survey among consumers about their views on AI in\n   selected EU Member States. It shows that more than one in two respondents\n   agree that companies are using AI to manipulate consumer decisions. In\n   addition, almost half of respondents believe that personalised content and\n   adverts on e-commerce platforms do not have an added value (44 %).\n   Slightly more than half of the survey respondents expressed low trust that\n   governments effectively control AI.75\n   In the interviews conducted for this study, consumer protection was only\n   mentioned at the margins, when discussing risks of using AI and fundamental\n   rights. However, some respondents from businesses refer to consumer\n   protection legislation as a relevant framework also applying to their use\n   of AI. Moreover, some respondents deem consumer protection authorities\n   potentially relevant oversight bodies when AI is used.\n   In general terms, many interviewees in the business sector stress the\n   importance of consumer satisfaction. For example, a company using video\n   surveillance for the security of customers at their premises mention that\n   consumer protection regulations are relevant for such technical solutions,\n   and that the use of the systems should aim to improve the situation of\n   consumers while also preserving their rights. Several AI tools are built to\n   understand and profile consumers to enable businesses to improve their\n   services and marketing.\n   Data protection is an important aspect for business. This is also linked to the\n   fact that breaching data protection rules is considered a business risk, as\n   mentioned above. One major concern of companies is obtaining and managing\n   consent from consumers and customers to process their data, when using\n   AI tools for marketing purposes. Interviewees report that the GDPR has had\n   an impact, improving their systems to handle consent.\n80\n4.9.\t RIGHT TO GOOD ADMINISTRATION\nThe right to good administration is a well-established general principle of\nEU law elaborated by the CJEU. As such, it is binding on all EU Member States.76\nIt is also a fundamental right enshrined in Article 41 of the Charter, although\nonly for actions of EU institutions, bodies and agencies.77\nAs a general principle of EU law, it requires EU Member States to apply the\nrequirements of the right to good administration in all public action. This right\nincludes, but is not limited to, the right of an individual to have access to\ntheir file and the obligation of any public authority to give sufficient reasons\nfor its decisions.78\nAccess to the file facilitates understanding of the evidentiary basis on which\na decision has been made, and/or of the reasons underlying it. This places\nthe individual in a better position to put forward counter-arguments when\nexercising the right to be heard and the right to an effective remedy.79\nThe obligation to give reasons makes, from the perspective of the individuals\naffected, the decision-making process more transparent, so that the person\nconcerned can know and understand why a measure or action has been\ntaken. Transparency is also an enabling principle that provides foundations\nfor other rights,80 including the exercise of the right to an effective remedy.\nAccording to the CJEU, the context in which individual decisions are made is\nimportant in determining the extent of the duty to give reasons.81 In France, for\ninstance, the Code on the Relations between the Public and the Administration\nrequires written explanations of the factual and legal considerations on which\na decision has been based.82\nThe right to good administration also applies when AI systems process\npersonal data and support decision making by public authorities. Although\nthe right to good administration may be subjected to certain limitations,\nthe question arises of how to ensure that the potentially huge number of\nindividuals all have access to their files (personal data used in AI systems).\nAnother question is how to make sure that public authorities always give\nsufficient reasons when the operation of AI-driven technologies cannot be\nfully explained due to their inherent opacity and complexity.\nThe use of a system to categorise unemployed people, set up in Poland,\nhighlighted problems linked to public administration and the use of algorithms.\nBased on questions answered by unemployed people, a categorisation was\ndeveloped through a statistical algorithm. The system received a lot of criticism\nfrom civil society with respect to the lack of opportunities to complain and\npotential discrimination.83 In the end, a complaint by the Ombudsinstitution –\nbased on administrative grounds – led to a constitutional court ruling that\nput an end to the system’s use.84\nThe intent to increase efficiency drives the use of AI in the public sector – an\naim that directly speaks to improving administration and benefiting citizens.\nRespondents in public administration by far most often indicate efficiency\nas the reason for considering the use of AI or for presently using AI. One\nrespondent, who advises ministries on digital strategies and their use of\nAI, said that the main reasons for adopting AI are to improve the service to\ncitizens and to reduce the costs of these services for public administration.\nInterviewees also indicate that public administration has particular\nrequirements, meaning AI cannot be used for all purposes and needs particular\nattention when it comes to decision making. However, the efficiency of\na system is also considered an important added value.\n                                                                                  81\n   In this sense, a respondent working on the digitalisation of migration\n   management indicates that building too complex AI systems is a risk,\n   because afterwards it would require a lot of work to understand the system\n   in retrospect. The interviewee indicates that their team needs to be careful\n   not to allow AI to make final decisions, which have to be taken by a human –\n   because society and clients are not ready for this, according to the interviewee.\n   Although some systems are appealing, they do not work effectively, and this\n   could result in extra work and negative results. However, the interviewee\n   also indicates that the dimension of efficiency “is often side-lined when\n   discussing data protection”.\n   The requirements for good administration also directly link the issues raised\n   above with respect to data protection, non-discrimination and the right to an\n   effective remedy and fair trial. Public administration can only process data\n   on a legal basis. Decisions need to be fair and transparent and pathways\n   to challenge decisions need to be available and accessible. As a result, the\n   requirements for good administration are directly linked to the discussion\n   and analysis above with respect to the legal processing of data (under data\n   protection), fair decisions (linked to the discussion about non-discrimination),\n   alongside transparency and ways to challenge and explain decisions (with\n   respect to access to justice).\n82\nEndnotes\n1   See European Commission, European enterprise survey on the use of technologies based on artificial intelligence, Luxembourg, July\n    2020.\n2 FRA (2020), What do fundamental rights mean for people in the EU, Luxembourg, Publications Office, p. 28.\n3 See webpage on Three-level IT Baseline Security System ISKE on ther website of Estonia’s Information System Authority.\n4 See the website of the PCI Security Standards Council.\n5 Barak, A. (2019), ‘Human dignity as a framework right (motherright)’, in Barak, A., Human Dignity: The Constitutional Value and the\n    Constitutional Right, Cambridge, Cambridge University Press, 2015, Ch. 9 (pp. 156-169).\n6 CJEU, C-377/98, Netherlands v. European Parliament and Council, 9 October 2001, paras. 70-77.\n7 For a discussion on the malicious use of AI, see for example, Brundage, M. et al. (2018), The Malicious Use of Artificial Intelligence:\n    Forecasting, Prevention, and Mitigation.\n8 FRA (2019), Facial recognition technology: fundamental rights considerations in the context of law enforcement, Luxembourg,\n    Publications Office, November 2019.\n9 CJEU, Joined Cases C-92/09 and C-93/09, Volker und Markus Schecke and Eifert GbR and Hartmut Eifert, Opinion of Advocate General\n    Sharpston, 17 June 2010, para. 71.\n10 FRA, Council of Europe and EDPS (2018), Handbook on European data protection law. 2018 Edition, Luxembourg, Publications Office, June\n    2018, p. 19.\n11 See also Ibid., pp. 35-52.\n12 ECtHR (2019), Guide on Article 8 of the European Convention on Human Rights – Right to respect for private and family life, home and\n    correspondence, Strasbourg, Council of Europe, updated on 31 August 2019, paras. 133 and 136.\n13 ECtHR, López Ribalda and Others v. Spain, Nos. 1874/13 and 8567/13, 17 October 2019, para. 87. For a comprehensive legal analysis of\n    the meaning and content of ‘privacy’, see also Koops, B.-J. et al. (2017), ‘A Typology of Privacy’, University of Pennsylvania Journal of\n    International Law, Vol. 38, Issue 2, pp. 483-575.\n14 Vermeulen, M. (2015), SURVEILLE Deliverable D4.7 – The scope of the right to private life in public places, July 2014, p. 2.\n15 UN, Human Rights Committee, General Comment No. 37 (2020) on the right of peaceful assembly (article 21), CCPR/C/GC/37, 17\n    September 2020, para. 62.\n16 Costello, Róisín Áine (2020), The Impacts of AdTech on Privacy Rights and the Rule of Law,\nTechnology and Regulation.\n17 Norwegian Consumer Council (2020), Out of Control. How consumers are exploited by the online advertising industry.\n18 FRA (2020), Your rights matter: Data protection and privacy – Fundamental Rights Survey, Luxembourg, Publications Office.\n19 Rocher, L., Hendrickx, J. M. and de Montjoye Y. (2019), Estimating the success of re-identifications in incomplete datasets using\n    generative models, Nature Communications 10, No. 3069.\n20 Hacker, P. (2020), A Legal Framework for AI Training Data. Law, Innovation and Technology (forthcoming), available at SSRN; Article\n    29 Data Protection Working Party (2014), Opinion 05/2014 on Anonymisation Techniques; see also Finck, Michèle and Pallas, Frank,\n    They Who Must Not Be Identified - Distinguishing Personal from Non-Personal Data Under the GDPR (October 1, 2019), Forthcoming,\n    International Data Privacy Law, 2020, Max Planck Institute for Innovation & Competition Research Paper No. 19-14, available at SSRN; and\n    Sartor G. and Lagioia F. (2020), The impact of the General Data Protection Regulation (GDPR) on artificial intelligence, study prepared for\n    the panel for the Future of Science and Technology (STOA) of the European Parliament.\n21 See, for example, the UK Data Service’s blog on “Access to sensitive data for research: ‘The 5 Safes’”; see also the discussion in Ohm, P.\n    (2010), “Broken promises of privacy: responding to the surprising failure of anonymization”, UCLA Law Review, p. 1701.\n22 GDPR, Art. 22 (3); and Law Enforcement Directive, Art. 11 (1).\n23 Veale, M. and Edwards, L. (2018), ‘Clarity, surprises, and further questions in the Article 29 Working Party draft guidance on automated\n    decision-making and profiling’, Computer Law & Security Review, Vol. 34 (2), April 2018, pp. 398-404.\n24 Article 29 Data Protection Working Party (2018), Guidelines on Automated individual decision-making and Profiling for the purposes of\n    Regulation 2016/679, Adopted on 3 October 2017, as last Revised and Adopted on 6 February 2018.\n25 Green, B. And Chen, Y. (2019), ‘Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments’, In FAT* ’19:\n    Conference on Fairness, Accountability, and Transparency (FAT* ’19), January 29-31, 2019.\n26 González Fuster, G. (2020), Artificial Intelligence and Law Enforcement – Impact on Fundamental Rights, European Parliament, Policy\n    Department for Citizens’ Rights and Constitutional Affairs, Directorate-General for Internal Policies, PE 656.295, July 2020, p. 17; Brkan,\n    M. (2019), ‘Do algorithms rule the world? Algorithmic decision-making and data protection in the framework of the GDPR and beyond’,\n    International Journal of Law and Information Technology, Vol. 27 (2), p. 98; Article 29 Working Party, Guidelines on Automated individual\n    decision-making and Profiling for the purposes of Regulation 2016/679, Adopted on 3 October 2017, as last Revised and Adopted on 6\n    February 2018, WP251rev.0, p. 19.\n27 Misuraca, G., and van Noordt, C. (2020), Overview of the use and impact of AI in public services in the EU, European Commission Joint\n    Research Centre, Luxembourg.\n28 Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial\n    or ethnic origin, OJ L 180, 19.7.2000, pp. 22-26, Art. 2; and Council Directive 2000/78/EC of 27 November 2000 establishing a general\n    framework for equal treatment in employment and occupation, OJ L 303, 2.12.2000, pp. 16-22, Art. 2.\n29 FRA and CoE (2018), Handbook on European non-discrimination law. 2018 edition, Luxembourg, Publications Office, June 2018, p. 35.\n30 CJEU, C-236/09, Association Belge des Consommateurs Test-Achats ASBL and Others v. Conseil des ministres, 14 January 2011.\n31 European Commission (2012), EU rules on gender-neutral pricing in insurance industry enter into force, Press release, IP/11/1581, 20\n    December 2012.\n32 Elizabeth E. Joh (2015), ‘The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing’, UC Davis Legal Studies Research\n    Paper No. 473, pp. 17-18.\n33 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, p. 14. DOI:\n    10.1177/1477370819876762.\n34 See also European Commission, White Paper On Artificial Intelligence – A European approach to excellence and trust, COM(2020) 65 final,\n    Brussels, 19 February 2020, p. 1.\n35 FRA (2018), #BigData: Discrimination in data-supported decision making, Luxembourg, Publications Office, June 2018, p. 3.\n36 Ibid.\n37 FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications\n    Office.\n                                                                                                                                                   83\n   38 Korff, D. and Browne, I. (2013) ‘The use of the Internet & related services, private life & data protection: trends, technologies, threats\n      and implications’, Council of Europe, T-PD(2013)07.\n   39 See National Non-discrimination and Equality Tribunal of Finland, decision no. 216/2017 from 21 March 2018. See also the SyRI case\n      discussed above and UK, Court of Appeal, R (Bridges) v. CC South Wales, [2020] EWCA Civ 1058, 11 August 2020.\n   40 See also Equinet (2020), Regulating for an equal AI: A new role for equality bodies, Brussels, report prepared by Allen R. and Masters D.\n   41 Tolan S., Miron M., Gomez E. and Castillo C (2019), ‘Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for\n      Juvenile Justice in Catalonia’, Best Paper Award, International Conference on AI and Law, 2019; Richardson R., Schultz J. and Crawford K.\n      (2019), Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice, 94 N.Y.U. L.\n      REV. ONLINE 192 (2019), available at SSRN.\n   42 FRA (2014), Violence against women: an EU-wide survey. Main results report, Luxembourg, Publications Office, p. 61.\n   43 FRA (2018), Second European Union Minorities and Discrimination Survey. Main results, Luxembourg, Publications Office, p. 66.\n   44 Erik Bakke (2018), “Predictive policing: The argument for public transparency”, New York University Annual Survey of American Law, Vol.\n      74, pp. 139-140; Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, Washington University Law Review, Vol. 94, pp. 1146-1150;\n      andCouncil of Europe Committee of experts on internet intermediaries (MSI-NET) (2017), Algorithms and Human Rights, Council of Europe\n      DGI(2017)12, p. 11.\n   45 Elizabeth E. Joh (2015), ‘The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing’, UC Davis Legal Studies Research\n      Paper No. 473, p. 18.\n   46 Gstrein, O. J., Bunnik, A., and Zwitter, A. (2019), ‘Ethical, Legal and Social Challenges of Predictive Policing’, Católica Law Review 3:3, pp.\n      77-98; Albert Meijer & Martijn Wessels (2019), ‘Predictive Policing: Review of Benefits and Drawbacks’, International Journal of Public\n      Administration 42:12, p. 1036, DOI: 10.1080/01900692.2019.1575664.\n   47 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, pp. 8-9.\n      DOI: 10.1177/1477370819876762.\n   48 Wachter, Sandra (2020), ‘Affinity Profiling and Discrimination by Association in Online Behavioural Advertising’, Berkeley Technology Law\n      Journal, Vol. 35, No. 2, 2020, (forthcoming), available at SSRN.\n   49 On the use of AI in financial industries leading to unequal access to financial services, see in the legal literature e.g. Boyd, D., Levy K. &\n      Marwick, A. (2014), ‘The Networked Nature of Algorithmic Discrimination’ in Gangadharan, S. P., Eubanks, V. & Barocas, S. (eds), Data and\n      Discrimination: Collected Essays, Open Technology Institute, pp. 53-62.\n   50 For an overview of child rights issues, see UNICEF Innovation, Human Rights Center, UC Berkeley (2019), Artificial Intelligence and\n      Children’s Rights.\n   51 EU Network of Independent Experts on Fundamental Rights, Commentary on the Charter on Fundamental Rights of the European Union,\n      June 2006, p. 360. See also: FRA and CoE (2016), Handbook on European law relating to access to justice, Luxembourg, Publications\n      Office, June 2016, p. 92.\n   52 CJEU, C-432/05, Unibet (London) Ltd, Unibet (International) Ltd v. Justitiekanslern, 13 March 2007, para. 37; CJEU, C-93/12, ET\n      Agrokonsulting-04-Velko Stoyanov v. Izpalnitelen direktor na Darzhaven fond ‘Zemedelie’ – Razplashtatelna agentsia, 27 June 2013, para.\n      59; CJEU, C-562/13, Centre public d’action sociale d’Ottignies-Louvain-la-Neuve v. Moussa Abdida, 18 December 2014, para. 45.\n   53 Law Enforcement Directive, Art. 54; and GDPR, Art. 79.\n   54 Law Enforcement Directive, Art. 53; and GDPR, Art. 78.\n   55 Law Enforcement Directive, Art. 52; and GDPR, Art. 77.\n   56 Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights impacts of\n      algorithmic systems (adopted by the Committee of Ministers on 8 April 2020 at the 1373rd meeting of the Ministers’ Deputies), Appendix,\n      para. B.4.5.\n   57 Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, 94 Washington University Law Review, pp. 1165-1167.\n   58 Gstrein, O. J., Bunnik, A., & Zwitter, A. (2019), Ethical, Legal and Social Challenges of Predictive Policing’, Católica Law Review, 3:3, pp. 80-\n      81; Yeung K. (2019), A study of the implications of advanced digital technologies (including AI systems) for the concept of responsibility\n      within a human rights framework, Prepared by the Council of Europe Expert Committee on human rights dimensions of automated data\n      processing and different forms of artificial intelligence (MSI-AUT).\n   59 Council of Europe, Algorithms and human rights, pp.11 and 24.\n   60 International Technology Law Association (2019), ‘Responsible AI: A Global Policy Framework’, pp. 258-282.\n   61 The lack of expertise on AI is also reflected in the survey among companies in the EU, where the lack of skills among existing staff and\n      difficulties in hiring new staff are the most prominent obstacle for further AI adoption (European Commission (2020), European enterprise\n      survey on the use of technologies based on artificial intelligence, Luxembourg, July 2020, p. 11).\n   62 See a detailed assessment of the impact of predictive policing on the presumption of innocence in Mendola, Marco (2016), One Step\n      Further in the ‘Surveillance Society’: The Case of Predictive Policing.\n   63 See e.g. Egorov, A. and Wujczyk, M. (eds.) (2016), The Right to Social Security in the Constitutions of the World: Broadening the moral\n      and legal space for social justice, Geneva, ILO Global Study, Vol. 1: Europe, pp. xv-xvii and 1-6.\n   64 These include Arts. 153 and 156 of the TFEU; Arts. 12 and 13 of the 1961 European Social Charter; as well as points 2 and 10 of the 1989\n      Community Charter on the Fundamental Social Rights of Workers (see Explanations relating to the Charter of Fundamental Rights, OJ\n      C 303, 14.12.2007, pp. 17-35).\n   65 Explanations relating to the Charter of Fundamental Rights (OJ C 303, 14.12.2007, pp. 17-35), Explanation on Article 52 — Scope and\n      interpretation of rights and principles.\n   66 Łukasz Bojarski, Dieter Schindlauer and Katrin Wladasch (2014), The European Charter of Fundamental Rights as a Living Instrument –\n      Manual, Rome/Warsaw/Vienna, pp. 61-62.\n   67 De Becker, E. (2016), ‘The (Possible) Role of the Right to Social Security in the EU Economic Monitoring Process’, German Law Journal, Vol.\n      17, No. 3, pp. 297, 304; Paju, J. (2017), The European Union and Social Security Law, Oxford, Hart Publishing, sub-section 7.5.2.\n   68 Ibid., pp. 297-298; Peers, S. & Prechal, S. (2014), ‘Scope and Interpretation of Rights and Principles’, in Hervey, T., Kenner, J., Peers, S. and\n      Ward, A. (eds.), The EU Charter of Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1455, 1508.\n   69 With the exception of Poland and the United Kingdom, see Protocol (No. 30) on the application of the Charter of Fundamental Rights of\n      the European Union to Poland and to the United Kingdom (OJ C 115, 9.5.2008, pp. 313-314), Art. 1 (2).\n   70 Christiaan van Veen and Ben Zevenbergen, ‘Conference on Social Protection by Artificial Intelligence: Decoding Human Rights in a Digital\n      Age’, Freedom to Tinker – Research and Expert Commentary on Digital Technologies in Public Life, 29 May 2019.\n   71 Art. 51 (1) of the Charter; see also Explanations relating to the Charter of Fundamental Rights (OJ C 303, 14.12.2007, pp. 17-35),\n      Explanation on Article 52 — Scope and interpretation of rights and principles.\n   72 Łukasz Bojarski, Dieter Schindlauer and Katrin Wladasch (2014), The European Charter of Fundamental Rights as a Living Instrument –\n      Manual, Rome/Warsaw/Vienna, p. 67.\n84\n73 Sartor, Giovanni (2020), New aspects and challenges in consumer protection, study for the committee on the Internal Market and\n   Consumer Protection, Policy Department for Economic, Scientific and Quality of Life Policies, European Parliament, Luxembourg.\n74 The European Consumer Organisation (BEUC) (2018), Digital Health, Principles and Recommendations.\n75 BEUC (2020), Artificial Intelligence: what consumers say. Findings and policy recommendations of a multi-country survey on AI.\n76 In recent case law, see CJEU, C-604/12, H. N. v. Minister for Justice, Equality and Law Reform, Ireland, Attorney General, 8 May 2014, para.\n   49.\n77 Also confirmed by the CJEU, Joined Cases C-141/12 and C-372/12, YS v. Minister voor Immigratie, Integratie en Asiel, and Minister voor\n   Immigratie, Integratie en Asiel v. M, S, 17 July 2014, paras. 66-70).\n78 These components, initially developed by the CJEU case law, have been codified in Article 41 (2) of the Charter. For more on this right in\n   leading academic literature, see Craig, P. (2014), ‘Article 41 – Right to Good Administration’, in Hervey, T., Kenner, J., Peers, S. and Ward, A.\n   (eds.), The EU Charter of Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1069-1098.\n79 Ibid., p. 1082.\n80 Finck, M. (2019), ‘Automated Decision-Making and Administrative Law’, Max Planck Institute for Innovation & Competition Research\n   Paper No. 19-10, p. 8.\n81 Craig, P. (2014), ‘Article 41 – Right to Good Administration’, in Hervey, T., Kenner, J., Peers, S. and Ward, A. (eds.), The EU Charter of\n   Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1086-1087.\n82 France, Code des relations entre le public et l’administration, Article L2111-5.\n83 Panoptykon Foundation (2015), Profiling the unemployed in Poland: Social and political implications of algorithmic decision making; see\n   also Algorithm Watch (2019), Poland to scrap controversial unemployment scoring system.\n84 See Decision K 53/16, available on the Constitutional Tribunal’s website.\n                                                                                                                                                     85\n865\nFUNDAMENTAL RIGHTS IMPACT\nASSESSMENT – A PRACTICAL TOOL\nFOR PROTECTING FUNDAMENTAL\nRIGHTS\n             Chapter 4 illustrated the extent to which using AI affects different fundamental\n             rights. This chapter analyses how fundamental rights impact assessments\n             (FRIA) could reduce the negative impacts that using AI can have on\n             fundamental rights.\n             Section 5.1 provides a brief overview of the current discussion on the need\n             for fundamental rights impact assessments in this field. Section 5.2 analyses\n             current practices in addressing fundamental rights implications, based on the\n             interviews conducted for this report. Interviewees were asked about what\n             sort of testing was done before the system was used, and who controls the\n             tasks affected by the use of the technology.\n             The chapter ends with suggestions on how to assess the fundamental rights\n             impact when using AI and related technologies.\n             5.1.\t CALLING FOR A FUNDAMENTAL RIGHTS IMPACT\n                     ASSESSMENT – AVAILABLE GUIDANCE AND TOOLS\n             International organisations,1 academics2 and civil society3 have called for\n             fundamental rights impact assessments to be conducted when using AI or\n             related technologies.\n             For example, the Committee of Ministers of the Council of Europe’s guidelines\n             on addressing the human rights impacts of algorithmic systems recommend\n             that states should conduct “impact assessments prior to public procurement,\n             during development, at regular milestones, and throughout their context-\n             specific deployment in order to identify the risks of rights-adverse outcomes”.4\n             There is a need for flexible impact assessments that can adapt to different\n             situations given that fundamental rights violations are always contextual.\n             Scholars exemplify this based on EU anti-discrimination law, where equality\n             is always contextual and depends on the case at hand.5\n             Fundamental rights compliance cannot be automated and hard-coded into\n             computer software. Rather, each use case needs separate examination\n             to determine whether any fundamental rights issue arises. Nevertheless,\n             assessments can follow a systematic approach and provide similar information.\n             Existing standards provide guidance on how to do a fundamental rights impact\n             assessment of AI and related technology. These include hard law, soft law\n                                                                                              87\n   instruments (such as recommendations or declarations), and practical tools\n   (e.g. guidelines and checklists).\n   Beyond the requirements flowing from data protection legislation (see box),\n   there are few examples of laws requiring mandatory assessments of the\n   effects of AI in general. In view of the increasing uptake of AI, the Canadian\n   government has issued guidelines, including mandatory requirements for\n   assessing AI for use by public administration. It applies to any system, tool,\n   or statistical model used to recommend or make an administrative decision\n   about a client.6\n88\n            European data protection law requires a data protection impact assessment (DPIA).a The\nLearning    CoE Modernised Convention No. 108 provides for a general obligation to examine the likely\nfrom data   impact of data processing on individuals’ rights and fundamental freedoms before their use.\nprotection  Following the assessment, controllers should design the processing in such a manner to\nimpact      prevent or minimise identified risks.b\nassessments\n            EU law imposes a similar, more detailed, obligation. The GDPR foresees a Data Protection\n            Impact Assessment (DPIA) for data processing that is likely “to result in a high risk to the\n            rights and freedoms of natural persons.”c Therefore, where required by law, a DPIA for an\n            AI technology could potentially also address the broader fundamental rights implications,\n            besides the impact on the right to privacy,d and be used as a tool to further investigate\n            algorithms and their impacts.e\n            However, under the GDPR (Article 35), the DPIA is limited to ‘high risk’ cases processing\n            personal data. It therefore may miss other high risk cases that are not primarily or obviously\n            related to protection of personal data. At the same time, the GDPR is delimited to its specific\n            field of application, with accompanying expertise in this field. This means that the potential\n            extension of the scope of a DPIA to other fundamental rights might be limited.\n            The GDPR also gives some indications about the modalities to undertake a DPIA. First,\n            a DPIA should be conducted before any high risk processing.f Second, a DPIA should provide\n            for a systematic description of envisaged operations, the purpose and the legitimate\n            interests pursued. It must also assess the necessity and proportionality of the processing\n            and the possible risks to the rights of individuals. In addition, it must contain the planned\n            security measures to address the risks identified.g\n            While pointing out that different methodologies can apply, the Article 29 Working Party (WP\n            29) proposes – in a check list form – minimum criteria that a controller should use to assess\n            if the DPIA comprehensively complies with the GDPR.h\n            Finally, the GDPR foresees prior mandatory consultation of the relevant supervisory\n            authority, if the impact assessment indicates that processing presents risks that cannot be\n            mitigated. i This gives a crucial role to DPAs, as independent bodies established by law.j\n            The European Data Protection Supervisor (EDPS) provides guidance on carrying out DPIAs.k\n            Data protection authorities have also discussed, and provide guidance on, how to assess AI\n            technologies.l\n            a\n              For more information on Data Protection Impact Assessment, see: FRA, Council of Europe and\n            EDPS (2018), Handbook on European data protection law. 2018 edition, p. 179-181.\n            b\n              Council of Europe Modernised Convention No. 108, Art. 10 (2).\n            c\n              GDPR, Art. 35 (1).\n            d\n              GDPR, Recitals (2) and (75); Article 29 Working Party, Guidelines on Data Protection Impact\n            Assessment (DPIA), wp248rev.01, 13 October 2017.\n            e\n              Edwards and Veale (2018); FRA (2018), #BigData: Discrimination in data-supported decision\n            making, Luxembourg, Publications Office, June 2018.\n            f\n              GDPR, Art. 35 (1). The WP29 specifies that ‘carrying out a DPIA is a continual process, not a one-\n            time exercise.’\n            g\n              GDPR, Art. 35 (7), as well as recitals (84) and (90).\n            h\n              Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13\n            October 2017, Annex 2.\n            i\n              GDPR, Art. 36.\n            j\n              GDPR, Art. 35.\n            k\n              EDPS (2019), Accountability on the ground Part II: Data Protection Impact Assessments & Prior\n            Consultation, v.1.3, July 2019.\n            l\n              See, for example, the Declaration on ethics and data protection in AI, adopted by the 40th\n            International Conference of Data Protection and Privacy Commissioners (ICDPPC), in 2018.\n                                                                                                                 89\n   There are many more examples of non-binding guidelines. At the global\n   level, the United Nations Guiding Principles on Business and Human Rights\n   recommend that enterprises integrate the findings from human rights impact\n   assessments across relevant internal functions and processes, and take\n   appropriate action.7 Although they do not refer specifically to AI, the guidelines\n   are relevant in supporting the development of AI technology in a rights\n   compliant manner.8\n   At the EU level, the Ethics Guidelines for Trustworthy AI prepared by the\n   European Commission’s High-Level Group on Artificial Intelligence9 also\n   recommend performing a FRIA, before a system’s development, when\n   “there are risks that fundamental rights can negatively be affected by the\n   technology”.10 They also emphasise the need to put in place mechanisms\n   to receive external feedback on AI systems that potentially infringe on\n   fundamental rights.\n   In addition, private companies, 11 associations of private companies12 or of both\n   public and private interests,13 as well as NGOs14 and other organisations15 have\n   developed different types of guidance to support AI impact assessments. These\n   documents do not usually contain clear guidelines on impact assessment.\n   Instead, they highlight the different aspects and criteria that should be taken\n   into account when developing and carrying out an impact assessment.\n   Broad categories include the purpose of the system, the description of the\n   technology, the assessment of the impact and targeted population/individual,\n   evaluating fairness and diversity, the description of the audits planned or\n   performed, as well as accountability. Some explicitly refer to applicable\n   international human rights law standards.16\n   Various codes of ethics or conducts,17 standards,18 as well as certification\n   schemes are also in place.19\n   Several practical tools are available to assess the impact of AI technologies and\n   mitigate risks, developed by a wide range of actors. These include checklists,20\n   lists of questions,21 online self-evaluation tools,22 and risk management\n   frameworks.23\n   Some focus specifically on assessing fundamental rights risks.24 Others focus\n   on ethical, societal or economic implications.25 These can be useful references\n   when performing a thorough fundamental rights impact assessment of AI\n   technologies.\n   In July 2020, for example, the High-Level Group on Artificial Intelligence\n   issued an “Assessment List for Trustworthy AI” (ALTAI),26 after a six month\n   pilot involving more than 350 stakeholders. ALTAI helps organisations\n   self-evaluate – on a voluntary basis – the reliability and trustworthiness of\n   AI and reduce potential risks for users. It supports businesses and public\n   administrations to ask the right questions around the seven requirements\n   for responsible AI identified in the Ethics Guidelines for Trustworthy AI.27\n   ALTAI specifically refers to the need to perform a fundamental rights impact\n   assessment. It includes examples of questions to assess impact on non-\n   discrimination/equality; the right to privacy; the rights of the child; the\n   freedom of expression; as well as the freedom of information and association.28\n   Several online assessment tools target the use of AI by public authorities.\n   The Canadian Government developed an Algorithmic Impact Assessment tool\n   (AIA),29 pursuant to the Canadian Directive on Automated Decision-Making.30\n   The AIA represents an automated assessment consisting of more than 50\n   questions that unfold the requirements of the directive. Questions relate to\n90\nfundamental rights concerns – such as an AI system’s impact on the freedom\nof movement, on the likelihood of incarceration of an individual, on the legal\nstatus, on access to funding or benefits, or on indigenous people. A score\nis attributed to each reply and a final impact scoring is provided and made\npublically available on the government’s website.\nAs another example, the Ethics Toolkit31 is a freely accessible tool designed\nfor local governments. Based on a risk management approach, it supports fair\nautomated decisions and minimising unintentional harm to individuals in the\nfield of the criminal justice, higher-education, social media and other areas.\nAmong national human rights bodies, the Danish Institute for Human\nRights proposed a human rights compliance “quick check.32 This involves\nan interactive online computer programme that allows companies to select\nand modify the information in a database to suit their type of business and\narea of operations to check rights compliance. The quick check is based on\nthe Human Rights Compliance Assessment tool,33 which runs on a database\nof over 350 questions and 1,000 corresponding human rights indicators. It\nuses international human rights law standards as benchmarks. Applying to\nall fields of operations, it can provide guidance when developing impact\nassessment for AI technology.\nAcademic work has also suggested operational frameworks for assessing\nrisks in using AI technology. Some focus specifically on identifying and\naddressing fundamental rights implications by the private sector.34 Some\nfocus on developing ethical and values-oriented models (analysing the\nsocietal impact of the data used) with the creation of ad hoc expert (review)\ncommittee.35\nOthers have developed guidance frameworks for specific case studies.\nFor example, in the field of criminal justice, the ALGO CARE framework36\nintroduced a step-by-step assessment to evaluate the key legal and practical\nconcerns that should be considered in relation to police using algorithmic\nrisk assessment tools.\nSome have argued for participatory ways to involve and consider the views\nof the affected rights-holders and other stakeholders communities when\ndeveloping an impact assessment and publically engage with them from\nthe start.37 Others have joined cross-discipline expertise of science and law\nto design practical frameworks.38\n5.2.\t IMPACT ASSESSMENTS AND TESTING IN PRACTICE\nVirtually all the systems discussed in the interviews were subject to some\nsort of testing, which included elements of impact assessment. However,\nthese were mainly technical and data protection (impact) assessments. These\nrarely address potential impacts on other fundamental rights.\n                                                                               91\n   Some interviewees argue against conducting a fundamental rights impact\n   assessment, because in their view the system does not negatively affect\n   fundamental rights or because they are unsure about it. For example,\n   a respondent working on traffic management, using cameras for monitoring\n   traffic, indicated that they only tested for accuracy of the system, but not\n   fundamental rights, apart from respecting data protection rules.\n   Some respondents simply did not know if fundamental rights were assessed\n   as part of a general impact assessment that was carried out.\n   Testing and stages of development\n   Much testing is done before any new AI system is used. As respondents            “When testing the system, we did\n   highlighted, moving an AI system into production is a very challenging task.     not really look at the legal aspects,\n   As mentioned, public administration as well as private companies are usually     we looked at whether the system is\n   careful when using AI. Many projects that interviewees refer to are still in     profitable.”\n   development or in the pilot phase, and some had not started concrete testing.    (Private company, Estonia)\n   Testing can be done in several stages. These include the development stage\n   (so-called proof-of-concept), pilot stages before deployment, and tests during\n   and after deployment. If possible, live experimentation is carried out at the\n   initial stages, which often involves staged deployment.\n   For example, the organisation interviewed that tests different applications\n   to support job seekers conducts continuous, step-by-step testing. Selected\n   members of the organisation test the tool in real situations, using check lists.\n   The interviewee mentioned that it is challenging to move to the deployment\n   stage and it is planned to supervise the tool in real time.\n   In another example, involving automated rule-based granting of social\n   benefits, different assessments were carried out. Before implementation,\n   a group of lawyers, data protection specialists, compensation specialists\n   and accountants performed a general impact assessment. After this, the\n   department responsible for using the system conducted tests to decide\n   whether the system could be used.\n   Following this, the system was monitored in its implementation, using a step-\n   by-step approach. In a first step, about half of the decisions were taken by\n   the system. In a next step, the decisions taken automatically were expanded\n   to all negative decisions. After this, another area of decisions was added,\n92\n                                     including all decisions on ending compensation payments. At the time the\n                                     interviews were conducted, about 95 % of decisions were automated. The\n                                     interviewee indicated that, after carrying out these tests, they feel sure that\n                                     the system is secure, and that there are no outstanding risks.\n                                     A company working on a fraud detection system replaced their rule-based\n                                     system with a machine learning tool. Before changing the system, the old\n                                     and new system were run in parallel to see if the machine learning system\n                                     performs better than the rule-based one. The interviewee mentioned that\n                                     “[there] was rigorous analysis behind it and direct feedback where we saw\n                                     what would be the impact on losses versus how many good customers we\n                                     were impacting negatively”. The interviewee added that, when they “were\n                                     comfortable that [the machine learning system] was better [than the static\n                                     rule system] in all aspects, we deployed it in its entirety”.\n                                     In other use cases, no previous automated system existed and tests were\n                                     reviewed by humans. For example, an automated transcription service was\n                                     tested during court hearings, when allowed by the judge. This included\n                                     regular feedback on the correctness of the transcription services from judges.\n                                     One interviewee from law enforcement, working on a tool to detect domestic\n                                     violence, identifies issues with precision and accuracy when using the system.\n                                     If a police officer does not have sufficient training and knowledge about the\n                                     system, the indicators required by the system cannot gather the required\n                                     information, which could lead to miscalculation. They highlight that the\n                                     robustness of the system is tested annually to assure the quality of the two\n                                     questionnaires used, the completeness of the data, and the training of the\n                                     police officers using the AI system. This process also considers how personal\n                                     data protection laws and protocols are applied. The tests discussed focus\n                                     strongly on technical aspects and general operations.\n                                     Fundamental rights and data protection impact assessments\n                                     Apart from data protection, which all respondents mentioned, other\n                                     fundamental rights were typically not considered. Respondents only reflected\n                                     about other potential impacts on fundamental rights, or mentioned that these\n                                     aspects were considered, when prompted by the interviewer.\n                                     Many respondents are generally aware of discrimination issues – but often\n                                     discussed this only after being explicitly asked about discrimination. Yet they\n                                     gave no information about any formal, in-depth tests for discrimination.\n                                     Generally, respondents ruled out the possibility that their system discriminates\n                                     based on protected attributes. For example, one interviewee states that they\n                                     test the system against data protection laws and specific applicable legal\n                                     acts, but not fundamental rights. However, the interviewee did consider\n                                     potential discrimination, but ruled it out. It needs to be kept in mind for future\n                                     technologies, the interviewee stated.\n                                     However, there are cases where non-discrimination was generally considered\n                                     during the testing phase of AI systems. One respondent from a municipal\n                                     authority mentioned that they cannot assess the fairness of a model, because\n                                     they cannot access data needed due to data protection reasons. According\n                                     to the interviewee, “there is a huge tension surrounding the GDPR. So we\n                                     want to do well, but might in fact be worse off, because interpretation of\n                                     the data then turns out to be impossible”.\n“Yes, we assess the legality of\npersonal data protection and the     Most respondents reported that a data protection impact assessment, as\nconformity with their specific legal required by law, was conducted, although these took different forms. A bank\nacts.”                               tested a tool for analysing speech from customer calls to find out about\n(Public administration, Estonia)     reoccurring problems, and carried out a data protection impact assessment\n                                                                                                                        93\n   (DPIA) specifically for testing the tool. The outcome was that the system\n   can be tested if data were only used for the testing phase and are deleted\n   after a certain period after the test, and if access to the data by employees\n   is restricted to the testing phase and supervised. For the deployment of the\n   tool, another DPIA is required in this case.\n   There is sometimes a lack of clarity as to what extent the use of AI and\n   related technologies, most notably the use of algorithms, belongs to an DPIA.\n   In the area of predictive policing, for instance, some DPIAs were done for\n   the underlying architecture of the system, rather than the specific AI tool.\n   Another interviewee using algorithms in financial services also mentioned\n   not assessing the machine learning tool as such within the framework of\n   a DPIA, because of the belief that it does not apply to the machine learning\n   system (but the underlying data).\n   One interviewee felt that the data protection impact assessment for the crime\n   heat map example was not sufficiently in depth to safeguard the quality of\n   the model, and that the system was not equipped to deal with cross-sectoral\n   use of data, where different rules might apply. They indicated that further\n   standards were required.\n   A respondent working on migration management indicated having data\n   protection officers involved in their analysis. The legal service has a specialised\n   quality control AI tool to study the data protection aspects of their system.\n   However, the respondent also mentioned that more guidance is needed.\n   The companies working on targeted advertising all looked into data protection\n   issues, although not all respondents were sure if an impact assessment was\n   conducted. The companies assessed, for example, whether only people who\n   consented are approached in targeted communication. For targeted ads,\n   they assessed whether information on possible re-identification is deleted,\n   including whether cookies and trackers are anonymised.\n   With respect to DPIAs generally, some respondents did not know, as this\n   was not their area of responsibility. Others knew they had a positive DPIA,\n   but were not aware of any details. It appears that the legal assessment\n   is sometimes detached from the technical side, with technical people not\n   knowing about legal assessments. One interviewee from a private company\n   working on credit risk scoring mentioned: “I make suggestions how some\n   system could be developed and then the compliance manager tells me if it\n   is in conformity with the laws”.\n   Audits and working with external (oversight) bodies\n   The public administrations and private companies involved in FRA’s research\n   all carry out tests before deploying any AI. These are often linked to existing\n   internal and external oversight processes. The use of AI is frequently subjected\n   to internal review processes within companies and public administration,\n   although these are not necessarily formalised review processes. Some\n   interviewees mentioned that they are working on formalising existing internal\n   review processes for overseeing AI systems.\n94\n                                                                      Interviewees from the public sector say\n                                                                      that they have to be particularly cautious\n                                                                      before using any AI to support decisions.\n                                                                      A representative working on migration\n                                                                      management at a public administration\n                                                                      indicates that “[i]n the private sector, [wrong\n                                                                      results] might cause business-related losses,\n                                                                      in the police it impacts people’s lives and their\n                                                                      fundamental rights”.\n                                                                      Yet it is not always clear to public administration,\n                                                                      or to businesses, who is responsible for\n                                                                      checking and overseeing the use of AI. Public\n                                                                      administrations appear to be under stronger\n                                                                      scrutiny when it comes to oversight of their AI\n                                                                      systems. Such oversight is often done through\n                                                                      regular audits, for example connected to\n                                                                      budgetary review.\n                                     Some interviewees, from public and private organisations, report that their\n                                     AI systems are currently checked in the framework of an existing IT review\n                                     (e.g. regular database checks), in the absence of review processes that\n                                     specifically look into the use of AI. In addition, interviewees report about\n                                     sector-specific certification schemes that also look into the use of AI – for\n                                     example, in the area of health or financial services.\n                                     Several interviewees mentioned that they were in contact with data protection\n                                     authorities. Some companies and public administrations sought permission\n                                     from the data protection authorities before using their AI system or at least\n                                     were generally in contact with them. For example, one company working on\n                                     targeted advertising mentioned discussing their use of personal data with\n                                     the national data protection authority.\n                                     Experts interviewed for this report further highlighted the relevance of data\n                                     protection authorities for overseeing AI systems with respect to the use of\n                                     personal data. However, experts strongly highlighted that data protection\n                                     authorities are under-resourced for this task for two reasons. Data protection\n                                     authorities often do not have relevant AI-related expertise.39 Additionally,\n                                     their budgets are overstretched and their workload heavy.\n                                     Experts’ views differ with respect to the need of additional oversight bodies,\n                                     and the potential creation of an AI specific institution. However, they agree that\n                                     existing bodies all have to work on topics linked to AI within their mandates.\n                                     Equality bodies, as well as other human rights institutions, are mentioned\n                                     by some interviewed experts as providing oversight concerning possible\n                                     discrimination when using AI. They highlighted that these institutions need\n                                     to build up expertise in this area to better contribute to the oversight of AI.\n                                     However, similar to data protection authorities, this is a challenging task for\n                                     equality bodies given their lack of resources.\n“We are proactive not only among\n                                     Several interviewees mentioned consumer protection authorities as potentially\nourselves to mitigate risks, but we\n                                     providing relevant oversight on the use of AI. One respondent, working for\nalso get additional audits. We also\n                                     a retail company, would like to have an advisory agency that could be consulted\nsee sometimes that some regulatory\n                                     about possible use of AI for innovation without being investigated right away.\naudits are quite sloppy. For us that\n                                     At the moment, the company prefers to consult consumer authorities over\nis not good because we have lots of\n                                     data protection authorities about potential future marketing campaigns.\ncustomer data.”\n                                     This is because data protection authorities might start an investigation into\n(Private company, Estonia)\n                                     their efforts.\n                                                                                                                           95\n   When discussing oversight, those developing and using AI, as well as experts,\n   repeatedly mention the challenge to really understand the impact when using\n   AI. Despite the need to engage existing oversight bodies, responsibilities to\n   oversee the use of AI from a fundamental rights perspective remain unclear.\n   5.3.\t FUNDAMENTAL RIGHTS IMPACT ASSESSMENT IN\n            PRACTICE\n   Many key actors in the field of fundamental rights have called for conducting\n   fundamental rights impact assessments before using any AI-driven systems.\n   This section highlights some of the elements that could be incorporated into\n   such an assessment.\n   Fundamental rights impact assessments are needed given that a contextualised\n   assessment is required. This is because uses of AI vary considerably in terms of\n   complexity, level of automation, potential errors and harm, scale of application,\n   as well as area of use. The more complex an AI system is, the more difficult\n   it is to assess its potential impact.\n   While the fundamental rights implicated will vary depending on the area of\n   application, the full spectrum of rights needs to be considered for each use\n   of AI. However, uses of AI are likely to involve some of the rights most often\n   affected by AI systems. The discussion in the preceding chapter makes clear\n   that issues linked to data protection, non-discrimination, as well as access to\n   effective remedies and a fair trial, are relevant for all uses of AI.\n   Thus, the following horizontal points could be a basic starting point for\n   considering the impact of AI on selected rights:\n   ――The legal processing of data needs to be confirmed in line with data\n        protection laws.\n   If personal data are used, the full data protection framework applies. This\n   ensures that processing is legal and does not violate a person’s rights to\n   respect for a private and family life, and data protection.\n   ――The processing should not lead to unfair treatment or discrimination of\n        protected groups.\n   Assessing non-discrimination needs to be at the core of assessing AI. Even\n   apparently miniscule differences can scale up and create risks contravening\n   the principle of non-discrimination. The disadvantage to people depends\n   on the nature (kind of harm), severity (strength of harm) and significance\n96\n(how many people are put at a disadvantage compared to another group of\npeople). Statistical assessments on group differences are an important tool\nto assess unfair and discriminatory uses of AI.40\n――People subjected to AI and related technologies should be able to complain\n    and receive effective remedies.\nThere should be accessible ways for people to complain about potential\ndecisions being made and to effectively access remedies. This includes\navailability of information that allows the explanation of decisions.\nIn addition, other relevant rights in the Charter apply. Public administrations\nusing AI need to consider good administration principles. Businesses have\nto take consumer protection into account.\nOther rights are relevant depending on the area of application. Some examples\ninclude:\n――the right to social protection, when working with social benefits;\n――the right to freedom of expression and information, when using AI to\n    support online content moderation;\n――the right of assembly and of association, when considering the use of\n    facial recognition technology in public spaces;\n――the right to education, when using AI in the education sector;\n――the right to asylum, when using AI to support migration management;\n――the right of collective bargaining and action, when using AI in the ‘gig-\n    economy’;\n――the right to fair and just working conditions, when using AI at the workplace;\n――the right to access preventive health care, when using AI in health services;\n――and the right to the presumption of innocence and the right to defence,\n    when using AI in the justice sector or for law-enforcement purposes.\nInformation needed to assess the potential impact on fundamental\nrights before implementing AI\nGiven the variety of tools, purposes and area of application, assessments\nare contextual. To be able to meaningfully respond to the horizontal points\nraised above, and to assess specific rights linked to different use cases, at\nleast the following information needs to be available:\n――A description of the purpose and context of the system, as well as the\n    legal basis.\n――A description of possible harm of using the system, including questions\n    around false positives, false negatives, and other possible harm due to\n    the automation and scale of use.\n――A description of the technology used. This includes information on the\n    data used for building the system and its legal basis for processing.\n    A description of relevant information to include is provided in FRA’s paper\n    on data quality and AI.41\n――An evidence-based description of the accuracy of the AI system in terms\n    of outcomes based on training data and possible tests and experiments in\n    real life situations, if appropriate. Here, false positives and false negatives\n    should be considered separately. These should include breakdowns for\n    as many groups as possible to allow for checking potential discrimination\n    (e.g. differences in the accuracy between women and men).\n――Where already available, the provision of information about compliance\n    with existing standards and potential certifications obtained.\nEx-post assessments and safeguards\nLastly, envisaging ex-post safeguards further contributes to the fundamental\nrights compliant use of AI. These could include:\n                                                                                    97\n   ――Regular repetition of assessments after deployment, where appropriate.\n       This is important to learn about potential feedback loops and in case\n       rules are updated. This also requires recording information on the use\n       and outcomes of the system to the extent data protection is respected.\n   ――Making people subjected to AI systems aware that they are subjected\n       to this technology, as they can otherwise not challenge any decision\n       affecting them.\n   ――Making available easily accessible channels for effectively complaining\n       about decisions made based on the AI system.\n   Engaging external experts, stakeholders and oversight bodies\n   The above information could be the basis for consultation with different\n   stakeholders and experts before a particular AI system is used. Depending on\n   the nature of the application and its legal basis, a consultation with relevant\n   stakeholders would ensure that no potential harm has been omitted and\n   different perspectives are brought into the assessment. Stakeholders could\n   include civil society; different public and private organisations; as well as\n   experts from different fields of fundamental rights, including data protection.\n   As the ten experts interviewed for this report highlighted, existing oversight\n   bodies are also responsible for AI oversight within their mandates. Sector-\n   specific bodies and certification schemes are doing this to some extent, the\n   interviews suggest – for example, in health care and financial oversight.\n   To monitor, comprehend and effectively respond to the potential impacts\n   of AI on a wide spectrum of fundamental rights, data protection authorities,\n   equality bodies, ombuds institutions and national human rights institutions\n   could play an important role, providing input and oversight from their various\n   points of expertise. However, as interviews indicated, extensive upskilling\n   and resource allocation is needed to underpin this.\n98\nEndnotes\n1  Council of Europe, Commissioner for Human Rights (2019), Unboxing Artificial Intelligence: 10 steps to protect Human Rights –\n   Recommendation, Council of Europe, Strasbourg, May 2019.\n2  Heleen L Janssen (2020), ‘An approach for a fundamental rights impact assessment to automated decision-making, International\n   Data Privacy Law’, International Data Privacy Law,Vol. 10, Issue 1, February 2020, pp. 76-106; Alessandro Mantelero, ‘AI and Big Data:\n   A blueprint for a human rights, social and ethical impact assessment’, Computer Law & Security Review Vol. 34, Issue 4, August 2018, pp.\n   754-772; Edwards, Lilian and Veale, Michael (2017), Slave to the Algorithm? Why a ‘Right to an Explanation’ Is Probably Not the Remedy\n   You Are Looking For (May 23, 2017), 16 Duke Law & Technology Review 18.\n3  AccessNow (2020), Access Now’s submission to the Consultation on the “White Paper on Artificial Intelligence - a European approach\n   to excellence and trust”.\n4  Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States\n   on the human rights impacts of algorithmic systems, 8 April 2020, para 5.2. (human rights impact assessment).\n5  For a detailed discussion with respect to non-discrimination, see: Wachter S., Mittelstatt, B., and Russel C. (2020), Why fairness cannot be\n   automated: bridging the gap between EU non-discrimination law and AI.\n6  Government of Canada, (2019), Directive on Automated Decision-Making.\n7  United Nations, UN Guiding principles on Business and Human Rights, endorsed by Human Rights Council Resolution 17/4, A/HRC/\n   RES/17/4, 6 July 2011, Principles 18, 19, 20.\n8  Heleen L Janssen, An approach for a fundamental rights impact assessment to automated decision-making, International Data Privacy\n   Law, Vol. 10, Issue 1, February 2020, pp. 76-106.\n9  High-Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI, 8 April 2019, Chapter III.\n10 Ibid, p. 15.\n11 See for example: IBM, Everyday Ethics for Artificial Intelligence, 2019; Sony, Sony Group AI Ethics Guidelines, 2019; Vodaphone,\n   Vodaphone’s AI framework, 2019; Arborus International and Orange, International Charter for Inclusive AI, 21 April 2020, signed by more\n   than 40 private companies, including Camfil, Danone, EDF, L’Oréal, Metro, Sodexo, etc.\n12 Information Technology Industry Council (ITI), ITI AI Policy Principles, 2017.\n13 ECP Platform for the Information Society, Artificial Intelligence Impact Assessment, The Netherlands, 14 November 2019.\n14 Amnesty International, Access Now, Human Rights Watch, Wikimedia Foundation, The Toronto Declaration: Protecting the rights to\n   equality and non-discrimination in machine learning systems, 16 May 2018 at RightsCon Toronto; University of Montreal, Montreal\n   Declaration Responsible AI, 2018.\n15 Electrical and Electronics Engineers (IEEE), Global Initiative on Ethics of Autonomous and Intelligent Systems, Ethically Aligned Design:\n   Prioritizing Human Wellbeing with Autonomous and Intelligent Systems, 2019; Future of Life Institute, Asilomar AI Principles, Conference\n   outcome of the Future of Life Institute’s second conference on the future of artificial intelligence, 2017.\n16 See for example: ECP Platform for the Information Society, Artificial Intelligence Impact Assessment, The Netherlands, 14 November\n   2019; IEEE Initiative.\n17 Association for Computer Machinery (ACM), ACM Code of Ethics and Professional Conduct, 22 June 2018.\n18 Future of Humanity Institute, University of Oxford, Standards for AI Governance: International Standards to Enable Global Coordination in\n   AI Research & Development, April 2019.\n19 ISO, Standards by iso/iec jtc 1/sc 42, Artificial intelligence,Sstandard and/or project under the direct responsibility of iso/iec jtc 1/sc\n   42 secretariat, ISO, ISO/IEC TR 24028:2020 standard Information technology — Artificial intelligence — Overview of trustworthiness in\n   artificial intelligence, May 2020. It establishes, among others, the “approaches to assess and achieve availability, resiliency, reliability,\n   accuracy, safety, security and privacy of AI systems.” Other ISO standards under development as of September 2020: ISO/IEC CD\n   23894 Information Technology — Artificial Intelligence — Risk Management, ISO/IEC AWI TR 24027 Information technology — Artificial\n   Intelligence (AI) — Bias in AI systems and AI aided decision making, or ISO/IEC AWI TR 24368 Information technology — Artificial\n   intelligence — Overview of ethical and societal concerns, more information available on ISO’s website; Electrical and Electronics Engineers\n   (IEEE), IEEE P7003™ Algorithmic Bias Considerations; German AI Federal Association (KI Bundesverband), German AI Federal Association:\n   seal of quality (KI Bundesverband Guetesiegel), 22 March 2019.\n20 Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13 October 2017, Annex 2 – Criteria for\n   an acceptable DPIA.\n21 High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17\n   July 2020.\n22 Government of Canada, Algorithmic Impact Assessment Tool, 2019; Danish Institute for Human Rights, Human rights compliance\n   assessment quick check, 7 June 2016.\n23 Center of Government Excellence, Johns Hopkins University, Ethics & Algorithm toolkit, 2018; Government of Canada, Algorithmic Impact\n   Assessment Tool, 2019.\n24 Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13 October 2017, Annex 2 (data\n   protection focus); Danish Institute for Human Rights, Human Rights Impact Assessment Guidance and Toolbox, 2016; AI Pulse, Creating\n   a Tool to Reproducibly Estimate the Ethical Impact of Artificial Intelligence, 26 September 2019.\n25 Fairness, Accountability, and Transparency in Machine Learning (FAT/ML), Principles for Accountable Algorithms and a Social Impact\n   Statement for Algorithms, 2019; FAT/ML, Social Impact Statement for Algorithms, 2019.\n26 High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17\n   July 2020.\n27 See e.g. human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-\n   discrimination and fairness; societal and environmental well-being; accountability. High-Level Expert Group on Artificial Intelligence, Ethics\n   Guidelines for Trustworthy AI, 8 April 2019.\n28 High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17\n   July 2020, p.5.\n29 Government of Canada, Algorithmic Impact Assessment tool, 2019.\n30 Government of Canada, Directive on Automated Decision-Making, 2019, Article 6 an Appendix C.\n31 Center of Government Excellence, Johns Hopkins University, Ethics & Algorithm toolkit, 2018.\n32 Danish Institute for Human Rights, Human rights compliance assessment quick check, 7 June 2016.\n33 Danish Institute for Human Rights, Human Rights Impact Assessment Guidance and Toolbox, 2016.\n34 Heleen L Janssen, An approach for a fundamental rights impact assessment to automated decision-making, International Data Privacy\n                                                                                                                                                  99\n       Law, International Data Privacy Law,Vol. 10, Issue 1, February 2020.\n    35 Alessandro Mantelero, AI and Big Data: A blueprint for a human rights, social and ethical impact assessment, Computer Law & Security\n       Review, Vol. 34, Issue 4, August 2018, pp. 754-772; AI Pulse - Program on Understanding Law, Science, and Evidence (PULSE), UCLA School\n       of Law, Creating a Tool to Reproducibly Estimate the Ethical Impact of Artificial Intelligence, 26 September 2019. This model includes\n       a series of questions for Assessing the Human Rights Impact of AI-Enabled Projects.\n    36 Marion Oswald, Jamie Grace, Sheena Urwin & Geoffrey C. Barnes, Algorithmic risk assessment policing models: lessons from the Durham\n       HART model and ‘Experimental’ proportionality, Journal, Vol. 27, 2018 – Issue 2.\n    37 AINOW, Algorithmic Impact Assessments: a Practical Framework for Public Agency Accountability, April 2018.\n    38 The Institute for Ethical AI & Machine Learning (Ethical ML Network (BETA)), The Machine Learning Maturity Model, 2019.\n    39 Brave (2020), Europe’s governments are failing the GDPR.\n    40 See Wachter S., Mittelstatt, B., and Russel C. (2020), Why fairness cannot be automated: bridging the gap between EU non-\n       discrimination law and AI.\n    41 FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications\n       Office, June 2019.\n100\n6\nMOVING FORWARD: CHALLENGES\nAND OPPORTUNITIES\n                                     This report is published amidst ongoing European legislative and policy\n                                     developments on artificial intelligence and the global fight against the\n                                     coronavirus. The Covid-19 pandemic has potentially quickened acceptance\n                                     of innovative technologies. Yet it has also shown that AI is not the panacea\n                                     to all problems, and comes with various challenges.\n                                     This report clearly shows that using AI systems engages a wide range\n                                     of fundamental rights. It also shows that many businesses and public\n                                     administrations are already using or planning to use AI and related technologies.\n                                     However, these technologies involve different levels of complexity. Most\n                                     examples use relatively simple algorithms. The level of automation also\n                                     varies. Most – but not all – decision making is subject to human review.\n                                     The applications currently used are also often only in the development\n                                     stage. EU and national legislators and policymakers should keep this reality\n                                     in mind – especially when presented with optimistic expectations of AI’s\n                                     potential vis-à-vis the challenges related to using new technologies and the\n                                     need to regulate them.\n“We try to look into the future. We  The vast majority of public administrations and businesses interviewed plan\nwill automate more and more.”        to keep working on or using AI. Only two interviewees indicated that they\n(Private company, Estonia)           will not further use or develop AI. Another two interviewees are cautious.\n                                     They plan to wait and see what others are doing, including because of a lack\n                                     of resources for further work on using AI.\n“The next steps are related to\ntransparency and open data: that is  However, most said that they will further develop or continue to test tools\nto say, publish not only information and (data) infrastructure with respect to the use of AI. This includes starting\nin pdf, but also information in      new or continuing ongoing pilots, evaluating existing efforts, sharing data\nreusable formatting so that it could and results with others, increasing data quality, or trying to obtain other\nbe reused internally and by the      data sources.\nprivate sector.”\n (Public administration, Spain)      Some interviewees mentioned that they are engaged in ongoing debates and\n                                     expressed the desire to contribute to the further development of legislation.\n                                     They still see the current situation – the absence of harmonised law in the\n“AI is a great thing but we must\n                                     area – as an obstacle to the further use of AI. In addition, some respondents\nlearn to use it.”\n                                     said they are working on issues linked to the interpretability of AI. This\n(Private company, Spain)\n                                     means that they are working on methods that enhance understanding and\n                                     explanation of decisions based on more complex AI. Some indicated a desire\n                                     to look more closely into ethical and legal matters.\n                                     Figure 7 shows correlations of words interviewees often use when talking\n                                     about their future use of AI. The figure indicates topics that are often raised.\n                                     For example, interviewees often used the term ‘data’ when discussing future\n                                     developments.\n                                                                                                                       101\n    FIGURE 7:     CORRELATIONS OF WORDS RESPONDENTS MOST OFTEN\n                  MENTION WHEN DISCUSSING FUTURE PLANS TO USE AI\n                                                                   technologies\n                                                                                         systems\n                                                    plans                                                    data\n                                  development                            company\n                                ai\n                                                                                                     tool             police\n                                   management\n                                                             solutions\n                                                                                   companies\n               level\n                              system                                                                                  project\n                                                                                                                                    technology\n                                                                                              administration\n                                                                               tax                            analysis\n             organisation                                    service\n                                                                              processing                                        lot\n                                        translation    process\n                     related\n                                                                  customer\n                                            easy                                                                   national          phase\n                                                   payment\n                     information\n                                                                services    application                                             time\n                              quality                                                                              improve\n                                              learning\n               business                                                  human                   energy\n                               algorithms                              decisions                                        support\n                                             machine\n                                                                 ml                                             continue\n                                                     potential\n                                                                                   people      develop\n                                                               future\n    Notes:\t\u0007Based on text from interview summaries, when respondents spoke about\n              their future use of AI, including words mentioned at least ten times. The\n              lines connecting words indicate the strength of word correlations within\n              text passages. The size of the dots indicate the frequency of the words\n              used.\n    Source: FRA, 2020\n    Effectively and adequately protecting fundamental rights in the EU is a key\n    objective of the current efforts to better regulate the use of AI. In the context\n    of upcoming EU legislation on AI, the European Commission’s White Paper\n    addresses current gaps, helping to mitigate the uncertainty around the use\n    of AI with respect to fundamental rights, and making the use of AI more\n    transparent and accountable in terms of fundamental rights. It includes\n    requirements for AI use that directly link to the information needed to assess\n    the impact of AI on fundamental rights, as discussed above.\n    Requirements linked to the description of training data, data and record\n    keeping, information to be provided to those subjected to AI, robustness and\n    accuracy, as well as human oversight are all highly relevant when assessing\n    and protecting fundamental rights. In this respect, the body of evidence\n    presented in this report offers general insights into how different technologies\n    can affect fundamental rights and what safeguards are needed to ensure\n    fully fundamental rights-compliant use of AI in practice.\n    At the same time, further research into the fundamental rights implications\n    of the use of AI in specific areas will further support policy and legislative\n    efforts at the EU level aiming to shape Europe’s digital future more widely.\n102\nFRA will continue to look into the fundamental implications of AI by carrying\nout more focussed analysis of specific use cases. To increase knowledge\non what can potentially go wrong and consequently help mitigate and\nprevent fundamental rights violations, FRA will look into potential simulation\nstudies. These can showcase how biased algorithms can negatively affect\nfundamental rights.\nThe use of AI often involves automating tasks that were previously carried\nout by humans. Here we need to acknowledge that human behaviour is\nsometimes not in line with fundamental rights, both when using AI and when\nnot using AI. For example, the police might engage in unlawful profiling.\nDecisions by public administration or companies might sometimes be driven\nby negative stereotypes.\nCurrent developments in the use of AI need to acknowledge the potential for\ndiscrimination with respect to the data on which an AI system is built, and\nwith respect to the underlying assumptions that humans in turn may feed\ninto the development and deployment of a system. Automating certain tasks\nwithout fully understanding what is being automated could lead to unlawful\nprocessing of data, the use of technology that treats people unfairly, and might\nmake it impossible to challenge certain outcomes – to name some challenges.\nHowever, the increased availability of data and technological tools can also\nbe used to better understand where and how unequal treatment occurs.\nCurrent technological developments and the increased availability of data\nalso provide a unique opportunity to better understand the structures of\nsociety, which can be used to support fundamental rights compliance. The\nopportunities created by AI can also contribute to better understanding and\nconsequently mitigation of fundamental rights violations.\n                                                                                 103\nGetting in touch with the EU\nIn person\nAll over the European Union there are hundreds of Europe Direct information centres.\nYou can find the address of the centre nearest you at:\nhttps://europa.eu/european-union/contact_en\nOn the phone or by email\nEurope Direct is a service that answers your questions about\nthe European Union. You can contact this service:\n—b  \u0007 y freephone: 00 800 6 7 8 9 10 11\n    (certain operators may charge for these calls),\n— at the following standard number: +32 22999696 or\n— by email via: https://europa.eu/european-union/contact_en\nFinding information about the EU\nOnline\nInformation about the European Union in all the official languages of the EU is available\non the Europa website at: https:// europa.eu/european-union/index_en\nEU publications\nYou can download or order free and priced EU publications at: https://op.europa.eu/\nen/publications. Multiple copies of free publications may be obtained by contacting\nEurope Direct or your local information centre (see https://europa.eu/european-union/\ncontact_en).\nEU law and related documents\nFor access to legal information from the EU, including all EU law since 1952 in all the\nofficial language versions, go to EUR- Lex at:\nhttp://eur-lex.europa.eu\nOpen data from the EU\nThe EU Open Data Portal (http://data.europa.eu/euodp/en) provides access to datasets\nfrom the EU. Data can be downloaded and reused for free, for both commercial and\nnon-commercial purposes.\n\nPROMOTING AND PROTECTING\nYOUR FUNDAMENTAL RIGHTS\nACROSS THE EU\n―\nArtificial intelligence (AI) already plays a role in deciding what\nunemployment benefits someone gets, where a burglary is likely to\ntake place, whether someone is at risk of cancer, or who sees that\ncatchy advertisement for low mortgage rates. Its use keeps growing,\npresenting seemingly endless possibilities. But we need to make sure\nto fully uphold fundamental rights standards when using AI.\nThis report presents concrete examples of how companies and public\nadministrations in the EU are using, or trying to use, AI. It focuses on\nfour core areas – social benefits, predictive policing, health services\nand targeted advertising.\nThe report discusses the potential implications for fundamental rights\nand analyses how such rights are taken into account when using\nor developing AI applications. In so doing, it aims to help ensure\nthat the future EU regulatory framework for AI is firmly grounded in\nrespect for human and fundamental rights.\n   EU Charter of\nFundamental Rights Access to justice Non-discrimination Information society\nFRA – EUROPEAN UNION AGENCY FOR FUNDAMENTAL RIGHTS\nSchwarzenbergplatz 11 – 1040 Vienna – Austria\nTEL. +43 158030-0 – FAX +43 158030-699\nfra.europa.eu\n       facebook.com/fundamentalrights\n       twitter.com/EURightsAgency\n       linkedin.com/company/eu-fundamental-rights-agency\n","count":1},{"name":"3 others","count":3}]}},{"name":"cleanText","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"application ai insurtech real estate technology introduction insurtech professor christopher geczy phd introduction insurtech * insurance industry global diversified across applications subsegments * gross written premium gwp trillion * amount insurance written including commissions costs * net income worldwide grew last year vs actual growth ernst young \"global insurance trends analysis \" june introduction insurtech * insurance industry needs respond technological change disruption * wearables * driverless vehicles * internet things * \"big data\" * natural language processing * blockchain * distributed ledger technologies * climate change * etc * new technologies offer opportunities increase efficiency industry serve new markets ernst young \"global insurance trends analysis \" june introduction insurtech * standardized definition insurtech * said revolutionizing insurance industry changing way insurers business milken institute \"insurtech rising profile insurtech landscape \" december \" insurtech described \" insurance company intermediary insurance value chain segment specialist utilizes technology either compete provide valued added benefits insurance industry \" sia partners key emerging technologies leveraged insurtech companies source capgemini world insurtech report application ai insurtech real estate technology emerging technologies ai machine learning professor christopher geczy phd key emerging technologies leveraged insurtech companies * artificial intelligence * descriptor software perform functions ordinarily associated human reasoning * iterative learning * self awareness emotions * insurers hope exploit ai chatbots * allstate' allstate business insurance expert abie * provides answers real time customer owners questions * insurers followed source \" insurtech insurers using \" https www thebalancesmb com insurtech graphic accenture presentation \"accenture' technology vision insurance \" key emerging technologies leveraged insurtech companies machine learning * enables computers learn time * using algorithms mathematical models simulate neural networks human brain * allows computers extract patterns raw data rather following specific instructions * gives appearance closer activities human brain * insurance companies amass large amounts data * yet according national association insurance commissioners insurers use data collect source \" insurtech insurers using \" https www thebalancesmb com insurtech key emerging technologies leveraged insurtech companies * machine learning could allow insurers mine data effectively extract valuable information * risk modeling analyze claims data predict risk future losses * demand modeling predict demand products future estimate premiums * detecting fraud identify patterns behavior obvious human adjusters * processing claims automate claim reporting processing * underwriting help underwriters analyze data collected applicants * computers aid decision making process flag risks inconsistencies data underwriters might able see * also check external sources social media verify accuracy data source \" insurtech insurers using \" https www thebalancesmb com insurtech application ai insurtech real estate technology redefining insurance industry professor christopher geczy phd redefining insurance industry product design selling marketing front office underwriting policy administration claims management insurtech redefining insurance industry policy claims product design front office underwriting administration management marketing actuarial models underwriting policy acquisition claims servicing distribution product design new policies servicing payout channel management * deg view * extended multi * real time * segment market * streamlined claims customer' needs device mobility information based servicing process low * personalized offering capturing desires waiting time product designs * integrated * advanced risk * automated systems * instant notification * design new omnichannel analytics enabling straight claim products offerings touch risk based pricing processing proactive status * adjust products points * automated workflow stp capabilities updates real time * real time updates management rules * automated * real time claims * disaggregate interactions engines premium reminders status monitoring product mix clients * customer value led renewal notice * advanced analytics seamlessly * quick identification promotions * anytime access based fraud * design deliver cross selling discounts policy details view detection products end selling * digitized systems customer want opportunities less reliance * detecting client data customer satisfaction needs provide * information availability price transparency source capgemini world insurtech report application ai insurtech real estate technology classification insurtech companies professor christopher geczy phd segmentation insurtech firms * different classification methods insurtech firms initiatives rotate similar concepts * \"traditional\" view full stack agents brokers * nuanced view sub segments carriers enablers distributors segmentation insurtech firms * milken institute three main classifications * full stack insurers platforms underwrite policies assume risk cases manage process beginning end * agents platforms act behalf carrier essentially acting extension incumbent carrier * brokers platforms provide customers variety policies offered incumbent carriers insurgent insurtech platforms * may may paid commission based policies sold platform * may require customers scroll policies offered automatically connect customers preferred policy algorithms employed based user' response set questions milken institute \"insurtech rising profile insurtech landscape \" december segmentation insurtech firms * make classification yes insurer full insurance one agent company partnered one multiple insurance companies multiple broker milken institute \"insurtech rising profile insurtech landscape \" december classification insurtech firms capgemnini * second classification system used capgemini * categorizes insurtech providers role distribution chain * enablers * distributors * full carriers source capgemini world insurtech report classification insurtech firms capgemnini source capgemini world insurtech report segmentation insurtech firms model insurtech platform models number insurtechs ful l stack agent broker insurtech platform models milken institute \"insurtech rising profile insurtech landscape \" december product offerings h ut ea lth nc l li nd bi fe ke iv u f ni ily rs al h al om te th rm ... e r u en ni li nc nk l ed tit ... le ab ili su ty ra b p nc us er e es na e ia nd nc bi ow l lit sh ... ip en pi ng r et tu tr ir em rn av en ... el product offering categories nc p l en si pe ri ci al p ty su ra nc nc l e bi ... cy cl type insurtech platform product offering e ob ile ... segmentation insurtech firms model product milken institute \"insurtech rising profile broker agent full stack insurtech landscape \" december segmentation insurtech firms * significant portion approximately insurtech companies could better described technology solution providers * human resources earned benefits solution providers * data solution providers * infrastructure solution providers milken institute \"insurtech rising profile insurtech landscape \" december segmentation insurtech firms * human resources earned benefits solution providers * platforms using deploying technology help firms manage human capital efficiently cost effectively * data solution providers * platforms specialize collecting aggregating analyzing vast quantities data support insurance carriers startups stakeholders * infrastructure solution providers * platforms focus making back end processes efficient use application programming interfaces apis provide means platforms integrate build customizable insurance products services milken institute \"insurtech rising profile insurtech landscape \" december number technology solution providers insurtech market insurtech platform models number providers human resources data solution provider infrastructure solution earned benefits solution provider provider small mild large businesses milken institute \"insurtech rising profile type tech solution providers insurtech landscape \" december examples full carrier insurtech firms insurtech firm products services offered example type zhongan chinese property insurer uses traditional insurance model digital carrier online channel sell products handle conducted online mobile claims risk sharing network group associated individuals pools inspeer france based community insurance p2p insurer premiums insure risk platform allows members pool money within full carriers generally stands benefit group covering group member' deductible regarding premium returns smaller insurance packages leveraging mobile technology bima offers micro insurer lower premiums typically lower affordable insurance products low income coverage populations emerging markets demand insurance coverage new york based sure offers demand personal demand insurer purchased online well episodic policies user buy either via via mobile apps website app us based metromile offers auto insurance usage based premiums prices per usage risky fees based number miles insured' insurer behavior displayed customer car logs source capgemini world insurtech report examples full distributor insurtech firms policybazaar specializes comparative analysis online site enables individuals marketplace products various insurers based price compare plans different insurers quality key benefits artificially intelligent insurance advisory application one stop app allows customers brolly delivers contextually relevant insights personal financial manage policies obtain web mobile applications customers assistant coverage recommendations manage policies one place know compare purchase plans distributors coverage may duplicated missing online platform allows customers licensed broker coverfox offers insurance products digital broker compare purchase policies vehicles home health services travel online site enables commercial coverhound us based insurtech firm offers b2b digital customers compare plans comparison platform personal small distributor different insurers commercial insurance products london based bought many uses social media customized flexible front office data connect people similar insurance needs value adding solutions via partnership uses group' collective buying power intermediary insurer reinsurer risk management negotiate insurers deals ' available individuals source capgemini world insurtech report examples full enabled insurtech firms premfina' white label solution brokers allows front office solution process improvement solutions extend premium financing options providers front office customers manage insurance policies riskgenius applies artificial intelligence streamline policy plan process improvement solutions work insurance professionals retrieving management underwriting policy plan details specific coverage exclusion analyzing solution provider administration policies extracting relevant information premium limit deductible enablers rightindem white label self service insurance claims management process improvement solutions claims platform insurers allows customers solution provider specifically claims management interact claim time carpe data leverages fata various channels online content social media connected data capture analytics solutions data specialist devices offers predictive scoring data use cases across value chain products insurers enabling predict risk better solutions based specific technology betterview drone technology specialist allows technology blockchain source capgemini world insurtech report specialist drone based inspection property assessment drones source capgemini world insurtech report application ai insurtech real estate technology investment market size insurtech industry professor christopher geczy phd insurtech market size * ' clear ' large growing * global insurtech market revenue 7mm * market revenue expected reach billion cagr * asiapacific highest regional cagr growing financial hubs hong kong singapore india * health insurance segment expected higher segment cagr * total insurtech investments billion * total deals deals billion * involved insurer reinsurer investor * estimated year cagr orbis research \"global insurance technology insurtech market size \" dec ernst young \"global insurance trends analysis \" june size insurtech market global private investment vc pe insurtech capital invested b deal count source kpmg international pulse fintech global analysis investment fintech january insurtech startup count number insurtech startups exploded years source sma - strategy meets action insurtech disruptors top insurtech disruptors insurtech key techs * insurers moving key business functions cloud * tech research firm ovum' annual survey penetration software service saas grew last year source ovum survey data cited deloitte \" insurance industry outlook\" insurtech key techs source ovum survey data cited deloitte \" insurance industry outlook\" insurtech key techs * aia hong kong launched blockchain app share life policy data bank distributors * axa europe offering flight delay insurance blockchain platform featuring smart contracts * ovum' annual survey penetration software service saas grew last year * carriers consortiums expected launch impactful blockchain initiatives due concerns around data technology profile insurtech market * u insurtech deals total value billion announced q4 * compared q4 deal count q4 increased funding volume also increased * transactions q4 - higher q3 lower q1 q2 * globally * uk investment last quarter * china second largest investor q4 u * uk responsible total investment since * investment international markets remains strong transactions outside u account total transactions since 4th quarter cbinsights quarterly insurtech briefing q4 profile insurtech market * early stage investments remain strong * seed series account total transactions since quarter last quarter * insurtech funding maturing mid - later stages - financings took place series b c stages * could lead consolidation cbinsights quarterly insurtech briefing q4 profile insurtech market * property casualty p c funding volume increased q3 increased q4 * p c transactions quarter marginally higher transactions q3 marks * life health l h funding volume increased q3 marked increase q4 * hits record level insurtech investment driven large investments deal count increased q3 funding volume increased cbinsights quarterly insurtech briefing q4 size insurtech market quarterly insurtech funding volume - stages millions property casualty life health q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 deal count p c l h cbinsights quarterly insurtech briefing q4 size insurtech market quarterly insurtech transaction target country united states united states united kingdom china - q4 china united kingdom q4 germany germany india south africa france q4 transactions q4 transactions quarterly insurtech transaction investments stage seed angel seed angel - q4 series series q4 series b series b series c series c series series series e series e q4 transactions q4 transactions cbinsights quarterly insurtech briefing q4 size private technology investment insurers reinsurers private technology investments insurers q1 q2 q3 q4 cbinsights quarterly insurtech briefing q4 application ai insurtech real estate technology insurtech fintech financial inclusion professor christopher geczy phd microinsurance * investment firm omidyar network * started ebay founder pierre omidyar * model combines profit llc grantmaking c * 5b committed since inception pierre pam omidyar * 676mm profit investments 782mm grants total commitments year ytd https www omidyar com financials microinsurance * common feature microinsurance products low income low net worth population served otherwise limited access insurance * microinsurance products type group individual coverage * term life * health accident disability * casualty crop insurance livestock theft fire natural disaster * certain forms retirement savings plans * microinsurance underwriter delivery channel type * large multinational insurance companies * credit unions mutual associations * government ngos * small community organizations microinsurance * benefits microinsurance * financial protection risk pooling * insureds assume risk * crop insurance vs drought enables small farmers plant crops higher yields \"good\" years poorer yields drought years * safeguard vs families falling back poverty due illness death breadwinner housing destroyed etc * indian ministry health found one quarter hospitalizations pushed individual family poverty due cost treatment tina rosenberg \" microinsurance revolution \" new york times opinionator blog june http opinionator blogs nytimes com microinsurance revolution microinsurance * benefits microinsurance * target specific risk populations * hiv positive living flood zone microentrepreneurs * complement social welfare programs bolster microfinance initiatives * assign term life policy secure business education mortgage loan tina rosenberg \" microinsurance revolution \" new york times opinionator blog june http opinionator blogs nytimes com microinsurance revolution microinsurance * bima - insurance business disruptor \"insurtech\" leader * swedish founded mobile insurance health company provides accident life health insurance products million low income consumers countries africa asia latam * largest markets ghana sri lanka bangladesh pakistan * mobile technology lowered prices brought affordable insurance world' poorest techcrunch https techcrunch com bima raises 97m allianz microinsurance aimed emerging markets bima website http www bimamobile com bima us new microinsurance * business model bima provides microinsurance subscription via basic mobile phone service little 60c month \"pay go\" rolling monthly cover * offers payouts family insured person dies * sign takes minutes payments collected basic mobile phone service * customer growth thousand new customers month bima * customers live less day bima * subscribers never insurance * educating customers top priority * 300mm valuation dec sale leapfrog investments' stake allianz 97mm techcrunch https techcrunch com bima raises 97m allianz microinsurance aimed emerging markets bima website http www bimamobile com bima us new size private technology investment insurers reinsurers private technology investments insurers target country united states united states france - q4 china china q4 united kingdom germany germany united kingdom canada q4 transactions q4 transactions private technology investments insurers investment stage seed angel seed series series - q4 series b series b q4 series c series c series series e series e q4 transactions q4 transactions cbinsights quarterly insurtech briefing q4","count":1},{"name":"getting future right -- artificial intelligence fundamental rights report (c) european union agency fundamental rights reproduction authorised provided source acknowledged use reproduction photos material european union agency fundamental rights copyright permission must sought directly copyright holders neither european union agency fundamental rights person acting behalf agency responsible use might made following information luxembourg publications office european union print isbn doi tk en c pdf isbn doi tk en n (c) photo credits cover hquality adobe stock page copyright (c) coded bias rights reserved page mimi potter adobe stock page siberian art adobe stock page monsitj adobe stock page good studio adobe stock page monsitj adobe stock page sikov adobe stock page mykola mazuryk adobe stock page robsonphoto adobe stock page metamorworks adobe stock page thodonal adobe stock page gorodenkoff adobe stock page blackboard adobe stock page dimco adobe stock page blackboard adobe stock page videoflow adobe stock page monopoly919 adobe stock page zapp2photo adobe stock page gorodenkoff adobe stock page bestforbest adobe stock page freedomz adobe stock page zapp2photo adobe stock page copyright (c) coded bias rights reserved page european communities page copyright (c) coded bias rights reserved page blacksalmon adobe stock foreword know artificial intelligence already plays role deciding unemployment benefits someone gets burglary likely take place whether someone risk cancer sees catchy advertisement low mortgage rates speak artificial intelligence ai machines kind things people used able today ai present lives realise - use keeps growing possibilities seem endless fully uphold fundamental rights standards using ai report presents concrete examples companies public administrations eu using trying use ai discusses potential implications fundamental rights shows whether using ai taking rights account fra interviewed hundred public administration officials private company staff well diverse experts - including supervisory oversight authorities non governmental organisations lawyers - variously work ai field based interviews report analyses fundamental rights taken consideration using developing ai applications focuses four core areas - social benefits predictive policing health services targeted advertising ai uses differ terms complex much automation involved potential impact people widely applied findings underscore lot work lies ahead - everyone one way foster rights protection ensure people seek remedies something goes awry need know ai used also means organisations using ai need able explain ai systems deliver decisions based yet systems issue truly complex using ai systems responsible regulating use acknowledge always fully understand hiring staff technical expertise key awareness potential rights implications also lacking know data protection concern refer non discrimination less aware rights - human dignity access justice consumer protection among others - also risk surprisingly developers review potential impact ai systems tend focus technical aspects tackle challenges let' encourage working human rights protection working ai cooperate share much needed knowledge - tech rights develop use ai also need right tools assess comprehensively fundamental rights implications many may immediately obvious accessible fundamental rights impact assessments encourage reflection help ensure ai uses comply legal standards interviews suggest ai use eu growing still infancy technology moves quicker law need seize chance ensure future eu regulatory framework ai firmly grounded respect human fundamental rights hope empirical evidence analysis presented report spurs policymakers embrace challenge michael 'flaherty director contents foreword key findings fra opinions ai fundamental rights - relevant policymaking report mean artificial intelligence ai fundamental rights eu policy framework moving towards regulation endnotes putting fundamental rights context - selected use cases ai eu examples ai use public administration examples ai use private sector endnotes fundamental rights framework applicable ai fundamental rights framework governing use ai 'use case' examples requirements justified interferences fundamental rights endnotes impact current use ai selected fundamental rights perceived risks general awareness fundamental rights legal frameworks ai context human dignity right privacy data protection - selected challenges equality non discrimination access justice right social security social assistance consumer protection right good administration endnotes fundamental rights impact assessment - practical tool protecting fundamental rights calling fundamental rights impact assessment - available guidance tools impact assessments testing practice fundamental rights impact assessment practice endnotes moving forward challenges opportunities figures figure companies using ai member state figure examples different automation complexity levels use cases covered figure words interviewees often used describe ai 'use cases' figure awareness gdpr right opt direct marketing eu united kingdom country region figure awareness right say decisions automated age gender difficulty paying bills figure awareness risks discrimination using ai country figure correlations words respondents often mention discussing future plans use ai key findings fra opinions new technologies profoundly changed organise live lives particular new data driven technologies spurred development artificial intelligence ai including increased automation tasks usually carried humans covid health crisis boosted ai adoption data sharing - creating new opportunities also challenges threats human fundamental rights developments ai received wide attention media civil society academia human rights bodies policymakers much attention focuses potential support economic growth different technologies affect fundamental rights received less attention date yet large body empirical evidence wide range rights ai implicates safeguards needed ensure use ai complies fundamental rights practice february european commission published white paper artificial intelligence - european approach excellence trust outlines main principles future eu regulatory framework ai europe white paper notes vital framework grounded eu' fundamental values including respect human rights - article treaty european union teu report supports goal analysing fundamental rights implications using artificial intelligence based concrete 'use cases' ai selected areas focuses situation ground terms fundamental rights challenges opportunities using ai overarching fundamental rights framework applies use ai legal eu consists charter fundamental rights eu charter well framework european convention human rights multiple council europe international human rights instruments relevant include universal declaration human rights major un human rights conventions addition sector specific secondary eu law notably eu data protection acquis eu non discrimination legislation helps safeguard fundamental rights context ai finally national laws eu member states also apply see fra bringing rights life fundamental rights landscape european union luxembourg publications office european union major conventions include international covenant civil political rights international covenant economic social cultural rights international convention elimination forms racial discrimination convention elimination forms discrimination women convention torture convention rights child convention rights persons disabilities international convention protection persons enforced disappearance universal international human rights law framework including enforcement mechanisms see e g de schutter international human rights law cases materials commentary cambridge cambridge university press 2nd edition report based interviews officials public administration staff private companies selected eu member states asked use ai awareness fundamental rights issues involved practices terms assessing mitigating risks linked use ai moreover interviews conducted experts deal various ways potential fundamental rights challenges ai group included public bodies supervisory oversight authorities non governmental organisations lawyers safeguarding fundamental rights - scope impact assessments accountability considering full scope fundamental rights respect ai fra opinion using ai systems engages wide range fundamental introducing new policies rights regardless field application adopting new legislation ai include - also go beyond - privacy data protection eu legislator member states non discrimination access justice acting within scope eu law must ensure respect full eu charter fundamental rights charter spectrum fundamental rights became legally binding december enshrined charter eu legal value eu treaties brings together treaties taken account specific civil political economic social rights single text fundamental rights safeguards need pursuant article charter institutions accompany relevant policies laws bodies offices agencies union respect eu member rights embodied charter eu member states rely robust evidence states implementing union concerning ai' impact fundamental law applies equally ai field rights ensure restrictions fieldwork research shows large certain fundamental rights respect variety systems used heading ai principles necessity technologies analysed entail different levels proportionality automation complexity also vary terms relevant safeguards need scale potential impact people provided law effectively fra' findings show using ai systems implicate protect arbitrary interference wide spectrum fundamental rights regardless fundamental rights give field application include also go beyond legal certainty ai developers privacy data protection non discrimination users voluntary schemes access justice yet addressing impact ai observing safeguarding respect fundamental rights interviews show fundamental rights development scope often delimited specific rights use ai help mitigate rights violations line wider range rights need considered minimum requirements legal clarity using ai depending technology area use - basic principle rule addition rights concerning privacy data protection law prerequisite securing equality non discrimination access justice fundamental rights - legislator rights could considered include take due care defining example human dignity right social security scope ai law social assistance right good administration mostly relevant public sector consumer protection given variety technology particularly important businesses depending subsumed term ai context ai use right protected lack knowledge full charter needs consideration scope potential fundamental rights impact legal definition ai related terms might need assessed regular basis using effective impact assessments prevent negative effects fra opinion prior impact assessments mainly focus technical eu legislator consider making issues rarely address potential effects mandatory impact assessments fundamental rights knowledge cover full spectrum fundamental rights cover private ai affects rights lacking public sectors applied deploying ai systems engages wide spectrum ai system used fundamental rights regardless field application impact assessments take pursuant article charter eu member states account varying nature scope must respect rights embodied charter ai technologies including level implementing union law line existing automation complexity well international standards - notably united national potential harm include guiding principles business human rights ungps basic screening requirements - businesses place \" human rights due also serve raise awareness potential diligence process identify prevent mitigate account fundamental rights implications address impacts human rights\" impact assessments draw principles irrespective size established good practice sector encompasses businesses working ai fields regularly repeated pursuing commitments ungps eu deployment appropriate adopted several legislative acts addressing sector assessments conducted specific instruments particular context due transparent manner outcomes diligence related obligations human rights discussions recommendations currently underway proposing new eu secondary public domain extent possible law law would require businesses carry due aid impact assessment process diligence potential human rights environmental companies public administration impacts operations supply chains law required collect would likely cross sectoral provide sanctions information needed thoroughly non compliance - encompass use assessing potential fundamental ai see fra' recent report business human rights rights impact - access remedy calls improved horizontal eu member states human rights diligence rules eu based companies consider targeted actions support impact assessments important tool businesses developing using planning public administration alike mitigate potential use ai systems ensure effective negative impact activities fundamental rights compliance fundamental eu law specific sectors requires forms impact rights impact assessment obligations assessments data protection impact assessments actions could include funding general data protection regulation gdpr guidelines training awareness many interviewees reported data protection impact raising particularly - assessment required law conducted however exclusively - target private took different forms moreover prior assessments sector conducted focus mainly technical aspects eu member states rarely address potential impacts fundamental rights consider using existing tools according interviewees fundamental rights impact checklists self evaluation tools assessments carried ai system developed european international appears affect fundamental rights negatively level include developed research shows interviewees' knowledge eu high level group artificial fundamental rights - data protection intelligence extent non discrimination - limited majority acknowledge however use ai impact fundamental rights interviewees indicate systems affect fundamental rights extent linked tasks ai systems used respondents aware data protection issues respondents also realise discrimination could - generally - problem ai used however exact meaning applicability rights related data protection non discrimination remains unclear many respondents research findings show differences private public sector interviewees private sector often less aware wider range fundamental rights could affected data protection issues known private sector however rights non discrimination access justice related rights less well known among business representatives work ai fully aware potential problems others said responsibility checking fundamental rights issues lies clients ensuring effective oversight overall accountability fra opinion businesses public administrations developing using ai contact various eu member states ensure bodies responsible overseeing ai related effective accountability systems place systems within respective mandates sectors monitor needed effectively address negative impact ai systems bodies include data protection authorities fundamental rights consider using ai always sure bodies addition fundamental rights impact responsible overseeing ai systems assessments see fra opinion introducing specific safeguards ensure line well established international human rights accountability regime effective could standards - example article european include legal requirement make available convention human rights echr article enough information allow assessment charter - states obliged secure people' rights fundamental rights impact ai systems freedoms effectively comply states - among would enable external monitoring others - put place effective monitoring enforcement human rights oversight competent bodies mechanisms applies equally respect ai eu member states also level monitoring findings point make better use existing oversight expert important role specialised bodies established specific structures protect fundamental rights sectors also responsible ai oversight within using ai include data protection mandates include example oversight authorities equality bodies national human area banking data protection authorities rights institutions ombuds institutions variety bodies potentially relevant consumer protection bodies oversight ai fundamental rights perspective additional resources earmarked however responsibilities bodies concerning establish effective accountability systems oversight ai remains unclear many 'upskilling' diversifying staff working interviewed private public sector oversight bodies would allow public administrations' use ai sometimes audited deal complex issues linked developing part regular audits private companies using ai specific sectors also specialised oversight bodies similarly appropriate bodies example area health financial services equipped sufficient resources powers also check use ai related technologies - importantly - expertise prevent example part certification schemes private assess fundamental rights violations sector interviewees expressed wish bodies could effectively support whose fundamental provide expert advice possibilities legality rights affected ai potential ai uses facilitating cooperation appropriate eu well developed set independent bodies national european level help bodies mandate protect promote fundamental share expertise experience engaging rights include data protection authorities equality actors relevant expertise - bodies national human rights institutions ombuds specialist civil society organisations - institutions research shows using also help implementing actions planning use ai often contacted different bodies national level member states consider use ai consumer protection bodies using available eu funding mechanisms often users ai contacted data protection authorities seek guidance input approval personal data processing involved interviewed experts highlight relevance data protection authorities overseeing ai systems respect use personal data however also note data protection authorities resourced task lack specific expertise ai issues experts including working oversight bodies equality bodies data protection authorities agree expertise existing oversight bodies needs strengthened allow provide effective oversight ai related issues according experts challenging given bodies' resources already stretched also highlighted important role relevant civil society organisations specialised fields technology digital rights algorithms enhance accountability use ai systems non discrimination data protection access justice three horizontal themes research shows use ai affects various fundamental rights apart context related specific aspects affect different rights varying extent fundamental rights topics emerged research repeatedly apply ai cases include need ensure non discriminatory use ai right discriminated requirement process data legally right personal data protection possibility complain ai based decisions seek redress right effective remedy fair trial two main fundamental rights highlighted interviews data protection non discrimination addition effective ways complain use ai came repeatedly linked right fair trial effective remedy following three fra opinions reflect findings read alongside opinions call comprehensive recognition response full range fundamental rights affected ai specific safeguards ensure non discrimination using ai fra opinion interviewees rarely mentioned carrying detailed assessments potential discrimination using ai eu member states consider encouraging companies public suggests lack depth assessments administration assess potentially discrimination automated decision making discriminatory outcomes using ai systems obligation respect principle non european commission member discrimination enshrined article teu states consider providing funding article tfeu requiring union combat targeted research potentially discrimination number grounds articles discriminatory impacts use ai charter equality law non algorithms research would discrimination range grounds specific benefit adaptation established detailed provisions several eu directives also enshrine research methodologies social principle varying scopes application sciences employed identify automation use ai greatly increase potential discrimination different areas efficiency services scale tasks - ranging recruitment customer humans would able undertake however profiling necessary ensure services decisions based building results research ai discriminatory recognising european guidance tools support commission recently highlighted need additional using ai detect possible discriminatory outcomes developed legislation safeguard non discrimination using ai eu anti racism action plan interviewees principle aware discrimination might happen yet rarely raised issue believe systems could actually discriminate interviewees also rarely mentioned detailed assessments potential discrimination meaning lack depth assessment potential discrimination common perception omitting information protected attributes gender age ethnic origin guarantee ai system discriminate necessarily true however information potentially indicating protected characteristics proxies often found datasets could lead discrimination certain cases ai systems also used test detect discriminatory behaviour encoded datasets however interviewees mentioned possibility collecting information disadvantaged groups detect potential discrimination absence depth analysis potential discrimination actual use ai systems also almost discussion analysis potential positive effect using algorithms make decisions fairer moreover none interviewees working ai mentioned using ai detect possible discrimination positive outcome sense discrimination better detected data analysed potential bias since detecting potential discrimination use ai algorithms remains challenging interviewees briefly addressed issue different measures needed address include requirement consider issues linked discrimination assessing use ai investment studies potential discrimination use diverse range methodologies could involve example discrimination testing could build similar established methodologies testing bias everyday life respect job applications applicant' name changed indirectly identify ethnicity relation ai applications tests could involve possible creation fake profiles online tools differ respect protected attributes way outcomes checked respect potential discrimination research could also benefit advanced statistical analysis detect differences datasets concerning protected groups therefore used basis exploring potential discrimination finally research interviews underscored results complex machine learning algorithms often difficult understand explain thus research better understand explain results called 'explainable ai' also help better detect discrimination using ai guidance data protection clarity needed scope meaning fra opinion legal provisions regarding automated decision making european data protection board data protection critical development use edpb european data ai article charter article tfeu protection supervisor edps provide everyone right protection consider providing guidance personal data gdpr law enforcement support effectively implement directive directive eu elaborate gdpr provisions directly apply right include many provisions applicable use ai safeguarding use ai fundamental rights particular regards meaning personal data interviewees indicated ai systems use ai including ai training employ use personal data meaning data protection datasets affected many different ways however applications - according interviewees - high level uncertainty use personal data use anonymised data concerning meaning automated hence data protection law would apply personal decision making right data used data protection related principles human review linked use ai provisions apply automated decision making thus edpb edps also report highlights important issue linked data consider clarifying concepts protection also relevant fundamental 'automated decision making' rights respect automated decision making 'human review' according eurobarometer survey mentioned eu law europeans know say decisions automated knowledge right considerably addition national data protection higher among working ai - majority bodies provide practical interviewees raised issue however many guidance data protection interviewees including experts argued clarity provisions apply use needed scope meaning legal provisions ai guidance could include automated decision making recommendations checklists based concrete use cases ai area social benefits interviewees mentioned support compliance data one example fully automated rule based decisions protection provisions applications mentioned reviewed humans interviewees public administration stressed importance human review decisions however rarely described human review actually involves information used reviewing output ai systems interviewees disagree whether existing legislation sufficient many called concrete interpretation existing data protection rules respect automated decision making enshrined article gdpr effective access justice cases involving ai based decisions fra opinion effectively contest decisions based use ai people need know ai used eu legislator member states complain organisations using ai need able ensure effective access justice individuals cases involving explain ai system decisions based ai ai based decisions access justice process goal crucial ensure available remedies individuals seeking benefit procedural accessible practice eu legislator substantive rights encompasses number core member states could consider human rights include right fair trial introducing legal duty public effective remedy article echr administration private companies article eu charter fundamental rights using ai systems provide accordingly notion access justice obliges states seeking redress information guarantee individual' right go court - operation ai systems circumstances alternative dispute resolution includes information body - obtain remedy found individual' ai systems arrive automated rights violated decisions obligation would help achieve equality arms cases accordance standards victim human individuals seeking justice would rights violation arising development use also support effectiveness ai system public private entity provided external monitoring human access remedy national authority line rights oversight ai systems see relevant case law article charter fra opinion article echr remedy must \"effective practice well law\" view difficulty explaining complex ai systems eu jointly research findings identify following preconditions member states remedy effective practice cases consider developing guidelines involving ai systems impact fundamental support transparency efforts rights everyone needs aware ai used area draw informed complain organisations expertise national human rights using ai must ensure public informed bodies civil society organisations ai system decisions based active field findings show explaining ai systems make decisions layman terms challenging intellectual property rights hamper provision detailed information algorithm works addition certain ai systems complex makes difficult provide meaningful information way system works related decisions tackle problem companies interviewed avoid using complex methods certain decision making altogether would able explain decisions alternatively use simpler data analysis methods problem obtain understanding main factors influencing certain outcomes private sector interviewees pointed efforts made gradually improve understanding ai technology ai fundamental rights - relevant policymaking artificial intelligence ai increasingly used private public sectors affecting daily life see ai end human control machines others view technology help humanity address pressing challenges neither portrayal may accurate concerns ai' fundamental rights impact clearly mounting meriting scrutiny use human rights actors examples potential problems using ai related technologies relation fundamental rights increasingly emerged include ---- algorithm used recruit human resources found generally prefer men women ---- online chatbot2 became 'racist' within couple hours ----machine translations showed gender bias ----facial recognition systems detect gender well white men black women ---- public administration' use algorithms categorise unemployed people comply law ---- court stopped algorithmic system supporting social benefit decisions breaching data protection laws examples raise profound questions whether modern ai systems fit purpose fundamental rights standards upheld using considering using ai systems report addresses questions providing snapshot current use ai related technologies eu - based selected use cases - implications fundamental rights report main publication stemming fra' project artificial intelligence fra' work big data fundamental rights project aims assess positive negative ai big fundamental rights implications new technologies including ai big data data fundamental current report builds findings number earlier papers rights * facial recognition technology fundamental rights considerations context law enforcement paper outlines analyses fundamental rights challenges triggered public authorities deploy live frt law enforcement purposes also briefly presents steps take help avoid rights violations * data quality artificial intelligence - mitigating bias error protect fundamental rights paper highlights importance awareness avoidance poor data quality * bigdata discrimination data supported decision making focus paper discusses discrimination occur suggests possible solutions part project fra also exploring feasibility studying concrete examples fundamental rights challenges using algorithms decision making either online experiments simulation studies several fra publications address relevant issues * guide preventing unlawful profiling today future illustrates profiling legal frameworks regulate conducting profiling lawfully necessary comply fundamental rights crucial effective policing border management * handbook european data protection law edition designed familiarise legal practitioners specialised data protection area law * data fra' fundamental rights survey surveyed random sample people across eu including findings people' opinions experiences linked data protection technology security * fra' report business human rights - access remedy analyses obstacles promising practices relation access remedies victims business related human rights abuses analysing complaints mechanisms eu member states research maps hinders facilitates access remedies report growing attention ai potential drive economic growth matched body evidence different technologies affect fundamental rights - positively negatively concrete examples allow thorough examination whether extent applying technology interferes various fundamental rights - whether interference justified line principles necessity proportionality report provides fundamental rights based analysis concrete 'use cases' - case studies 'use case' term software engineering report loosely defines specific application technology certain goal used specified actor report illustrates ways companies public sector eu looking use ai support work whether - - taking fundamental rights considerations account way contributes empirical evidence analysed fundamental rights perspective inform eu national policymaking efforts regulate use ai tools research cover fra conducted fieldwork research five eu member states estonia finland france netherlands spain collected information involved designing using ai systems key private public sectors address relevant fundamental rights issues research - based personal interviews - gathered information ---- purpose practical application ai technologies ---- assessments conducted using ai applicable legal framework oversight mechanisms ---- awareness fundamental rights issues potential safeguards place ----future plans addition experts involved monitoring observing potential fundamental rights violations concerning use ai including civil society lawyers oversight bodies interviewed presenting main findings report presents main findings fieldwork particular report includes ---- overview use ai eu across range sectors focus social benefits predictive policing healthcare targeted advertising ---- analysis awareness fundamental rights implications selected rights focus four use cases ---- discussion measures assess mitigate impact ai related technologies people' fundamental rights two annexes available fra' website supplement report ----annex gives detailed description research methodology questions asked interviews ----annex provides examples potential errors using ai selected areas addition country specific information five member states covered complements fieldwork research delivered contractor also available fra' website maps policy developments ai legal framework governing use different sectors supporting rights compliant policymaking report provides evidence extent fundamental rights considerations brought discussions activities develop test employ monitor ai systems eu also highlights different technologies affect rights set charter reflects protect rights ai becomes widespread sophisticated analysis selected fundamental rights challenges help eu member states well stakeholders assess fundamental rights compatibility ai systems different contexts findings report current views practices among using ai supports policymakers identifying actions needed report aim provide comprehensive mapping use different ai systems five eu member states covered research provide depth technical information different systems mentioned interviewees work conducting interviews report based semi structured interviews representatives public administration private companies involved use ai services businesses fra intentionally provided general definition ai interviewed part research based existing definitions organisations interviewed active public administration general working law enforcement private companies include working health retail pricing marketing financial services insurance employment transport energy importantly except two interviewees research include companies sell ai companies instead entities use ai support operations addition ten interviews conducted experts dealing potential challenges ai public administration e g supervisory authorities non governmental organisations lawyers working field interviews carried five eu member states estonia finland france netherlands spain countries selected based different levels uptake ai technology policy development area ai well incorporate experience across different parts eu fra outsourced fieldwork ecorys fra staff supervised work developed research questions methodology interviewers received dedicated training conducting fieldwork interviews carried anonymously consequence information identifying organisation concerned provided report addition certain details applications described - notably country - omitted protect respondents' anonymity communicated interviewees increasing level trust allowing speak freely work also proved useful recruiting respondents mean artificial intelligence universally accepted definition ai rather referring concrete applications reflects recent technological developments encompass variety technologies although ai usually defined widely survey conducted behalf european commission among companies eu showed eight ten people working companies eu say know ai slightly two respondents companies eu know sure ai fra' research apply strict definition ai use high level cases presents interviews ai defined broadly reference definition provided high level expert group expert group artificial intelligence ai hleg artificial interviewees also expressed variety ways think ai identifying use cases explore research intelligence project focused applications support decision making based data machine learning applications \"artificial intelligence ai refers systems systems contribute automating tasks usually display intelligent behaviour analysing environment taking actions - degree undertaken humans cannot undertaken autonomy - achieve specific goals ai based humans due large scale use cases systems purely software based acting report provide insight different technologies virtual world e g voice assistants image used discussed selected areas broad heading analysis software search engines speech ai may contention concerning whether face recognition systems ai embedded certain use cases constitute ai current level use hardware devices e g advanced robots report often refers 'ai related technologies' autonomous cars drones internet things applications \" past years seen enormous increase computing initial definition ai hleg subject power increased availability data development discussion groups see ai hleg new technologies analysing data increased amount definition ai main capabilities disciplines variety data sometimes available almost real time internet often referred big data machine learning technologies related algorithms including deep learning benefit enormously increased computing power data availability development use flourishing use terms however limited use even prove counterproductive triggers ideas linked science fiction rather real application ai variety myths exist ai often spread via social media example claim ai act form entity distracts fact ai systems made humans computers follow instructions made given humans human centric approach ai important note ai never anything - human beings use technology achieve certain goals however human work decision making behind ai systems often visible centre attention \"currently lawyer entire studies many discussions explored possible ai definitions tell definition ai european commission' joint research centre analysed ai definitions ' asked around pretty highlights often refer issues linked perception thoroughly one tell \" environment e way system receives input data environment public administration netherlands e g sensors information processing decision making achievement specific goals definitions frequently refer machines behaving like humans taking tasks associated human intelligence given difficulty defining intelligence many definitions remain vague makes use ai hard measure practice10 equally challenging define law report discusses use ai based concrete applications differ terms complexity level automation potential impact individuals scale application discussion around actual use ai involves deploying machine learning technologies seen sub domain ai also confusion around term \"learning\" implies machines learn like humans reality much current machine learning based statistical learning methodologies machine learning uses statistical methods find rules form correlations help predict certain outcomes different traditional statistical analysis involve detailed checks predictions produced often referred 'black boxes' traditional statistical analysis based specific theoretical assumptions data generation processes correlations used machine learning geared towards producing accurate outcomes used automating workflows decisions acceptable level accuracy obtained usual example email spam filter uses statistical methods predict email spam important know certain email blocked spam predicted high accuracy really need understand algorithm works e based rules emails get blocked however depending complexity task prediction always possible high accuracy moreover report highlights understanding certain outcomes predicted acceptable certain tasks area machine learning incorporates several approaches often machine learning refers finding rules link data certain outcome based dataset includes outcomes supervised learning example dataset emails labelled spam 'ham' used find correlations rules associated spam emails dataset rules used 'predict' degree likelihood future email spam sometimes machine learning used find hidden groups datasets without defining certain outcome unsupervised learning - example segmenting people groups based similarities demographics finally rules correlations found trial error reinforcement learning systems try optimise certain goal experimentation update rules automatically best possible output systems need enormous amounts data hardly used humans involves experimentation mainly responsible success winning board games humans often sensationalised media ai fundamental rights eu policy framework moving towards regulation policymakers time highlighted potential ai related technologies improve efficiency drive economic growth yet public authorities international organisations recently reflected fundamental rights challenges associated technologies coupled growing use accuracy ai systems turned attention whether regulate use european parliament resolution marked milestone eu' recognition fundamental rights implications ai resolution stressed \"prospects opportunities big data fully tapped citizens public private sectors academia scientific community public trust technologies ensured strong enforcement fundamental rights\" called european commission member states data protection authorities \" develop strong common ethical framework transparent processing personal data automated decision making may guide data usage ongoing enforcement union law\" later year european council called \"sense urgency address emerging trends\" including \"issues artificial intelligence ... time ensuring high level data protection digital rights ethical standards\" european council invited european commission put forward european approach ai responding calls european commission published communication ai europe18 set high level expert group ai initiatives include strong reference fundamental rights commission facilitated high level expert group made independent experts academia civil society industry including representative fra published 'ethics guidelines trustworthy ai' 'policy investment recommendations trustworthy ai' developed work triggered discussion importance framing ai human rights terms alongside ethical considerations led development ethics guidelines refer charter place fundamental rights consideration respect ai ethics guidelines include assessment list trustworthy ai translated checklist guide develop deploy ai indicating political support highest level european council calls strategic guidelines \"ensure europe digitally sovereign\" policy \"shaped way embodies societal values\" similarly commission president von der leyen committed \"put forward legislation coordinated european approach human ethical implications ai \" prompted significant moves towards setting eu legal framework govern development use ai related technologies including respect impact fundamental rights february european commission published white paper artificial intelligence sets policy options meeting twin objectives \"promoting uptake ai addressing risks associated certain uses new technology\" paper promotes common european approach ai deems necessary \" reach sufficient scale avoid fragmentation single market\" notes \" introduction national initiatives risks endanger legal certainty weaken citizens' trust prevent emergence dynamic european industry\" legal uncertainty also concern companies planning use ai commission white paper ai highlights risks fundamental rights one main concerns associated ai acknowledges \" use ai affect values eu founded lead breaches fundamental rights result flaws overall design ai systems use data without correcting possible bias\" also lists wide range rights affected white paper ai indicates commission' preference possible new regulatory framework follow risk based approach mandatory requirements would principle apply high risk applications would determined basis two cumulative criteria employed sector healthcare transport parts public sector significant risks expected occur used manner significant risks likely arise latter risk could assessed based impact affected parties adding harm based element white paper also highlights instances ai use certain purposes considered high risk irrespective sector include use ai applications recruitment processes remote biometric identification including facial recognition technologies following public consultation ran february june commission expected propose legislation ai first quarter ahead proposal eu' co legislators considered various aspects potential legal framework october european parliament adopted resolutions recommendations european commission framework ethical aspects ai robotics related technologies civil liability regime ai also adopted resolution intellectual property rights development artificial intelligence technologies continues work resolutions ai criminal law use police judicial authorities criminal matters ai education culture audio visual sector also established special committee artificial intelligence digital age following meeting october heads state government eu member states declared \"eu needs global leader development secure trustworthy ethical artificial intelligence\" invited commission \"provide clear objective definition high risk artificial intelligence systems addition council eu adopted conclusions shaping europe' digital future35 seizing opportunities digitalisation access justice included dedicated section deploying ai systems justice sector german presidency council eu published conclusions charter fundamental rights context artificial intelligence digital change text supported objected member states growing reference fundamental rights discussions indicates fundamental rights framework alongside legal frameworks38 necessary effective human rights compliant evaluation many opportunities challenges brought new technologies many existing ai initiatives guided ethical frameworks typically voluntary fundamental rights centred approach ai underpinned legal regulation responsibility respecting protecting fulfilling rights rests state guarantee high level legal protection possible misuse new technologies also provides clear legal basis develop ai reference fundamental rights - application practice - fully embedded addition steps towards legal regulation eu taking significant policy financial actions support development ai related technologies alongside white paper commission published european data strategy aims set single market data including nine common european data spaces covering areas health data financial data proposal multiannual financial framework would create digital europe programme worth EUR billion invest eu' \"strategic digital capacities\" including ai addition funding horizon europe connecting europe facility international actors also considering steps regulate ai notably council europe active player field ai related technologies september committee ministers council europe set ad hoc committee artificial intelligence cahai aims examine \" feasibility potential elements legal framework development design application ai based council europe' standards human rights democracy rule law\" april committee ministers council europe adopted recommendations human rights impact algorithmic systems addition organisation economic cooperation development oecd adopted ai principles created ai policy observatory global level unesco starting develop global standard setting instrument ai selected examples wide range legal policy initiatives aiming contribute standard setting area ai includes amongst others actual draft legislation soft law guidelines recommendations use ai reports recommendations law policy fra put together non exhaustive list initiatives linked ai policymaking also include legislative initiatives eu member states many organisations businesses launched initiatives tackle ethical concerns ai however useful tackle potential problems ai ethical approaches often rely voluntary action sufficiently address obligation respect fundamental rights fra pointed fundamental rights report \" rights based approach guarantees high level protection possible misuse new technologies wrongdoings using \" european commission' initiative regulating ai helps avoid disjointed responses ai across member states undermine businesses across eu entities outside eu endnotes reuters 'amazon scraps secret ai recruiting tool showed bias women' october chatbot chatterbot common ai feature embedded messaging applications simulate human conversation voice text independent 'ai robots learning racism sexism prejudices humans study finds' april prates avelar p lamb l 'assessing gender bias machine translation - case study google translate' march gender shades project evaluating accuracy ai powered gender classification products see example der standard datenschutzbehorde kippt umstrittenen ams algorithmus algorithmwatch poland government scrap controversial unemployment scoring system privacy first dutch risk profiling system syri banned following court decision european commission european enterprise survey use technologies based artificial intelligence luxembourg july see example website \"ai myths\" samoili lopez cobo gomez e de prato g martinez plumed f delipetrev b ai watch defining artificial intelligence towards operational definition taxonomy artificial intelligence luxembourg schuett j legal definition ai arxiv hastie tibshirani r friedman j elements statistical learning data mining inference prediction springer see example pasquale f black box society secret algorithms control money information harvard university press cambrigde london rai 'explainable ai black box glass box' journal academy marketing science vol pp seminal paper describing difference breiman l 'statistical modeling two cultures' statistical science vol pp european parliament resolution march fundamental rights implications big data privacy data protection non discrimination security law enforcement ini para ibid para european council european council meeting october - conclusions euco brussels october p european commission communication commission european parliament european council council european economic social committee committee regions artificial intelligence europe com final april information available webpage high level expert group high level expert group artificial intelligence ethics guidelines trustworthy artificial intelligence policy investment recommendations trustworthy ai high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment european council new strategic agenda p vonder leyen ursula union strives agenda europe p european commission white paper artificial intelligence - european approach excellence trust com final brussels february p ibid p european commission white paper artificial intelligence public consultation towards european approach excellence trust july european commission adjusted commission work programme annex new initiatives may european parliament legislative observatory framework ethical aspects artificial intelligence robotics related technologies inl european parliament resolution october recommendations commission civil liability regime artificial intelligence inl european parliament resolution october intellectual property rights development artificial intelligence technologies ini european parliament artificial intelligence criminal law use police judicial authorities criminal matters ini european parliament legislative observatory artificial intelligence education culture audiovisual sector ini european parliament decision june setting special committee artificial intelligence digital age defining responsibilities numerical strength term office rso european council special meeting european council october - conclusions euco october council european union shaping europe' digital future - council conclusions june council european union council conclusions \"access justice - seizing opportunities digitalisation\" october council european union presidency conclusions - charter fundamental rights context artificial intelligence digital change october see e g pagallo u casanovas p madelin r ' middle approach assessing models legal governance data protection artificial intelligence web data' theory practice legislation pp see fra fundamental rights report luxembourg publications office chapter communication commission european parliament council european economic social committee committee regions european strategy data com final european council conclusions special meeting european council july euco july council europe ad hoc committee artificial intelligence cahai factsheet governance digital transformation council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems adopted committee ministers april 1373rd meeting ministers' deputies see dedicated oecd website see dedicated unesco website see overview fra ai policy initiatives council europe website fra fundamental rights report luxembourg publications office p putting fundamental rights context - selected use cases ai eu eu use ai related technologies relatively wide spread recent survey shows companies use ai related technologies - plan chapter presents selected cases ai use note - typically referred 'use cases' ai field fra collected information cases interviewees use cases presented five eu member states estonia france finland netherlands spain involve chapter based information different areas application across public obtained interviews public private sector administration private companies special representatives focus put use ai areas social benefits predictive policing health services interviewed representatives targeted advertising public administration work areas health services chapter provides information current infrastructure energy use ai well basic information eu judiciary law enforcement competence select areas use cases migration border management social benefits tax well provide good sense kind ai transportation traffic control related technologies currently used interviewees private companies examples also offer context mainly work retail marketing fundamental rights analysis looking pricing health sector broad variety use cases provides important financial services energy insurance insights actual use ai affect employment transport well people' fundamental rights chapter includes cross cutting areas focus discussion fundamental rights implications ai development different sectors makes reference cases described chapter according european enterprise survey beginning companies use ai eu said use technologies depend ai percentage ranges companies estonia cyprus czechia see figure another companies eu planning use ai future survey indicates ai used mostly sector technologies used comprise variety applications aiming process equipment optimisation anomaly detection process automation forecasting price optimisation decision making figure companies using ai member state eu ro uk se si nl bg lu el cz lt mt lv dk hr de hu fr pt pl fi es ie ee cy sk currently using ai planning use ai notes survey asked use plans use ten different ai related technologies speech recognition visual diagnostics fraud detection analysis emotions forecasting based machine learning includes percentage companies using least one ai technologies n source fra based data extracted european commission european enterprise survey use technologies based artificial intelligence luxembourg july noted report focuses four broad ai 'use cases' ----social benefits ----predictive policing ----health services ----targeted advertising areas particularly sensitive regards fundamental rights two \"ai machine learning cover mainly public administration' use ai social benefits allocation different concepts ai umbrella predictive policing two concern private companies health term \" services targeted advertising use cases provide basis private company estonia report' fundamental rights analysis offering necessary context appropriate report also highlights findings interviews cover areas four areas detailed studies taxonomy ai available providing \" see categorisations technology noted introduction interviewees everyone something different views ai also stated machine learning labelling clear definition ai 'ai' \" public administration netherlands report discusses specific use cases without classifying technology applied yet use ai cases examined differed use technology described interviewees involved varying levels complexity varying levels automation figure provides overview different examples use interviewees discussed heading ai applications relatively straightforward understand rule based decision making algorithms defined based ' rules' example person income certain threshold eligible certain benefits algorithms used area social benefits different levels automation examples full partial human review involved applications used traditional statistical methods inform decisions include example regression analysis classical statistical method analyses correlation several pieces information 'variables' outcome credit score example others used complex machine learning methodologies feed production forecasts statistics government reports also algorithms much higher levels complexity deep learning diagnosis support area health tools still include high level human review hence include high level automation contrast targeted advertising example potentially using highly complex algorithms without human review output decision also using highly complex algorithms including deep learning reinforcement learning see chapter descriptions terms human review would also possible area due scale algorithms operate figure examples different automation complexity levels use cases covered high rule based regression analysis deep learning automated predictions fully reinforcement areas examples decision making automated credit learning social benefits scoring advertising social welfare rule based regression analysis facial recognition marketing automation decision predictions technology positive human reviewed identification law enforcement outcomes credit scoring human social benefits review health services financial services rule based machine learning medical images human decisions supported analysis social production diagnosis benefits forecasts low low complexity high source fra ai systems also vary according potential harm could result notes examples financial erroneous decision based use ai depending area services use facial recognition application wrong decisions - based erroneous outputs technology covered system - different impacts using ai decision making detailed use case descriptions consequences different decision affirmative wrong false mentioned interviews positive negative wrong false negative examples illustrate different levels complexity automation used practice issues particularly important machine learning used based statistical calculations always come degree error rule based algorithms also make mistakes especially grow complex risks lower deterministic nature rules developed example using ai make decisions social benefits false positive means person may erroneously receive benefits necessarily negative impact person concerned unless error found later money needs paid back however negatively impacts public administration money paid line good administration practices contrast false negative would negative impact individual would receive benefits entitled annex available fra' website provides hypothetical examples effects wrong decisions based use cases discussed importantly automating tasks impact could also scale potentially exacerbating negative effect society whole severity scale potential harm one aspect needs taken consideration analysing potential limitations fundamental rights respect use ai example small error rates using facial recognition technology used law enforcement might still lead flagging many innocent people technology used places many people analysed might apply airports train stations thousands people could scanned daily basis potential bias error rates could lead disproportionally targeting certain groups society interviewees mostly mention 'machine learning' including use neural networks technologies extensions see chapter description machine learning respondents used across either directly mentioned mentioned subfields machine learning image cases recognition facial recognition technology frt identified research often interviewees mentioned use 'supervised machine learning' mainly used optimise specifically defined outcome yet sometimes 'unsupervised machine learning' also used categorise cluster data one case referred use 'reinforcement learning' without going much detail several respondents used 'natural language processing nlp ' technology analyse text speech sometimes combined machine learning algorithms mention examples involve rule based algorithms meaning rules algorithm follow directly encoded e based ' rules' cases interviewees disclose could provide detailed information technology used generally interviewees referred one use case asked focus one application interviews ----importantly fieldwork shows companies public administrations often still beginning looking use ai two thirds use cases actually use deployed practice many use cases described interviewees pilot stage development still research phase ----two ai driven applications halted tests figure words interviewees often used describe ai 'use cases' notes fra visualisation words frequently used descriptions use cases bigger size word often interviewees mentioned terms source fra figure shows frequently used words describe use cases covered report highlights importance data using ai systems well relevance supporting decision making fra previously highlighted thorough description data used ai applications essential identifying mitigating potential fundamental rights challenges variety data used ai systems covered report however difficult obtain detailed information data used respondents remained rather vague data sources rather generically many respondents mentioned using 'open data' 'historical data' 'metadata' concretely respondents mentioned using customer data e g purchases browsing behaviour administrative records \" mostly used save time ... data social benefits taxes interviewees also mentioned go lot medical records police records court records well social media material \" traffic data data included text data e g e mails audio recordings video public administration netherlands geolocation data data come internal databases companies public administration also external sources \" important deal cases efficiently ' single important reason using ai increased efficiency making use workforce vast majority respondents across public private sector mentioned people handle cases using ai greater speed fewer errors cost reduction fewer human effectively possible \" resources needed interviewees law enforcement also said public administration netherlands use ai safety security well crime prevention humans previously performed many use cases respondents said use ai entails fewer mistakes humans carry certain tasks respondents also use ai tasks humans previously carry quantity information could processed humans - example area genome analysis traffic predictions importantly half respondents interviewed use ai relevant decision making however ai mainly used support decision making final decisions remain largely hands humans interviewees pointed enthusiastic public administration companies still cautious deploying ai many use cases still testing phase described stopped phase nevertheless almost interviewees aware plans reduce level technology used fact expressed intentions invest innovation new ways employ currently available ai systems examples ai use public administration use case automating social welfare systems - using algorithms area social benefits background eu legal framework united nations special rapporteur extreme poverty human rights philip alston warned october report introducing 'digital welfare' state including use ai lead \"digital welfare dystopia\" digitalisation welfare systems often accompanied reductions overall welfare budgets narrowing beneficiary pool measures reduce availability welfare digitalisation also increases power states offering opportunities control people particularly worrying countries significant rule law deficits use algorithms public administration welfare raises major concerns respect potentially negative impact poverty inequality applied erroneously area social benefits includes areas child welfare services6 unemployment benefits yet public authorities keen use new technologies make decision making social security benefits efficient potentially fairer globally new technologies used many ways administer welfare systems include identity verification eligibility assessments benefit calculations fraud prevention detection risk scoring need classification well communication authorities beneficiaries oecd defines social benefits transfers made households need certain events particular circumstances arisen including sickness unemployment retirement housing education family circumstances however commonly agreed definition social benefits social benefits particular social insurance systems different private insurance schemes involve compulsory contributions made employees employers sometimes form taxation social policy including social security social protection area shared competence eu member states article b tfeu pursuant article tfeu eu pursues objectives among things promote \"improved living working conditions\" \"proper social protection\" end eu supports complements activities member states number fields including social security social protection workers combating social exclusion article tfeu eu actions encourage cooperation member states adopt directives minimum requirements moreover decisions social security social protection adopted special legislative procedure unanimous vote council backdrop eu member states mostly free shape social security social protection policies since virtually harmonisation social security systems differ significantly across eu terms benefits provided conditions eligibility benefits calculated contributions need paid etc public administrations eu member states working implementing ai related technologies area public welfare however information applications limited fra collected information use cases linked ----using algorithms comes compensating job seekers ----processing social benefits applications ----machine learning supported data analysis use pensions several private insurance companies interviewed research use ai related private technologies includes handling requests customers complementary health insurance insurance insurance compensation decision support evaluating credit risk companies' individuals insurance pricing insurance claims management decision making support use ai related management functions credit decisions private insurance companies generally embrace ai related technologies help make business profitable oecd report highlights importance technology sector also argues risk classification could lead exclusion belonging certain vulnerable groups ways undesirable societal political perspective oecd impact big data artificial intelligence ai insurance sector use practice use cases outlined exemplify challenges using planning use ai area social benefits linked algorithmic decision making experimenting new technologies support jobseekers course three year project public organisation experimented several ai related technologies concerning work related processing benefits job seekers assisting return work representative interviewed states tested technologies improve foster relationship job seekers improve advice given job seekers companies testing completed organisation decide apply technologies day day work tests include machine learning based detection attractiveness job offers system detecting whether job seekers still actively looking job tests also looking profiling job seekers provide advice would include calculating probability someone offered available job within given time identifying parameters make job offers relevant may also reflected advice companies best practices formulating job offers profiling would allow organisation determine appropriate services according profile background job seeker rather analysis advice drawn employees practically would done requiring job seekers complete monthly diary job search however still consideration whether programme limited providing descriptive analyses whether go provide recommendations organisation hesitant latter aspect additionally natural language processing system tested analysing content job seekers' e mails e mails categorised relevant data extracted urgency relevance e mails identified using chatbot using automatic replies emails considered data used systems come several sources within organisation data job seekers background including personal tax data well data salaries social security allowances used strict conditions derived highly regulated data sources e g salary statements cannot accessed data job offers companies also used generate knowledge job market organisation currently use external data professional social media networks legal provisions place using data processing housing benefits - failure success public body responsible processing social benefits piloted ai tool process applications subsequently support staff making decisions housing benefits system selected cases new benefit applications relatively straightforward calculate include new applications housing benefits submitted individual living alone children individual income government benefits overall cases deemed simple result always individual receives benefits technological solution based decision tree model following rules housing benefits calculating general housing benefits requires income estimates advance data used testing stemmed internal database contains data benefit application processes data pseudonymised need use personal information simple statistical model linear regression used input income cost limits output amount benefit however even simplified cases found difficult use ai practice frequent changes legislation test terminated according interviewee lack legal basis using machine learning allow using administrative decisions plans use ai support decision making social benefits organisation pursuing particular project due aforementioned legal challenges interviewee noted potential applications solutions area future noted ai related technologies support operations without legal impact particularly good organisation syri case netherlands called time organisation using image processing social benefits applications 'system risk indication' syri generally benefit applicants complete developed government tool several forms attachments often alert dutch public administration submitted paper format efficient fraud risk citizens time saving handling documents processing linking large amounts agency' staff hard copies received personal data public scanned classified automated authorities system broad coalition civil society organisations dealing privacy first step turn images right way issues initiated lawsuit prompting round algorithms align documents district court hague aligned properly scrutinise algorithm based syri scanned remove spots clean edit colouring document identify court ruled syri impinges disproportionately private columns paragraphs tables elements life citizens court found distinctive blocks recognise script etc everyone whose data analysed application checks received syri exposed risk application form attachment marked addition due opacity correctly e g document marked algorithm used citizens could invoice system determines whether \"neither anticipate intrusion correct private life guard \" turning classification images good description syri done image recognition optical found ilja braun high risk character recognition ocr technologies citizens algorithm watch recognise text stemming images including photographs scans documents ruling february handwritten notes ocr technology dutch available online converts recognised text text data privacy first dutch machine readable pattern recognition risk profiling system syri banned process input scanned images following court decision first isolated compared 'glyphs' e variations letters stored system pixel pixel basis agency continue processing images develop example potentially making possible scan bar codes attachments would help speed confirmation correctness documents attachments also solutions related natural language processing automating unemployment benefits one countries selected decisions unemployment benefits fully automated national institution responsible unemployment insurance benefits updated system fully automate processing benefit applications decisions done relevant legislation adapted allow automated decisions person registers unemployed lodges application benefits system draws information applicant various databases includes example population register tax authorities' databases containing information salaries work experience etc conditions receiving unemployment benefits fulfilled system calculates period payments based length person contributed insurance system amount benefits based average daily salary procedure fully automated however employee institution must intervene necessary information cannot extracted databases contradictory information databases decision case involves level discretion e decision cannot definitively determined based data available human leeway deciding case main reason using system improved efficiency addition system believed achieve consistency processes every application subject discretion handled way use case predictive policing - trying anticipate crime advance fra activity background eu legal framework ai technologies used law enforcement particularly predictive policing preventing existing research tools affect fundamental rights unlawful profiling highlighted particular issues concerning discrimination among rights one recurrent concern potential predictive policing reproduce today entrench existing discriminatory practices particularly reliance future guide historical crime data may biased incomplete developing using algorithmic many crimes - domestic violence hate crime - remain largely profiling bias may introduced unreported therefore counted official police statistics step process avoid subsequent potential violations focus certain crimes violence drug related crime public fundamental rights experts places - rather business fraud non payment taxes example - officers interpreting data also make law enforcement responses less equitable clear understanding former often associated certain demographics neighbourhoods fundamental rights ultimately undermine police relations particular communities fra guide explains profiling criminological research crime 'hotspots' around several legal frameworks regulate decades - notably uk usa uses police data map certain conducting profiling crimes undertakes statistical tests explore crime probabilities various lawfully necessary comply police forces used developed address different types fundamental rights crucial crime concentrations clusters 'hotspots' effective policing border management recently adaptations area applied research used ai information see fra tool enhance effectiveness suggesting using algorithmic preventing unlawful profiling today tools could reduce police' reliance subjective human judgments future guide may reflect biases stereotypes studies also indicated predictive policing could potentially reduce unnecessary surveillance questioning physical checks searches reducing humiliation harassment individuals may occur activities predictive policing aims forecast probability crime anticipate emerging trends patterns inform crime prevention intervention strategies may also part investigation crime already taken place authoritative definition predictive policing typically characterised analysing data identify common patterns trends crime using algorithms create models based analysis used forecast criminal activity may occur future ai technologies area generally either aim 'predict' crimes 'predict' individuals either commit victims crimes tools aiming predict crimes generally fed historical data - largely official sources - time place type crimes committed complemented environmental variables population density presence certain public places services major events holidays generally use personal data applied fra activity contrast ai systems focused predicting potential perpetrators victims facial recognition crime employ historical real time personal data could include criminal records data addresses phone numbers location data data extracted technology social media information known associates health income rise data combined criminal environmental data fundamental rights eu member states shared competence area freedom considerations security justice article j tfeu includes judicial cooperation criminal matters police cooperation articles law enforcement tfeu already treaty lisbon adopted annexed declaration protection personal data judicial cooperation eu law recognises 'sensitive data' criminal matters police cooperation observed \"specific rules people' facial images protection personal data free movement data form biometric data processed fields ... police cooperation based article tfeu ... prove facial recognition software necessary specific nature fields \" images also quite easy capture public places although accuracy within framework predictive policing collection storage processing matches improving risk analysis exchange information particularly relevant processing errors remains real - particularly personal data context law enforcement operations regulated certain minority groups people whose eu level law enforcement directive directive eu sets images captured processed comprehensive standards safeguards processing including might know happening safeguarding prevention threats public security - cannot challenge possible misuses use practice fra paper outlines analyses use cases collected fra signal variety ways law fundamental rights enforcement authorities already use plan use ai related technologies challenges triggered support work public authorities deploy live frt law enforcement purposes also examples mentioned interviewees range data mining systems briefly presents steps take help designed map crime patterns detecting online hate speech making avoid rights violations risk assessments gender based violence automating certain prison information see fra guard duties use cases include detecting illicit objects satellite facial recognition technology images generally recognising objects images addition tool fundamental rights considerations mentioned research used private sector fraud prevention context law enforcement crime detection money transfers interviewees emphasised ai related technology systems used automate speed tasks previously done humans thus freeing better distributing resources mapping crime support efficient allocation investigation capacity national intelligence agency public prosecutor' office employ data driven system help employees make choices use available investigation capacity aim improve allocation human resources ensuring officers present right time place interviewees suggest system could make precise assessments compared humans often rely gut feeling decisions still system always used combination human appraisal non ai systems make operational decisions based system generated outcomes analysts create 'heat map' outlines prevalence certain crimes certain areas replicates long standing manual version crime anticipation system whereby police officers put pins map indicate specific risk areas using ai increase speed process also makes reliable users believe analyse data system based data mining machine learning processes primarily built unique police data contained crime reports witness statements suspect declarations gaps extent possible addressed using data sources criminology research social demographic information obtained national office statistics system also uses data open sources specific parameters calculation depend type crime predictive factors vary relevance across crime areas example case burglaries data burglaries collected combined data place residence known criminals distance burgled fra activity houses relevant criteria preselected allow system produce heat map detecting hate location based predictions made next six months indicate speech online time location burglary may occur result map small public agency combatting hate squares risk crime occurring indicated different shades crime uses ai based tool detect interviewees indicated visualisation helps officers analyse online hate speech analysing neighbourhoods observe correlations different locations patterns speech online basis processing system assessing risk gender based domestic violence determines social groups targeted helps law enforcement national police force uses internal system track cases gender adopt measures protect based domestic violence system helps police officers take decisions threats realised distribute resources across domestic violence cases system categorises cases basis assessed risk relapse repetition order although tool aims identify focus 'riskiest' cases potential victims rather perpetrators law enforcement specialist team could complete risk analysis without using ai however use information generated system able compute large amount data short amount system ask social media providers time assist untrained non specialist police officers risk analysis information users pursue criminal investigations case alleged gender based domestic violence reported police one particular challenge officer starts initial investigation includes collecting evidence taking understanding context witness statements - potentially - making arrest using information statements made example gathered process officer fills two detailed questionnaires journalists academics may use assess complaints evaluate probability reoffending examine words associated hate speech evolution case assess behaviour perpetrator report analyse occurrence victim police officers also indicate level gravity nature threats faced attitudes concerning victim fra plans initiate research online hate present social system produces risk 'score' three point scale police media allow fra provide officer raise level risk manually cannot lower risk level input policy developments indicated system level confirmed specific area online content moderation measures applied line established police protocols system uses ai also informs judge potentially 'severe cases' automated system examples ai use private sector use case ai health - analysing medical records save lives background eu legal framework healthcare particularly prominent discussions use ai medical data online applications potential support improved health outcomes - result - wider socio economic benefits covid pandemic increased focus interest area particularly terms potential online data applications enhance ability governments health services track spread disease health also prominent general population' views uses ai eurobarometer survey found every second european thinks ai best used improve medical diagnostics develop personalised medicine improve surgery use case covers applications ai related technologies public private sector stakeholders area medical records disease prediction feeding data electronic medical records emr electronic health records ehr ai systems related technologies support development preventative medicine recognises early risks disease designs appropriate interventions researchers predict clinical events mortality hospitalisation readmissions length stay hospital beyond disease prediction medical record data analysed predict patients' adherence treatment keeping medical appointments technologies potential support improved health outcomes well increase efficiency healthcare system article tfeu eu supporting competence protecting improving human health member states retain full responsibility defining health policies organising managing health systems delivering health services article tfeu within eu competence union action complement national policies directed towards improving public health preventing physical mental illness diseases obviating sources danger physical mental health action cover health information education well monitoring early warning combating serious cross border threats health article tfeu latter areas eu adopt incentive measures excluding harmonisation laws regulations member states rules policies adopted eu level aim ensure free movement citizens equal treatment non discrimination abroad well availability safety medical products services single market considering development technologies application health care exchange medical records patients' rights cross border situations disease prediction matter public health particularly relevant gdpr health genetic data considered special category data article called 'sensitive data' require specific protection processing could create significant risks data subjects' health genetic data shared specific circumstances article gdpr gdpr provides exemption purpose limitation principle data used research purposes line article researchers required ensure technical organisational safeguards - pseudonymisation anonymity - place using patient data eu also taken action regarding exchange medical records european commission recommendation c european electronic health record exchange format24 \"seeks facilitate cross border interoperability ehrs eu supporting members states efforts ensure citizens securely access exchange health data wherever eu \" recommendation lays technical specifications exchange data eu member states european data strategy february also strong focus health data 'common european health data space' one nine common european data spaces whose establishment european commission support early warning response system ewrs owned european commission operated european centre disease prevention control aims \"notifying eu level serious cross border threats health\" enabling \" european commission eu countries permanent communication purposes alerting assessing public health risks determining measures may required protect public health \" emr computerised medical record created patients healthcare organisation ehr contains patient' medical history beyond one organisation involve sharing data across healthcare system include large amount personal data encompass among others name contact details individual next kin demographic information diagnoses test results medication treatment may also include patient generated data wearable devices uniform emr ehr system operating across eu member states germany national emr ehr system others - including belgium denmark - different emr ehr systems regional level systems differ considerably depending data recorded access data european commission stakeholders highlighted diversity country level emr ehr systems lack interoperability major barrier digital single market health studies highlight potential ai related technologies enable earlier diagnosis widen possibilities disease prevention improve patient safety strengthening right access preventive healthcare benefit medical treatment emr ehr may also help make healthcare personalised possibility rapid sharing data facilitate coordinated timely treatment however use emr ehr presents significant data protection risks healthcare sector leads terms personal data breaches amount personal data stored highest among industries combined large data sharing network number access points makes healthcare sector attractive target hackers quality data emr ehr also raises concern studies patients shown medical files asked accuracy found information incomplete erroneous lot important data emr ehr unstructured form free text reduces data quality low levels accuracy completeness overall data quality increases risk medical error use practice applications described interviews include simple advanced models employed public private sectors largest number use cases refer image based diagnosis tools however interviewees also discussed tools automate various working procedures mapping text data filing medical records analyses measurements body tissues nerve fibres smaller number examples touched advanced projects systems monitor remotely certain health indicators heart rate case systems complement expertise health professionals next sections present examples diagnostic remote monitoring tools image based tools help detect diagnose disease tools used support detection diagnosis diseases described interviewees work similar ways example privately owned hospital uses ai system interpret images ct scans stroke patients stroke imaging used detect damage brain occurred may blockages blood supply brain also generate measures compared particular values medical specialist interviewee feels application helps determine characteristics images quickly potentially - depending uses tool - improving quality diagnosis however highlight necessarily efficient rely ai application since medical professional must present could examine image rather tool offer support - example specialist finds difficult interpret certain image find abnormalities system built trained validated using dataset partially based large scientific study hospital contributed supplemented purchasing foreign datasets algorithm trained adapted future based new data new versions released developers feel allowing system continue learn would make difficult using ai public authority responsible validate operation target health inspecting food safety standards restaurants uses machine learning private company developed algorithm supports detection breast cancer inspections process customer review data major online platforms helps mammography exams tool gives decide conduct inspections previously process probability degree certainty based complaints help radiologists speed analysis authority received previous results decide whether additional reports since introduction tests warranted algorithm detects tool rate non compliant characterises anomalies mammography restaurants identified doubled cancerous around interviewee indicates first step involves text mining algorithm identifies reviews system low rate false containing key words may negatives false positives note indicate health safety issues many cases deliver clear outcome 'sick 'nausea' 'rodents' system trained radiography second step authority mammography data europe eu compared results coming written reports past biopsies acting customer reviews previous control data inspection reports improve algorithm' accuracy reliability monitoring patients' vital statistics remotely hospital piloting system support early detection potential illness monitoring patients' health indicators - example blood pressure heart rate - typically takes place manually captures situation specific moment time constantly monitoring indicators potential identify trends doctors may otherwise recognise detect health issues early prevent illness system uses biosensor - kind plaster - gathers hemodynamic data patients continuously constantly monitoring heart pulsation respiration data used system come hospital patient data anonymised shared third party provider information besides gathered monitoring plaster used build train system data environmental factors incorporated pilot interviewee pointed could contain biases future system combine information gathered biosensor separate information patients' emr draw conclusions trends observed monitoring use case targeted advertising - profiling consumers boost profit background eu legal framework internet transformed way live many people make use internet services often offered free daily basis companies offering services free mainly generate revenue advertising adverts automatically targeted individual consumers based information availability data online individual behaviour combined machine learning technologies considerably improved ability commercial enterprises target individuals could even go far manipulating consumers predicting reactions based irrational aspects psychology reasoned choice cambridge analytica scandal underscored particularly negative impact uses political purposes case company illegally obtained personal data millions social media users target political adverts different social groups based certain psychological profiles recent declaration committee ministers council europe highlights lack knowledge manipulative power algorithms \" effects targeted use constantly expanding volumes aggregated data exercise human rights broader sense significantly beyond current notions personal data protection privacy remain understudied require serious consideration \" concerns also raised online advertising powered ai technologies affect data protection privacy consumer protection right non discrimination even way democracies work word 'advertising' associated messages designed influence consumer behaviour advertising one form another always targeted specific groups based characteristics behaviour growth social media however taken targeted advertising another level using direct access consumer data micro targeting directed towards specific groups - data gathered online activities targeted activities social media providers platforms like google amazon gather comprehensive user data monitoring various activities users advertisers access detailed specific information area targeted advertising systems recommend content e g news movies one real life examples also involves called reinforcement learning technology based optimising certain goal experimenting updating rules automatically best possible output means systems tries different ad placements trial error finds best way optimise revenue - including element self learning little knowledge actual use reinforcement learning available european countries major companies working area researching issue issues related targeted advertising fall consumer protection falls shared eu competence member states article f tfeu eu consumer protection measures seek protect health safety economic interests consumers promote right information education organise safeguard interests article tfeu eu adopt minimum harmonisation measures achieve high level consumer protection article tfeu yet allowing eu member states introduce even stringent measures nationally secondary eu legislation rules advertising covered directive ec concerning misleading comparative advertising directive provides minimum level protection misleading advertising also harmonises rules comparative advertising across eu provisions directive ec apply consumer business business business relations however practically applied latter53 directive ec unfair business consumer commercial practices internal market practices54 took effect directive ec services internal market55 covers services include advertising additionally directive ec certain legal aspects information society services particular electronic commerce internal market e commerce directive also applies directive forms part legal framework digital services eu meet significant developments area new online services practices e commerce directive currently revised part digital services act package package aims \"strengthen single market digital services foster innovation competitiveness european online environment\" fra collected information actual use cases six european companies engaged placing online ads content recommendation personalised marketing use practice examples covered include ----placing ads online based click predictions e learning likelihood online users click certain links adverts automated bidding auctions online advertisement space ----personalised targeted marketing communication via email tasks fully automated examples concern analyses user preferences activity calculations probabilities clicks purchases including measurement effectiveness previously made recommendations also includes methods targeted communication basis identified target groups build long term trust clients service providers targeted online ads based click predictions business models working click predictions targeted advertisements often follow 'click buy' policy companies purchase advertising space media platforms optimise display adverts analysing interests preferences website users showing advertisements interest purpose increase relevance advertisements shown better matching interests see present example company gets paid people click advertisement buy something additionally company uses ai detect inappropriate content advertising advertisements alcohol firearms political content company uses range machine learning techniques field computational advertising estimate probability user clicking advertisement displayed specific context optimising called click rate customers' interests relevance products measured via mapping individuals' browsing histories transaction patterns information derived individuals' navigation merchant websites worldwide advertising company works done via anonymised third party cookies trackers placed merchant websites outline individuals' navigation across also list products seen purchased profiles individuals linked devices used although ip addresses anonymised product purchased recommender system algorithm tries determine products customer could also buy case 'fresh' data valued higher older data browsing histories stored maximum one year interests change purchases older year longer necessarily considered relevant advertisements shown respective person immediately adapted accordingly vary across websites also match content latter advertisement posted continuously analysed combination elements taken account individual' interest confirmed purchase made data shared across platforms includes informing others purchase made stop advertisements particular item purchase made formula reviewed algorithm adapted individuals' continuous online behaviour future company covered example expects work optimising timing terms places advertisements within given budget certain time frame also expects focus displayed ads impact consumers another example based european online market place links buyers sellers range specialised products ai used optimise advertising campaigns categorise products based advertisements shown website market place improve search engine experience predicting complementary substitutable products detect fraud attempts company uses machine learning predict value clicks customers buy advertisement space offered real time auctions examples company indicates ai enables make decisions otherwise would possible without ai would significantly scaled targeted communication customers clients case retail company focusing specialised supplies sold across physical stores online direct marketing personalised advertising used increase appeal customers time measure efficiency particular instance marketing advertising according company issue example marketing emails opened average particularly customers recognise relevant favourite products offered marketing emails sent around registered individuals system used establish may considered relevant individuals done analysing purchases made respective individuals previous six months offers displayed directly based previous purchases meanwhile new suggestions e alternative products category previous purchases similar approach used bank sends emails clients messages offering specific services products sent certain clients data analysts calculate probability clients interested service product probability certain threshold client receive message system used yet include machine learning models fully automated points taken develop system third example grocery retailer uses loyalty cards increase customers' interaction personalise offers loyalty card systems predict many customers likely engage product offering system covered example also suggests new products customers tracks results suggestions groups buyers similar behavioural patterns segments make personalised suggestions every week company' loyalty card owners receive personalised offers email website mobile application access offers store terminals ai system selects offerings based individual purchase history recommends new items might catch buyer' interest prompt purchase endnotes see instance samoili et al ai watch defining artificial intelligence towards operational definition taxonomy artificial intelligence luxembourg karanasiou pinotsis ' study layers automated decision making emergent normative legal aspects deep learning' international review law computers technology pp see fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office p fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office june un human rights council report special rapporteur extreme poverty human rights philip alston eubanks v automating inequality hightech tools profile police punish poor st martin' press redden joanna dencik lina warne harry datafied child welfare services unpacking politics economics power policy studies panoptykon foundation profiling unemployed poland social political implications algorithmic decision making see also algorithm watch poland government scrap controversial unemployment scoring system oecd glossary statistical terms social benefits definition accessed august j henry richardson chapter iv social insurance economic financial aspects social security university toronto press pieters social security overview eu competence domain regulation ec see paju j european union social security law oxford hart publishing ch erik bakke 'predictive policing argument public transparency' new york university annual survey american law vol pp andrew g ferguson 'policing predictive policing' washington university law review vol pp example one five women experienced violence brought serious incident attention police see fra violence women eu wide survey main results report luxembourg publications office p elizabeth e joh new surveillance discretion automated suspicion big data policing uc davis legal studies research paper p braga et al hot spots policing small geographic areas effects crime campbell systematic reviews vol elizabeth e joh new surveillance discretion automated suspicion big data policing uc davis legal studies research paper pp available ssrn erik bakke 'predictive policing argument public transparency' new york university annual survey american law vol pp wim hardyns anneleen rummens 'predictive policing new tool law enforcement recent developments challenges' eur j crim policy res p doi s10610 albert meijer martijn wessels 'predictive policing review benefits drawbacks' international journal public administration p doi law society commission use algorithm justice system algorithms criminal justice system p newbold j n 'predictive policing' 'preventative policing' 'intelligence led policing' future declaration annexed final act intergovernmental conference adopted treaty lisbon signed december directive eu european parliament council april protection natural persons regard processing personal data competent authorities purposes prevention investigation detection prosecution criminal offences execution criminal penalties free movement data repealing council framework decision jha oj l pp european commission standard eurobarometer report europeans artificial intelligence p european patients forum n new eu regulation protection personal data mean patients guide patients patients' organisations commission recommendation eu february european electronic health record exchange format oj l pp digital health society exchange electronic health records across eu february communication commission european parliament council european economic social committee committee regions european strategy data com final brussels february decision eu european parliament council october serious cross border threats health repealing decision ec oj l pp see commission webpage communicable diseases oecd european union healthcare glance europe p vera ehrenstein hadi kharrazi harold lehmann casey overby taylor 'obtaining data electronic health records' gliklich leavy mb dreyer na eds tools technologies registry interoperability registries evaluating patient outcomes user' guide 3rd ed addendum use data insurance industry currently potential see example spender c bullen l altmann richer j cripps r duffy c falkous farrell horn j wigzell w yeap 'wearables internet things considerations life health insurance industry' british actuarial journal pp see visualisation see short overview different ehr systems europe nurses' perspective healtheurope world cloud based services storing health data cloud college europe transformation health care digital single market synopsis report public consultation european commission study big data public health telemedine healthcare roberta pastorino corrado de vito giuseppe migliara katrin glocker ilona binenbaum walter ricciardi stefania boccia benefits challenges big data healthcare overview european initiatives european journal public health vol issue supplement pp - ministry health welfare sport netherlands digitalization health care benefits patient safety literature web reports according multiple reports different cybersecurity companies time see example sc magazine healthcare leads cost data breaches shannon williams new report reveals 'wall shame' health care data breaches tammy lovell statistics reveal healthcare sector affected personal data breaches sc magazine healthcare leads cost data breaches annet sollie reuse sharing electronic health record data focus primary care disease coding doctoral dissertation vrije univesiteit amsterdam pp vera ehrenstein hadi kharrazi harold lehmann casey overby taylor 'obtaining data electronic health records' gliklich leavy mb dreyer na eds tools technologies registry interoperability registries evaluating patient outcomes user' guide 3rd ed addendum mowafa househ bakheet aldosari abdullah alanazi show andre kushniruk elizabeth borycki 'big data big problems healthcare perspective' studies health technology informatics p sartor giovanni new aspects challenges consumer protection study committee internal market consumer protection policy department economic scientific quality life policies european parliament luxembourg neudert lisa marchal nahema polarisation use technology political campaigns communication study request panel future science technology stoa managed scientific foresight unit within directorate general parliamentary research services eprs secretariat european parliament information commissioner' office ico investigation use data analytics political campaigns council europe declaration committee ministers manipulative capabilities algorithmic processes decl example costello roisin aine impacts adtech privacy rights rule law technology regulation - edps opinion edps opinion online manipulation personal data sartor giovanni new aspects challenges consumer protection jablonowska agnieszka et al 'consumer law artificial intelligence challenges eu consumer law policy stemming business' use artificial intelligence' eui working papers law wachter sandra 'affinity profiling discrimination association online behavioural advertising' berkeley technology law journal vol forthcoming available ssrn zuboff shoshana age surveillance capitalism london edps opinion edps opinion online manipulation personal data martin gillian importance marketing segmentation american journal business education vol kaili lambe becca ricks basics microtargeting political ads facebook see example information recsys2020 workshop reveal bandit reinforcement learning user interactions accessed august directive ec european parliament council december concerning misleading comparative advertising oj l pp european commission misleading comparative advertising directive objective directive directive ec european parliament council may concerning unfair business consumer commercial practices internal market amending council directive eec directives ec ec ec european parliament council regulation ec european parliament council 'unfair commercial practices directive' oj l pp directive ec european parliament council december concerning misleading comparative advertising oj l see european commission' webpage digital services act package fundamental rights framework applicable ai use ai - presented four use cases discussed chapter - affect specific fundamental rights outlined chapter full compliance fundamental rights prerequisite using ai driven technologies irrespective area concerned chapter introduces general fundamental rights framework eu governs use ai including selected secondary eu legislation national law section fundamental rights framework provides normative basis benchmarks design development deployment ai tools helps determine whether specific use ai fundamental rights compliant requirements justified interferences fundamental rights outlined section fundamental rights framework governing use ai cornerstone instrument eu fundamental rights framework applicable use ai charter together unwritten general principles eu law main source fundamental rights eu charter enshrines wide array fundamental rights legal value eu treaties eu institutions bodies bound charter member states act within scope eu law article charter many charter rights set european convention human rights echr meaning scope must corresponding echr rights article charter however cannot prevent union law providing extensive protection fundamental rights also found provisions treaties see e g article teu titles v x tfeu eu secondary law rights safeguarded different pieces secondary eu law central piece eu secondary law context ai general data protection regulation gdpr - regulation eu governs automated processing personal data european economic area processing personal data means form part filing system - within scope eu law result gdpr apply national security related data processing gdpr coupled law enforcement directive applies police judicial cooperation criminal matters eu instruments include numerous provisions protection personal data determining key principles data processing lawfulness fairness transparency whether eu data protection legislation applies depends whether personal data processed ai driven applications use personal data example traffic data others use anonymised data cases data protection laws apply applicability entirely clear line personal non personal data blurred risk anonymised data ' identified' - ie anonymisation undone however identification usually illegal addition persons identifying data usually put major efforts potentially need access additional information individuals might included anonymised dataset identification section discusses topic detail linked results interviews carried report addition eu data protection acquis european non discrimination law key safeguarding fundamental rights context use ai related technologies article teu provides non discrimination one fundamental values eu article tfeu requires union combat discrimination number grounds moreover articles charter provide equality law non discrimination beyond several eu non discrimination directives enshrine specific detailed provisions varying scopes application include employment equality directive ec racial equality directive ec gender goods services directive ec recast gender equality directive ec eu member states also party international human rights conventions see list conventions key findings fra opinions section contain legally binding standards safeguards comply act areas fall within scope eu competence main instrument echr ratified eu member states accompanied additional protocols great majority eu member states parties echr wide reach also applies areas covered eu law addition council europe convention protection individuals regard automatic processing personal data13 another source pan european data protection obligations binding eu member states recently modernised sector specific eu national legislation also enshrines safeguards protection fundamental rights overview technical legislation beyond scope report however chapter provides examples relevant use cases discussed report complemented couple examples national laws five eu member states covered none five eu member states covered currently horizontal ai specific laws although countries looking potential need regulation eu countries finland issued recommendations self regulation development responsibility standards private sector estonia assessment concluded separate ai specific law required foreseeable future since current legal framework sufficient according relevant estonian long term strategy however legal environment must adapted avoid unnecessary hindrances implementing ai situation concerning sectoral legislation relevant use ai different sectors varies across eu member states however active policymaking ai recently emerged national level national action plans ai appeared remain core policy development member states countries working growing entrepreneurship others focused enacting market oriented policies compatible un agenda sustainable development educational activities promote ai increasing public use ai often identified ai related strategy goals investment research development also frequently outlined relevant goal domestic ai discussions potential legislative reforms remain attentive european initiatives national sector specific fundamental rights safeguards also enacted instance finland began considering overhaul domestic human rights safeguards public sector proposing broader across board legislative update opposed individual ai laws specific reference processing personal data immigration law finnish constitutional law committee put forward proposal strengthen safeguards finnish constitution overriding constitutional law shortcomings relation among others protection law accountability well ambiguity algorithms automated decision making whenever public authorities automate decision making processes processes must adhere constitutional principle rule law may endanger observance rules good administration due process proposal articulated vision requirements finnish constitution sets ai use automated decision making within public administration research identified initiatives policies linked ai fundamental rights five member states examined example estonian e state charter includes summary citizens' rights better communicating agencies electronically also targets ai relation right know data collected public authorities similarly ministry interior netherlands presented policy brief parliament ai public values fundamental rights brief stresses human centric approach ai applications strong influence human beings society whole also lists important risks ai fundamental rights discrimination result biased data reduced interpersonal relations ai takes certain forms interaction 'use case' examples social welfare use case regulating social welfare eu member states enacted rules aiming protect fundamental rights specifically area addition existing horizontal eu regulations see section mostly define rules processing protection personal data purpose social benefits insurance estonia example insurance activities act applicable types forms insurance regulates processing transmission personal data context states public authorities health care providers insurance undertakings third parties may transmit personal data request insurance undertaking personal health court data necessary insurance undertaking perform insurance contract right obligation disclose data derives law scope act also includes data transfers purpose data processing within ai systems social welfare act contains specific provisions data protection persons need social assistance notified processing data provide consent processing person established target group right opt data processing social welfare act also allows local authorities process including using algorithms personal data youth years age stored state registries identify youth employment education training finland act secondary use health social data applies using ai social care healthcare act based norms securing protecting sensitive personal data outlined gdpr aims establish conditions effective secure \"processing access personal health social data certain secondary purposes research statistics innovation development knowledge management teaching authority planning \" act regulates manner registered health data cannot processed several laws apply various types social benefits france code relations public administration applies purpose processing accessing personal data related social benefits minor amendments entry force gdpr code states \" algorithms used public administrations must published\" \" person subject automated decision making right informed\" predictive policing use case context predictive policing eu' law enforcement directive contains key fundamental rights safeguards stipulate law enforcement authorities apply main data protection principles set gdpr include requirement data controllers e competent law enforcement authorities provide data subjects information controller' data processing activities identity contact details data controller purposes processing information right lodge complaint article specific cases data controllers shall provide information - example legal basis processing - enable data subjects exercise rights right access article requires data controller confirm upon request data subject whether processing operations related case data subject shall able access data also request additional information including purposes legal basis processing categories personal data processed right information right access restricted number cases including avoid obstructing prejudicing prevention detection investigation prosecution criminal offences protect public security national security addition article law enforcement directive explicitly prohibits automated decision making prohibition limited authorised eu national law safeguards data subject' rights including \" least right obtain human intervention part controller\" see section cases scope implementing national legislation broader directive example finnish act processing personal data criminal matters connection maintaining national security strengthens right information distinguishing information provided general special circumstances healthcare use case regards eu level fundamental rights safeguards using ai healthcare gdpr empowers patients rights informed part granting control personal health data data qualifies 'sensitive data' found example medical records rights include rights access one' personal health data object processing personal data rectification erasure data well rights case breach gdpr administrative fines breaches processing data including health data allowed however estonia instance domestic law allows maximum penalty eur application misdemeanour procedure cases data protection inspectorate also impose similar fines misdemeanour procedure france data protection act public health code impose stricter requirements set gdpr regarding health data processing french data protection act amended law modernisation health system allow processing personal health data various purposes provided fall within scope one exceptions general principle prohibition sensitive data processing article gdpr targeted advertising use case considering fundamental rights safeguards relation targeted advertising underlying mechanisms regarding profiling particular eu legal framework privacy data protection provides relevant fundamental rights provisions protection privacy personal data holds status takes precedence economic benefits hence rules processing special categories personal data relevant companies operating area applying targeted advertising place companies certain obligations main legal provisions setting rules protecting personal data eu gdpr directive privacy electronic communications e privacy directive lex specialis gdpr gdpr directly applicable eu member states whenever company based eu processes personal data company based outside eu processes data relating individuals eu e privacy directive strong focus fundamental rights concerns processing personal data protection privacy electronic communications sector e g individuals use computer smartphone tablet european commission proposed e privacy regulation would replace current e privacy directive legislative proposal would broaden scope directive include specific provisions concerning unsolicited marketing cookies confidentiality requirements justified interferences fundamental rights chapter highlights selected fundamental rights - covered charter - particularly affected ai taking account four use cases discussed chapter rights absolute rights subject limitations line article charter accordingly analysing extent different fundamental rights impacted use ai section presents general steps need followed determine whether charter right limited fundamental rights affected ai absolute subject limitations interferences fundamental rights justified respect requirements charter echr case charter rights corresponding rights guaranteed echr article charter pursuant article charter limitation fundamental rights must ---- provided law ----genuinely meet objectives general interest recognised union need protect rights freedoms others ----respect essence right ---- necessary ---- proportionate court justice eu cjeu also emphasised limitation exercise rights freedoms recognised charter must respect \" essence\" rights freedoms means fundamental rights limited certain extent completely disregarded established inalienable essential core right violated measure next step conduct necessity proportionality test outlined charter respect non core aspects right interference charter right needs examined whether given legitimate aim could obtained means interfere less right guaranteed similar requirements also imposed echr interpreted european court human rights ecthr include 'essence right' concept derived object purpose echr whole respect use new technologies ecthr observed marper v uk states \"strike right balance\" protecting fundamental rights developing new technologies given wide range applications ai systems everyday life presented four selected use cases wide range fundamental rights may assessed taking account variety elements depending context particular area use notably specific purpose ai used functionality complexity scale deployed relevant assessing fundamental rights implications endnotes see also van veen c 'artificial intelligence ' human rights got ' data society points - blog data society research institute may barfield w pagallo u advanced introduction law artificial intelligence cheltenham northhampton edward elgar pp see also cjeu aklagaren v hans akerberg fransson gc february paras european convention protection human rights fundamental freedoms amended protocols nos november ets overview application charter see fra 2018a applying charter fundamental rights european union law policy making national level luxembourg publications office regulation eu european parliament council april protection natural persons regard processing personal data free movement data repealing directive ec general data protection regulation oj l pp see fra handbook european data protection law edition luxembourg publications office see example hacker p legal framework ai training data law innovation technology forthcoming available ssrn overview european non discrimination law see fra handbook european non discrimination law edition luxembourg publications office council directive ec november establishing general framework equal treatment employment occupation oj l pp council directive ec june implementing principle equal treatment persons irrespective racial ethnic origin oj l pp council directive ec december implementing principle equal treatment men women access supply goods services oj l pp directive ec european parliament council july implementation principle equal opportunities equal treatment men women matters employment occupation recast oj l pp convention protection individuals regard automatic processing personal data strasbourg january ets protocol amending convention protection individuals regard automatic processing personal data strasbourg october cets ai finland project' ethics working group ethics challenge added emphasis companies self regulation ai finland 'etiikkahaaste ethics challenge ' tekoaly uusi sahko finnish republic estonia report estonia' ai taskforce p estonian government launched preparation long term strategy example see netherlands ministry economic affairs climate policy strategic action ai strategic action plan ai strategisch actieplan ai - sapai example effort adapt goals development sustainable market see spain ministry science innovation universities national ai strategy spanish comprehensive overview see european commission national strategies artificial intelligence oecd ai policy observatory finnish constitutional law committee 'committee opinion pevl vp - vp draft proposal parliament law processing personal data immigration administration related laws' estonia national audit office chancellor justice everyone' rights e state e state charter netherlands ministry interior kingdom relations ai public values fundamental rights dutch elina saxlin hautamaki johanna lilja secondary use health data - new finnish act de donno french code \"des relations entre le public et l'administration\" new european era administrative procedure italian journal public law pp see fra preventing unlawful profiling today future guide luxembourg publications office tables sajfert j quintel data protection directive eu police criminal justice authorities available ssrn note article law enforcement directive seems apply automated decisions taken solely automated processing means safeguard apply human agency involved orla lynskey criminal justice profiling eu data protection law precarious protection predictive policing p english translation available via finlex website gdpr recital art european patients forum n new eu regulation protection personal data mean patients guide patients patients' organisations gdpr arts white case gdpr guide national implementation estonia merav griguer processing health data france look gdpr european commission proposal regulation european parliament council concerning respect private life protection personal data electronic communications repealing directive ec regulation privacy electronic communications com final brussels charter art \" far charter contains rights correspond rights guaranteed convention protection human rights fundamental freedoms meaning scope rights shall laid said convention \" also reiterated explained cjeu see example c satakunnan markkinaporssi satamedia december para joined cases c c volker und markus schecke eifert gbr hartmut eifert november para joined cases c c digital rights ireland ltd v minister communications marine natural resources others karntner landesregierung others april para c maximillian schrems v data protection commissioner october para c webmindlicenses kft v nemzeti ado es vamhivatal kiemelt ado es vam foigazgatosag december paras see cjeu c maximillian schrems v data protection commissioner october paras refer article charter see also scheinin martin sorell tom surveille deliverable d4 - synthesis report wp4 merging ethics law analysis discussing outcomes april p see e g brkan ' essence fundamental rights privacy data protection finding way maze cjeu' constitutional reasoning' german law journal p lenaerts k 'limits limitations essence fundamental rights eu' german law journal pp cjeu joined cases c c digital rights ireland ltd v minister communications marine natural resources others karntner landesregierung others april see instance khelili v switzerland october ecthr marper v united kingdom gc nos december ecthr k v finland july ecthr z v finland february ecthr huvig v france april ecthr leander v sweden march scheinin martin sorell tom surveille deliverable d4 - synthesis report wp4 merging ethics law analysis discussing outcomes april p ecthr marper v united kingdom gc nos december para see also council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems appendix para impact current use ai selected fundamental rights deploying ai systems engages wide range fundamental rights seen chapter use cases presented report involve range technologies varying levels complexity automation different phases development applied different contexts different purposes different scale rights affected depend factors number horizontal sector specific fundamental issues emerge chapter begins general overview risks perceived interviewees general awareness fundamental rights implications using ai chapter highlights selected fundamental rights affected ai related technologies reference four use cases analysed analysis takes account presents views practices awareness issues expressed interviews conducted report interviewees first asked general risks see using ai asked general fundamental rights awareness using ai concrete fundamental rights implications mostly linked data protection non discrimination availability complaints mechanisms perceived risks important recognise many issues cut across different rights example potentially biased decision made algorithm could involve right non discrimination protection personal data right effective remedy similarly particular issue seen perspective different rights instance good explanation decision made algorithm required right protection personal data right good administration right effective remedy fair trial asked general risks using ai interviewees always mention fundamental rights main risks although highlighted related topics private sector representatives often mentioned inaccuracy risk using ai followed potential bias proper legal basis processing personal data one respondent international retail company stated one business risk linked european customers extremely knowledgeable rights namely people hesitate ask data storage automated decision making customers properly informed might complain company may lose client addition interviewee continued breaching law possible fines linked breach another major business risk respect public administration bias often highlighted risk associated using ai addition public authorities often discussed inaccuracy data identification risks using ai example interviewees working social benefits algorithms stated incorrect results general risk occur potentially due rare cases well identified algorithm due errors input data also highlighted difficulties associated moving testing deploying system including technical challenges resources required potential different results deployed respondents working targeted advertising also highlighted business risks - example offering irrelevant inappropriate content one respondent mentioned potentially losing control automated systems addition interviewees indicate challenges linked difficulty interpreting results outputs ai systems one interviewee consultancy sector fears risk related lack absence sufficient ai knowledge understanding cause ongoing projects halted due company' inability explain clearly algorithms perform purpose another interviewee law enforcement sector looking possible use ai support decisions licence applications explains inherent risks system proposes certain response example potentially using ai support decisions license applications firearms respondent asserts would critical understand reasoning behind negative decisions also positive decisions several interviews showed major concern assign properly trained staff sufficient expertise trace explain interact ai system finding also corroborated results european commission survey among companies eu survey indicate obstacle adopting ai technologies difficulty hire new staff right skills mention complexity algorithms obstacle respect ability explain decisions based algorithms interviewee working public administration mentioned alternatives completely transparent making decisions room doubt similar vein respondent working area health private sector mentions 'self learning' algorithms forbidden area work fixed algorithms traced \" use ai bring many benefits also risks like risks reported without providing much additional information include nuclear energy \" cyber security data quality excessive monitoring people due use interviewee working private sector data algorithms job loss due automation profiling spain general awareness fundamental rights legal frameworks ai context everyone eu aware fundamental rights fra' fundamental rights survey shows slightly every second person eu aged older heard charter slightly people two three heard echr universal \" use ai impact declaration human rights might echr older human rights way terms established people' common knowledge decision process matter whether decision made majority people interviewed project acknowledge using machine human \" ai generally affect fundamental rights mention interviewee working public use ai potential impact fundamental rights administration estonia aware implications responses influenced different ways use ai also understanding fundamental rights example one respondent working production pension forecasts based machine learning says producing statistics impact fundamental rights apart data protection issues need addressed another respondent working social benefits algorithms argues impact depends \" widely human rights defined\" - example right receive correct pension \" rights related data none interviewees working targeted advertising believe protection ensured see use ai affects fundamental rights negatively one respondent working human rights relevance targeted communication customers stated one reason \" response relates lack knowledge exactly fundamental private company spain rights practically interviewees showed awareness rights privacy \" touch topic data protection well non discrimination rights assume human dignity right fair trial effective remedy also human rights issues involved mentioned albeit briefly activities within legal framework activities closer look interviewees' responses indicates diverging views across compliant data protection respondent groups respondents working private companies discuss good practices therefore data protection non discrimination rarely mention rights assume human challenges company working targeted advertising mentions rights issues related attentive issues linked freedom speech right information systems \" sense company promotes rights public administration spain posting adverts helps news websites obtain funding continue work one interviewee notes range rights awareness much broader among public sector representatives working ai referred rights human dignity presumption innocence working ai systems different fields application also highlight use systems also covered sector specific laws example system making decisions unemployment benefits regulated national legislation unemployment insurance administrative procedures data protection however respondents aware legal standards apply use ai unsure absence ai specific regulation several respondents mention ethics guidelines certification schemes work existing guidelines standards necessarily specifically aimed ai case example security system 'iske' estonia3 area financial services payment card industry data security standard respondents also refer standards developed international organization standardization iso institute electrical electronics engineers ieee european committee standardization cen respondent working targeted advertising argues certification needed field posting ads issues linked health sector work banks several interviewees noted \" think organisations developing internal guidelines regulate specific technology like ai sufficient general respondents mention guidelines developed eu principles technology neutral international level guidelines european commission' rules \" private sector estonia high level expert group ai oecd' guidelines unesco standards \"yes codes yes aware ongoing developments eu council europe level procedures codes procedures date refer need update sector specific regulations able using something innovate ai - example area health yet one interviewee created analog world states existing standards sufficient ai need digital world \" regulated separately private sector spain human dignity using ai driven technologies broadly implicates duty respect human dignity foundation fundamental rights guaranteed charter article charter states human dignity inviolable must respected protected times cjeu confirmed case law fundamental right dignity part eu law ai driven processing personal data must carried manner respects human dignity puts human centre discussions actions related ai rather technology 'human ' creating affected new technology needs focus taking human dignity starting point help ensure use ai benefits everyone - example supporting ageing access healthcare dignified manner use ai also risks infringing closely connected charter rights right life article right integrity person article context important consider avoid harmful use ai prevent violations rights example comes use ai people engaging criminal activities ai used weapons apart extreme cases preserving dignity includes avoiding subjecting people ai without knowledge informed consent strongly linked privacy data protection example people' applications social benefits decided upon use ai people need made aware consent use automated decisions taken give another example certain proportion population feel comfortable subjected biometric identification systems hence using without allowing opt could potentially violate dignity respondents public administration referred right dignity discussing fundamental rights one respondent considering use ai prisons mentions particular context first needs assessed whether risk violating fundamental rights would high right human dignity interviewees made general references right without discussing relation concrete use ai right privacy data protection - selected challenges right respect private life protection personal data articles charter core fundamental rights discussions around use ai closely related rights respect private life protection personal data distinct self standing rights described \"classic\" right protection privacy \"modern\" right right data protection strive protect similar values e autonomy human dignity individuals granting personal sphere freely develop personalities think shape opinions thus form essential prerequisite exercise fundamental rights freedom thought conscience religion article charter freedom expression information article charter freedom assembly association article charter given two rights absolute rights subject limitations however interference needs adequately justified11 cannot compromise essential inalienable core right explained section concept \"private life\" \"privacy\" complex broad susceptible exhaustive definition covers physical psychological integrity person therefore embrace multiple aspects person' physical social identity also zone interaction person others even public context may fall within scope \"privacy\" contexts ecthr used concept \"reasonable expectation privacy\" - referring extent people expect privacy public spaces without subjected surveillance - one factors albeit necessarily conclusive one decide violation right respect private life relevance scope application however appears limited similarly according un human rights committee mere fact participants assemblies public mean privacy cannot infringed applies monitoring social media glean information participation peaceful assemblies widespread use ai technologies may technologies continue develop raise unchartered issues novel concerns right respect private life ai driven technologies may change way think privacy algorithmic tools predict reveal information people' behaviour unprecedented ways - without people even realizing giving away information personal data obtained internet may instance used targeted advertising raising many fundamental rights concerns issues linked personal data sharing via smart phone apps particularly raises significant concerns including variety potential harmful effects manipulation exploitation vulnerabilities discrimination security issues fraud e g identity theft reduced trust digital economy using ai driven technologies often implies computerised processing large amounts personal data constitutes interference right protection personal data set article charter embodying pre existing eu data protection law well right private life article charter article echr awareness data protection issues use personal data eu people heard gdpr contrast virtually \" little anxious interviewees aware gdpr discussed data protection issues gdpr implemented data protection rules deriving gdpr national law clearly end meant managing datasets well known applied rights area ai fundamental access rights ... good rights less known reminder everything done \" discussing legal framework governing use ai public administration finland respondents mentioned data protection rules well sectoral laws clearly say legal framework apart data protection laws interviewee working spanish public \"actually ' concerned administration notes \" rely data protection regulation norms gdpr might hinder ai research ' available moment\" afraid large databases used previously cannot one interviewee reflecting image based diagnostic tool expressed used research anymore \" view gdpr could hinder research hospital using tool private company netherlands support diagnosis strokes clear rules data protection interviewee indicated although know whether data protection certification requested \" gdpr give specific rules gives others referred general data protection guidelines indicated principles comes aware documents ethical issues interpretation \" private company estonia respondents working target advertising aware privacy data protection issues although responsible data protection issues companies aware efforts protect data privacy one interviewee mentioned contrary earlier years personal data stored much securely handled care attention given properly handling consent data processing consequence high level awareness data protection privacy issues linked ai use however data protection law applies personal data processed example using anonymised data develop ai tools e training data likely permissible many instances would trigger gdpr research shows data often de anonymised however efforts often require expert knowledge potentially additional information illegal illegality de anonymisation necessarily preclude applicability gdpr important consider identification anonymised data reasonably likely anonymising data one aspect protecting privacy data subjects assessing risks identification aspects also important consider disseminating anonymised data include use data purpose outputs produced interviews respondents always entirely clear use personal data often superficially described data used mentioned chapter several instances interviewees indicated use non personal data anonymised data arguing data protection relevant cases example semi public organisation working environmental management uses aggregated data water consumption machine learning based predictions water consumption data available individual level interviewees said use personal data although data originally stem individuals tool supporting restaurant inspection collecting data online sources use personal data - interviewee indicated however indicated need careful mining data online even publicly available might include personal data usernames \" would great retrieve another example insurance company using chatbot make client data another service contact effective data used train system chat protocols client ' repeat conversation logs linked personal data however line go example linking data personal data might possible reusing data \" future according respondent public administration finland companies working targeted online advertising indicate using pseudo anonymised data done example excluding names social security keys encrypting data identity consumers relevant company interviewee mentioned indicate use non personal anonymised data others possible data used make predictions decisions specific individuals example interviewee company working credit rating mentioned need know identity consumers assessments case even important right forgotten according interviewee exhaustive discussion data protection issues possible report however two aspects clearly emerged interviews automated decision making linked right human review right obtain meaningful information decisions automated automated decision making article gdpr article law enforcement directive generally forbid automated decision making meaning \"decision based solely automated processing including profiling produces legal effects concerning similarly significantly affects \" article gdpr explicit consent needed decisions solely automated legal similarly significant effect people automated decision making authorised law authorisation union national law sole precondition law enforcement directive article processing decision considered fully automated instruments require human review controller however concept 'automated' decision making elusive requires discussion research example cases human intervention might limited 'signing ' outcomes ai system rendering virtually automated importantly human review must mean human signing recommendations outputs algorithm must done someone \"authority competence change decision\" considering relevant data hand humans review potentially override outcomes system must also evaluated research indicates humans overrule outcomes algorithms mainly result algorithm line stereotypes behaviour threatens possible added value automated processing potentially accurate even fairer humans may also put minority groups disadvantage therefore also relevant non discrimination issues discussed overall disagreement exact scope provisions eu data protection acquis whether impose general ban certain types automated decisions provide data subjects rights context certain types ai driven decision making using algorithms area social benefits health predictive policing clearly potential legal significant consequences interviews suggest working areas well aware concept human review decisions taken support ai many interviewees indicate automated decisions taken one exception automation unemployment benefits based national law fully automated decisions involve discretion another example another country positive decisions based pre defined rules automated student benefits case negative decisions made humans cases refer rule based decisions involving use statistics machine learning another respondent testing use ai systems including machine learning area social benefits mentions equality could negatively impacted automation makes human behaviour visible including existing biased practices makes precautions necessary consequence organisation would allow decisions made humans interviewees working health highlighted risks linked automation decisions interviewee discussing tool support stroke diagnosis feels important rely system avoid risk automation confirmation bias caution early positive experiences application could prompt users rely easily devote less attention assessment images interviewees raised similar concerns one interviewee discussing tool analyses images provide probability presence certain type lesion notes technology supports diagnosis simple cases expertise doctors particularly important - trusted - complex cases targeted advertising often considered significant effect people however may case example individual' vulnerabilities used successful advertising considering vulnerabilities particularly important people disadvantaged groups may aware opt direct marketing see box right say decisions automated absence case law area information research needed identify impact automated decisions e advertisement delivered answering questions challenging targeted advertising based highly complex technology scale eurobarometer survey asked people eu aware right awareness opt direct marketing overall eu citizens heard right right opt exercised people exercise right aware direct - becomes even important direct marketing made much marketing efficient machine learning among general awareness levels strongly vary across eu percentage people know population right opt direct marketing ranges bulgaria netherlands figure shows percentages also highlights - based fra' analysis eurobarometer data - strong variation within countries broken regions regions fewer one four heard right areas higher shares people risk poverty indicates general problem people disadvantaged society tend less aware right data show people working often struggle pay bills living rural areas older less aware right figure awareness gdpr right opt direct marketing eu united kingdom country region fi- se- ee- insufficient data available lv- dk- uk- lt- ie- nl- pl- de- - lu- cz- sk- - hu- fr- si- ro- hr- - bg- pt- es- el- mt- cy- note map show non eu countries uk light shading aware right dark shading less aware right results regions within countries represented light grey spaces excluded fewer respondents meaning numbers observations low reliable results n question \" general data protection regulation gdpr guarantees number rights heard following rights ... right object receiving direct marketing \" source fra calculations presentation based european commission eurobarometer experiences use cases general interviewed experts highlighted data protection law difficult interpret lacks clarity comes meaning automated decision making one expert france felt automated decision making difficult explain automated decision making banned meaning exceptions gdpr allow automated decision making removed pointed ai used decision support tool another expert independent lawyer netherlands views current laws standards sufficient says need concretised per sector particularly expert mentions scope existing rules permissible automated decision making clear remains unclear comprehensive assessment 'human loop' means also raised relation syri case remained unclear extent decisions reviewed another expert working supervisory authority generally sees need adapting data protection laws \" legislation quite comprehensive organisation supervision thereof also political behind \" concerns reflect findings research also raise serious \" risk much issues concerning right human review example responsible trust machine \" officers questioned results algorithmic system built profile public administration france unemployed people poland less one percent cases essentially makes supporting tool automated decision making tool \" huge tension surrounding linked question reviewing decisions outputs ai systems gdpr want well challenge clear lack knowledge ai works interviewees might fact worse often could explain detail system use works interpretation data data uses due lack knowledge lack transparency turns impossible \" meaningful information logic involved explaining outcomes public administration netherlands algorithms essential several fundamental rights crucial processing personal data also ensuring algorithms fair discriminate also necessary enable \" explain model people properly challenge decisions ai systems ' able model statistical explainable \" one interviewee working public administration explains complexity public administration france differs depending tasks licence administration systems relatively straightforward crime prevention analysis uses data sources makes harder understand another interviewee working law \"internally explain enforcement says current ai used police organisations yet decisions machine learning complex would make explanations difficult might models several means case future \" private sector estonia respondent working financial data transactions indicates traditional models straightforward understand however new methodologies difficult explain company invest resources \" systems black making models explainable still level explainability boxes information processes required gdpr clear respondent already take step forward defence human rights \" public administration spain \" strongly attached idea ai explainable \" public administration france people aware right say decisions awareness automated evidence suggests eurobarometer survey showed europeans right know data protection rights say decisions fra' analysis eurobarometer survey shows figure drops considerably automated among people lower socio economic status eu citizens report struggling pay bills time know right lack rights awareness among socially disadvantaged could contribute social exclusion already disadvantaged less aware challenge automated decisions see figure gender differences small yet women even less aware right women men older people considerably less aware among aged older figure awareness right say decisions automated age gender difficulty paying bills years older years age years years women gender men refusal difficulty paying bills almost never never time time time total notes n question \" general data protection regulation gdpr guarantees number rights heard following rights ... right say decisions automated e g algorithm decides granted loan \" source fra calculations presentation based european commission eurobarometer equality non discrimination equality law non discrimination enshrined articles charter discrimination \" one person treated less favourably another would treated comparable situation\" based perceived real personal characteristic28 called 'protected grounds characteristics' article charter prohibits discrimination based ground sex race colour ethnic social origin genetic features language religion belief political opinion membership national minority property birth disability age sexual orientation charter prohibition reflects corresponding rights echr article protocol echr article even broader establishes non exhaustive open list extending protection wide range new grounds unlike article echr charter right non discrimination freestanding right applies situations need covered charter provision main challenges discrimination crucial topic comes use ai purpose machine learning algorithms categorise classify separate one interviewed expert points making differences per se bad thing according expert deciding grant loan credit history used differentiate individuals basis protected attributes gender religion however many personal attributes life experiences often strongly correlated protected attributes credit history might systematically different men women due differences earnings job histories interviewees often mention efficiency main purpose using ai related technologies yet important note cannot justify unfair differential treatment often protected attributes might highly correlated risks example differences life situations among men women might often linked different insurance risks however acceptable test achats ruling shows case cjeu put end gender discrimination insurance pricing certain circumstances areas using algorithms could positively contribute reducing bias stereotyping algorithmic data analysis may produce results could dispel prejudicial attitudes example predictive policing might contexts lead equitable non discriminatory policing reducing reliance subjective human judgments predictive techniques may used identify called 'white collar crimes' financial crimes historically policed nevertheless direct indirect discrimination34 use algorithms involve big data considered one pressing challenges use ai driven technologies bias discrimination including gender based discrimination data supported algorithmic decision making occur several reasons many levels ai systems difficult detect mitigate often quality data biases within source potential discrimination unfair treatment discriminatory effects generated certain groups practice difficult individuals challenge far limited number court cases dealt discrimination relating ai systems first instance decision divisional court cardiff dismissed claim uk court concerning lawfulness south wales police' use \"afr locate\" face appeal police recognition system court appeal overturned decision use facial recognition found facial recognition programme used police unlawful court violates appeal ruled \" much discretion currently left individual police officers\" added \" clear placed watch list clear human rights criteria determining technology deployed\" court also held police sufficiently investigate software use exhibited race gender bias judgment first merit specifically matter europe considerably narrows scope permissible law enforcement agencies need fully comply human rights law uk court appeal r bridges v cc south wales ewca civ august ars technica 'police use facial recognition violates human rights uk court rules' august studies highlighted potential discrimination prompted use ai systems across areas covered report area predictive policing example particular risk relates potential automated decision making tools reproduce entrench existing discriminatory practices undermine equality law article charter historical crime data underpins predictive policing may biased reflecting inherent data gaps e g chronic underreporting certain types crime alongside issues data recorded e g human error also bias individual officers crime victimisation surveys consistently show large proportion crime never reported police public - particularly crimes involving physical sexual violence hate crimes example fra' survey violence women - respondents - showed one five women experienced violence partner anyone else brought serious incident attention police fra' eu midis ii survey respondents across eu showed three ten reported incidents racially motivated hate crime police organisation compared violent crime hate crime property crime - burglary - higher rate reporting police particularly developed countries may requirement claiming insurance policy sum - relying official crime statistics based reported crime looking develop ai models field predictive policing particularly problematic comes specific crimes specific groups variables used ai modelling proxies race ethnicity gender protected categories complexity algorithms makes harder identify remove biases instead providing objective analysis predictive policing software may turn 'echo chamber' cementing existing systemic flaws injustices 'stamp' appears scientific legitimacy use predictive policing may also make law enforcement responses less equitable focusing certain crimes areas predictive policing currently focused property crimes theft burglaries often associated certain demographics neighbourhoods result certain demographics neighbourhoods - individuals living - stigmatised meanwhile white collar crime - typically committed different demographics - less prioritised patterns policing - whereby certain neighbourhoods communities disproportionately policed - predates use ai however 'promise' ai 'objective' turn used counteract discriminatory policing needs verified practice oxford university researcher sandra wachter highlights discrimination may occur due information linked protected attributes targeted advertising newly created profiles purpose advertising might amount indirect discrimination potentially even require new characteristics added non discrimination legislation extend scope expanded areas experiences use cases \" want machine many interviewees noted use ai general discriminate discriminate basis sex systems working many indicated belief put variable sex excluding information protected attributes sufficient protection easy make examples discrimination however discrimination occur due information symmetrical notice sex contained datasets may indicate protected attributes traces certain relevance \" protected groups often hidden information public administration spain example public authority uses ai tax customs shows challenges linked identifying possible bias potential discrimination using algorithms scrutinising algorithms public administration body found higher degree errors tax declarations among recently issued national identification numbers almost always attributed immigrants prompted research correlation turned outputs people recent identification numbers often contained errors never filed taxes know also case non migrants also example proxy information parts number could indicate immigrant status another interviewee working potential use ai detecting benefits fraud mentioned respect \" want prevent discrimination based ethnicity instance suffice remove 'ethnicity label' neighbourhood composition often also determined ethnicity ethnicity plays role preventing discrimination often goes beyond 'direct' characteristics\" \" f access even respondents aware general potential sensitive personal data discrimination using ai often ruled system impossible check discriminates people based protected characteristics profiling basis \" respondents also believe tools positive impact terms public administration netherlands non discrimination one respondent testing ai social benefits decisions regrets able use ai data protection reasons even though respondent' view automation could process big datasets effectively without discrimination noting protection personal data needs observed respondent feels hinders prompt decision making non discrimination - \" automated automated\" respondents clear sure whether use ai could discriminate respondents repeatedly stated system cannot discriminate include data protected characteristics example several interviewees working predictive policing law enforcement felt potential discrimination ai systems use data return outcomes related protected grounds system aim identify people others working predictive policing felt discrimination could occur particular issues training data relation predictive policing 'heat map' case example one interviewee noted - dataset never fully neutral representative complete - strong risk bias possible discrimination towards particular groups identified sharing datasets increase amount data available one way mitigate risk felt impeded data protection regulations also indicated multi level teams task travel different police authorities check quality systems used set area targeted advertising interviewees mentioned discrimination potential problem mainly asked directly overall respondents think systems discriminate three respondents mention information gender age used consequently discrimination respect occur another interviewee sure information included \" discrimination ' complicated respondent working breast cancer detection tool highlighted diseases age gender ethnicity relevant factors population groups present certain ethnic groups likely develop certain types cancer respondents working predictions take account health highlighted potential discrimination also linked sexual ethnic genetic character uses system suggesting could become greater challenge discriminatory violation system used non medical staff human rights \" private sector france different related example comes respondent working credit rating private company selling credit scores individuals created algorithm company uses information gender age citizenship credit risk models information impact outcome credit scores example younger people non citizens higher credit risk score influence demographics much smaller compared credit history data according interviewee system \"certainly impact right non discrimination make decisions sell data data analytics creditors monitor discriminate\" another interviewee working data strategy financial institution private sector using ai analyse financial transactions clearly mentions challenges understanding non discrimination constitutes work interviewee mentions example clear extent illegal exclude older people receiving credit life expectancy expected lower mortgage repayment period asked findings point uncertainty ambiguity financial sector respect article charter - non discrimination - translates real life situations vulnerable groups much discussion research discrimination using ai linked biased results respect ethnic origin gender extent age although important analyse potential discrimination groups charter covers several grounds discrimination less often part discussions research grounds include example political opinion sexual orientation disability charter provides particular rights special groups beyond articles including rights child article rights elderly article rights persons disabilities article question age - respect older age groups younger adults - came interviews notably comes insurance credit see however none interviewees experts directly mentioned rights child might linked extent nature use cases investigated clearly reflects fact topic high agenda many working ai article charter emphasises best interests child must primary consideration activities public authorities private actors concern children applies course - equally - field ai two respondents public administration mentioned possible use ai area child custody distribution children schools address consideration rights child respondents wish go detail concerning use cases - potentially reflecting sensitivity topic finally issues linked integration people disabilities raised interviews eurobarometer survey included questions ai asked respondents areas awareness mostly concerned comes use ai including discrimination among general decision making unclear responsibility nobody complain population potential around eu citizens indicated concerned using ai could ai lead lead discrimination terms age gender race nationality - example taking decisions recruitment credit worthiness etc discrimination results vary across countries higher proportions people concerned discrimination netherlands luxembourg sweden lower proportions expressed concern estonia hungary lithuania see figure however question clear people know discrimination happen aware happen think problem figure awareness risks discrimination using ai country eu nl uk lu se fr el si de es cy ie hr dk cz fi bg pt sk mt lv ro pl ee hu lt notes includes people indicated concerned ai could lead discrimination among three possible issues three issues source fra calculations based european commission eurobarometer tackling gender charter stipulates equality inequality women men must ensured areas including design employment work pay article gender discrimination use ai major concern comes design use ai related technologies development side european economic social committee notes development ai taking place within homogenous environment principally consisting young white men results cultural gender disparities embedded ai technologies example training data prone manipulation may biased reflect cultural gender prejudices preferences contain errors see also european also reflected research commission white despite efforts achieve paper artificial intelligence - gender balance majority european approach interviewees men excellence trust com final disparities design brussels february deployment stage linked p systematic disadvantages affecting european economic women labour market social committee potential lack awareness artificial intelligence - gender biases recent study consequences showed increased use artificial intelligence industrial robots could widen digital single gender gap despite market production genders benefitting increased consumption automation analysis indicated employment society men medium high initiative opinion skill occupations would benefit may jo c p disproportionally aksoy c ozcan b looking ahead using data philipp j algorithms could help better robots gender mainstream gender equality pay gap europe iza discussion paper policies processes paying attention gendered datasets drawing discussions around see webpage data feminism gender inequalities use datasociety' website data 'data feminism' could help raise awareness criado perez c invisible male point view women exposing data taken default view bias world designed also finds way men london datasets access justice right effective remedy tribunal fair trial article charter one often used charter right legal proceedings highlights importance upholding fundamental rights rule law right horizontal character empowers individuals challenge measure affecting right conferred eu law respect guaranteed charter cjeu underlined article charter constitutes reaffirmation principle effective judicial protection characteristics remedy must determined manner consistent principle right effective remedy also covers decisions taken support ai technologies eu data protection law reconfirms right effective judicial remedy must provided relation decisions controller processor53 well supervisory authority data processed ai driven technologies exception crucial note possibility lodge administrative complaint supervisory authority provided gdpr law enforcement directive55 considered effective judicial remedy article charter court involved review judicial review always remain available accessible internal alternative dispute settlement mechanisms prove insufficient person concerned opts judicial review using ai challenge right effective remedy different ways one prominent concern lack transparency use operation new technologies algorithmic decision making notoriously opaque data collection algorithm training selection data modelling profiling situation around individual consent effectiveness error rates algorithm aspects often transparently reported without access information individuals may able defend assign responsibility decisions affecting appeal decision negatively affecting fair trial includes principle equality arms adversarial proceedings established ecthr requirements also form part corresponding charter right article view article charter main challenges issues reflected specific challenges right effective remedy fair trial interviewed experts outlined generally experts indicate difference accessing remedies private companies public administration public authorities often forced transparent use ai meanwhile companies appear secretive assessment several experts suggests however expert netherlands said people might readily complain companies reluctant complain public authorities public services often concern vulnerable people need social benefits would less inclined complain decisions opportunities successfully complain use ai challenge decisions based ai essential providing access justice interviews emphasised following important respect ----making people aware ai used ----making people aware complain ----making sure ai system decisions based ai explained first everyone needs know dealing ai system taken decision affects people e g social benefits people concerned might complain general - able complain use ai know ai involved expert explained general willingness complain biggest problem people often know ai used organisations transparent even though required gdpr several interviewees indicate informing people decision made based partly automated tools first step providing access complaints second everyone needs know complain may difficult people know body deals type complaints one expert pointed consumers often know complain - example bank might use algorithms deciding financial matters public administration issues automated decisions decided add names employees decisions provide contact persons potentially challenging automated decision interviewees indicated ways procedures complaints place procedures complaints linked use ai companies organisations use ai anonymised aggregated data indicate complaint mechanisms place finally complaining need enough information challenge underlying decision thorough information ai systems provides equality arms meaningfully challenge decisions however straightforward comes use ai particularly ----potential intellectual property rights issues ---- complex systems difficult explain intellectual property rights form one hurdle providing enough information decision made system works algorithms part implemented software technical invention may subject intellectual property rights - right protected article charter actors often seek copyright patent trade secret protection safeguard knowledge ai one interviewee insurance sector claims due highly competitive market \"one may share much workings used technology\" instance particular price given customer essentially competitors could benefit knowledge underlying software subject scrutiny another respondent using ai handle visa applications notes using systems developed external providers whose algorithms covered intellectual property rights hinder necessary transparency later stage another challenge successfully complaining automated decisions use ai general challenge explain decisions based complex systems interviewees working public administration suggest usually clear guidance complain administrative decision area interviewees highlight importance detailed explanations example systems automatically provide unemployment benefits cases involve discretion clients ask reasoning behind automated administrative acts interviewee indicates clients wish see calculations behind financial decisions may self service system organisation' website publications contain detailed descriptions calculations used interviewees recognise open transparent logic essential providing explanations regarding ai supported decisions often challenging impossible achieve one interviewee working bank mentions complex machine learning solutions cannot used certain decision making reasoning system cannot explained easily systems used purposes however interviewee working another bank indicates systems used use simpler methods addition complex ones get idea probable reasons decisions one expert raised problem companies internally might enough information way algorithms work lack expertise knowledge appears major hindrance practice seeking access effective remedy experiences use cases \" topic transparency respondents discussing predictive policing tools highlighted transparency important nowadays important many procedures publish information many automatic gender based violence use case felt sending police means help upload file outcome ai system judge informing victim information portals level risk attributed case police measures lot work done apply result enhances transparency terms transparency \" public administration spain interviewees discussing heat map example referred numerous requests police explain system' purpose works highlighted transparency way reduce public anxiety number interviewees pointed possibility individuals affected system make complaints police courts ombudsinstitution reference domestic violence case however interviewee indicated procedure place question system police protocol terms measures protect fundamental rights health services use cases several interviewees referred ethics committees well general legal safeguards data protection rules checks controls primarily mentioned take place external actors specific complaints procedures place organisations interviewees responded question interviewees highlighted doctors ultimately take responsibility decisions patients often know use ai tool first place example breast cancer detection example interviewee indicated possibility legal recourse developer tool radiologist makes decision diagnosis liable errors safeguards place targeted advertising cases mainly follow data protection requirements ensuring consent obtained respected one company makes sure clients engaged illicit practices rejects clients certain sectors political advertising complaints received organisations interviewed received complaints challenging use ai cases interviewees claim received complaints complainants aware ai used noticed incorrect outputs decision making example individuals lodged complaints regarding traffic fines whereby police officer stopped car driver upon hearing car driver' explanation fine wrongfully administered proceeded manually correct information system without able update system' historical data cases fines remain visible throughout system particular person would continue profiled high risk occasion even though organisations rarely received formal complaints respect \" number complaints use ai interviewees often state due early stages data use miniscule rather ai implementation nonetheless interviewees reported repeated people may asked delete requests access rectification personal data people information \" requested information removed well explanations private company estonia certain recommendation made majority interviewees claim procedures decision processed undertaken human hand interviewees showed interest opening new channels analyse explain redress decisions involving ai solutions rights linked access justice set charter also impacted notably use ai law enforcement include example presumption innocence article charter identifying people suspected committed crime police may target activities specifically one person put suspicion based flawed fragmented data algorithmic profiling uncritical reliance automated tools without proper human review takes account information might contribute discrimination decision making right social security social assistance right social security assistance enshrined article charter classic social right inspired various international european legal standards provision combining elements right principle great significance eu view free movement people within union instead tying issues social protection labour market charter right takes new communitarian approach broadly referring \"providing social protection cases maternity illness industrial accidents dependency old age case loss employment\" article however primarily programmatic statement prescribe minimum standard protection principle eu member states determine conditions entitlement access social benefits clarification needed cjeu yet article charter provides protection measures restricting abolishing existing social security rights addition access social rights guaranteed individuals legally residing within eu exercise right free movement regardless nationality subject eu national laws article thus creates justiciable rights national courts cjeu becoming increasingly apparent impact ai technologies social protection systems lives many individuals rely upon far reaching - potentially - problematic introducing ai driven technologies social welfare systems risks creating barriers access right example using ai social security needs account potential negative - discriminatory - effects non nationals eu citizens third country nationals exercising right freedom movement eu could negatively affected example system relies data job histories available moving eu member states one respondent addressed 'right receive correct pension' aspect wider definition human rights meanwhile none interviewed referred fundamental right social security social assistance could partly reflect nature use cases however lack references social rights among public sector interviewees notable consumer protection charter stipulates eu policies must ensure high level consumer protection based article tfeu eu institutions bodies needs observe principle member state authorities implementing eu law charter principle provides guarantee particular goal \" high level consumer protection\" article tfeu concrete also determines means achieve stated aim - example protecting health safety economic interest consumers well promoting right information education among use cases use ai targeted advertising use medical records companies particular importance comes targeted advertising consumers need aware opt targeted aware might subjected advertising want particularly problematic combination highly sophisticated ai systems advertising amount sort manipulation consumer preferences consumer protection also major relevance use health data ehrs european consumer organisation beuc noted ai area health brings challenges consumers recommends ai technologies must fully respect data protection rules transparent consumer avoid discrimination beuc also called updated regulation legislative measures market surveillance law enforcement efficient redress concerning digital health products services fully protect eu consumers beuc carried survey among consumers views ai selected eu member states shows one two respondents agree companies using ai manipulate consumer decisions addition almost half respondents believe personalised content adverts e commerce platforms added value slightly half survey respondents expressed low trust governments effectively control ai interviews conducted study consumer protection mentioned margins discussing risks using ai fundamental rights however respondents businesses refer consumer protection legislation relevant framework also applying use ai moreover respondents deem consumer protection authorities potentially relevant oversight bodies ai used general terms many interviewees business sector stress importance consumer satisfaction example company using video surveillance security customers premises mention consumer protection regulations relevant technical solutions use systems aim improve situation consumers also preserving rights several ai tools built understand profile consumers enable businesses improve services marketing data protection important aspect business also linked fact breaching data protection rules considered business risk mentioned one major concern companies obtaining managing consent consumers customers process data using ai tools marketing purposes interviewees report gdpr impact improving systems handle consent right good administration right good administration well established general principle eu law elaborated cjeu binding eu member states also fundamental right enshrined article charter although actions eu institutions bodies agencies general principle eu law requires eu member states apply requirements right good administration public action right includes limited right individual access file obligation public authority give sufficient reasons decisions access file facilitates understanding evidentiary basis decision made reasons underlying places individual better position put forward counter arguments exercising right heard right effective remedy obligation give reasons makes perspective individuals affected decision making process transparent person concerned know understand measure action taken transparency also enabling principle provides foundations rights including exercise right effective remedy according cjeu context individual decisions made important determining extent duty give reasons france instance code relations public administration requires written explanations factual legal considerations decision based right good administration also applies ai systems process personal data support decision making public authorities although right good administration may subjected certain limitations question arises ensure potentially huge number individuals access files personal data used ai systems another question make sure public authorities always give sufficient reasons operation ai driven technologies cannot fully explained due inherent opacity complexity use system categorise unemployed people set poland highlighted problems linked public administration use algorithms based questions answered unemployed people categorisation developed statistical algorithm system received lot criticism civil society respect lack opportunities complain potential discrimination end complaint ombudsinstitution - based administrative grounds - led constitutional court ruling put end system' use intent increase efficiency drives use ai public sector - aim directly speaks improving administration benefiting citizens respondents public administration far often indicate efficiency reason considering use ai presently using ai one respondent advises ministries digital strategies use ai said main reasons adopting ai improve service citizens reduce costs services public administration interviewees also indicate public administration particular requirements meaning ai cannot used purposes needs particular attention comes decision making however efficiency system also considered important added value sense respondent working digitalisation migration management indicates building complex ai systems risk afterwards would require lot work understand system retrospect interviewee indicates team needs careful allow ai make final decisions taken human - society clients ready according interviewee although systems appealing work effectively could result extra work negative results however interviewee also indicates dimension efficiency \" often side lined discussing data protection\" requirements good administration also directly link issues raised respect data protection non discrimination right effective remedy fair trial public administration process data legal basis decisions need fair transparent pathways challenge decisions need available accessible result requirements good administration directly linked discussion analysis respect legal processing data data protection fair decisions linked discussion non discrimination alongside transparency ways challenge explain decisions respect access justice endnotes see european commission european enterprise survey use technologies based artificial intelligence luxembourg july fra fundamental rights mean people eu luxembourg publications office p see webpage three level baseline security system iske ther website estonia' information system authority see website pci security standards council barak 'human dignity framework right motherright ' barak human dignity constitutional value constitutional right cambridge cambridge university press ch pp cjeu c netherlands v european parliament council october paras discussion malicious use ai see example brundage et al malicious use artificial intelligence forecasting prevention mitigation fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office november cjeu joined cases c c volker und markus schecke eifert gbr hartmut eifert opinion advocate general sharpston june para fra council europe edps handbook european data protection law edition luxembourg publications office june p see also ibid pp ecthr guide article european convention human rights - right respect private family life home correspondence strasbourg council europe updated august paras ecthr lopez ribalda others v spain nos october para comprehensive legal analysis meaning content 'privacy' see also koops b j et al ' typology privacy' university pennsylvania journal international law vol issue pp vermeulen surveille deliverable d4 - scope right private life public places july p un human rights committee general comment right peaceful assembly article ccpr c gc september para costello roisin aine impacts adtech privacy rights rule law technology regulation norwegian consumer council control consumers exploited online advertising industry fra rights matter data protection privacy - fundamental rights survey luxembourg publications office rocher l hendrickx j de montjoye estimating success identifications incomplete datasets using generative models nature communications hacker p legal framework ai training data law innovation technology forthcoming available ssrn article data protection working party opinion anonymisation techniques see also finck michele pallas frank must identified distinguishing personal non personal data gdpr october forthcoming international data privacy law max planck institute innovation competition research paper available ssrn sartor g lagioia f impact general data protection regulation gdpr artificial intelligence study prepared panel future science technology stoa european parliament see example uk data service' blog \"access sensitive data research ' safes'\" see also discussion ohm p \"broken promises privacy responding surprising failure anonymization\" ucla law review p gdpr art law enforcement directive art veale edwards l 'clarity surprises questions article working party draft guidance automated decision making profiling' computer law security review vol april pp article data protection working party guidelines automated individual decision making profiling purposes regulation adopted october last revised adopted february green b chen 'disparate interactions algorithm loop analysis fairness risk assessments' fat ' conference fairness accountability transparency fat ' january gonzalez fuster g artificial intelligence law enforcement - impact fundamental rights european parliament policy department citizens' rights constitutional affairs directorate general internal policies pe july p brkan ' algorithms rule world algorithmic decision making data protection framework gdpr beyond' international journal law information technology vol p article working party guidelines automated individual decision making profiling purposes regulation adopted october last revised adopted february wp251rev p misuraca g van noordt c overview use impact ai public services eu european commission joint research centre luxembourg council directive ec june implementing principle equal treatment persons irrespective racial ethnic origin oj l pp art council directive ec november establishing general framework equal treatment employment occupation oj l pp art fra coe handbook european non discrimination law edition luxembourg publications office june p cjeu c association belge des consommateurs test achats asbl others v conseil des ministres january european commission eu rules gender neutral pricing insurance industry enter force press release ip december elizabeth e joh ' new surveillance discretion automated suspicion big data policing' uc davis legal studies research paper pp ales zavrsnik 'algorithmic justice algorithms big data criminal justice settings' european journal criminology p doi see also european commission white paper artificial intelligence - european approach excellence trust com final brussels february p fra bigdata discrimination data supported decision making luxembourg publications office june p ibid fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office korff browne ' use internet related services private life data protection trends technologies threats implications' council europe pd see national non discrimination equality tribunal finland decision march see also syri case discussed uk court appeal r bridges v cc south wales ewca civ august see also equinet regulating equal ai new role equality bodies brussels report prepared allen r masters tolan miron gomez e castillo c ' machine learning may lead unfairness evidence risk assessment juvenile justice catalonia' best paper award international conference ai law richardson r schultz j crawford k dirty data bad predictions civil rights violations impact police data predictive policing systems justice n u l rev online available ssrn fra violence women eu wide survey main results report luxembourg publications office p fra second european union minorities discrimination survey main results luxembourg publications office p erik bakke \"predictive policing argument public transparency\" new york university annual survey american law vol pp andrew g ferguson 'policing predictive policing' washington university law review vol pp andcouncil europe committee experts internet intermediaries msi net algorithms human rights council europe dgi p elizabeth e joh ' new surveillance discretion automated suspicion big data policing' uc davis legal studies research paper p gstrein j bunnik zwitter 'ethical legal social challenges predictive policing' catolica law review pp albert meijer martijn wessels 'predictive policing review benefits drawbacks' international journal public administration p doi ales zavrsnik 'algorithmic justice algorithms big data criminal justice settings' european journal criminology pp doi wachter sandra 'affinity profiling discrimination association online behavioural advertising' berkeley technology law journal vol forthcoming available ssrn use ai financial industries leading unequal access financial services see legal literature e g boyd levy k marwick ' networked nature algorithmic discrimination' gangadharan p eubanks v barocas eds data discrimination collected essays open technology institute pp overview child rights issues see unicef innovation human rights center uc berkeley artificial intelligence children' rights eu network independent experts fundamental rights commentary charter fundamental rights european union june p see also fra coe handbook european law relating access justice luxembourg publications office june p cjeu c unibet london ltd unibet international ltd v justitiekanslern march para cjeu c et agrokonsulting velko stoyanov v izpalnitelen direktor na darzhaven fond 'zemedelie' - razplashtatelna agentsia june para cjeu c centre public 'action sociale 'ottignies louvain la neuve v moussa abdida december para law enforcement directive art gdpr art law enforcement directive art gdpr art law enforcement directive art gdpr art council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems adopted committee ministers april 1373rd meeting ministers' deputies appendix para b andrew g ferguson 'policing predictive policing' washington university law review pp gstrein j bunnik zwitter ethical legal social challenges predictive policing' catolica law review pp yeung k study implications advanced digital technologies including ai systems concept responsibility within human rights framework prepared council europe expert committee human rights dimensions automated data processing different forms artificial intelligence msi aut council europe algorithms human rights pp international technology law association 'responsible ai global policy framework' pp lack expertise ai also reflected survey among companies eu lack skills among existing staff difficulties hiring new staff prominent obstacle ai adoption european commission european enterprise survey use technologies based artificial intelligence luxembourg july p see detailed assessment impact predictive policing presumption innocence mendola marco one step 'surveillance society' case predictive policing see e g egorov wujczyk eds right social security constitutions world broadening moral legal space social justice geneva ilo global study vol europe pp xv xvii include arts tfeu arts european social charter well points community charter fundamental social rights workers see explanations relating charter fundamental rights oj c pp explanations relating charter fundamental rights oj c pp explanation article -- scope interpretation rights principles lukasz bojarski dieter schindlauer katrin wladasch european charter fundamental rights living instrument - manual rome warsaw vienna pp de becker e ' possible role right social security eu economic monitoring process' german law journal vol pp paju j european union social security law oxford hart publishing sub section ibid pp peers prechal 'scope interpretation rights principles' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp exception poland united kingdom see protocol application charter fundamental rights european union poland united kingdom oj c pp art christiaan van veen ben zevenbergen 'conference social protection artificial intelligence decoding human rights digital age' freedom tinker - research expert commentary digital technologies public life may art charter see also explanations relating charter fundamental rights oj c pp explanation article -- scope interpretation rights principles lukasz bojarski dieter schindlauer katrin wladasch european charter fundamental rights living instrument - manual rome warsaw vienna p sartor giovanni new aspects challenges consumer protection study committee internal market consumer protection policy department economic scientific quality life policies european parliament luxembourg european consumer organisation beuc digital health principles recommendations beuc artificial intelligence consumers say findings policy recommendations multi country survey ai recent case law see cjeu c h n v minister justice equality law reform ireland attorney general may para also confirmed cjeu joined cases c c ys v minister voor immigratie integratie en asiel minister voor immigratie integratie en asiel v july paras components initially developed cjeu case law codified article charter right leading academic literature see craig p 'article - right good administration' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp ibid p finck 'automated decision making administrative law' max planck institute innovation competition research paper p craig p 'article - right good administration' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp france code des relations entre le public et l'administration article l2111 panoptykon foundation profiling unemployed poland social political implications algorithmic decision making see also algorithm watch poland scrap controversial unemployment scoring system see decision k available constitutional tribunal' website fundamental rights impact assessment - practical tool protecting fundamental rights chapter illustrated extent using ai affects different fundamental rights chapter analyses fundamental rights impact assessments fria could reduce negative impacts using ai fundamental rights section provides brief overview current discussion need fundamental rights impact assessments field section analyses current practices addressing fundamental rights implications based interviews conducted report interviewees asked sort testing done system used controls tasks affected use technology chapter ends suggestions assess fundamental rights impact using ai related technologies calling fundamental rights impact assessment - available guidance tools international organisations academics2 civil society3 called fundamental rights impact assessments conducted using ai related technologies example committee ministers council europe' guidelines addressing human rights impacts algorithmic systems recommend states conduct \"impact assessments prior public procurement development regular milestones throughout context specific deployment order identify risks rights adverse outcomes\" need flexible impact assessments adapt different situations given fundamental rights violations always contextual scholars exemplify based eu anti discrimination law equality always contextual depends case hand fundamental rights compliance cannot automated hard coded computer software rather use case needs separate examination determine whether fundamental rights issue arises nevertheless assessments follow systematic approach provide similar information existing standards provide guidance fundamental rights impact assessment ai related technology include hard law soft law instruments recommendations declarations practical tools e g guidelines checklists beyond requirements flowing data protection legislation see box examples laws requiring mandatory assessments effects ai general view increasing uptake ai canadian government issued guidelines including mandatory requirements assessing ai use public administration applies system tool statistical model used recommend make administrative decision client european data protection law requires data protection impact assessment dpia learning coe modernised convention provides general obligation examine likely data impact data processing individuals' rights fundamental freedoms use protection following assessment controllers design processing manner impact prevent minimise identified risks b assessments eu law imposes similar detailed obligation gdpr foresees data protection impact assessment dpia data processing likely \" result high risk rights freedoms natural persons \"c therefore required law dpia ai technology could potentially also address broader fundamental rights implications besides impact right privacy used tool investigate algorithms impacts e however gdpr article dpia limited 'high risk' cases processing personal data therefore may miss high risk cases primarily obviously related protection personal data time gdpr delimited specific field application accompanying expertise field means potential extension scope dpia fundamental rights might limited gdpr also gives indications modalities undertake dpia first dpia conducted high risk processing f second dpia provide systematic description envisaged operations purpose legitimate interests pursued must also assess necessity proportionality processing possible risks rights individuals addition must contain planned security measures address risks identified g pointing different methodologies apply article working party wp proposes - check list form - minimum criteria controller use assess dpia comprehensively complies gdpr h finally gdpr foresees prior mandatory consultation relevant supervisory authority impact assessment indicates processing presents risks cannot mitigated gives crucial role dpas independent bodies established law j european data protection supervisor edps provides guidance carrying dpias k data protection authorities also discussed provide guidance assess ai technologies l information data protection impact assessment see fra council europe edps handbook european data protection law edition p b council europe modernised convention art c gdpr art gdpr recitals article working party guidelines data protection impact assessment dpia wp248rev october e edwards veale fra bigdata discrimination data supported decision making luxembourg publications office june f gdpr art wp29 specifies 'carrying dpia continual process one time exercise ' g gdpr art well recitals h article working party guidelines data protection impact assessment dpia wp248rev october annex gdpr art j gdpr art k edps accountability ground part ii data protection impact assessments prior consultation v july l see example declaration ethics data protection ai adopted 40th international conference data protection privacy commissioners icdppc many examples non binding guidelines global level united nations guiding principles business human rights recommend enterprises integrate findings human rights impact assessments across relevant internal functions processes take appropriate action although refer specifically ai guidelines relevant supporting development ai technology rights compliant manner eu level ethics guidelines trustworthy ai prepared european commission' high level group artificial intelligence9 also recommend performing fria system' development \" risks fundamental rights negatively affected technology\" also emphasise need put place mechanisms receive external feedback ai systems potentially infringe fundamental rights addition private companies associations private companies12 public private interests well ngos14 organisations15 developed different types guidance support ai impact assessments documents usually contain clear guidelines impact assessment instead highlight different aspects criteria taken account developing carrying impact assessment broad categories include purpose system description technology assessment impact targeted population individual evaluating fairness diversity description audits planned performed well accountability explicitly refer applicable international human rights law standards various codes ethics conducts standards well certification schemes also place several practical tools available assess impact ai technologies mitigate risks developed wide range actors include checklists lists questions online self evaluation tools risk management frameworks focus specifically assessing fundamental rights risks others focus ethical societal economic implications useful references performing thorough fundamental rights impact assessment ai technologies july example high level group artificial intelligence issued \"assessment list trustworthy ai\" altai six month pilot involving stakeholders altai helps organisations self evaluate - voluntary basis - reliability trustworthiness ai reduce potential risks users supports businesses public administrations ask right questions around seven requirements responsible ai identified ethics guidelines trustworthy ai altai specifically refers need perform fundamental rights impact assessment includes examples questions assess impact non discrimination equality right privacy rights child freedom expression well freedom information association several online assessment tools target use ai public authorities canadian government developed algorithmic impact assessment tool aia pursuant canadian directive automated decision making aia represents automated assessment consisting questions unfold requirements directive questions relate fundamental rights concerns - ai system' impact freedom movement likelihood incarceration individual legal status access funding benefits indigenous people score attributed reply final impact scoring provided made publically available government' website another example ethics toolkit31 freely accessible tool designed local governments based risk management approach supports fair automated decisions minimising unintentional harm individuals field criminal justice higher education social media areas among national human rights bodies danish institute human rights proposed human rights compliance \"quick check involves interactive online computer programme allows companies select modify information database suit type business area operations check rights compliance quick check based human rights compliance assessment tool runs database questions corresponding human rights indicators uses international human rights law standards benchmarks applying fields operations provide guidance developing impact assessment ai technology academic work also suggested operational frameworks assessing risks using ai technology focus specifically identifying addressing fundamental rights implications private sector focus developing ethical values oriented models analysing societal impact data used creation ad hoc expert review committee others developed guidance frameworks specific case studies example field criminal justice algo care framework36 introduced step step assessment evaluate key legal practical concerns considered relation police using algorithmic risk assessment tools argued participatory ways involve consider views affected rights holders stakeholders communities developing impact assessment publically engage start others joined cross discipline expertise science law design practical frameworks impact assessments testing practice virtually systems discussed interviews subject sort testing included elements impact assessment however mainly technical data protection impact assessments rarely address potential impacts fundamental rights interviewees argue conducting fundamental rights impact assessment view system negatively affect fundamental rights unsure example respondent working traffic management using cameras monitoring traffic indicated tested accuracy system fundamental rights apart respecting data protection rules respondents simply know fundamental rights assessed part general impact assessment carried testing stages development much testing done new ai system used respondents \" testing system highlighted moving ai system production challenging task really look legal aspects mentioned public administration well private companies usually looked whether system careful using ai many projects interviewees refer still profitable \" development pilot phase started concrete testing private company estonia testing done several stages include development stage called proof concept pilot stages deployment tests deployment possible live experimentation carried initial stages often involves staged deployment example organisation interviewed tests different applications support job seekers conducts continuous step step testing selected members organisation test tool real situations using check lists interviewee mentioned challenging move deployment stage planned supervise tool real time another example involving automated rule based granting social benefits different assessments carried implementation group lawyers data protection specialists compensation specialists accountants performed general impact assessment department responsible using system conducted tests decide whether system could used following system monitored implementation using step step approach first step half decisions taken system next step decisions taken automatically expanded negative decisions another area decisions added including decisions ending compensation payments time interviews conducted decisions automated interviewee indicated carrying tests feel sure system secure outstanding risks company working fraud detection system replaced rule based system machine learning tool changing system old new system run parallel see machine learning system performs better rule based one interviewee mentioned \" rigorous analysis behind direct feedback saw would impact losses versus many good customers impacting negatively\" interviewee added \" comfortable machine learning system better static rule system aspects deployed entirety\" use cases previous automated system existed tests reviewed humans example automated transcription service tested court hearings allowed judge included regular feedback correctness transcription services judges one interviewee law enforcement working tool detect domestic violence identifies issues precision accuracy using system police officer sufficient training knowledge system indicators required system cannot gather required information could lead miscalculation highlight robustness system tested annually assure quality two questionnaires used completeness data training police officers using ai system process also considers personal data protection laws protocols applied tests discussed focus strongly technical aspects general operations fundamental rights data protection impact assessments apart data protection respondents mentioned fundamental rights typically considered respondents reflected potential impacts fundamental rights mentioned aspects considered prompted interviewer many respondents generally aware discrimination issues - often discussed explicitly asked discrimination yet gave information formal depth tests discrimination generally respondents ruled possibility system discriminates based protected attributes example one interviewee states test system data protection laws specific applicable legal acts fundamental rights however interviewee consider potential discrimination ruled needs kept mind future technologies interviewee stated however cases non discrimination generally considered testing phase ai systems one respondent municipal authority mentioned cannot assess fairness model cannot access data needed due data protection reasons according interviewee \" huge tension surrounding gdpr want well might fact worse interpretation data turns impossible\" \"yes assess legality personal data protection respondents reported data protection impact assessment conformity specific legal required law conducted although took different forms bank acts \" tested tool analysing speech customer calls find public administration estonia reoccurring problems carried data protection impact assessment dpia specifically testing tool outcome system tested data used testing phase deleted certain period test access data employees restricted testing phase supervised deployment tool another dpia required case sometimes lack clarity extent use ai related technologies notably use algorithms belongs dpia area predictive policing instance dpias done underlying architecture system rather specific ai tool another interviewee using algorithms financial services also mentioned assessing machine learning tool within framework dpia belief apply machine learning system underlying data one interviewee felt data protection impact assessment crime heat map example sufficiently depth safeguard quality model system equipped deal cross sectoral use data different rules might apply indicated standards required respondent working migration management indicated data protection officers involved analysis legal service specialised quality control ai tool study data protection aspects system however respondent also mentioned guidance needed companies working targeted advertising looked data protection issues although respondents sure impact assessment conducted companies assessed example whether people consented approached targeted communication targeted ads assessed whether information possible identification deleted including whether cookies trackers anonymised respect dpias generally respondents know area responsibility others knew positive dpia aware details appears legal assessment sometimes detached technical side technical people knowing legal assessments one interviewee private company working credit risk scoring mentioned \" make suggestions system could developed compliance manager tells conformity laws\" audits working external oversight bodies public administrations private companies involved fra' research carry tests deploying ai often linked existing internal external oversight processes use ai frequently subjected internal review processes within companies public administration although necessarily formalised review processes interviewees mentioned working formalising existing internal review processes overseeing ai systems interviewees public sector say particularly cautious using ai support decisions representative working migration management public administration indicates \" n private sector wrong results might cause business related losses police impacts people' lives fundamental rights\" yet always clear public administration businesses responsible checking overseeing use ai public administrations appear stronger scrutiny comes oversight ai systems oversight often done regular audits example connected budgetary review interviewees public private organisations report ai systems currently checked framework existing review e g regular database checks absence review processes specifically look use ai addition interviewees report sector specific certification schemes also look use ai - example area health financial services several interviewees mentioned contact data protection authorities companies public administrations sought permission data protection authorities using ai system least generally contact example one company working targeted advertising mentioned discussing use personal data national data protection authority experts interviewed report highlighted relevance data protection authorities overseeing ai systems respect use personal data however experts strongly highlighted data protection authorities resourced task two reasons data protection authorities often relevant ai related expertise additionally budgets overstretched workload heavy experts' views differ respect need additional oversight bodies potential creation ai specific institution however agree existing bodies work topics linked ai within mandates equality bodies well human rights institutions mentioned interviewed experts providing oversight concerning possible discrimination using ai highlighted institutions need build expertise area better contribute oversight ai however similar data protection authorities challenging task equality bodies given lack resources \" proactive among several interviewees mentioned consumer protection authorities potentially mitigate risks providing relevant oversight use ai one respondent working also get additional audits also retail company would like advisory agency could consulted see sometimes regulatory possible use ai innovation without investigated right away audits quite sloppy us moment company prefers consult consumer authorities good lots data protection authorities potential future marketing campaigns customer data \" data protection authorities might start investigation private company estonia efforts discussing oversight developing using ai well experts repeatedly mention challenge really understand impact using ai despite need engage existing oversight bodies responsibilities oversee use ai fundamental rights perspective remain unclear fundamental rights impact assessment practice many key actors field fundamental rights called conducting fundamental rights impact assessments using ai driven systems section highlights elements could incorporated assessment fundamental rights impact assessments needed given contextualised assessment required uses ai vary considerably terms complexity level automation potential errors harm scale application well area use complex ai system difficult assess potential impact fundamental rights implicated vary depending area application full spectrum rights needs considered use ai however uses ai likely involve rights often affected ai systems discussion preceding chapter makes clear issues linked data protection non discrimination well access effective remedies fair trial relevant uses ai thus following horizontal points could basic starting point considering impact ai selected rights ---- legal processing data needs confirmed line data protection laws personal data used full data protection framework applies ensures processing legal violate person' rights respect private family life data protection ---- processing lead unfair treatment discrimination protected groups assessing non discrimination needs core assessing ai even apparently miniscule differences scale create risks contravening principle non discrimination disadvantage people depends nature kind harm severity strength harm significance many people put disadvantage compared another group people statistical assessments group differences important tool assess unfair discriminatory uses ai ----people subjected ai related technologies able complain receive effective remedies accessible ways people complain potential decisions made effectively access remedies includes availability information allows explanation decisions addition relevant rights charter apply public administrations using ai need consider good administration principles businesses take consumer protection account rights relevant depending area application examples include ---- right social protection working social benefits ---- right freedom expression information using ai support online content moderation ---- right assembly association considering use facial recognition technology public spaces ---- right education using ai education sector ---- right asylum using ai support migration management ---- right collective bargaining action using ai 'gig economy' ---- right fair working conditions using ai workplace ---- right access preventive health care using ai health services ---- right presumption innocence right defence using ai justice sector law enforcement purposes information needed assess potential impact fundamental rights implementing ai given variety tools purposes area application assessments contextual able meaningfully respond horizontal points raised assess specific rights linked different use cases least following information needs available ---- description purpose context system well legal basis ---- description possible harm using system including questions around false positives false negatives possible harm due automation scale use ---- description technology used includes information data used building system legal basis processing description relevant information include provided fra' paper data quality ai ---- evidence based description accuracy ai system terms outcomes based training data possible tests experiments real life situations appropriate false positives false negatives considered separately include breakdowns many groups possible allow checking potential discrimination e g differences accuracy women men ---- already available provision information compliance existing standards potential certifications obtained ex post assessments safeguards lastly envisaging ex post safeguards contributes fundamental rights compliant use ai could include ----regular repetition assessments deployment appropriate important learn potential feedback loops case rules updated also requires recording information use outcomes system extent data protection respected ----making people subjected ai systems aware subjected technology otherwise challenge decision affecting ----making available easily accessible channels effectively complaining decisions made based ai system engaging external experts stakeholders oversight bodies information could basis consultation different stakeholders experts particular ai system used depending nature application legal basis consultation relevant stakeholders would ensure potential harm omitted different perspectives brought assessment stakeholders could include civil society different public private organisations well experts different fields fundamental rights including data protection ten experts interviewed report highlighted existing oversight bodies also responsible ai oversight within mandates sector specific bodies certification schemes extent interviews suggest - example health care financial oversight monitor comprehend effectively respond potential impacts ai wide spectrum fundamental rights data protection authorities equality bodies ombuds institutions national human rights institutions could play important role providing input oversight various points expertise however interviews indicated extensive upskilling resource allocation needed underpin endnotes council europe commissioner human rights unboxing artificial intelligence steps protect human rights - recommendation council europe strasbourg may heleen l janssen ' approach fundamental rights impact assessment automated decision making international data privacy law' international data privacy law vol issue february pp alessandro mantelero 'ai big data blueprint human rights social ethical impact assessment' computer law security review vol issue august pp edwards lilian veale michael slave algorithm 'right explanation' probably remedy looking may duke law technology review accessnow access ' submission consultation \"white paper artificial intelligence european approach excellence trust\" council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems april para human rights impact assessment detailed discussion respect non discrimination see wachter mittelstatt b russel c fairness cannot automated bridging gap eu non discrimination law ai government canada directive automated decision making united nations un guiding principles business human rights endorsed human rights council resolution hrc res july principles heleen l janssen approach fundamental rights impact assessment automated decision making international data privacy law vol issue february pp high level expert group artificial intelligence ethics guidelines trustworthy ai april chapter iii ibid p see example ibm everyday ethics artificial intelligence sony sony group ai ethics guidelines vodaphone vodaphone' ai framework arborus international orange international charter inclusive ai april signed private companies including camfil danone edf l'oreal metro sodexo etc information technology industry council iti iti ai policy principles ecp platform information society artificial intelligence impact assessment netherlands november amnesty international access human rights watch wikimedia foundation toronto declaration protecting rights equality non discrimination machine learning systems may rightscon toronto university montreal montreal declaration responsible ai electrical electronics engineers ieee global initiative ethics autonomous intelligent systems ethically aligned design prioritizing human wellbeing autonomous intelligent systems future life institute asilomar ai principles conference outcome future life institute' second conference future artificial intelligence see example ecp platform information society artificial intelligence impact assessment netherlands november ieee initiative association computer machinery acm acm code ethics professional conduct june future humanity institute university oxford standards ai governance international standards enable global coordination ai research development april iso standards iso iec jtc sc artificial intelligence sstandard project direct responsibility iso iec jtc sc secretariat iso iso iec tr standard information technology -- artificial intelligence -- overview trustworthiness artificial intelligence may establishes among others \"approaches assess achieve availability resiliency reliability accuracy safety security privacy ai systems \" iso standards development september iso iec cd information technology -- artificial intelligence -- risk management iso iec awi tr information technology -- artificial intelligence ai -- bias ai systems ai aided decision making iso iec awi tr information technology -- artificial intelligence -- overview ethical societal concerns information available iso' website electrical electronics engineers ieee ieee p7003(tm) algorithmic bias considerations german ai federal association ki bundesverband german ai federal association seal quality ki bundesverband guetesiegel march article working party guidelines data protection impact assessment dpia wp248rev october annex - criteria acceptable dpia high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july government canada algorithmic impact assessment tool danish institute human rights human rights compliance assessment quick check june center government excellence johns hopkins university ethics algorithm toolkit government canada algorithmic impact assessment tool article working party guidelines data protection impact assessment dpia wp248rev october annex data protection focus danish institute human rights human rights impact assessment guidance toolbox ai pulse creating tool reproducibly estimate ethical impact artificial intelligence september fairness accountability transparency machine learning fat ml principles accountable algorithms social impact statement algorithms fat ml social impact statement algorithms high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july see e g human agency oversight technical robustness safety privacy data governance transparency diversity non discrimination fairness societal environmental well accountability high level expert group artificial intelligence ethics guidelines trustworthy ai april high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july p government canada algorithmic impact assessment tool government canada directive automated decision making article appendix c center government excellence johns hopkins university ethics algorithm toolkit danish institute human rights human rights compliance assessment quick check june danish institute human rights human rights impact assessment guidance toolbox heleen l janssen approach fundamental rights impact assessment automated decision making international data privacy law international data privacy law vol issue february alessandro mantelero ai big data blueprint human rights social ethical impact assessment computer law security review vol issue august pp ai pulse program understanding law science evidence pulse ucla school law creating tool reproducibly estimate ethical impact artificial intelligence september model includes series questions assessing human rights impact ai enabled projects marion oswald jamie grace sheena urwin geoffrey c barnes algorithmic risk assessment policing models lessons durham hart model 'experimental' proportionality journal vol - issue ainow algorithmic impact assessments practical framework public agency accountability april institute ethical ai machine learning ethical ml network beta machine learning maturity model brave europe' governments failing gdpr see wachter mittelstatt b russel c fairness cannot automated bridging gap eu non discrimination law ai fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office june moving forward challenges opportunities report published amidst ongoing european legislative policy developments artificial intelligence global fight coronavirus covid pandemic potentially quickened acceptance innovative technologies yet also shown ai panacea problems comes various challenges report clearly shows using ai systems engages wide range fundamental rights also shows many businesses public administrations already using planning use ai related technologies however technologies involve different levels complexity examples use relatively simple algorithms level automation also varies - - decision making subject human review applications currently used also often development stage eu national legislators policymakers keep reality mind - especially presented optimistic expectations ai' potential vis vis challenges related using new technologies need regulate \" try look future vast majority public administrations businesses interviewed plan automate \" keep working using ai two interviewees indicated private company estonia use develop ai another two interviewees cautious plan wait see others including lack resources work using ai \" next steps related transparency open data however said develop continue test tools say publish information data infrastructure respect use ai includes starting pdf also information new continuing ongoing pilots evaluating existing efforts sharing data reusable formatting could results others increasing data quality trying obtain reused internally data sources private sector \" public administration spain interviewees mentioned engaged ongoing debates expressed desire contribute development legislation still see current situation - absence harmonised law \"ai great thing must area - obstacle use ai addition respondents learn use \" said working issues linked interpretability ai private company spain means working methods enhance understanding explanation decisions based complex ai indicated desire look closely ethical legal matters figure shows correlations words interviewees often use talking future use ai figure indicates topics often raised example interviewees often used term 'data' discussing future developments figure correlations words respondents often mention discussing future plans use ai technologies systems plans data development company ai tool police management solutions companies level system project technology administration tax analysis organisation service processing lot translation process related customer easy national phase payment information services application time quality improve learning business human energy algorithms decisions support machine ml continue potential people develop future notes based text interview summaries respondents spoke future use ai including words mentioned least ten times lines connecting words indicate strength word correlations within text passages size dots indicate frequency words used source fra effectively adequately protecting fundamental rights eu key objective current efforts better regulate use ai context upcoming eu legislation ai european commission' white paper addresses current gaps helping mitigate uncertainty around use ai respect fundamental rights making use ai transparent accountable terms fundamental rights includes requirements ai use directly link information needed assess impact ai fundamental rights discussed requirements linked description training data data record keeping information provided subjected ai robustness accuracy well human oversight highly relevant assessing protecting fundamental rights respect body evidence presented report offers general insights different technologies affect fundamental rights safeguards needed ensure fully fundamental rights compliant use ai practice time research fundamental rights implications use ai specific areas support policy legislative efforts eu level aiming shape europe' digital future widely fra continue look fundamental implications ai carrying focussed analysis specific use cases increase knowledge potentially go wrong consequently help mitigate prevent fundamental rights violations fra look potential simulation studies showcase biased algorithms negatively affect fundamental rights use ai often involves automating tasks previously carried humans need acknowledge human behaviour sometimes line fundamental rights using ai using ai example police might engage unlawful profiling decisions public administration companies might sometimes driven negative stereotypes current developments use ai need acknowledge potential discrimination respect data ai system built respect underlying assumptions humans turn may feed development deployment system automating certain tasks without fully understanding automated could lead unlawful processing data use technology treats people unfairly might make impossible challenge certain outcomes - name challenges however increased availability data technological tools also used better understand unequal treatment occurs current technological developments increased availability data also provide unique opportunity better understand structures society used support fundamental rights compliance opportunities created ai also contribute better understanding consequently mitigation fundamental rights violations getting touch eu person european union hundreds europe direct information centres find address centre nearest https europa eu european union contact en phone email europe direct service answers questions european union contact service --b freephone certain operators may charge calls -- following standard number -- email via https europa eu european union contact en finding information eu online information european union official languages eu available europa website https europa eu european union index en eu publications download order free priced eu publications https op europa eu en publications multiple copies free publications may obtained contacting europe direct local information centre see https europa eu european union contact en eu law related documents access legal information eu including eu law since official language versions go eur lex http eur lex europa eu open data eu eu open data portal http data europa eu euodp en provides access datasets eu data downloaded reused free commercial non commercial purposes promoting protecting fundamental rights across eu -- artificial intelligence ai already plays role deciding unemployment benefits someone gets burglary likely take place whether someone risk cancer sees catchy advertisement low mortgage rates use keeps growing presenting seemingly endless possibilities need make sure fully uphold fundamental rights standards using ai report presents concrete examples companies public administrations eu using trying use ai focuses four core areas - social benefits predictive policing health services targeted advertising report discusses potential implications fundamental rights analyses rights taken account using developing ai applications aims help ensure future eu regulatory framework ai firmly grounded respect human fundamental rights eu charter fundamental rights access justice non discrimination information society fra - european union agency fundamental rights schwarzenbergplatz - vienna - austria tel - fax fra europa eu facebook com fundamentalrights twitter com eurightsagency linkedin com company eu fundamental rights agency","count":1},{"name":"3 others","count":3}]}},{"name":"NLP","dtype":"object","stats":{"unique_count":5,"nan_count":0,"categories":[{"name":"application ai insurtech real estate technology introduction insurtech professor christopher geczy phd introduction insurtech * insurance industry global diversified across applications subsegments * gross written premium gwp trillion * amount insurance written including commissions costs * net income worldwide grew last year vs actual growth ernst young \"global insurance trends analysis \" june introduction insurtech * insurance industry needs respond technological change disruption * wearables * driverless vehicles * internet things * \"big data\" * natural language processing * blockchain * distributed ledger technologies * climate change * etc * new technologies offer opportunities increase efficiency industry serve new markets ernst young \"global insurance trends analysis \" june introduction insurtech * standardized definition insurtech * said revolutionizing insurance industry changing way insurers business milken institute \"insurtech rising profile insurtech landscape \" december \" insurtech described \" insurance company intermediary insurance value chain segment specialist utilizes technology either compete provide valued added benefits insurance industry \" sia partners key emerging technologies leveraged insurtech companies source capgemini world insurtech report application ai insurtech real estate technology emerging technologies ai machine learning professor christopher geczy phd key emerging technologies leveraged insurtech companies * artificial intelligence * descriptor software perform functions ordinarily associated human reasoning * iterative learning * self awareness emotions * insurers hope exploit ai chatbots * allstate' allstate business insurance expert abie * provides answers real time customer owners questions * insurers followed source \" insurtech insurers using \" https www thebalancesmb com insurtech graphic accenture presentation \"accenture' technology vision insurance \" key emerging technologies leveraged insurtech companies machine learning * enables computers learn time * using algorithms mathematical models simulate neural networks human brain * allows computers extract patterns raw data rather following specific instructions * gives appearance closer activities human brain * insurance companies amass large amounts data * yet according national association insurance commissioners insurers use data collect source \" insurtech insurers using \" https www thebalancesmb com insurtech key emerging technologies leveraged insurtech companies * machine learning could allow insurers mine data effectively extract valuable information * risk modeling analyze claims data predict risk future losses * demand modeling predict demand products future estimate premiums * detecting fraud identify patterns behavior obvious human adjusters * processing claims automate claim reporting processing * underwriting help underwriters analyze data collected applicants * computers aid decision making process flag risks inconsistencies data underwriters might able see * also check external sources social media verify accuracy data source \" insurtech insurers using \" https www thebalancesmb com insurtech application ai insurtech real estate technology redefining insurance industry professor christopher geczy phd redefining insurance industry product design selling marketing front office underwriting policy administration claims management insurtech redefining insurance industry policy claims product design front office underwriting administration management marketing actuarial models underwriting policy acquisition claims servicing distribution product design new policies servicing payout channel management * deg view * extended multi * real time * segment market * streamlined claims customer' needs device mobility information based servicing process low * personalized offering capturing desires waiting time product designs * integrated * advanced risk * automated systems * instant notification * design new omnichannel analytics enabling straight claim products offerings touch risk based pricing processing proactive status * adjust products points * automated workflow stp capabilities updates real time * real time updates management rules * automated * real time claims * disaggregate interactions engines premium reminders status monitoring product mix clients * customer value led renewal notice * advanced analytics seamlessly * quick identification promotions * anytime access based fraud * design deliver cross selling discounts policy details view detection products end selling * digitized systems customer want opportunities less reliance * detecting client data customer satisfaction needs provide * information availability price transparency source capgemini world insurtech report application ai insurtech real estate technology classification insurtech companies professor christopher geczy phd segmentation insurtech firms * different classification methods insurtech firms initiatives rotate similar concepts * \"traditional\" view full stack agents brokers * nuanced view sub segments carriers enablers distributors segmentation insurtech firms * milken institute three main classifications * full stack insurers platforms underwrite policies assume risk cases manage process beginning end * agents platforms act behalf carrier essentially acting extension incumbent carrier * brokers platforms provide customers variety policies offered incumbent carriers insurgent insurtech platforms * may may paid commission based policies sold platform * may require customers scroll policies offered automatically connect customers preferred policy algorithms employed based user' response set questions milken institute \"insurtech rising profile insurtech landscape \" december segmentation insurtech firms * make classification yes insurer full insurance one agent company partnered one multiple insurance companies multiple broker milken institute \"insurtech rising profile insurtech landscape \" december classification insurtech firms capgemnini * second classification system used capgemini * categorizes insurtech providers role distribution chain * enablers * distributors * full carriers source capgemini world insurtech report classification insurtech firms capgemnini source capgemini world insurtech report segmentation insurtech firms model insurtech platform models number insurtechs ful l stack agent broker insurtech platform models milken institute \"insurtech rising profile insurtech landscape \" december product offerings h ut ea lth nc l li nd bi fe ke iv u f ni ily rs al h al om te th rm ... e r u en ni li nc nk l ed tit ... le ab ili su ty ra b p nc us er e es na e ia nd nc bi ow l lit sh ... ip en pi ng r et tu tr ir em rn av en ... el product offering categories nc p l en si pe ri ci al p ty su ra nc nc l e bi ... cy cl type insurtech platform product offering e ob ile ... segmentation insurtech firms model product milken institute \"insurtech rising profile broker agent full stack insurtech landscape \" december segmentation insurtech firms * significant portion approximately insurtech companies could better described technology solution providers * human resources earned benefits solution providers * data solution providers * infrastructure solution providers milken institute \"insurtech rising profile insurtech landscape \" december segmentation insurtech firms * human resources earned benefits solution providers * platforms using deploying technology help firms manage human capital efficiently cost effectively * data solution providers * platforms specialize collecting aggregating analyzing vast quantities data support insurance carriers startups stakeholders * infrastructure solution providers * platforms focus making back end processes efficient use application programming interfaces apis provide means platforms integrate build customizable insurance products services milken institute \"insurtech rising profile insurtech landscape \" december number technology solution providers insurtech market insurtech platform models number providers human resources data solution provider infrastructure solution earned benefits solution provider provider small mild large businesses milken institute \"insurtech rising profile type tech solution providers insurtech landscape \" december examples full carrier insurtech firms insurtech firm products services offered example type zhongan chinese property insurer uses traditional insurance model digital carrier online channel sell products handle conducted online mobile claims risk sharing network group associated individuals pools inspeer france based community insurance p2p insurer premiums insure risk platform allows members pool money within full carriers generally stands benefit group covering group member' deductible regarding premium returns smaller insurance packages leveraging mobile technology bima offers micro insurer lower premiums typically lower affordable insurance products low income coverage populations emerging markets demand insurance coverage new york based sure offers demand personal demand insurer purchased online well episodic policies user buy either via via mobile apps website app us based metromile offers auto insurance usage based premiums prices per usage risky fees based number miles insured' insurer behavior displayed customer car logs source capgemini world insurtech report examples full distributor insurtech firms policybazaar specializes comparative analysis online site enables individuals marketplace products various insurers based price compare plans different insurers quality key benefits artificially intelligent insurance advisory application one stop app allows customers brolly delivers contextually relevant insights personal financial manage policies obtain web mobile applications customers assistant coverage recommendations manage policies one place know compare purchase plans distributors coverage may duplicated missing online platform allows customers licensed broker coverfox offers insurance products digital broker compare purchase policies vehicles home health services travel online site enables commercial coverhound us based insurtech firm offers b2b digital customers compare plans comparison platform personal small distributor different insurers commercial insurance products london based bought many uses social media customized flexible front office data connect people similar insurance needs value adding solutions via partnership uses group' collective buying power intermediary insurer reinsurer risk management negotiate insurers deals ' available individuals source capgemini world insurtech report examples full enabled insurtech firms premfina' white label solution brokers allows front office solution process improvement solutions extend premium financing options providers front office customers manage insurance policies riskgenius applies artificial intelligence streamline policy plan process improvement solutions work insurance professionals retrieving management underwriting policy plan details specific coverage exclusion analyzing solution provider administration policies extracting relevant information premium limit deductible enablers rightindem white label self service insurance claims management process improvement solutions claims platform insurers allows customers solution provider specifically claims management interact claim time carpe data leverages fata various channels online content social media connected data capture analytics solutions data specialist devices offers predictive scoring data use cases across value chain products insurers enabling predict risk better solutions based specific technology betterview drone technology specialist allows technology blockchain source capgemini world insurtech report specialist drone based inspection property assessment drones source capgemini world insurtech report application ai insurtech real estate technology investment market size insurtech industry professor christopher geczy phd insurtech market size * ' clear ' large growing * global insurtech market revenue 7mm * market revenue expected reach billion cagr * asiapacific highest regional cagr growing financial hubs hong kong singapore india * health insurance segment expected higher segment cagr * total insurtech investments billion * total deals deals billion * involved insurer reinsurer investor * estimated year cagr orbis research \"global insurance technology insurtech market size \" dec ernst young \"global insurance trends analysis \" june size insurtech market global private investment vc pe insurtech capital invested b deal count source kpmg international pulse fintech global analysis investment fintech january insurtech startup count number insurtech startups exploded years source sma - strategy meets action insurtech disruptors top insurtech disruptors insurtech key techs * insurers moving key business functions cloud * tech research firm ovum' annual survey penetration software service saas grew last year source ovum survey data cited deloitte \" insurance industry outlook\" insurtech key techs source ovum survey data cited deloitte \" insurance industry outlook\" insurtech key techs * aia hong kong launched blockchain app share life policy data bank distributors * axa europe offering flight delay insurance blockchain platform featuring smart contracts * ovum' annual survey penetration software service saas grew last year * carriers consortiums expected launch impactful blockchain initiatives due concerns around data technology profile insurtech market * u insurtech deals total value billion announced q4 * compared q4 deal count q4 increased funding volume also increased * transactions q4 - higher q3 lower q1 q2 * globally * uk investment last quarter * china second largest investor q4 u * uk responsible total investment since * investment international markets remains strong transactions outside u account total transactions since 4th quarter cbinsights quarterly insurtech briefing q4 profile insurtech market * early stage investments remain strong * seed series account total transactions since quarter last quarter * insurtech funding maturing mid - later stages - financings took place series b c stages * could lead consolidation cbinsights quarterly insurtech briefing q4 profile insurtech market * property casualty p c funding volume increased q3 increased q4 * p c transactions quarter marginally higher transactions q3 marks * life health l h funding volume increased q3 marked increase q4 * hits record level insurtech investment driven large investments deal count increased q3 funding volume increased cbinsights quarterly insurtech briefing q4 size insurtech market quarterly insurtech funding volume - stages millions property casualty life health q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 deal count p c l h cbinsights quarterly insurtech briefing q4 size insurtech market quarterly insurtech transaction target country united states united states united kingdom china - q4 china united kingdom q4 germany germany india south africa france q4 transactions q4 transactions quarterly insurtech transaction investments stage seed angel seed angel - q4 series series q4 series b series b series c series c series series series e series e q4 transactions q4 transactions cbinsights quarterly insurtech briefing q4 size private technology investment insurers reinsurers private technology investments insurers q1 q2 q3 q4 cbinsights quarterly insurtech briefing q4 application ai insurtech real estate technology insurtech fintech financial inclusion professor christopher geczy phd microinsurance * investment firm omidyar network * started ebay founder pierre omidyar * model combines profit llc grantmaking c * 5b committed since inception pierre pam omidyar * 676mm profit investments 782mm grants total commitments year ytd https www omidyar com financials microinsurance * common feature microinsurance products low income low net worth population served otherwise limited access insurance * microinsurance products type group individual coverage * term life * health accident disability * casualty crop insurance livestock theft fire natural disaster * certain forms retirement savings plans * microinsurance underwriter delivery channel type * large multinational insurance companies * credit unions mutual associations * government ngos * small community organizations microinsurance * benefits microinsurance * financial protection risk pooling * insureds assume risk * crop insurance vs drought enables small farmers plant crops higher yields \"good\" years poorer yields drought years * safeguard vs families falling back poverty due illness death breadwinner housing destroyed etc * indian ministry health found one quarter hospitalizations pushed individual family poverty due cost treatment tina rosenberg \" microinsurance revolution \" new york times opinionator blog june http opinionator blogs nytimes com microinsurance revolution microinsurance * benefits microinsurance * target specific risk populations * hiv positive living flood zone microentrepreneurs * complement social welfare programs bolster microfinance initiatives * assign term life policy secure business education mortgage loan tina rosenberg \" microinsurance revolution \" new york times opinionator blog june http opinionator blogs nytimes com microinsurance revolution microinsurance * bima - insurance business disruptor \"insurtech\" leader * swedish founded mobile insurance health company provides accident life health insurance products million low income consumers countries africa asia latam * largest markets ghana sri lanka bangladesh pakistan * mobile technology lowered prices brought affordable insurance world' poorest techcrunch https techcrunch com bima raises 97m allianz microinsurance aimed emerging markets bima website http www bimamobile com bima us new microinsurance * business model bima provides microinsurance subscription via basic mobile phone service little 60c month \"pay go\" rolling monthly cover * offers payouts family insured person dies * sign takes minutes payments collected basic mobile phone service * customer growth thousand new customers month bima * customers live less day bima * subscribers never insurance * educating customers top priority * 300mm valuation dec sale leapfrog investments' stake allianz 97mm techcrunch https techcrunch com bima raises 97m allianz microinsurance aimed emerging markets bima website http www bimamobile com bima us new size private technology investment insurers reinsurers private technology investments insurers target country united states united states france - q4 china china q4 united kingdom germany germany united kingdom canada q4 transactions q4 transactions private technology investments insurers investment stage seed angel seed series series - q4 series b series b q4 series c series c series series e series e q4 transactions q4 transactions cbinsights quarterly insurtech briefing q4","count":1},{"name":"getting future right -- artificial intelligence fundamental rights report (c) european union agency fundamental rights reproduction authorised provided source acknowledged use reproduction photos material european union agency fundamental rights copyright permission must sought directly copyright holders neither european union agency fundamental rights person acting behalf agency responsible use might made following information luxembourg publications office european union print isbn doi tk en c pdf isbn doi tk en n (c) photo credits cover hquality adobe stock page copyright (c) coded bias rights reserved page mimi potter adobe stock page siberian art adobe stock page monsitj adobe stock page good studio adobe stock page monsitj adobe stock page sikov adobe stock page mykola mazuryk adobe stock page robsonphoto adobe stock page metamorworks adobe stock page thodonal adobe stock page gorodenkoff adobe stock page blackboard adobe stock page dimco adobe stock page blackboard adobe stock page videoflow adobe stock page monopoly919 adobe stock page zapp2photo adobe stock page gorodenkoff adobe stock page bestforbest adobe stock page freedomz adobe stock page zapp2photo adobe stock page copyright (c) coded bias rights reserved page european communities page copyright (c) coded bias rights reserved page blacksalmon adobe stock foreword know artificial intelligence already plays role deciding unemployment benefits someone gets burglary likely take place whether someone risk cancer sees catchy advertisement low mortgage rates speak artificial intelligence ai machines kind things people used able today ai present lives realise - use keeps growing possibilities seem endless fully uphold fundamental rights standards using ai report presents concrete examples companies public administrations eu using trying use ai discusses potential implications fundamental rights shows whether using ai taking rights account fra interviewed hundred public administration officials private company staff well diverse experts - including supervisory oversight authorities non governmental organisations lawyers - variously work ai field based interviews report analyses fundamental rights taken consideration using developing ai applications focuses four core areas - social benefits predictive policing health services targeted advertising ai uses differ terms complex much automation involved potential impact people widely applied findings underscore lot work lies ahead - everyone one way foster rights protection ensure people seek remedies something goes awry need know ai used also means organisations using ai need able explain ai systems deliver decisions based yet systems issue truly complex using ai systems responsible regulating use acknowledge always fully understand hiring staff technical expertise key awareness potential rights implications also lacking know data protection concern refer non discrimination less aware rights - human dignity access justice consumer protection among others - also risk surprisingly developers review potential impact ai systems tend focus technical aspects tackle challenges let' encourage working human rights protection working ai cooperate share much needed knowledge - tech rights develop use ai also need right tools assess comprehensively fundamental rights implications many may immediately obvious accessible fundamental rights impact assessments encourage reflection help ensure ai uses comply legal standards interviews suggest ai use eu growing still infancy technology moves quicker law need seize chance ensure future eu regulatory framework ai firmly grounded respect human fundamental rights hope empirical evidence analysis presented report spurs policymakers embrace challenge michael 'flaherty director contents foreword key findings fra opinions ai fundamental rights - relevant policymaking report mean artificial intelligence ai fundamental rights eu policy framework moving towards regulation endnotes putting fundamental rights context - selected use cases ai eu examples ai use public administration examples ai use private sector endnotes fundamental rights framework applicable ai fundamental rights framework governing use ai 'use case' examples requirements justified interferences fundamental rights endnotes impact current use ai selected fundamental rights perceived risks general awareness fundamental rights legal frameworks ai context human dignity right privacy data protection - selected challenges equality non discrimination access justice right social security social assistance consumer protection right good administration endnotes fundamental rights impact assessment - practical tool protecting fundamental rights calling fundamental rights impact assessment - available guidance tools impact assessments testing practice fundamental rights impact assessment practice endnotes moving forward challenges opportunities figures figure companies using ai member state figure examples different automation complexity levels use cases covered figure words interviewees often used describe ai 'use cases' figure awareness gdpr right opt direct marketing eu united kingdom country region figure awareness right say decisions automated age gender difficulty paying bills figure awareness risks discrimination using ai country figure correlations words respondents often mention discussing future plans use ai key findings fra opinions new technologies profoundly changed organise live lives particular new data driven technologies spurred development artificial intelligence ai including increased automation tasks usually carried humans covid health crisis boosted ai adoption data sharing - creating new opportunities also challenges threats human fundamental rights developments ai received wide attention media civil society academia human rights bodies policymakers much attention focuses potential support economic growth different technologies affect fundamental rights received less attention date yet large body empirical evidence wide range rights ai implicates safeguards needed ensure use ai complies fundamental rights practice february european commission published white paper artificial intelligence - european approach excellence trust outlines main principles future eu regulatory framework ai europe white paper notes vital framework grounded eu' fundamental values including respect human rights - article treaty european union teu report supports goal analysing fundamental rights implications using artificial intelligence based concrete 'use cases' ai selected areas focuses situation ground terms fundamental rights challenges opportunities using ai overarching fundamental rights framework applies use ai legal eu consists charter fundamental rights eu charter well framework european convention human rights multiple council europe international human rights instruments relevant include universal declaration human rights major un human rights conventions addition sector specific secondary eu law notably eu data protection acquis eu non discrimination legislation helps safeguard fundamental rights context ai finally national laws eu member states also apply see fra bringing rights life fundamental rights landscape european union luxembourg publications office european union major conventions include international covenant civil political rights international covenant economic social cultural rights international convention elimination forms racial discrimination convention elimination forms discrimination women convention torture convention rights child convention rights persons disabilities international convention protection persons enforced disappearance universal international human rights law framework including enforcement mechanisms see e g de schutter international human rights law cases materials commentary cambridge cambridge university press 2nd edition report based interviews officials public administration staff private companies selected eu member states asked use ai awareness fundamental rights issues involved practices terms assessing mitigating risks linked use ai moreover interviews conducted experts deal various ways potential fundamental rights challenges ai group included public bodies supervisory oversight authorities non governmental organisations lawyers safeguarding fundamental rights - scope impact assessments accountability considering full scope fundamental rights respect ai fra opinion using ai systems engages wide range fundamental introducing new policies rights regardless field application adopting new legislation ai include - also go beyond - privacy data protection eu legislator member states non discrimination access justice acting within scope eu law must ensure respect full eu charter fundamental rights charter spectrum fundamental rights became legally binding december enshrined charter eu legal value eu treaties brings together treaties taken account specific civil political economic social rights single text fundamental rights safeguards need pursuant article charter institutions accompany relevant policies laws bodies offices agencies union respect eu member rights embodied charter eu member states rely robust evidence states implementing union concerning ai' impact fundamental law applies equally ai field rights ensure restrictions fieldwork research shows large certain fundamental rights respect variety systems used heading ai principles necessity technologies analysed entail different levels proportionality automation complexity also vary terms relevant safeguards need scale potential impact people provided law effectively fra' findings show using ai systems implicate protect arbitrary interference wide spectrum fundamental rights regardless fundamental rights give field application include also go beyond legal certainty ai developers privacy data protection non discrimination users voluntary schemes access justice yet addressing impact ai observing safeguarding respect fundamental rights interviews show fundamental rights development scope often delimited specific rights use ai help mitigate rights violations line wider range rights need considered minimum requirements legal clarity using ai depending technology area use - basic principle rule addition rights concerning privacy data protection law prerequisite securing equality non discrimination access justice fundamental rights - legislator rights could considered include take due care defining example human dignity right social security scope ai law social assistance right good administration mostly relevant public sector consumer protection given variety technology particularly important businesses depending subsumed term ai context ai use right protected lack knowledge full charter needs consideration scope potential fundamental rights impact legal definition ai related terms might need assessed regular basis using effective impact assessments prevent negative effects fra opinion prior impact assessments mainly focus technical eu legislator consider making issues rarely address potential effects mandatory impact assessments fundamental rights knowledge cover full spectrum fundamental rights cover private ai affects rights lacking public sectors applied deploying ai systems engages wide spectrum ai system used fundamental rights regardless field application impact assessments take pursuant article charter eu member states account varying nature scope must respect rights embodied charter ai technologies including level implementing union law line existing automation complexity well international standards - notably united national potential harm include guiding principles business human rights ungps basic screening requirements - businesses place \" human rights due also serve raise awareness potential diligence process identify prevent mitigate account fundamental rights implications address impacts human rights\" impact assessments draw principles irrespective size established good practice sector encompasses businesses working ai fields regularly repeated pursuing commitments ungps eu deployment appropriate adopted several legislative acts addressing sector assessments conducted specific instruments particular context due transparent manner outcomes diligence related obligations human rights discussions recommendations currently underway proposing new eu secondary public domain extent possible law law would require businesses carry due aid impact assessment process diligence potential human rights environmental companies public administration impacts operations supply chains law required collect would likely cross sectoral provide sanctions information needed thoroughly non compliance - encompass use assessing potential fundamental ai see fra' recent report business human rights rights impact - access remedy calls improved horizontal eu member states human rights diligence rules eu based companies consider targeted actions support impact assessments important tool businesses developing using planning public administration alike mitigate potential use ai systems ensure effective negative impact activities fundamental rights compliance fundamental eu law specific sectors requires forms impact rights impact assessment obligations assessments data protection impact assessments actions could include funding general data protection regulation gdpr guidelines training awareness many interviewees reported data protection impact raising particularly - assessment required law conducted however exclusively - target private took different forms moreover prior assessments sector conducted focus mainly technical aspects eu member states rarely address potential impacts fundamental rights consider using existing tools according interviewees fundamental rights impact checklists self evaluation tools assessments carried ai system developed european international appears affect fundamental rights negatively level include developed research shows interviewees' knowledge eu high level group artificial fundamental rights - data protection intelligence extent non discrimination - limited majority acknowledge however use ai impact fundamental rights interviewees indicate systems affect fundamental rights extent linked tasks ai systems used respondents aware data protection issues respondents also realise discrimination could - generally - problem ai used however exact meaning applicability rights related data protection non discrimination remains unclear many respondents research findings show differences private public sector interviewees private sector often less aware wider range fundamental rights could affected data protection issues known private sector however rights non discrimination access justice related rights less well known among business representatives work ai fully aware potential problems others said responsibility checking fundamental rights issues lies clients ensuring effective oversight overall accountability fra opinion businesses public administrations developing using ai contact various eu member states ensure bodies responsible overseeing ai related effective accountability systems place systems within respective mandates sectors monitor needed effectively address negative impact ai systems bodies include data protection authorities fundamental rights consider using ai always sure bodies addition fundamental rights impact responsible overseeing ai systems assessments see fra opinion introducing specific safeguards ensure line well established international human rights accountability regime effective could standards - example article european include legal requirement make available convention human rights echr article enough information allow assessment charter - states obliged secure people' rights fundamental rights impact ai systems freedoms effectively comply states - among would enable external monitoring others - put place effective monitoring enforcement human rights oversight competent bodies mechanisms applies equally respect ai eu member states also level monitoring findings point make better use existing oversight expert important role specialised bodies established specific structures protect fundamental rights sectors also responsible ai oversight within using ai include data protection mandates include example oversight authorities equality bodies national human area banking data protection authorities rights institutions ombuds institutions variety bodies potentially relevant consumer protection bodies oversight ai fundamental rights perspective additional resources earmarked however responsibilities bodies concerning establish effective accountability systems oversight ai remains unclear many 'upskilling' diversifying staff working interviewed private public sector oversight bodies would allow public administrations' use ai sometimes audited deal complex issues linked developing part regular audits private companies using ai specific sectors also specialised oversight bodies similarly appropriate bodies example area health financial services equipped sufficient resources powers also check use ai related technologies - importantly - expertise prevent example part certification schemes private assess fundamental rights violations sector interviewees expressed wish bodies could effectively support whose fundamental provide expert advice possibilities legality rights affected ai potential ai uses facilitating cooperation appropriate eu well developed set independent bodies national european level help bodies mandate protect promote fundamental share expertise experience engaging rights include data protection authorities equality actors relevant expertise - bodies national human rights institutions ombuds specialist civil society organisations - institutions research shows using also help implementing actions planning use ai often contacted different bodies national level member states consider use ai consumer protection bodies using available eu funding mechanisms often users ai contacted data protection authorities seek guidance input approval personal data processing involved interviewed experts highlight relevance data protection authorities overseeing ai systems respect use personal data however also note data protection authorities resourced task lack specific expertise ai issues experts including working oversight bodies equality bodies data protection authorities agree expertise existing oversight bodies needs strengthened allow provide effective oversight ai related issues according experts challenging given bodies' resources already stretched also highlighted important role relevant civil society organisations specialised fields technology digital rights algorithms enhance accountability use ai systems non discrimination data protection access justice three horizontal themes research shows use ai affects various fundamental rights apart context related specific aspects affect different rights varying extent fundamental rights topics emerged research repeatedly apply ai cases include need ensure non discriminatory use ai right discriminated requirement process data legally right personal data protection possibility complain ai based decisions seek redress right effective remedy fair trial two main fundamental rights highlighted interviews data protection non discrimination addition effective ways complain use ai came repeatedly linked right fair trial effective remedy following three fra opinions reflect findings read alongside opinions call comprehensive recognition response full range fundamental rights affected ai specific safeguards ensure non discrimination using ai fra opinion interviewees rarely mentioned carrying detailed assessments potential discrimination using ai eu member states consider encouraging companies public suggests lack depth assessments administration assess potentially discrimination automated decision making discriminatory outcomes using ai systems obligation respect principle non european commission member discrimination enshrined article teu states consider providing funding article tfeu requiring union combat targeted research potentially discrimination number grounds articles discriminatory impacts use ai charter equality law non algorithms research would discrimination range grounds specific benefit adaptation established detailed provisions several eu directives also enshrine research methodologies social principle varying scopes application sciences employed identify automation use ai greatly increase potential discrimination different areas efficiency services scale tasks - ranging recruitment customer humans would able undertake however profiling necessary ensure services decisions based building results research ai discriminatory recognising european guidance tools support commission recently highlighted need additional using ai detect possible discriminatory outcomes developed legislation safeguard non discrimination using ai eu anti racism action plan interviewees principle aware discrimination might happen yet rarely raised issue believe systems could actually discriminate interviewees also rarely mentioned detailed assessments potential discrimination meaning lack depth assessment potential discrimination common perception omitting information protected attributes gender age ethnic origin guarantee ai system discriminate necessarily true however information potentially indicating protected characteristics proxies often found datasets could lead discrimination certain cases ai systems also used test detect discriminatory behaviour encoded datasets however interviewees mentioned possibility collecting information disadvantaged groups detect potential discrimination absence depth analysis potential discrimination actual use ai systems also almost discussion analysis potential positive effect using algorithms make decisions fairer moreover none interviewees working ai mentioned using ai detect possible discrimination positive outcome sense discrimination better detected data analysed potential bias since detecting potential discrimination use ai algorithms remains challenging interviewees briefly addressed issue different measures needed address include requirement consider issues linked discrimination assessing use ai investment studies potential discrimination use diverse range methodologies could involve example discrimination testing could build similar established methodologies testing bias everyday life respect job applications applicant' name changed indirectly identify ethnicity relation ai applications tests could involve possible creation fake profiles online tools differ respect protected attributes way outcomes checked respect potential discrimination research could also benefit advanced statistical analysis detect differences datasets concerning protected groups therefore used basis exploring potential discrimination finally research interviews underscored results complex machine learning algorithms often difficult understand explain thus research better understand explain results called 'explainable ai' also help better detect discrimination using ai guidance data protection clarity needed scope meaning fra opinion legal provisions regarding automated decision making european data protection board data protection critical development use edpb european data ai article charter article tfeu protection supervisor edps provide everyone right protection consider providing guidance personal data gdpr law enforcement support effectively implement directive directive eu elaborate gdpr provisions directly apply right include many provisions applicable use ai safeguarding use ai fundamental rights particular regards meaning personal data interviewees indicated ai systems use ai including ai training employ use personal data meaning data protection datasets affected many different ways however applications - according interviewees - high level uncertainty use personal data use anonymised data concerning meaning automated hence data protection law would apply personal decision making right data used data protection related principles human review linked use ai provisions apply automated decision making thus edpb edps also report highlights important issue linked data consider clarifying concepts protection also relevant fundamental 'automated decision making' rights respect automated decision making 'human review' according eurobarometer survey mentioned eu law europeans know say decisions automated knowledge right considerably addition national data protection higher among working ai - majority bodies provide practical interviewees raised issue however many guidance data protection interviewees including experts argued clarity provisions apply use needed scope meaning legal provisions ai guidance could include automated decision making recommendations checklists based concrete use cases ai area social benefits interviewees mentioned support compliance data one example fully automated rule based decisions protection provisions applications mentioned reviewed humans interviewees public administration stressed importance human review decisions however rarely described human review actually involves information used reviewing output ai systems interviewees disagree whether existing legislation sufficient many called concrete interpretation existing data protection rules respect automated decision making enshrined article gdpr effective access justice cases involving ai based decisions fra opinion effectively contest decisions based use ai people need know ai used eu legislator member states complain organisations using ai need able ensure effective access justice individuals cases involving explain ai system decisions based ai ai based decisions access justice process goal crucial ensure available remedies individuals seeking benefit procedural accessible practice eu legislator substantive rights encompasses number core member states could consider human rights include right fair trial introducing legal duty public effective remedy article echr administration private companies article eu charter fundamental rights using ai systems provide accordingly notion access justice obliges states seeking redress information guarantee individual' right go court - operation ai systems circumstances alternative dispute resolution includes information body - obtain remedy found individual' ai systems arrive automated rights violated decisions obligation would help achieve equality arms cases accordance standards victim human individuals seeking justice would rights violation arising development use also support effectiveness ai system public private entity provided external monitoring human access remedy national authority line rights oversight ai systems see relevant case law article charter fra opinion article echr remedy must \"effective practice well law\" view difficulty explaining complex ai systems eu jointly research findings identify following preconditions member states remedy effective practice cases consider developing guidelines involving ai systems impact fundamental support transparency efforts rights everyone needs aware ai used area draw informed complain organisations expertise national human rights using ai must ensure public informed bodies civil society organisations ai system decisions based active field findings show explaining ai systems make decisions layman terms challenging intellectual property rights hamper provision detailed information algorithm works addition certain ai systems complex makes difficult provide meaningful information way system works related decisions tackle problem companies interviewed avoid using complex methods certain decision making altogether would able explain decisions alternatively use simpler data analysis methods problem obtain understanding main factors influencing certain outcomes private sector interviewees pointed efforts made gradually improve understanding ai technology ai fundamental rights - relevant policymaking artificial intelligence ai increasingly used private public sectors affecting daily life see ai end human control machines others view technology help humanity address pressing challenges neither portrayal may accurate concerns ai' fundamental rights impact clearly mounting meriting scrutiny use human rights actors examples potential problems using ai related technologies relation fundamental rights increasingly emerged include ---- algorithm used recruit human resources found generally prefer men women ---- online chatbot2 became 'racist' within couple hours ----machine translations showed gender bias ----facial recognition systems detect gender well white men black women ---- public administration' use algorithms categorise unemployed people comply law ---- court stopped algorithmic system supporting social benefit decisions breaching data protection laws examples raise profound questions whether modern ai systems fit purpose fundamental rights standards upheld using considering using ai systems report addresses questions providing snapshot current use ai related technologies eu - based selected use cases - implications fundamental rights report main publication stemming fra' project artificial intelligence fra' work big data fundamental rights project aims assess positive negative ai big fundamental rights implications new technologies including ai big data data fundamental current report builds findings number earlier papers rights * facial recognition technology fundamental rights considerations context law enforcement paper outlines analyses fundamental rights challenges triggered public authorities deploy live frt law enforcement purposes also briefly presents steps take help avoid rights violations * data quality artificial intelligence - mitigating bias error protect fundamental rights paper highlights importance awareness avoidance poor data quality * bigdata discrimination data supported decision making focus paper discusses discrimination occur suggests possible solutions part project fra also exploring feasibility studying concrete examples fundamental rights challenges using algorithms decision making either online experiments simulation studies several fra publications address relevant issues * guide preventing unlawful profiling today future illustrates profiling legal frameworks regulate conducting profiling lawfully necessary comply fundamental rights crucial effective policing border management * handbook european data protection law edition designed familiarise legal practitioners specialised data protection area law * data fra' fundamental rights survey surveyed random sample people across eu including findings people' opinions experiences linked data protection technology security * fra' report business human rights - access remedy analyses obstacles promising practices relation access remedies victims business related human rights abuses analysing complaints mechanisms eu member states research maps hinders facilitates access remedies report growing attention ai potential drive economic growth matched body evidence different technologies affect fundamental rights - positively negatively concrete examples allow thorough examination whether extent applying technology interferes various fundamental rights - whether interference justified line principles necessity proportionality report provides fundamental rights based analysis concrete 'use cases' - case studies 'use case' term software engineering report loosely defines specific application technology certain goal used specified actor report illustrates ways companies public sector eu looking use ai support work whether - - taking fundamental rights considerations account way contributes empirical evidence analysed fundamental rights perspective inform eu national policymaking efforts regulate use ai tools research cover fra conducted fieldwork research five eu member states estonia finland france netherlands spain collected information involved designing using ai systems key private public sectors address relevant fundamental rights issues research - based personal interviews - gathered information ---- purpose practical application ai technologies ---- assessments conducted using ai applicable legal framework oversight mechanisms ---- awareness fundamental rights issues potential safeguards place ----future plans addition experts involved monitoring observing potential fundamental rights violations concerning use ai including civil society lawyers oversight bodies interviewed presenting main findings report presents main findings fieldwork particular report includes ---- overview use ai eu across range sectors focus social benefits predictive policing healthcare targeted advertising ---- analysis awareness fundamental rights implications selected rights focus four use cases ---- discussion measures assess mitigate impact ai related technologies people' fundamental rights two annexes available fra' website supplement report ----annex gives detailed description research methodology questions asked interviews ----annex provides examples potential errors using ai selected areas addition country specific information five member states covered complements fieldwork research delivered contractor also available fra' website maps policy developments ai legal framework governing use different sectors supporting rights compliant policymaking report provides evidence extent fundamental rights considerations brought discussions activities develop test employ monitor ai systems eu also highlights different technologies affect rights set charter reflects protect rights ai becomes widespread sophisticated analysis selected fundamental rights challenges help eu member states well stakeholders assess fundamental rights compatibility ai systems different contexts findings report current views practices among using ai supports policymakers identifying actions needed report aim provide comprehensive mapping use different ai systems five eu member states covered research provide depth technical information different systems mentioned interviewees work conducting interviews report based semi structured interviews representatives public administration private companies involved use ai services businesses fra intentionally provided general definition ai interviewed part research based existing definitions organisations interviewed active public administration general working law enforcement private companies include working health retail pricing marketing financial services insurance employment transport energy importantly except two interviewees research include companies sell ai companies instead entities use ai support operations addition ten interviews conducted experts dealing potential challenges ai public administration e g supervisory authorities non governmental organisations lawyers working field interviews carried five eu member states estonia finland france netherlands spain countries selected based different levels uptake ai technology policy development area ai well incorporate experience across different parts eu fra outsourced fieldwork ecorys fra staff supervised work developed research questions methodology interviewers received dedicated training conducting fieldwork interviews carried anonymously consequence information identifying organisation concerned provided report addition certain details applications described - notably country - omitted protect respondents' anonymity communicated interviewees increasing level trust allowing speak freely work also proved useful recruiting respondents mean artificial intelligence universally accepted definition ai rather referring concrete applications reflects recent technological developments encompass variety technologies although ai usually defined widely survey conducted behalf european commission among companies eu showed eight ten people working companies eu say know ai slightly two respondents companies eu know sure ai fra' research apply strict definition ai use high level cases presents interviews ai defined broadly reference definition provided high level expert group expert group artificial intelligence ai hleg artificial interviewees also expressed variety ways think ai identifying use cases explore research intelligence project focused applications support decision making based data machine learning applications \"artificial intelligence ai refers systems systems contribute automating tasks usually display intelligent behaviour analysing environment taking actions - degree undertaken humans cannot undertaken autonomy - achieve specific goals ai based humans due large scale use cases systems purely software based acting report provide insight different technologies virtual world e g voice assistants image used discussed selected areas broad heading analysis software search engines speech ai may contention concerning whether face recognition systems ai embedded certain use cases constitute ai current level use hardware devices e g advanced robots report often refers 'ai related technologies' autonomous cars drones internet things applications \" past years seen enormous increase computing initial definition ai hleg subject power increased availability data development discussion groups see ai hleg new technologies analysing data increased amount definition ai main capabilities disciplines variety data sometimes available almost real time internet often referred big data machine learning technologies related algorithms including deep learning benefit enormously increased computing power data availability development use flourishing use terms however limited use even prove counterproductive triggers ideas linked science fiction rather real application ai variety myths exist ai often spread via social media example claim ai act form entity distracts fact ai systems made humans computers follow instructions made given humans human centric approach ai important note ai never anything - human beings use technology achieve certain goals however human work decision making behind ai systems often visible centre attention \"currently lawyer entire studies many discussions explored possible ai definitions tell definition ai european commission' joint research centre analysed ai definitions ' asked around pretty highlights often refer issues linked perception thoroughly one tell \" environment e way system receives input data environment public administration netherlands e g sensors information processing decision making achievement specific goals definitions frequently refer machines behaving like humans taking tasks associated human intelligence given difficulty defining intelligence many definitions remain vague makes use ai hard measure practice10 equally challenging define law report discusses use ai based concrete applications differ terms complexity level automation potential impact individuals scale application discussion around actual use ai involves deploying machine learning technologies seen sub domain ai also confusion around term \"learning\" implies machines learn like humans reality much current machine learning based statistical learning methodologies machine learning uses statistical methods find rules form correlations help predict certain outcomes different traditional statistical analysis involve detailed checks predictions produced often referred 'black boxes' traditional statistical analysis based specific theoretical assumptions data generation processes correlations used machine learning geared towards producing accurate outcomes used automating workflows decisions acceptable level accuracy obtained usual example email spam filter uses statistical methods predict email spam important know certain email blocked spam predicted high accuracy really need understand algorithm works e based rules emails get blocked however depending complexity task prediction always possible high accuracy moreover report highlights understanding certain outcomes predicted acceptable certain tasks area machine learning incorporates several approaches often machine learning refers finding rules link data certain outcome based dataset includes outcomes supervised learning example dataset emails labelled spam 'ham' used find correlations rules associated spam emails dataset rules used 'predict' degree likelihood future email spam sometimes machine learning used find hidden groups datasets without defining certain outcome unsupervised learning - example segmenting people groups based similarities demographics finally rules correlations found trial error reinforcement learning systems try optimise certain goal experimentation update rules automatically best possible output systems need enormous amounts data hardly used humans involves experimentation mainly responsible success winning board games humans often sensationalised media ai fundamental rights eu policy framework moving towards regulation policymakers time highlighted potential ai related technologies improve efficiency drive economic growth yet public authorities international organisations recently reflected fundamental rights challenges associated technologies coupled growing use accuracy ai systems turned attention whether regulate use european parliament resolution marked milestone eu' recognition fundamental rights implications ai resolution stressed \"prospects opportunities big data fully tapped citizens public private sectors academia scientific community public trust technologies ensured strong enforcement fundamental rights\" called european commission member states data protection authorities \" develop strong common ethical framework transparent processing personal data automated decision making may guide data usage ongoing enforcement union law\" later year european council called \"sense urgency address emerging trends\" including \"issues artificial intelligence ... time ensuring high level data protection digital rights ethical standards\" european council invited european commission put forward european approach ai responding calls european commission published communication ai europe18 set high level expert group ai initiatives include strong reference fundamental rights commission facilitated high level expert group made independent experts academia civil society industry including representative fra published 'ethics guidelines trustworthy ai' 'policy investment recommendations trustworthy ai' developed work triggered discussion importance framing ai human rights terms alongside ethical considerations led development ethics guidelines refer charter place fundamental rights consideration respect ai ethics guidelines include assessment list trustworthy ai translated checklist guide develop deploy ai indicating political support highest level european council calls strategic guidelines \"ensure europe digitally sovereign\" policy \"shaped way embodies societal values\" similarly commission president von der leyen committed \"put forward legislation coordinated european approach human ethical implications ai \" prompted significant moves towards setting eu legal framework govern development use ai related technologies including respect impact fundamental rights february european commission published white paper artificial intelligence sets policy options meeting twin objectives \"promoting uptake ai addressing risks associated certain uses new technology\" paper promotes common european approach ai deems necessary \" reach sufficient scale avoid fragmentation single market\" notes \" introduction national initiatives risks endanger legal certainty weaken citizens' trust prevent emergence dynamic european industry\" legal uncertainty also concern companies planning use ai commission white paper ai highlights risks fundamental rights one main concerns associated ai acknowledges \" use ai affect values eu founded lead breaches fundamental rights result flaws overall design ai systems use data without correcting possible bias\" also lists wide range rights affected white paper ai indicates commission' preference possible new regulatory framework follow risk based approach mandatory requirements would principle apply high risk applications would determined basis two cumulative criteria employed sector healthcare transport parts public sector significant risks expected occur used manner significant risks likely arise latter risk could assessed based impact affected parties adding harm based element white paper also highlights instances ai use certain purposes considered high risk irrespective sector include use ai applications recruitment processes remote biometric identification including facial recognition technologies following public consultation ran february june commission expected propose legislation ai first quarter ahead proposal eu' co legislators considered various aspects potential legal framework october european parliament adopted resolutions recommendations european commission framework ethical aspects ai robotics related technologies civil liability regime ai also adopted resolution intellectual property rights development artificial intelligence technologies continues work resolutions ai criminal law use police judicial authorities criminal matters ai education culture audio visual sector also established special committee artificial intelligence digital age following meeting october heads state government eu member states declared \"eu needs global leader development secure trustworthy ethical artificial intelligence\" invited commission \"provide clear objective definition high risk artificial intelligence systems addition council eu adopted conclusions shaping europe' digital future35 seizing opportunities digitalisation access justice included dedicated section deploying ai systems justice sector german presidency council eu published conclusions charter fundamental rights context artificial intelligence digital change text supported objected member states growing reference fundamental rights discussions indicates fundamental rights framework alongside legal frameworks38 necessary effective human rights compliant evaluation many opportunities challenges brought new technologies many existing ai initiatives guided ethical frameworks typically voluntary fundamental rights centred approach ai underpinned legal regulation responsibility respecting protecting fulfilling rights rests state guarantee high level legal protection possible misuse new technologies also provides clear legal basis develop ai reference fundamental rights - application practice - fully embedded addition steps towards legal regulation eu taking significant policy financial actions support development ai related technologies alongside white paper commission published european data strategy aims set single market data including nine common european data spaces covering areas health data financial data proposal multiannual financial framework would create digital europe programme worth EUR billion invest eu' \"strategic digital capacities\" including ai addition funding horizon europe connecting europe facility international actors also considering steps regulate ai notably council europe active player field ai related technologies september committee ministers council europe set ad hoc committee artificial intelligence cahai aims examine \" feasibility potential elements legal framework development design application ai based council europe' standards human rights democracy rule law\" april committee ministers council europe adopted recommendations human rights impact algorithmic systems addition organisation economic cooperation development oecd adopted ai principles created ai policy observatory global level unesco starting develop global standard setting instrument ai selected examples wide range legal policy initiatives aiming contribute standard setting area ai includes amongst others actual draft legislation soft law guidelines recommendations use ai reports recommendations law policy fra put together non exhaustive list initiatives linked ai policymaking also include legislative initiatives eu member states many organisations businesses launched initiatives tackle ethical concerns ai however useful tackle potential problems ai ethical approaches often rely voluntary action sufficiently address obligation respect fundamental rights fra pointed fundamental rights report \" rights based approach guarantees high level protection possible misuse new technologies wrongdoings using \" european commission' initiative regulating ai helps avoid disjointed responses ai across member states undermine businesses across eu entities outside eu endnotes reuters 'amazon scraps secret ai recruiting tool showed bias women' october chatbot chatterbot common ai feature embedded messaging applications simulate human conversation voice text independent 'ai robots learning racism sexism prejudices humans study finds' april prates avelar p lamb l 'assessing gender bias machine translation - case study google translate' march gender shades project evaluating accuracy ai powered gender classification products see example der standard datenschutzbehorde kippt umstrittenen ams algorithmus algorithmwatch poland government scrap controversial unemployment scoring system privacy first dutch risk profiling system syri banned following court decision european commission european enterprise survey use technologies based artificial intelligence luxembourg july see example website \"ai myths\" samoili lopez cobo gomez e de prato g martinez plumed f delipetrev b ai watch defining artificial intelligence towards operational definition taxonomy artificial intelligence luxembourg schuett j legal definition ai arxiv hastie tibshirani r friedman j elements statistical learning data mining inference prediction springer see example pasquale f black box society secret algorithms control money information harvard university press cambrigde london rai 'explainable ai black box glass box' journal academy marketing science vol pp seminal paper describing difference breiman l 'statistical modeling two cultures' statistical science vol pp european parliament resolution march fundamental rights implications big data privacy data protection non discrimination security law enforcement ini para ibid para european council european council meeting october - conclusions euco brussels october p european commission communication commission european parliament european council council european economic social committee committee regions artificial intelligence europe com final april information available webpage high level expert group high level expert group artificial intelligence ethics guidelines trustworthy artificial intelligence policy investment recommendations trustworthy ai high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment european council new strategic agenda p vonder leyen ursula union strives agenda europe p european commission white paper artificial intelligence - european approach excellence trust com final brussels february p ibid p european commission white paper artificial intelligence public consultation towards european approach excellence trust july european commission adjusted commission work programme annex new initiatives may european parliament legislative observatory framework ethical aspects artificial intelligence robotics related technologies inl european parliament resolution october recommendations commission civil liability regime artificial intelligence inl european parliament resolution october intellectual property rights development artificial intelligence technologies ini european parliament artificial intelligence criminal law use police judicial authorities criminal matters ini european parliament legislative observatory artificial intelligence education culture audiovisual sector ini european parliament decision june setting special committee artificial intelligence digital age defining responsibilities numerical strength term office rso european council special meeting european council october - conclusions euco october council european union shaping europe' digital future - council conclusions june council european union council conclusions \"access justice - seizing opportunities digitalisation\" october council european union presidency conclusions - charter fundamental rights context artificial intelligence digital change october see e g pagallo u casanovas p madelin r ' middle approach assessing models legal governance data protection artificial intelligence web data' theory practice legislation pp see fra fundamental rights report luxembourg publications office chapter communication commission european parliament council european economic social committee committee regions european strategy data com final european council conclusions special meeting european council july euco july council europe ad hoc committee artificial intelligence cahai factsheet governance digital transformation council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems adopted committee ministers april 1373rd meeting ministers' deputies see dedicated oecd website see dedicated unesco website see overview fra ai policy initiatives council europe website fra fundamental rights report luxembourg publications office p putting fundamental rights context - selected use cases ai eu eu use ai related technologies relatively wide spread recent survey shows companies use ai related technologies - plan chapter presents selected cases ai use note - typically referred 'use cases' ai field fra collected information cases interviewees use cases presented five eu member states estonia france finland netherlands spain involve chapter based information different areas application across public obtained interviews public private sector administration private companies special representatives focus put use ai areas social benefits predictive policing health services interviewed representatives targeted advertising public administration work areas health services chapter provides information current infrastructure energy use ai well basic information eu judiciary law enforcement competence select areas use cases migration border management social benefits tax well provide good sense kind ai transportation traffic control related technologies currently used interviewees private companies examples also offer context mainly work retail marketing fundamental rights analysis looking pricing health sector broad variety use cases provides important financial services energy insurance insights actual use ai affect employment transport well people' fundamental rights chapter includes cross cutting areas focus discussion fundamental rights implications ai development different sectors makes reference cases described chapter according european enterprise survey beginning companies use ai eu said use technologies depend ai percentage ranges companies estonia cyprus czechia see figure another companies eu planning use ai future survey indicates ai used mostly sector technologies used comprise variety applications aiming process equipment optimisation anomaly detection process automation forecasting price optimisation decision making figure companies using ai member state eu ro uk se si nl bg lu el cz lt mt lv dk hr de hu fr pt pl fi es ie ee cy sk currently using ai planning use ai notes survey asked use plans use ten different ai related technologies speech recognition visual diagnostics fraud detection analysis emotions forecasting based machine learning includes percentage companies using least one ai technologies n source fra based data extracted european commission european enterprise survey use technologies based artificial intelligence luxembourg july noted report focuses four broad ai 'use cases' ----social benefits ----predictive policing ----health services ----targeted advertising areas particularly sensitive regards fundamental rights two \"ai machine learning cover mainly public administration' use ai social benefits allocation different concepts ai umbrella predictive policing two concern private companies health term \" services targeted advertising use cases provide basis private company estonia report' fundamental rights analysis offering necessary context appropriate report also highlights findings interviews cover areas four areas detailed studies taxonomy ai available providing \" see categorisations technology noted introduction interviewees everyone something different views ai also stated machine learning labelling clear definition ai 'ai' \" public administration netherlands report discusses specific use cases without classifying technology applied yet use ai cases examined differed use technology described interviewees involved varying levels complexity varying levels automation figure provides overview different examples use interviewees discussed heading ai applications relatively straightforward understand rule based decision making algorithms defined based ' rules' example person income certain threshold eligible certain benefits algorithms used area social benefits different levels automation examples full partial human review involved applications used traditional statistical methods inform decisions include example regression analysis classical statistical method analyses correlation several pieces information 'variables' outcome credit score example others used complex machine learning methodologies feed production forecasts statistics government reports also algorithms much higher levels complexity deep learning diagnosis support area health tools still include high level human review hence include high level automation contrast targeted advertising example potentially using highly complex algorithms without human review output decision also using highly complex algorithms including deep learning reinforcement learning see chapter descriptions terms human review would also possible area due scale algorithms operate figure examples different automation complexity levels use cases covered high rule based regression analysis deep learning automated predictions fully reinforcement areas examples decision making automated credit learning social benefits scoring advertising social welfare rule based regression analysis facial recognition marketing automation decision predictions technology positive human reviewed identification law enforcement outcomes credit scoring human social benefits review health services financial services rule based machine learning medical images human decisions supported analysis social production diagnosis benefits forecasts low low complexity high source fra ai systems also vary according potential harm could result notes examples financial erroneous decision based use ai depending area services use facial recognition application wrong decisions - based erroneous outputs technology covered system - different impacts using ai decision making detailed use case descriptions consequences different decision affirmative wrong false mentioned interviews positive negative wrong false negative examples illustrate different levels complexity automation used practice issues particularly important machine learning used based statistical calculations always come degree error rule based algorithms also make mistakes especially grow complex risks lower deterministic nature rules developed example using ai make decisions social benefits false positive means person may erroneously receive benefits necessarily negative impact person concerned unless error found later money needs paid back however negatively impacts public administration money paid line good administration practices contrast false negative would negative impact individual would receive benefits entitled annex available fra' website provides hypothetical examples effects wrong decisions based use cases discussed importantly automating tasks impact could also scale potentially exacerbating negative effect society whole severity scale potential harm one aspect needs taken consideration analysing potential limitations fundamental rights respect use ai example small error rates using facial recognition technology used law enforcement might still lead flagging many innocent people technology used places many people analysed might apply airports train stations thousands people could scanned daily basis potential bias error rates could lead disproportionally targeting certain groups society interviewees mostly mention 'machine learning' including use neural networks technologies extensions see chapter description machine learning respondents used across either directly mentioned mentioned subfields machine learning image cases recognition facial recognition technology frt identified research often interviewees mentioned use 'supervised machine learning' mainly used optimise specifically defined outcome yet sometimes 'unsupervised machine learning' also used categorise cluster data one case referred use 'reinforcement learning' without going much detail several respondents used 'natural language processing nlp ' technology analyse text speech sometimes combined machine learning algorithms mention examples involve rule based algorithms meaning rules algorithm follow directly encoded e based ' rules' cases interviewees disclose could provide detailed information technology used generally interviewees referred one use case asked focus one application interviews ----importantly fieldwork shows companies public administrations often still beginning looking use ai two thirds use cases actually use deployed practice many use cases described interviewees pilot stage development still research phase ----two ai driven applications halted tests figure words interviewees often used describe ai 'use cases' notes fra visualisation words frequently used descriptions use cases bigger size word often interviewees mentioned terms source fra figure shows frequently used words describe use cases covered report highlights importance data using ai systems well relevance supporting decision making fra previously highlighted thorough description data used ai applications essential identifying mitigating potential fundamental rights challenges variety data used ai systems covered report however difficult obtain detailed information data used respondents remained rather vague data sources rather generically many respondents mentioned using 'open data' 'historical data' 'metadata' concretely respondents mentioned using customer data e g purchases browsing behaviour administrative records \" mostly used save time ... data social benefits taxes interviewees also mentioned go lot medical records police records court records well social media material \" traffic data data included text data e g e mails audio recordings video public administration netherlands geolocation data data come internal databases companies public administration also external sources \" important deal cases efficiently ' single important reason using ai increased efficiency making use workforce vast majority respondents across public private sector mentioned people handle cases using ai greater speed fewer errors cost reduction fewer human effectively possible \" resources needed interviewees law enforcement also said public administration netherlands use ai safety security well crime prevention humans previously performed many use cases respondents said use ai entails fewer mistakes humans carry certain tasks respondents also use ai tasks humans previously carry quantity information could processed humans - example area genome analysis traffic predictions importantly half respondents interviewed use ai relevant decision making however ai mainly used support decision making final decisions remain largely hands humans interviewees pointed enthusiastic public administration companies still cautious deploying ai many use cases still testing phase described stopped phase nevertheless almost interviewees aware plans reduce level technology used fact expressed intentions invest innovation new ways employ currently available ai systems examples ai use public administration use case automating social welfare systems - using algorithms area social benefits background eu legal framework united nations special rapporteur extreme poverty human rights philip alston warned october report introducing 'digital welfare' state including use ai lead \"digital welfare dystopia\" digitalisation welfare systems often accompanied reductions overall welfare budgets narrowing beneficiary pool measures reduce availability welfare digitalisation also increases power states offering opportunities control people particularly worrying countries significant rule law deficits use algorithms public administration welfare raises major concerns respect potentially negative impact poverty inequality applied erroneously area social benefits includes areas child welfare services6 unemployment benefits yet public authorities keen use new technologies make decision making social security benefits efficient potentially fairer globally new technologies used many ways administer welfare systems include identity verification eligibility assessments benefit calculations fraud prevention detection risk scoring need classification well communication authorities beneficiaries oecd defines social benefits transfers made households need certain events particular circumstances arisen including sickness unemployment retirement housing education family circumstances however commonly agreed definition social benefits social benefits particular social insurance systems different private insurance schemes involve compulsory contributions made employees employers sometimes form taxation social policy including social security social protection area shared competence eu member states article b tfeu pursuant article tfeu eu pursues objectives among things promote \"improved living working conditions\" \"proper social protection\" end eu supports complements activities member states number fields including social security social protection workers combating social exclusion article tfeu eu actions encourage cooperation member states adopt directives minimum requirements moreover decisions social security social protection adopted special legislative procedure unanimous vote council backdrop eu member states mostly free shape social security social protection policies since virtually harmonisation social security systems differ significantly across eu terms benefits provided conditions eligibility benefits calculated contributions need paid etc public administrations eu member states working implementing ai related technologies area public welfare however information applications limited fra collected information use cases linked ----using algorithms comes compensating job seekers ----processing social benefits applications ----machine learning supported data analysis use pensions several private insurance companies interviewed research use ai related private technologies includes handling requests customers complementary health insurance insurance insurance compensation decision support evaluating credit risk companies' individuals insurance pricing insurance claims management decision making support use ai related management functions credit decisions private insurance companies generally embrace ai related technologies help make business profitable oecd report highlights importance technology sector also argues risk classification could lead exclusion belonging certain vulnerable groups ways undesirable societal political perspective oecd impact big data artificial intelligence ai insurance sector use practice use cases outlined exemplify challenges using planning use ai area social benefits linked algorithmic decision making experimenting new technologies support jobseekers course three year project public organisation experimented several ai related technologies concerning work related processing benefits job seekers assisting return work representative interviewed states tested technologies improve foster relationship job seekers improve advice given job seekers companies testing completed organisation decide apply technologies day day work tests include machine learning based detection attractiveness job offers system detecting whether job seekers still actively looking job tests also looking profiling job seekers provide advice would include calculating probability someone offered available job within given time identifying parameters make job offers relevant may also reflected advice companies best practices formulating job offers profiling would allow organisation determine appropriate services according profile background job seeker rather analysis advice drawn employees practically would done requiring job seekers complete monthly diary job search however still consideration whether programme limited providing descriptive analyses whether go provide recommendations organisation hesitant latter aspect additionally natural language processing system tested analysing content job seekers' e mails e mails categorised relevant data extracted urgency relevance e mails identified using chatbot using automatic replies emails considered data used systems come several sources within organisation data job seekers background including personal tax data well data salaries social security allowances used strict conditions derived highly regulated data sources e g salary statements cannot accessed data job offers companies also used generate knowledge job market organisation currently use external data professional social media networks legal provisions place using data processing housing benefits - failure success public body responsible processing social benefits piloted ai tool process applications subsequently support staff making decisions housing benefits system selected cases new benefit applications relatively straightforward calculate include new applications housing benefits submitted individual living alone children individual income government benefits overall cases deemed simple result always individual receives benefits technological solution based decision tree model following rules housing benefits calculating general housing benefits requires income estimates advance data used testing stemmed internal database contains data benefit application processes data pseudonymised need use personal information simple statistical model linear regression used input income cost limits output amount benefit however even simplified cases found difficult use ai practice frequent changes legislation test terminated according interviewee lack legal basis using machine learning allow using administrative decisions plans use ai support decision making social benefits organisation pursuing particular project due aforementioned legal challenges interviewee noted potential applications solutions area future noted ai related technologies support operations without legal impact particularly good organisation syri case netherlands called time organisation using image processing social benefits applications 'system risk indication' syri generally benefit applicants complete developed government tool several forms attachments often alert dutch public administration submitted paper format efficient fraud risk citizens time saving handling documents processing linking large amounts agency' staff hard copies received personal data public scanned classified automated authorities system broad coalition civil society organisations dealing privacy first step turn images right way issues initiated lawsuit prompting round algorithms align documents district court hague aligned properly scrutinise algorithm based syri scanned remove spots clean edit colouring document identify court ruled syri impinges disproportionately private columns paragraphs tables elements life citizens court found distinctive blocks recognise script etc everyone whose data analysed application checks received syri exposed risk application form attachment marked addition due opacity correctly e g document marked algorithm used citizens could invoice system determines whether \"neither anticipate intrusion correct private life guard \" turning classification images good description syri done image recognition optical found ilja braun high risk character recognition ocr technologies citizens algorithm watch recognise text stemming images including photographs scans documents ruling february handwritten notes ocr technology dutch available online converts recognised text text data privacy first dutch machine readable pattern recognition risk profiling system syri banned process input scanned images following court decision first isolated compared 'glyphs' e variations letters stored system pixel pixel basis agency continue processing images develop example potentially making possible scan bar codes attachments would help speed confirmation correctness documents attachments also solutions related natural language processing automating unemployment benefits one countries selected decisions unemployment benefits fully automated national institution responsible unemployment insurance benefits updated system fully automate processing benefit applications decisions done relevant legislation adapted allow automated decisions person registers unemployed lodges application benefits system draws information applicant various databases includes example population register tax authorities' databases containing information salaries work experience etc conditions receiving unemployment benefits fulfilled system calculates period payments based length person contributed insurance system amount benefits based average daily salary procedure fully automated however employee institution must intervene necessary information cannot extracted databases contradictory information databases decision case involves level discretion e decision cannot definitively determined based data available human leeway deciding case main reason using system improved efficiency addition system believed achieve consistency processes every application subject discretion handled way use case predictive policing - trying anticipate crime advance fra activity background eu legal framework ai technologies used law enforcement particularly predictive policing preventing existing research tools affect fundamental rights unlawful profiling highlighted particular issues concerning discrimination among rights one recurrent concern potential predictive policing reproduce today entrench existing discriminatory practices particularly reliance future guide historical crime data may biased incomplete developing using algorithmic many crimes - domestic violence hate crime - remain largely profiling bias may introduced unreported therefore counted official police statistics step process avoid subsequent potential violations focus certain crimes violence drug related crime public fundamental rights experts places - rather business fraud non payment taxes example - officers interpreting data also make law enforcement responses less equitable clear understanding former often associated certain demographics neighbourhoods fundamental rights ultimately undermine police relations particular communities fra guide explains profiling criminological research crime 'hotspots' around several legal frameworks regulate decades - notably uk usa uses police data map certain conducting profiling crimes undertakes statistical tests explore crime probabilities various lawfully necessary comply police forces used developed address different types fundamental rights crucial crime concentrations clusters 'hotspots' effective policing border management recently adaptations area applied research used ai information see fra tool enhance effectiveness suggesting using algorithmic preventing unlawful profiling today tools could reduce police' reliance subjective human judgments future guide may reflect biases stereotypes studies also indicated predictive policing could potentially reduce unnecessary surveillance questioning physical checks searches reducing humiliation harassment individuals may occur activities predictive policing aims forecast probability crime anticipate emerging trends patterns inform crime prevention intervention strategies may also part investigation crime already taken place authoritative definition predictive policing typically characterised analysing data identify common patterns trends crime using algorithms create models based analysis used forecast criminal activity may occur future ai technologies area generally either aim 'predict' crimes 'predict' individuals either commit victims crimes tools aiming predict crimes generally fed historical data - largely official sources - time place type crimes committed complemented environmental variables population density presence certain public places services major events holidays generally use personal data applied fra activity contrast ai systems focused predicting potential perpetrators victims facial recognition crime employ historical real time personal data could include criminal records data addresses phone numbers location data data extracted technology social media information known associates health income rise data combined criminal environmental data fundamental rights eu member states shared competence area freedom considerations security justice article j tfeu includes judicial cooperation criminal matters police cooperation articles law enforcement tfeu already treaty lisbon adopted annexed declaration protection personal data judicial cooperation eu law recognises 'sensitive data' criminal matters police cooperation observed \"specific rules people' facial images protection personal data free movement data form biometric data processed fields ... police cooperation based article tfeu ... prove facial recognition software necessary specific nature fields \" images also quite easy capture public places although accuracy within framework predictive policing collection storage processing matches improving risk analysis exchange information particularly relevant processing errors remains real - particularly personal data context law enforcement operations regulated certain minority groups people whose eu level law enforcement directive directive eu sets images captured processed comprehensive standards safeguards processing including might know happening safeguarding prevention threats public security - cannot challenge possible misuses use practice fra paper outlines analyses use cases collected fra signal variety ways law fundamental rights enforcement authorities already use plan use ai related technologies challenges triggered support work public authorities deploy live frt law enforcement purposes also examples mentioned interviewees range data mining systems briefly presents steps take help designed map crime patterns detecting online hate speech making avoid rights violations risk assessments gender based violence automating certain prison information see fra guard duties use cases include detecting illicit objects satellite facial recognition technology images generally recognising objects images addition tool fundamental rights considerations mentioned research used private sector fraud prevention context law enforcement crime detection money transfers interviewees emphasised ai related technology systems used automate speed tasks previously done humans thus freeing better distributing resources mapping crime support efficient allocation investigation capacity national intelligence agency public prosecutor' office employ data driven system help employees make choices use available investigation capacity aim improve allocation human resources ensuring officers present right time place interviewees suggest system could make precise assessments compared humans often rely gut feeling decisions still system always used combination human appraisal non ai systems make operational decisions based system generated outcomes analysts create 'heat map' outlines prevalence certain crimes certain areas replicates long standing manual version crime anticipation system whereby police officers put pins map indicate specific risk areas using ai increase speed process also makes reliable users believe analyse data system based data mining machine learning processes primarily built unique police data contained crime reports witness statements suspect declarations gaps extent possible addressed using data sources criminology research social demographic information obtained national office statistics system also uses data open sources specific parameters calculation depend type crime predictive factors vary relevance across crime areas example case burglaries data burglaries collected combined data place residence known criminals distance burgled fra activity houses relevant criteria preselected allow system produce heat map detecting hate location based predictions made next six months indicate speech online time location burglary may occur result map small public agency combatting hate squares risk crime occurring indicated different shades crime uses ai based tool detect interviewees indicated visualisation helps officers analyse online hate speech analysing neighbourhoods observe correlations different locations patterns speech online basis processing system assessing risk gender based domestic violence determines social groups targeted helps law enforcement national police force uses internal system track cases gender adopt measures protect based domestic violence system helps police officers take decisions threats realised distribute resources across domestic violence cases system categorises cases basis assessed risk relapse repetition order although tool aims identify focus 'riskiest' cases potential victims rather perpetrators law enforcement specialist team could complete risk analysis without using ai however use information generated system able compute large amount data short amount system ask social media providers time assist untrained non specialist police officers risk analysis information users pursue criminal investigations case alleged gender based domestic violence reported police one particular challenge officer starts initial investigation includes collecting evidence taking understanding context witness statements - potentially - making arrest using information statements made example gathered process officer fills two detailed questionnaires journalists academics may use assess complaints evaluate probability reoffending examine words associated hate speech evolution case assess behaviour perpetrator report analyse occurrence victim police officers also indicate level gravity nature threats faced attitudes concerning victim fra plans initiate research online hate present social system produces risk 'score' three point scale police media allow fra provide officer raise level risk manually cannot lower risk level input policy developments indicated system level confirmed specific area online content moderation measures applied line established police protocols system uses ai also informs judge potentially 'severe cases' automated system examples ai use private sector use case ai health - analysing medical records save lives background eu legal framework healthcare particularly prominent discussions use ai medical data online applications potential support improved health outcomes - result - wider socio economic benefits covid pandemic increased focus interest area particularly terms potential online data applications enhance ability governments health services track spread disease health also prominent general population' views uses ai eurobarometer survey found every second european thinks ai best used improve medical diagnostics develop personalised medicine improve surgery use case covers applications ai related technologies public private sector stakeholders area medical records disease prediction feeding data electronic medical records emr electronic health records ehr ai systems related technologies support development preventative medicine recognises early risks disease designs appropriate interventions researchers predict clinical events mortality hospitalisation readmissions length stay hospital beyond disease prediction medical record data analysed predict patients' adherence treatment keeping medical appointments technologies potential support improved health outcomes well increase efficiency healthcare system article tfeu eu supporting competence protecting improving human health member states retain full responsibility defining health policies organising managing health systems delivering health services article tfeu within eu competence union action complement national policies directed towards improving public health preventing physical mental illness diseases obviating sources danger physical mental health action cover health information education well monitoring early warning combating serious cross border threats health article tfeu latter areas eu adopt incentive measures excluding harmonisation laws regulations member states rules policies adopted eu level aim ensure free movement citizens equal treatment non discrimination abroad well availability safety medical products services single market considering development technologies application health care exchange medical records patients' rights cross border situations disease prediction matter public health particularly relevant gdpr health genetic data considered special category data article called 'sensitive data' require specific protection processing could create significant risks data subjects' health genetic data shared specific circumstances article gdpr gdpr provides exemption purpose limitation principle data used research purposes line article researchers required ensure technical organisational safeguards - pseudonymisation anonymity - place using patient data eu also taken action regarding exchange medical records european commission recommendation c european electronic health record exchange format24 \"seeks facilitate cross border interoperability ehrs eu supporting members states efforts ensure citizens securely access exchange health data wherever eu \" recommendation lays technical specifications exchange data eu member states european data strategy february also strong focus health data 'common european health data space' one nine common european data spaces whose establishment european commission support early warning response system ewrs owned european commission operated european centre disease prevention control aims \"notifying eu level serious cross border threats health\" enabling \" european commission eu countries permanent communication purposes alerting assessing public health risks determining measures may required protect public health \" emr computerised medical record created patients healthcare organisation ehr contains patient' medical history beyond one organisation involve sharing data across healthcare system include large amount personal data encompass among others name contact details individual next kin demographic information diagnoses test results medication treatment may also include patient generated data wearable devices uniform emr ehr system operating across eu member states germany national emr ehr system others - including belgium denmark - different emr ehr systems regional level systems differ considerably depending data recorded access data european commission stakeholders highlighted diversity country level emr ehr systems lack interoperability major barrier digital single market health studies highlight potential ai related technologies enable earlier diagnosis widen possibilities disease prevention improve patient safety strengthening right access preventive healthcare benefit medical treatment emr ehr may also help make healthcare personalised possibility rapid sharing data facilitate coordinated timely treatment however use emr ehr presents significant data protection risks healthcare sector leads terms personal data breaches amount personal data stored highest among industries combined large data sharing network number access points makes healthcare sector attractive target hackers quality data emr ehr also raises concern studies patients shown medical files asked accuracy found information incomplete erroneous lot important data emr ehr unstructured form free text reduces data quality low levels accuracy completeness overall data quality increases risk medical error use practice applications described interviews include simple advanced models employed public private sectors largest number use cases refer image based diagnosis tools however interviewees also discussed tools automate various working procedures mapping text data filing medical records analyses measurements body tissues nerve fibres smaller number examples touched advanced projects systems monitor remotely certain health indicators heart rate case systems complement expertise health professionals next sections present examples diagnostic remote monitoring tools image based tools help detect diagnose disease tools used support detection diagnosis diseases described interviewees work similar ways example privately owned hospital uses ai system interpret images ct scans stroke patients stroke imaging used detect damage brain occurred may blockages blood supply brain also generate measures compared particular values medical specialist interviewee feels application helps determine characteristics images quickly potentially - depending uses tool - improving quality diagnosis however highlight necessarily efficient rely ai application since medical professional must present could examine image rather tool offer support - example specialist finds difficult interpret certain image find abnormalities system built trained validated using dataset partially based large scientific study hospital contributed supplemented purchasing foreign datasets algorithm trained adapted future based new data new versions released developers feel allowing system continue learn would make difficult using ai public authority responsible validate operation target health inspecting food safety standards restaurants uses machine learning private company developed algorithm supports detection breast cancer inspections process customer review data major online platforms helps mammography exams tool gives decide conduct inspections previously process probability degree certainty based complaints help radiologists speed analysis authority received previous results decide whether additional reports since introduction tests warranted algorithm detects tool rate non compliant characterises anomalies mammography restaurants identified doubled cancerous around interviewee indicates first step involves text mining algorithm identifies reviews system low rate false containing key words may negatives false positives note indicate health safety issues many cases deliver clear outcome 'sick 'nausea' 'rodents' system trained radiography second step authority mammography data europe eu compared results coming written reports past biopsies acting customer reviews previous control data inspection reports improve algorithm' accuracy reliability monitoring patients' vital statistics remotely hospital piloting system support early detection potential illness monitoring patients' health indicators - example blood pressure heart rate - typically takes place manually captures situation specific moment time constantly monitoring indicators potential identify trends doctors may otherwise recognise detect health issues early prevent illness system uses biosensor - kind plaster - gathers hemodynamic data patients continuously constantly monitoring heart pulsation respiration data used system come hospital patient data anonymised shared third party provider information besides gathered monitoring plaster used build train system data environmental factors incorporated pilot interviewee pointed could contain biases future system combine information gathered biosensor separate information patients' emr draw conclusions trends observed monitoring use case targeted advertising - profiling consumers boost profit background eu legal framework internet transformed way live many people make use internet services often offered free daily basis companies offering services free mainly generate revenue advertising adverts automatically targeted individual consumers based information availability data online individual behaviour combined machine learning technologies considerably improved ability commercial enterprises target individuals could even go far manipulating consumers predicting reactions based irrational aspects psychology reasoned choice cambridge analytica scandal underscored particularly negative impact uses political purposes case company illegally obtained personal data millions social media users target political adverts different social groups based certain psychological profiles recent declaration committee ministers council europe highlights lack knowledge manipulative power algorithms \" effects targeted use constantly expanding volumes aggregated data exercise human rights broader sense significantly beyond current notions personal data protection privacy remain understudied require serious consideration \" concerns also raised online advertising powered ai technologies affect data protection privacy consumer protection right non discrimination even way democracies work word 'advertising' associated messages designed influence consumer behaviour advertising one form another always targeted specific groups based characteristics behaviour growth social media however taken targeted advertising another level using direct access consumer data micro targeting directed towards specific groups - data gathered online activities targeted activities social media providers platforms like google amazon gather comprehensive user data monitoring various activities users advertisers access detailed specific information area targeted advertising systems recommend content e g news movies one real life examples also involves called reinforcement learning technology based optimising certain goal experimenting updating rules automatically best possible output means systems tries different ad placements trial error finds best way optimise revenue - including element self learning little knowledge actual use reinforcement learning available european countries major companies working area researching issue issues related targeted advertising fall consumer protection falls shared eu competence member states article f tfeu eu consumer protection measures seek protect health safety economic interests consumers promote right information education organise safeguard interests article tfeu eu adopt minimum harmonisation measures achieve high level consumer protection article tfeu yet allowing eu member states introduce even stringent measures nationally secondary eu legislation rules advertising covered directive ec concerning misleading comparative advertising directive provides minimum level protection misleading advertising also harmonises rules comparative advertising across eu provisions directive ec apply consumer business business business relations however practically applied latter53 directive ec unfair business consumer commercial practices internal market practices54 took effect directive ec services internal market55 covers services include advertising additionally directive ec certain legal aspects information society services particular electronic commerce internal market e commerce directive also applies directive forms part legal framework digital services eu meet significant developments area new online services practices e commerce directive currently revised part digital services act package package aims \"strengthen single market digital services foster innovation competitiveness european online environment\" fra collected information actual use cases six european companies engaged placing online ads content recommendation personalised marketing use practice examples covered include ----placing ads online based click predictions e learning likelihood online users click certain links adverts automated bidding auctions online advertisement space ----personalised targeted marketing communication via email tasks fully automated examples concern analyses user preferences activity calculations probabilities clicks purchases including measurement effectiveness previously made recommendations also includes methods targeted communication basis identified target groups build long term trust clients service providers targeted online ads based click predictions business models working click predictions targeted advertisements often follow 'click buy' policy companies purchase advertising space media platforms optimise display adverts analysing interests preferences website users showing advertisements interest purpose increase relevance advertisements shown better matching interests see present example company gets paid people click advertisement buy something additionally company uses ai detect inappropriate content advertising advertisements alcohol firearms political content company uses range machine learning techniques field computational advertising estimate probability user clicking advertisement displayed specific context optimising called click rate customers' interests relevance products measured via mapping individuals' browsing histories transaction patterns information derived individuals' navigation merchant websites worldwide advertising company works done via anonymised third party cookies trackers placed merchant websites outline individuals' navigation across also list products seen purchased profiles individuals linked devices used although ip addresses anonymised product purchased recommender system algorithm tries determine products customer could also buy case 'fresh' data valued higher older data browsing histories stored maximum one year interests change purchases older year longer necessarily considered relevant advertisements shown respective person immediately adapted accordingly vary across websites also match content latter advertisement posted continuously analysed combination elements taken account individual' interest confirmed purchase made data shared across platforms includes informing others purchase made stop advertisements particular item purchase made formula reviewed algorithm adapted individuals' continuous online behaviour future company covered example expects work optimising timing terms places advertisements within given budget certain time frame also expects focus displayed ads impact consumers another example based european online market place links buyers sellers range specialised products ai used optimise advertising campaigns categorise products based advertisements shown website market place improve search engine experience predicting complementary substitutable products detect fraud attempts company uses machine learning predict value clicks customers buy advertisement space offered real time auctions examples company indicates ai enables make decisions otherwise would possible without ai would significantly scaled targeted communication customers clients case retail company focusing specialised supplies sold across physical stores online direct marketing personalised advertising used increase appeal customers time measure efficiency particular instance marketing advertising according company issue example marketing emails opened average particularly customers recognise relevant favourite products offered marketing emails sent around registered individuals system used establish may considered relevant individuals done analysing purchases made respective individuals previous six months offers displayed directly based previous purchases meanwhile new suggestions e alternative products category previous purchases similar approach used bank sends emails clients messages offering specific services products sent certain clients data analysts calculate probability clients interested service product probability certain threshold client receive message system used yet include machine learning models fully automated points taken develop system third example grocery retailer uses loyalty cards increase customers' interaction personalise offers loyalty card systems predict many customers likely engage product offering system covered example also suggests new products customers tracks results suggestions groups buyers similar behavioural patterns segments make personalised suggestions every week company' loyalty card owners receive personalised offers email website mobile application access offers store terminals ai system selects offerings based individual purchase history recommends new items might catch buyer' interest prompt purchase endnotes see instance samoili et al ai watch defining artificial intelligence towards operational definition taxonomy artificial intelligence luxembourg karanasiou pinotsis ' study layers automated decision making emergent normative legal aspects deep learning' international review law computers technology pp see fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office p fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office june un human rights council report special rapporteur extreme poverty human rights philip alston eubanks v automating inequality hightech tools profile police punish poor st martin' press redden joanna dencik lina warne harry datafied child welfare services unpacking politics economics power policy studies panoptykon foundation profiling unemployed poland social political implications algorithmic decision making see also algorithm watch poland government scrap controversial unemployment scoring system oecd glossary statistical terms social benefits definition accessed august j henry richardson chapter iv social insurance economic financial aspects social security university toronto press pieters social security overview eu competence domain regulation ec see paju j european union social security law oxford hart publishing ch erik bakke 'predictive policing argument public transparency' new york university annual survey american law vol pp andrew g ferguson 'policing predictive policing' washington university law review vol pp example one five women experienced violence brought serious incident attention police see fra violence women eu wide survey main results report luxembourg publications office p elizabeth e joh new surveillance discretion automated suspicion big data policing uc davis legal studies research paper p braga et al hot spots policing small geographic areas effects crime campbell systematic reviews vol elizabeth e joh new surveillance discretion automated suspicion big data policing uc davis legal studies research paper pp available ssrn erik bakke 'predictive policing argument public transparency' new york university annual survey american law vol pp wim hardyns anneleen rummens 'predictive policing new tool law enforcement recent developments challenges' eur j crim policy res p doi s10610 albert meijer martijn wessels 'predictive policing review benefits drawbacks' international journal public administration p doi law society commission use algorithm justice system algorithms criminal justice system p newbold j n 'predictive policing' 'preventative policing' 'intelligence led policing' future declaration annexed final act intergovernmental conference adopted treaty lisbon signed december directive eu european parliament council april protection natural persons regard processing personal data competent authorities purposes prevention investigation detection prosecution criminal offences execution criminal penalties free movement data repealing council framework decision jha oj l pp european commission standard eurobarometer report europeans artificial intelligence p european patients forum n new eu regulation protection personal data mean patients guide patients patients' organisations commission recommendation eu february european electronic health record exchange format oj l pp digital health society exchange electronic health records across eu february communication commission european parliament council european economic social committee committee regions european strategy data com final brussels february decision eu european parliament council october serious cross border threats health repealing decision ec oj l pp see commission webpage communicable diseases oecd european union healthcare glance europe p vera ehrenstein hadi kharrazi harold lehmann casey overby taylor 'obtaining data electronic health records' gliklich leavy mb dreyer na eds tools technologies registry interoperability registries evaluating patient outcomes user' guide 3rd ed addendum use data insurance industry currently potential see example spender c bullen l altmann richer j cripps r duffy c falkous farrell horn j wigzell w yeap 'wearables internet things considerations life health insurance industry' british actuarial journal pp see visualisation see short overview different ehr systems europe nurses' perspective healtheurope world cloud based services storing health data cloud college europe transformation health care digital single market synopsis report public consultation european commission study big data public health telemedine healthcare roberta pastorino corrado de vito giuseppe migliara katrin glocker ilona binenbaum walter ricciardi stefania boccia benefits challenges big data healthcare overview european initiatives european journal public health vol issue supplement pp - ministry health welfare sport netherlands digitalization health care benefits patient safety literature web reports according multiple reports different cybersecurity companies time see example sc magazine healthcare leads cost data breaches shannon williams new report reveals 'wall shame' health care data breaches tammy lovell statistics reveal healthcare sector affected personal data breaches sc magazine healthcare leads cost data breaches annet sollie reuse sharing electronic health record data focus primary care disease coding doctoral dissertation vrije univesiteit amsterdam pp vera ehrenstein hadi kharrazi harold lehmann casey overby taylor 'obtaining data electronic health records' gliklich leavy mb dreyer na eds tools technologies registry interoperability registries evaluating patient outcomes user' guide 3rd ed addendum mowafa househ bakheet aldosari abdullah alanazi show andre kushniruk elizabeth borycki 'big data big problems healthcare perspective' studies health technology informatics p sartor giovanni new aspects challenges consumer protection study committee internal market consumer protection policy department economic scientific quality life policies european parliament luxembourg neudert lisa marchal nahema polarisation use technology political campaigns communication study request panel future science technology stoa managed scientific foresight unit within directorate general parliamentary research services eprs secretariat european parliament information commissioner' office ico investigation use data analytics political campaigns council europe declaration committee ministers manipulative capabilities algorithmic processes decl example costello roisin aine impacts adtech privacy rights rule law technology regulation - edps opinion edps opinion online manipulation personal data sartor giovanni new aspects challenges consumer protection jablonowska agnieszka et al 'consumer law artificial intelligence challenges eu consumer law policy stemming business' use artificial intelligence' eui working papers law wachter sandra 'affinity profiling discrimination association online behavioural advertising' berkeley technology law journal vol forthcoming available ssrn zuboff shoshana age surveillance capitalism london edps opinion edps opinion online manipulation personal data martin gillian importance marketing segmentation american journal business education vol kaili lambe becca ricks basics microtargeting political ads facebook see example information recsys2020 workshop reveal bandit reinforcement learning user interactions accessed august directive ec european parliament council december concerning misleading comparative advertising oj l pp european commission misleading comparative advertising directive objective directive directive ec european parliament council may concerning unfair business consumer commercial practices internal market amending council directive eec directives ec ec ec european parliament council regulation ec european parliament council 'unfair commercial practices directive' oj l pp directive ec european parliament council december concerning misleading comparative advertising oj l see european commission' webpage digital services act package fundamental rights framework applicable ai use ai - presented four use cases discussed chapter - affect specific fundamental rights outlined chapter full compliance fundamental rights prerequisite using ai driven technologies irrespective area concerned chapter introduces general fundamental rights framework eu governs use ai including selected secondary eu legislation national law section fundamental rights framework provides normative basis benchmarks design development deployment ai tools helps determine whether specific use ai fundamental rights compliant requirements justified interferences fundamental rights outlined section fundamental rights framework governing use ai cornerstone instrument eu fundamental rights framework applicable use ai charter together unwritten general principles eu law main source fundamental rights eu charter enshrines wide array fundamental rights legal value eu treaties eu institutions bodies bound charter member states act within scope eu law article charter many charter rights set european convention human rights echr meaning scope must corresponding echr rights article charter however cannot prevent union law providing extensive protection fundamental rights also found provisions treaties see e g article teu titles v x tfeu eu secondary law rights safeguarded different pieces secondary eu law central piece eu secondary law context ai general data protection regulation gdpr - regulation eu governs automated processing personal data european economic area processing personal data means form part filing system - within scope eu law result gdpr apply national security related data processing gdpr coupled law enforcement directive applies police judicial cooperation criminal matters eu instruments include numerous provisions protection personal data determining key principles data processing lawfulness fairness transparency whether eu data protection legislation applies depends whether personal data processed ai driven applications use personal data example traffic data others use anonymised data cases data protection laws apply applicability entirely clear line personal non personal data blurred risk anonymised data ' identified' - ie anonymisation undone however identification usually illegal addition persons identifying data usually put major efforts potentially need access additional information individuals might included anonymised dataset identification section discusses topic detail linked results interviews carried report addition eu data protection acquis european non discrimination law key safeguarding fundamental rights context use ai related technologies article teu provides non discrimination one fundamental values eu article tfeu requires union combat discrimination number grounds moreover articles charter provide equality law non discrimination beyond several eu non discrimination directives enshrine specific detailed provisions varying scopes application include employment equality directive ec racial equality directive ec gender goods services directive ec recast gender equality directive ec eu member states also party international human rights conventions see list conventions key findings fra opinions section contain legally binding standards safeguards comply act areas fall within scope eu competence main instrument echr ratified eu member states accompanied additional protocols great majority eu member states parties echr wide reach also applies areas covered eu law addition council europe convention protection individuals regard automatic processing personal data13 another source pan european data protection obligations binding eu member states recently modernised sector specific eu national legislation also enshrines safeguards protection fundamental rights overview technical legislation beyond scope report however chapter provides examples relevant use cases discussed report complemented couple examples national laws five eu member states covered none five eu member states covered currently horizontal ai specific laws although countries looking potential need regulation eu countries finland issued recommendations self regulation development responsibility standards private sector estonia assessment concluded separate ai specific law required foreseeable future since current legal framework sufficient according relevant estonian long term strategy however legal environment must adapted avoid unnecessary hindrances implementing ai situation concerning sectoral legislation relevant use ai different sectors varies across eu member states however active policymaking ai recently emerged national level national action plans ai appeared remain core policy development member states countries working growing entrepreneurship others focused enacting market oriented policies compatible un agenda sustainable development educational activities promote ai increasing public use ai often identified ai related strategy goals investment research development also frequently outlined relevant goal domestic ai discussions potential legislative reforms remain attentive european initiatives national sector specific fundamental rights safeguards also enacted instance finland began considering overhaul domestic human rights safeguards public sector proposing broader across board legislative update opposed individual ai laws specific reference processing personal data immigration law finnish constitutional law committee put forward proposal strengthen safeguards finnish constitution overriding constitutional law shortcomings relation among others protection law accountability well ambiguity algorithms automated decision making whenever public authorities automate decision making processes processes must adhere constitutional principle rule law may endanger observance rules good administration due process proposal articulated vision requirements finnish constitution sets ai use automated decision making within public administration research identified initiatives policies linked ai fundamental rights five member states examined example estonian e state charter includes summary citizens' rights better communicating agencies electronically also targets ai relation right know data collected public authorities similarly ministry interior netherlands presented policy brief parliament ai public values fundamental rights brief stresses human centric approach ai applications strong influence human beings society whole also lists important risks ai fundamental rights discrimination result biased data reduced interpersonal relations ai takes certain forms interaction 'use case' examples social welfare use case regulating social welfare eu member states enacted rules aiming protect fundamental rights specifically area addition existing horizontal eu regulations see section mostly define rules processing protection personal data purpose social benefits insurance estonia example insurance activities act applicable types forms insurance regulates processing transmission personal data context states public authorities health care providers insurance undertakings third parties may transmit personal data request insurance undertaking personal health court data necessary insurance undertaking perform insurance contract right obligation disclose data derives law scope act also includes data transfers purpose data processing within ai systems social welfare act contains specific provisions data protection persons need social assistance notified processing data provide consent processing person established target group right opt data processing social welfare act also allows local authorities process including using algorithms personal data youth years age stored state registries identify youth employment education training finland act secondary use health social data applies using ai social care healthcare act based norms securing protecting sensitive personal data outlined gdpr aims establish conditions effective secure \"processing access personal health social data certain secondary purposes research statistics innovation development knowledge management teaching authority planning \" act regulates manner registered health data cannot processed several laws apply various types social benefits france code relations public administration applies purpose processing accessing personal data related social benefits minor amendments entry force gdpr code states \" algorithms used public administrations must published\" \" person subject automated decision making right informed\" predictive policing use case context predictive policing eu' law enforcement directive contains key fundamental rights safeguards stipulate law enforcement authorities apply main data protection principles set gdpr include requirement data controllers e competent law enforcement authorities provide data subjects information controller' data processing activities identity contact details data controller purposes processing information right lodge complaint article specific cases data controllers shall provide information - example legal basis processing - enable data subjects exercise rights right access article requires data controller confirm upon request data subject whether processing operations related case data subject shall able access data also request additional information including purposes legal basis processing categories personal data processed right information right access restricted number cases including avoid obstructing prejudicing prevention detection investigation prosecution criminal offences protect public security national security addition article law enforcement directive explicitly prohibits automated decision making prohibition limited authorised eu national law safeguards data subject' rights including \" least right obtain human intervention part controller\" see section cases scope implementing national legislation broader directive example finnish act processing personal data criminal matters connection maintaining national security strengthens right information distinguishing information provided general special circumstances healthcare use case regards eu level fundamental rights safeguards using ai healthcare gdpr empowers patients rights informed part granting control personal health data data qualifies 'sensitive data' found example medical records rights include rights access one' personal health data object processing personal data rectification erasure data well rights case breach gdpr administrative fines breaches processing data including health data allowed however estonia instance domestic law allows maximum penalty eur application misdemeanour procedure cases data protection inspectorate also impose similar fines misdemeanour procedure france data protection act public health code impose stricter requirements set gdpr regarding health data processing french data protection act amended law modernisation health system allow processing personal health data various purposes provided fall within scope one exceptions general principle prohibition sensitive data processing article gdpr targeted advertising use case considering fundamental rights safeguards relation targeted advertising underlying mechanisms regarding profiling particular eu legal framework privacy data protection provides relevant fundamental rights provisions protection privacy personal data holds status takes precedence economic benefits hence rules processing special categories personal data relevant companies operating area applying targeted advertising place companies certain obligations main legal provisions setting rules protecting personal data eu gdpr directive privacy electronic communications e privacy directive lex specialis gdpr gdpr directly applicable eu member states whenever company based eu processes personal data company based outside eu processes data relating individuals eu e privacy directive strong focus fundamental rights concerns processing personal data protection privacy electronic communications sector e g individuals use computer smartphone tablet european commission proposed e privacy regulation would replace current e privacy directive legislative proposal would broaden scope directive include specific provisions concerning unsolicited marketing cookies confidentiality requirements justified interferences fundamental rights chapter highlights selected fundamental rights - covered charter - particularly affected ai taking account four use cases discussed chapter rights absolute rights subject limitations line article charter accordingly analysing extent different fundamental rights impacted use ai section presents general steps need followed determine whether charter right limited fundamental rights affected ai absolute subject limitations interferences fundamental rights justified respect requirements charter echr case charter rights corresponding rights guaranteed echr article charter pursuant article charter limitation fundamental rights must ---- provided law ----genuinely meet objectives general interest recognised union need protect rights freedoms others ----respect essence right ---- necessary ---- proportionate court justice eu cjeu also emphasised limitation exercise rights freedoms recognised charter must respect \" essence\" rights freedoms means fundamental rights limited certain extent completely disregarded established inalienable essential core right violated measure next step conduct necessity proportionality test outlined charter respect non core aspects right interference charter right needs examined whether given legitimate aim could obtained means interfere less right guaranteed similar requirements also imposed echr interpreted european court human rights ecthr include 'essence right' concept derived object purpose echr whole respect use new technologies ecthr observed marper v uk states \"strike right balance\" protecting fundamental rights developing new technologies given wide range applications ai systems everyday life presented four selected use cases wide range fundamental rights may assessed taking account variety elements depending context particular area use notably specific purpose ai used functionality complexity scale deployed relevant assessing fundamental rights implications endnotes see also van veen c 'artificial intelligence ' human rights got ' data society points - blog data society research institute may barfield w pagallo u advanced introduction law artificial intelligence cheltenham northhampton edward elgar pp see also cjeu aklagaren v hans akerberg fransson gc february paras european convention protection human rights fundamental freedoms amended protocols nos november ets overview application charter see fra 2018a applying charter fundamental rights european union law policy making national level luxembourg publications office regulation eu european parliament council april protection natural persons regard processing personal data free movement data repealing directive ec general data protection regulation oj l pp see fra handbook european data protection law edition luxembourg publications office see example hacker p legal framework ai training data law innovation technology forthcoming available ssrn overview european non discrimination law see fra handbook european non discrimination law edition luxembourg publications office council directive ec november establishing general framework equal treatment employment occupation oj l pp council directive ec june implementing principle equal treatment persons irrespective racial ethnic origin oj l pp council directive ec december implementing principle equal treatment men women access supply goods services oj l pp directive ec european parliament council july implementation principle equal opportunities equal treatment men women matters employment occupation recast oj l pp convention protection individuals regard automatic processing personal data strasbourg january ets protocol amending convention protection individuals regard automatic processing personal data strasbourg october cets ai finland project' ethics working group ethics challenge added emphasis companies self regulation ai finland 'etiikkahaaste ethics challenge ' tekoaly uusi sahko finnish republic estonia report estonia' ai taskforce p estonian government launched preparation long term strategy example see netherlands ministry economic affairs climate policy strategic action ai strategic action plan ai strategisch actieplan ai - sapai example effort adapt goals development sustainable market see spain ministry science innovation universities national ai strategy spanish comprehensive overview see european commission national strategies artificial intelligence oecd ai policy observatory finnish constitutional law committee 'committee opinion pevl vp - vp draft proposal parliament law processing personal data immigration administration related laws' estonia national audit office chancellor justice everyone' rights e state e state charter netherlands ministry interior kingdom relations ai public values fundamental rights dutch elina saxlin hautamaki johanna lilja secondary use health data - new finnish act de donno french code \"des relations entre le public et l'administration\" new european era administrative procedure italian journal public law pp see fra preventing unlawful profiling today future guide luxembourg publications office tables sajfert j quintel data protection directive eu police criminal justice authorities available ssrn note article law enforcement directive seems apply automated decisions taken solely automated processing means safeguard apply human agency involved orla lynskey criminal justice profiling eu data protection law precarious protection predictive policing p english translation available via finlex website gdpr recital art european patients forum n new eu regulation protection personal data mean patients guide patients patients' organisations gdpr arts white case gdpr guide national implementation estonia merav griguer processing health data france look gdpr european commission proposal regulation european parliament council concerning respect private life protection personal data electronic communications repealing directive ec regulation privacy electronic communications com final brussels charter art \" far charter contains rights correspond rights guaranteed convention protection human rights fundamental freedoms meaning scope rights shall laid said convention \" also reiterated explained cjeu see example c satakunnan markkinaporssi satamedia december para joined cases c c volker und markus schecke eifert gbr hartmut eifert november para joined cases c c digital rights ireland ltd v minister communications marine natural resources others karntner landesregierung others april para c maximillian schrems v data protection commissioner october para c webmindlicenses kft v nemzeti ado es vamhivatal kiemelt ado es vam foigazgatosag december paras see cjeu c maximillian schrems v data protection commissioner october paras refer article charter see also scheinin martin sorell tom surveille deliverable d4 - synthesis report wp4 merging ethics law analysis discussing outcomes april p see e g brkan ' essence fundamental rights privacy data protection finding way maze cjeu' constitutional reasoning' german law journal p lenaerts k 'limits limitations essence fundamental rights eu' german law journal pp cjeu joined cases c c digital rights ireland ltd v minister communications marine natural resources others karntner landesregierung others april see instance khelili v switzerland october ecthr marper v united kingdom gc nos december ecthr k v finland july ecthr z v finland february ecthr huvig v france april ecthr leander v sweden march scheinin martin sorell tom surveille deliverable d4 - synthesis report wp4 merging ethics law analysis discussing outcomes april p ecthr marper v united kingdom gc nos december para see also council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems appendix para impact current use ai selected fundamental rights deploying ai systems engages wide range fundamental rights seen chapter use cases presented report involve range technologies varying levels complexity automation different phases development applied different contexts different purposes different scale rights affected depend factors number horizontal sector specific fundamental issues emerge chapter begins general overview risks perceived interviewees general awareness fundamental rights implications using ai chapter highlights selected fundamental rights affected ai related technologies reference four use cases analysed analysis takes account presents views practices awareness issues expressed interviews conducted report interviewees first asked general risks see using ai asked general fundamental rights awareness using ai concrete fundamental rights implications mostly linked data protection non discrimination availability complaints mechanisms perceived risks important recognise many issues cut across different rights example potentially biased decision made algorithm could involve right non discrimination protection personal data right effective remedy similarly particular issue seen perspective different rights instance good explanation decision made algorithm required right protection personal data right good administration right effective remedy fair trial asked general risks using ai interviewees always mention fundamental rights main risks although highlighted related topics private sector representatives often mentioned inaccuracy risk using ai followed potential bias proper legal basis processing personal data one respondent international retail company stated one business risk linked european customers extremely knowledgeable rights namely people hesitate ask data storage automated decision making customers properly informed might complain company may lose client addition interviewee continued breaching law possible fines linked breach another major business risk respect public administration bias often highlighted risk associated using ai addition public authorities often discussed inaccuracy data identification risks using ai example interviewees working social benefits algorithms stated incorrect results general risk occur potentially due rare cases well identified algorithm due errors input data also highlighted difficulties associated moving testing deploying system including technical challenges resources required potential different results deployed respondents working targeted advertising also highlighted business risks - example offering irrelevant inappropriate content one respondent mentioned potentially losing control automated systems addition interviewees indicate challenges linked difficulty interpreting results outputs ai systems one interviewee consultancy sector fears risk related lack absence sufficient ai knowledge understanding cause ongoing projects halted due company' inability explain clearly algorithms perform purpose another interviewee law enforcement sector looking possible use ai support decisions licence applications explains inherent risks system proposes certain response example potentially using ai support decisions license applications firearms respondent asserts would critical understand reasoning behind negative decisions also positive decisions several interviews showed major concern assign properly trained staff sufficient expertise trace explain interact ai system finding also corroborated results european commission survey among companies eu survey indicate obstacle adopting ai technologies difficulty hire new staff right skills mention complexity algorithms obstacle respect ability explain decisions based algorithms interviewee working public administration mentioned alternatives completely transparent making decisions room doubt similar vein respondent working area health private sector mentions 'self learning' algorithms forbidden area work fixed algorithms traced \" use ai bring many benefits also risks like risks reported without providing much additional information include nuclear energy \" cyber security data quality excessive monitoring people due use interviewee working private sector data algorithms job loss due automation profiling spain general awareness fundamental rights legal frameworks ai context everyone eu aware fundamental rights fra' fundamental rights survey shows slightly every second person eu aged older heard charter slightly people two three heard echr universal \" use ai impact declaration human rights might echr older human rights way terms established people' common knowledge decision process matter whether decision made majority people interviewed project acknowledge using machine human \" ai generally affect fundamental rights mention interviewee working public use ai potential impact fundamental rights administration estonia aware implications responses influenced different ways use ai also understanding fundamental rights example one respondent working production pension forecasts based machine learning says producing statistics impact fundamental rights apart data protection issues need addressed another respondent working social benefits algorithms argues impact depends \" widely human rights defined\" - example right receive correct pension \" rights related data none interviewees working targeted advertising believe protection ensured see use ai affects fundamental rights negatively one respondent working human rights relevance targeted communication customers stated one reason \" response relates lack knowledge exactly fundamental private company spain rights practically interviewees showed awareness rights privacy \" touch topic data protection well non discrimination rights assume human dignity right fair trial effective remedy also human rights issues involved mentioned albeit briefly activities within legal framework activities closer look interviewees' responses indicates diverging views across compliant data protection respondent groups respondents working private companies discuss good practices therefore data protection non discrimination rarely mention rights assume human challenges company working targeted advertising mentions rights issues related attentive issues linked freedom speech right information systems \" sense company promotes rights public administration spain posting adverts helps news websites obtain funding continue work one interviewee notes range rights awareness much broader among public sector representatives working ai referred rights human dignity presumption innocence working ai systems different fields application also highlight use systems also covered sector specific laws example system making decisions unemployment benefits regulated national legislation unemployment insurance administrative procedures data protection however respondents aware legal standards apply use ai unsure absence ai specific regulation several respondents mention ethics guidelines certification schemes work existing guidelines standards necessarily specifically aimed ai case example security system 'iske' estonia3 area financial services payment card industry data security standard respondents also refer standards developed international organization standardization iso institute electrical electronics engineers ieee european committee standardization cen respondent working targeted advertising argues certification needed field posting ads issues linked health sector work banks several interviewees noted \" think organisations developing internal guidelines regulate specific technology like ai sufficient general respondents mention guidelines developed eu principles technology neutral international level guidelines european commission' rules \" private sector estonia high level expert group ai oecd' guidelines unesco standards \"yes codes yes aware ongoing developments eu council europe level procedures codes procedures date refer need update sector specific regulations able using something innovate ai - example area health yet one interviewee created analog world states existing standards sufficient ai need digital world \" regulated separately private sector spain human dignity using ai driven technologies broadly implicates duty respect human dignity foundation fundamental rights guaranteed charter article charter states human dignity inviolable must respected protected times cjeu confirmed case law fundamental right dignity part eu law ai driven processing personal data must carried manner respects human dignity puts human centre discussions actions related ai rather technology 'human ' creating affected new technology needs focus taking human dignity starting point help ensure use ai benefits everyone - example supporting ageing access healthcare dignified manner use ai also risks infringing closely connected charter rights right life article right integrity person article context important consider avoid harmful use ai prevent violations rights example comes use ai people engaging criminal activities ai used weapons apart extreme cases preserving dignity includes avoiding subjecting people ai without knowledge informed consent strongly linked privacy data protection example people' applications social benefits decided upon use ai people need made aware consent use automated decisions taken give another example certain proportion population feel comfortable subjected biometric identification systems hence using without allowing opt could potentially violate dignity respondents public administration referred right dignity discussing fundamental rights one respondent considering use ai prisons mentions particular context first needs assessed whether risk violating fundamental rights would high right human dignity interviewees made general references right without discussing relation concrete use ai right privacy data protection - selected challenges right respect private life protection personal data articles charter core fundamental rights discussions around use ai closely related rights respect private life protection personal data distinct self standing rights described \"classic\" right protection privacy \"modern\" right right data protection strive protect similar values e autonomy human dignity individuals granting personal sphere freely develop personalities think shape opinions thus form essential prerequisite exercise fundamental rights freedom thought conscience religion article charter freedom expression information article charter freedom assembly association article charter given two rights absolute rights subject limitations however interference needs adequately justified11 cannot compromise essential inalienable core right explained section concept \"private life\" \"privacy\" complex broad susceptible exhaustive definition covers physical psychological integrity person therefore embrace multiple aspects person' physical social identity also zone interaction person others even public context may fall within scope \"privacy\" contexts ecthr used concept \"reasonable expectation privacy\" - referring extent people expect privacy public spaces without subjected surveillance - one factors albeit necessarily conclusive one decide violation right respect private life relevance scope application however appears limited similarly according un human rights committee mere fact participants assemblies public mean privacy cannot infringed applies monitoring social media glean information participation peaceful assemblies widespread use ai technologies may technologies continue develop raise unchartered issues novel concerns right respect private life ai driven technologies may change way think privacy algorithmic tools predict reveal information people' behaviour unprecedented ways - without people even realizing giving away information personal data obtained internet may instance used targeted advertising raising many fundamental rights concerns issues linked personal data sharing via smart phone apps particularly raises significant concerns including variety potential harmful effects manipulation exploitation vulnerabilities discrimination security issues fraud e g identity theft reduced trust digital economy using ai driven technologies often implies computerised processing large amounts personal data constitutes interference right protection personal data set article charter embodying pre existing eu data protection law well right private life article charter article echr awareness data protection issues use personal data eu people heard gdpr contrast virtually \" little anxious interviewees aware gdpr discussed data protection issues gdpr implemented data protection rules deriving gdpr national law clearly end meant managing datasets well known applied rights area ai fundamental access rights ... good rights less known reminder everything done \" discussing legal framework governing use ai public administration finland respondents mentioned data protection rules well sectoral laws clearly say legal framework apart data protection laws interviewee working spanish public \"actually ' concerned administration notes \" rely data protection regulation norms gdpr might hinder ai research ' available moment\" afraid large databases used previously cannot one interviewee reflecting image based diagnostic tool expressed used research anymore \" view gdpr could hinder research hospital using tool private company netherlands support diagnosis strokes clear rules data protection interviewee indicated although know whether data protection certification requested \" gdpr give specific rules gives others referred general data protection guidelines indicated principles comes aware documents ethical issues interpretation \" private company estonia respondents working target advertising aware privacy data protection issues although responsible data protection issues companies aware efforts protect data privacy one interviewee mentioned contrary earlier years personal data stored much securely handled care attention given properly handling consent data processing consequence high level awareness data protection privacy issues linked ai use however data protection law applies personal data processed example using anonymised data develop ai tools e training data likely permissible many instances would trigger gdpr research shows data often de anonymised however efforts often require expert knowledge potentially additional information illegal illegality de anonymisation necessarily preclude applicability gdpr important consider identification anonymised data reasonably likely anonymising data one aspect protecting privacy data subjects assessing risks identification aspects also important consider disseminating anonymised data include use data purpose outputs produced interviews respondents always entirely clear use personal data often superficially described data used mentioned chapter several instances interviewees indicated use non personal data anonymised data arguing data protection relevant cases example semi public organisation working environmental management uses aggregated data water consumption machine learning based predictions water consumption data available individual level interviewees said use personal data although data originally stem individuals tool supporting restaurant inspection collecting data online sources use personal data - interviewee indicated however indicated need careful mining data online even publicly available might include personal data usernames \" would great retrieve another example insurance company using chatbot make client data another service contact effective data used train system chat protocols client ' repeat conversation logs linked personal data however line go example linking data personal data might possible reusing data \" future according respondent public administration finland companies working targeted online advertising indicate using pseudo anonymised data done example excluding names social security keys encrypting data identity consumers relevant company interviewee mentioned indicate use non personal anonymised data others possible data used make predictions decisions specific individuals example interviewee company working credit rating mentioned need know identity consumers assessments case even important right forgotten according interviewee exhaustive discussion data protection issues possible report however two aspects clearly emerged interviews automated decision making linked right human review right obtain meaningful information decisions automated automated decision making article gdpr article law enforcement directive generally forbid automated decision making meaning \"decision based solely automated processing including profiling produces legal effects concerning similarly significantly affects \" article gdpr explicit consent needed decisions solely automated legal similarly significant effect people automated decision making authorised law authorisation union national law sole precondition law enforcement directive article processing decision considered fully automated instruments require human review controller however concept 'automated' decision making elusive requires discussion research example cases human intervention might limited 'signing ' outcomes ai system rendering virtually automated importantly human review must mean human signing recommendations outputs algorithm must done someone \"authority competence change decision\" considering relevant data hand humans review potentially override outcomes system must also evaluated research indicates humans overrule outcomes algorithms mainly result algorithm line stereotypes behaviour threatens possible added value automated processing potentially accurate even fairer humans may also put minority groups disadvantage therefore also relevant non discrimination issues discussed overall disagreement exact scope provisions eu data protection acquis whether impose general ban certain types automated decisions provide data subjects rights context certain types ai driven decision making using algorithms area social benefits health predictive policing clearly potential legal significant consequences interviews suggest working areas well aware concept human review decisions taken support ai many interviewees indicate automated decisions taken one exception automation unemployment benefits based national law fully automated decisions involve discretion another example another country positive decisions based pre defined rules automated student benefits case negative decisions made humans cases refer rule based decisions involving use statistics machine learning another respondent testing use ai systems including machine learning area social benefits mentions equality could negatively impacted automation makes human behaviour visible including existing biased practices makes precautions necessary consequence organisation would allow decisions made humans interviewees working health highlighted risks linked automation decisions interviewee discussing tool support stroke diagnosis feels important rely system avoid risk automation confirmation bias caution early positive experiences application could prompt users rely easily devote less attention assessment images interviewees raised similar concerns one interviewee discussing tool analyses images provide probability presence certain type lesion notes technology supports diagnosis simple cases expertise doctors particularly important - trusted - complex cases targeted advertising often considered significant effect people however may case example individual' vulnerabilities used successful advertising considering vulnerabilities particularly important people disadvantaged groups may aware opt direct marketing see box right say decisions automated absence case law area information research needed identify impact automated decisions e advertisement delivered answering questions challenging targeted advertising based highly complex technology scale eurobarometer survey asked people eu aware right awareness opt direct marketing overall eu citizens heard right right opt exercised people exercise right aware direct - becomes even important direct marketing made much marketing efficient machine learning among general awareness levels strongly vary across eu percentage people know population right opt direct marketing ranges bulgaria netherlands figure shows percentages also highlights - based fra' analysis eurobarometer data - strong variation within countries broken regions regions fewer one four heard right areas higher shares people risk poverty indicates general problem people disadvantaged society tend less aware right data show people working often struggle pay bills living rural areas older less aware right figure awareness gdpr right opt direct marketing eu united kingdom country region fi- se- ee- insufficient data available lv- dk- uk- lt- ie- nl- pl- de- - lu- cz- sk- - hu- fr- si- ro- hr- - bg- pt- es- el- mt- cy- note map show non eu countries uk light shading aware right dark shading less aware right results regions within countries represented light grey spaces excluded fewer respondents meaning numbers observations low reliable results n question \" general data protection regulation gdpr guarantees number rights heard following rights ... right object receiving direct marketing \" source fra calculations presentation based european commission eurobarometer experiences use cases general interviewed experts highlighted data protection law difficult interpret lacks clarity comes meaning automated decision making one expert france felt automated decision making difficult explain automated decision making banned meaning exceptions gdpr allow automated decision making removed pointed ai used decision support tool another expert independent lawyer netherlands views current laws standards sufficient says need concretised per sector particularly expert mentions scope existing rules permissible automated decision making clear remains unclear comprehensive assessment 'human loop' means also raised relation syri case remained unclear extent decisions reviewed another expert working supervisory authority generally sees need adapting data protection laws \" legislation quite comprehensive organisation supervision thereof also political behind \" concerns reflect findings research also raise serious \" risk much issues concerning right human review example responsible trust machine \" officers questioned results algorithmic system built profile public administration france unemployed people poland less one percent cases essentially makes supporting tool automated decision making tool \" huge tension surrounding linked question reviewing decisions outputs ai systems gdpr want well challenge clear lack knowledge ai works interviewees might fact worse often could explain detail system use works interpretation data data uses due lack knowledge lack transparency turns impossible \" meaningful information logic involved explaining outcomes public administration netherlands algorithms essential several fundamental rights crucial processing personal data also ensuring algorithms fair discriminate also necessary enable \" explain model people properly challenge decisions ai systems ' able model statistical explainable \" one interviewee working public administration explains complexity public administration france differs depending tasks licence administration systems relatively straightforward crime prevention analysis uses data sources makes harder understand another interviewee working law \"internally explain enforcement says current ai used police organisations yet decisions machine learning complex would make explanations difficult might models several means case future \" private sector estonia respondent working financial data transactions indicates traditional models straightforward understand however new methodologies difficult explain company invest resources \" systems black making models explainable still level explainability boxes information processes required gdpr clear respondent already take step forward defence human rights \" public administration spain \" strongly attached idea ai explainable \" public administration france people aware right say decisions awareness automated evidence suggests eurobarometer survey showed europeans right know data protection rights say decisions fra' analysis eurobarometer survey shows figure drops considerably automated among people lower socio economic status eu citizens report struggling pay bills time know right lack rights awareness among socially disadvantaged could contribute social exclusion already disadvantaged less aware challenge automated decisions see figure gender differences small yet women even less aware right women men older people considerably less aware among aged older figure awareness right say decisions automated age gender difficulty paying bills years older years age years years women gender men refusal difficulty paying bills almost never never time time time total notes n question \" general data protection regulation gdpr guarantees number rights heard following rights ... right say decisions automated e g algorithm decides granted loan \" source fra calculations presentation based european commission eurobarometer equality non discrimination equality law non discrimination enshrined articles charter discrimination \" one person treated less favourably another would treated comparable situation\" based perceived real personal characteristic28 called 'protected grounds characteristics' article charter prohibits discrimination based ground sex race colour ethnic social origin genetic features language religion belief political opinion membership national minority property birth disability age sexual orientation charter prohibition reflects corresponding rights echr article protocol echr article even broader establishes non exhaustive open list extending protection wide range new grounds unlike article echr charter right non discrimination freestanding right applies situations need covered charter provision main challenges discrimination crucial topic comes use ai purpose machine learning algorithms categorise classify separate one interviewed expert points making differences per se bad thing according expert deciding grant loan credit history used differentiate individuals basis protected attributes gender religion however many personal attributes life experiences often strongly correlated protected attributes credit history might systematically different men women due differences earnings job histories interviewees often mention efficiency main purpose using ai related technologies yet important note cannot justify unfair differential treatment often protected attributes might highly correlated risks example differences life situations among men women might often linked different insurance risks however acceptable test achats ruling shows case cjeu put end gender discrimination insurance pricing certain circumstances areas using algorithms could positively contribute reducing bias stereotyping algorithmic data analysis may produce results could dispel prejudicial attitudes example predictive policing might contexts lead equitable non discriminatory policing reducing reliance subjective human judgments predictive techniques may used identify called 'white collar crimes' financial crimes historically policed nevertheless direct indirect discrimination34 use algorithms involve big data considered one pressing challenges use ai driven technologies bias discrimination including gender based discrimination data supported algorithmic decision making occur several reasons many levels ai systems difficult detect mitigate often quality data biases within source potential discrimination unfair treatment discriminatory effects generated certain groups practice difficult individuals challenge far limited number court cases dealt discrimination relating ai systems first instance decision divisional court cardiff dismissed claim uk court concerning lawfulness south wales police' use \"afr locate\" face appeal police recognition system court appeal overturned decision use facial recognition found facial recognition programme used police unlawful court violates appeal ruled \" much discretion currently left individual police officers\" added \" clear placed watch list clear human rights criteria determining technology deployed\" court also held police sufficiently investigate software use exhibited race gender bias judgment first merit specifically matter europe considerably narrows scope permissible law enforcement agencies need fully comply human rights law uk court appeal r bridges v cc south wales ewca civ august ars technica 'police use facial recognition violates human rights uk court rules' august studies highlighted potential discrimination prompted use ai systems across areas covered report area predictive policing example particular risk relates potential automated decision making tools reproduce entrench existing discriminatory practices undermine equality law article charter historical crime data underpins predictive policing may biased reflecting inherent data gaps e g chronic underreporting certain types crime alongside issues data recorded e g human error also bias individual officers crime victimisation surveys consistently show large proportion crime never reported police public - particularly crimes involving physical sexual violence hate crimes example fra' survey violence women - respondents - showed one five women experienced violence partner anyone else brought serious incident attention police fra' eu midis ii survey respondents across eu showed three ten reported incidents racially motivated hate crime police organisation compared violent crime hate crime property crime - burglary - higher rate reporting police particularly developed countries may requirement claiming insurance policy sum - relying official crime statistics based reported crime looking develop ai models field predictive policing particularly problematic comes specific crimes specific groups variables used ai modelling proxies race ethnicity gender protected categories complexity algorithms makes harder identify remove biases instead providing objective analysis predictive policing software may turn 'echo chamber' cementing existing systemic flaws injustices 'stamp' appears scientific legitimacy use predictive policing may also make law enforcement responses less equitable focusing certain crimes areas predictive policing currently focused property crimes theft burglaries often associated certain demographics neighbourhoods result certain demographics neighbourhoods - individuals living - stigmatised meanwhile white collar crime - typically committed different demographics - less prioritised patterns policing - whereby certain neighbourhoods communities disproportionately policed - predates use ai however 'promise' ai 'objective' turn used counteract discriminatory policing needs verified practice oxford university researcher sandra wachter highlights discrimination may occur due information linked protected attributes targeted advertising newly created profiles purpose advertising might amount indirect discrimination potentially even require new characteristics added non discrimination legislation extend scope expanded areas experiences use cases \" want machine many interviewees noted use ai general discriminate discriminate basis sex systems working many indicated belief put variable sex excluding information protected attributes sufficient protection easy make examples discrimination however discrimination occur due information symmetrical notice sex contained datasets may indicate protected attributes traces certain relevance \" protected groups often hidden information public administration spain example public authority uses ai tax customs shows challenges linked identifying possible bias potential discrimination using algorithms scrutinising algorithms public administration body found higher degree errors tax declarations among recently issued national identification numbers almost always attributed immigrants prompted research correlation turned outputs people recent identification numbers often contained errors never filed taxes know also case non migrants also example proxy information parts number could indicate immigrant status another interviewee working potential use ai detecting benefits fraud mentioned respect \" want prevent discrimination based ethnicity instance suffice remove 'ethnicity label' neighbourhood composition often also determined ethnicity ethnicity plays role preventing discrimination often goes beyond 'direct' characteristics\" \" f access even respondents aware general potential sensitive personal data discrimination using ai often ruled system impossible check discriminates people based protected characteristics profiling basis \" respondents also believe tools positive impact terms public administration netherlands non discrimination one respondent testing ai social benefits decisions regrets able use ai data protection reasons even though respondent' view automation could process big datasets effectively without discrimination noting protection personal data needs observed respondent feels hinders prompt decision making non discrimination - \" automated automated\" respondents clear sure whether use ai could discriminate respondents repeatedly stated system cannot discriminate include data protected characteristics example several interviewees working predictive policing law enforcement felt potential discrimination ai systems use data return outcomes related protected grounds system aim identify people others working predictive policing felt discrimination could occur particular issues training data relation predictive policing 'heat map' case example one interviewee noted - dataset never fully neutral representative complete - strong risk bias possible discrimination towards particular groups identified sharing datasets increase amount data available one way mitigate risk felt impeded data protection regulations also indicated multi level teams task travel different police authorities check quality systems used set area targeted advertising interviewees mentioned discrimination potential problem mainly asked directly overall respondents think systems discriminate three respondents mention information gender age used consequently discrimination respect occur another interviewee sure information included \" discrimination ' complicated respondent working breast cancer detection tool highlighted diseases age gender ethnicity relevant factors population groups present certain ethnic groups likely develop certain types cancer respondents working predictions take account health highlighted potential discrimination also linked sexual ethnic genetic character uses system suggesting could become greater challenge discriminatory violation system used non medical staff human rights \" private sector france different related example comes respondent working credit rating private company selling credit scores individuals created algorithm company uses information gender age citizenship credit risk models information impact outcome credit scores example younger people non citizens higher credit risk score influence demographics much smaller compared credit history data according interviewee system \"certainly impact right non discrimination make decisions sell data data analytics creditors monitor discriminate\" another interviewee working data strategy financial institution private sector using ai analyse financial transactions clearly mentions challenges understanding non discrimination constitutes work interviewee mentions example clear extent illegal exclude older people receiving credit life expectancy expected lower mortgage repayment period asked findings point uncertainty ambiguity financial sector respect article charter - non discrimination - translates real life situations vulnerable groups much discussion research discrimination using ai linked biased results respect ethnic origin gender extent age although important analyse potential discrimination groups charter covers several grounds discrimination less often part discussions research grounds include example political opinion sexual orientation disability charter provides particular rights special groups beyond articles including rights child article rights elderly article rights persons disabilities article question age - respect older age groups younger adults - came interviews notably comes insurance credit see however none interviewees experts directly mentioned rights child might linked extent nature use cases investigated clearly reflects fact topic high agenda many working ai article charter emphasises best interests child must primary consideration activities public authorities private actors concern children applies course - equally - field ai two respondents public administration mentioned possible use ai area child custody distribution children schools address consideration rights child respondents wish go detail concerning use cases - potentially reflecting sensitivity topic finally issues linked integration people disabilities raised interviews eurobarometer survey included questions ai asked respondents areas awareness mostly concerned comes use ai including discrimination among general decision making unclear responsibility nobody complain population potential around eu citizens indicated concerned using ai could ai lead lead discrimination terms age gender race nationality - example taking decisions recruitment credit worthiness etc discrimination results vary across countries higher proportions people concerned discrimination netherlands luxembourg sweden lower proportions expressed concern estonia hungary lithuania see figure however question clear people know discrimination happen aware happen think problem figure awareness risks discrimination using ai country eu nl uk lu se fr el si de es cy ie hr dk cz fi bg pt sk mt lv ro pl ee hu lt notes includes people indicated concerned ai could lead discrimination among three possible issues three issues source fra calculations based european commission eurobarometer tackling gender charter stipulates equality inequality women men must ensured areas including design employment work pay article gender discrimination use ai major concern comes design use ai related technologies development side european economic social committee notes development ai taking place within homogenous environment principally consisting young white men results cultural gender disparities embedded ai technologies example training data prone manipulation may biased reflect cultural gender prejudices preferences contain errors see also european also reflected research commission white despite efforts achieve paper artificial intelligence - gender balance majority european approach interviewees men excellence trust com final disparities design brussels february deployment stage linked p systematic disadvantages affecting european economic women labour market social committee potential lack awareness artificial intelligence - gender biases recent study consequences showed increased use artificial intelligence industrial robots could widen digital single gender gap despite market production genders benefitting increased consumption automation analysis indicated employment society men medium high initiative opinion skill occupations would benefit may jo c p disproportionally aksoy c ozcan b looking ahead using data philipp j algorithms could help better robots gender mainstream gender equality pay gap europe iza discussion paper policies processes paying attention gendered datasets drawing discussions around see webpage data feminism gender inequalities use datasociety' website data 'data feminism' could help raise awareness criado perez c invisible male point view women exposing data taken default view bias world designed also finds way men london datasets access justice right effective remedy tribunal fair trial article charter one often used charter right legal proceedings highlights importance upholding fundamental rights rule law right horizontal character empowers individuals challenge measure affecting right conferred eu law respect guaranteed charter cjeu underlined article charter constitutes reaffirmation principle effective judicial protection characteristics remedy must determined manner consistent principle right effective remedy also covers decisions taken support ai technologies eu data protection law reconfirms right effective judicial remedy must provided relation decisions controller processor53 well supervisory authority data processed ai driven technologies exception crucial note possibility lodge administrative complaint supervisory authority provided gdpr law enforcement directive55 considered effective judicial remedy article charter court involved review judicial review always remain available accessible internal alternative dispute settlement mechanisms prove insufficient person concerned opts judicial review using ai challenge right effective remedy different ways one prominent concern lack transparency use operation new technologies algorithmic decision making notoriously opaque data collection algorithm training selection data modelling profiling situation around individual consent effectiveness error rates algorithm aspects often transparently reported without access information individuals may able defend assign responsibility decisions affecting appeal decision negatively affecting fair trial includes principle equality arms adversarial proceedings established ecthr requirements also form part corresponding charter right article view article charter main challenges issues reflected specific challenges right effective remedy fair trial interviewed experts outlined generally experts indicate difference accessing remedies private companies public administration public authorities often forced transparent use ai meanwhile companies appear secretive assessment several experts suggests however expert netherlands said people might readily complain companies reluctant complain public authorities public services often concern vulnerable people need social benefits would less inclined complain decisions opportunities successfully complain use ai challenge decisions based ai essential providing access justice interviews emphasised following important respect ----making people aware ai used ----making people aware complain ----making sure ai system decisions based ai explained first everyone needs know dealing ai system taken decision affects people e g social benefits people concerned might complain general - able complain use ai know ai involved expert explained general willingness complain biggest problem people often know ai used organisations transparent even though required gdpr several interviewees indicate informing people decision made based partly automated tools first step providing access complaints second everyone needs know complain may difficult people know body deals type complaints one expert pointed consumers often know complain - example bank might use algorithms deciding financial matters public administration issues automated decisions decided add names employees decisions provide contact persons potentially challenging automated decision interviewees indicated ways procedures complaints place procedures complaints linked use ai companies organisations use ai anonymised aggregated data indicate complaint mechanisms place finally complaining need enough information challenge underlying decision thorough information ai systems provides equality arms meaningfully challenge decisions however straightforward comes use ai particularly ----potential intellectual property rights issues ---- complex systems difficult explain intellectual property rights form one hurdle providing enough information decision made system works algorithms part implemented software technical invention may subject intellectual property rights - right protected article charter actors often seek copyright patent trade secret protection safeguard knowledge ai one interviewee insurance sector claims due highly competitive market \"one may share much workings used technology\" instance particular price given customer essentially competitors could benefit knowledge underlying software subject scrutiny another respondent using ai handle visa applications notes using systems developed external providers whose algorithms covered intellectual property rights hinder necessary transparency later stage another challenge successfully complaining automated decisions use ai general challenge explain decisions based complex systems interviewees working public administration suggest usually clear guidance complain administrative decision area interviewees highlight importance detailed explanations example systems automatically provide unemployment benefits cases involve discretion clients ask reasoning behind automated administrative acts interviewee indicates clients wish see calculations behind financial decisions may self service system organisation' website publications contain detailed descriptions calculations used interviewees recognise open transparent logic essential providing explanations regarding ai supported decisions often challenging impossible achieve one interviewee working bank mentions complex machine learning solutions cannot used certain decision making reasoning system cannot explained easily systems used purposes however interviewee working another bank indicates systems used use simpler methods addition complex ones get idea probable reasons decisions one expert raised problem companies internally might enough information way algorithms work lack expertise knowledge appears major hindrance practice seeking access effective remedy experiences use cases \" topic transparency respondents discussing predictive policing tools highlighted transparency important nowadays important many procedures publish information many automatic gender based violence use case felt sending police means help upload file outcome ai system judge informing victim information portals level risk attributed case police measures lot work done apply result enhances transparency terms transparency \" public administration spain interviewees discussing heat map example referred numerous requests police explain system' purpose works highlighted transparency way reduce public anxiety number interviewees pointed possibility individuals affected system make complaints police courts ombudsinstitution reference domestic violence case however interviewee indicated procedure place question system police protocol terms measures protect fundamental rights health services use cases several interviewees referred ethics committees well general legal safeguards data protection rules checks controls primarily mentioned take place external actors specific complaints procedures place organisations interviewees responded question interviewees highlighted doctors ultimately take responsibility decisions patients often know use ai tool first place example breast cancer detection example interviewee indicated possibility legal recourse developer tool radiologist makes decision diagnosis liable errors safeguards place targeted advertising cases mainly follow data protection requirements ensuring consent obtained respected one company makes sure clients engaged illicit practices rejects clients certain sectors political advertising complaints received organisations interviewed received complaints challenging use ai cases interviewees claim received complaints complainants aware ai used noticed incorrect outputs decision making example individuals lodged complaints regarding traffic fines whereby police officer stopped car driver upon hearing car driver' explanation fine wrongfully administered proceeded manually correct information system without able update system' historical data cases fines remain visible throughout system particular person would continue profiled high risk occasion even though organisations rarely received formal complaints respect \" number complaints use ai interviewees often state due early stages data use miniscule rather ai implementation nonetheless interviewees reported repeated people may asked delete requests access rectification personal data people information \" requested information removed well explanations private company estonia certain recommendation made majority interviewees claim procedures decision processed undertaken human hand interviewees showed interest opening new channels analyse explain redress decisions involving ai solutions rights linked access justice set charter also impacted notably use ai law enforcement include example presumption innocence article charter identifying people suspected committed crime police may target activities specifically one person put suspicion based flawed fragmented data algorithmic profiling uncritical reliance automated tools without proper human review takes account information might contribute discrimination decision making right social security social assistance right social security assistance enshrined article charter classic social right inspired various international european legal standards provision combining elements right principle great significance eu view free movement people within union instead tying issues social protection labour market charter right takes new communitarian approach broadly referring \"providing social protection cases maternity illness industrial accidents dependency old age case loss employment\" article however primarily programmatic statement prescribe minimum standard protection principle eu member states determine conditions entitlement access social benefits clarification needed cjeu yet article charter provides protection measures restricting abolishing existing social security rights addition access social rights guaranteed individuals legally residing within eu exercise right free movement regardless nationality subject eu national laws article thus creates justiciable rights national courts cjeu becoming increasingly apparent impact ai technologies social protection systems lives many individuals rely upon far reaching - potentially - problematic introducing ai driven technologies social welfare systems risks creating barriers access right example using ai social security needs account potential negative - discriminatory - effects non nationals eu citizens third country nationals exercising right freedom movement eu could negatively affected example system relies data job histories available moving eu member states one respondent addressed 'right receive correct pension' aspect wider definition human rights meanwhile none interviewed referred fundamental right social security social assistance could partly reflect nature use cases however lack references social rights among public sector interviewees notable consumer protection charter stipulates eu policies must ensure high level consumer protection based article tfeu eu institutions bodies needs observe principle member state authorities implementing eu law charter principle provides guarantee particular goal \" high level consumer protection\" article tfeu concrete also determines means achieve stated aim - example protecting health safety economic interest consumers well promoting right information education among use cases use ai targeted advertising use medical records companies particular importance comes targeted advertising consumers need aware opt targeted aware might subjected advertising want particularly problematic combination highly sophisticated ai systems advertising amount sort manipulation consumer preferences consumer protection also major relevance use health data ehrs european consumer organisation beuc noted ai area health brings challenges consumers recommends ai technologies must fully respect data protection rules transparent consumer avoid discrimination beuc also called updated regulation legislative measures market surveillance law enforcement efficient redress concerning digital health products services fully protect eu consumers beuc carried survey among consumers views ai selected eu member states shows one two respondents agree companies using ai manipulate consumer decisions addition almost half respondents believe personalised content adverts e commerce platforms added value slightly half survey respondents expressed low trust governments effectively control ai interviews conducted study consumer protection mentioned margins discussing risks using ai fundamental rights however respondents businesses refer consumer protection legislation relevant framework also applying use ai moreover respondents deem consumer protection authorities potentially relevant oversight bodies ai used general terms many interviewees business sector stress importance consumer satisfaction example company using video surveillance security customers premises mention consumer protection regulations relevant technical solutions use systems aim improve situation consumers also preserving rights several ai tools built understand profile consumers enable businesses improve services marketing data protection important aspect business also linked fact breaching data protection rules considered business risk mentioned one major concern companies obtaining managing consent consumers customers process data using ai tools marketing purposes interviewees report gdpr impact improving systems handle consent right good administration right good administration well established general principle eu law elaborated cjeu binding eu member states also fundamental right enshrined article charter although actions eu institutions bodies agencies general principle eu law requires eu member states apply requirements right good administration public action right includes limited right individual access file obligation public authority give sufficient reasons decisions access file facilitates understanding evidentiary basis decision made reasons underlying places individual better position put forward counter arguments exercising right heard right effective remedy obligation give reasons makes perspective individuals affected decision making process transparent person concerned know understand measure action taken transparency also enabling principle provides foundations rights including exercise right effective remedy according cjeu context individual decisions made important determining extent duty give reasons france instance code relations public administration requires written explanations factual legal considerations decision based right good administration also applies ai systems process personal data support decision making public authorities although right good administration may subjected certain limitations question arises ensure potentially huge number individuals access files personal data used ai systems another question make sure public authorities always give sufficient reasons operation ai driven technologies cannot fully explained due inherent opacity complexity use system categorise unemployed people set poland highlighted problems linked public administration use algorithms based questions answered unemployed people categorisation developed statistical algorithm system received lot criticism civil society respect lack opportunities complain potential discrimination end complaint ombudsinstitution - based administrative grounds - led constitutional court ruling put end system' use intent increase efficiency drives use ai public sector - aim directly speaks improving administration benefiting citizens respondents public administration far often indicate efficiency reason considering use ai presently using ai one respondent advises ministries digital strategies use ai said main reasons adopting ai improve service citizens reduce costs services public administration interviewees also indicate public administration particular requirements meaning ai cannot used purposes needs particular attention comes decision making however efficiency system also considered important added value sense respondent working digitalisation migration management indicates building complex ai systems risk afterwards would require lot work understand system retrospect interviewee indicates team needs careful allow ai make final decisions taken human - society clients ready according interviewee although systems appealing work effectively could result extra work negative results however interviewee also indicates dimension efficiency \" often side lined discussing data protection\" requirements good administration also directly link issues raised respect data protection non discrimination right effective remedy fair trial public administration process data legal basis decisions need fair transparent pathways challenge decisions need available accessible result requirements good administration directly linked discussion analysis respect legal processing data data protection fair decisions linked discussion non discrimination alongside transparency ways challenge explain decisions respect access justice endnotes see european commission european enterprise survey use technologies based artificial intelligence luxembourg july fra fundamental rights mean people eu luxembourg publications office p see webpage three level baseline security system iske ther website estonia' information system authority see website pci security standards council barak 'human dignity framework right motherright ' barak human dignity constitutional value constitutional right cambridge cambridge university press ch pp cjeu c netherlands v european parliament council october paras discussion malicious use ai see example brundage et al malicious use artificial intelligence forecasting prevention mitigation fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office november cjeu joined cases c c volker und markus schecke eifert gbr hartmut eifert opinion advocate general sharpston june para fra council europe edps handbook european data protection law edition luxembourg publications office june p see also ibid pp ecthr guide article european convention human rights - right respect private family life home correspondence strasbourg council europe updated august paras ecthr lopez ribalda others v spain nos october para comprehensive legal analysis meaning content 'privacy' see also koops b j et al ' typology privacy' university pennsylvania journal international law vol issue pp vermeulen surveille deliverable d4 - scope right private life public places july p un human rights committee general comment right peaceful assembly article ccpr c gc september para costello roisin aine impacts adtech privacy rights rule law technology regulation norwegian consumer council control consumers exploited online advertising industry fra rights matter data protection privacy - fundamental rights survey luxembourg publications office rocher l hendrickx j de montjoye estimating success identifications incomplete datasets using generative models nature communications hacker p legal framework ai training data law innovation technology forthcoming available ssrn article data protection working party opinion anonymisation techniques see also finck michele pallas frank must identified distinguishing personal non personal data gdpr october forthcoming international data privacy law max planck institute innovation competition research paper available ssrn sartor g lagioia f impact general data protection regulation gdpr artificial intelligence study prepared panel future science technology stoa european parliament see example uk data service' blog \"access sensitive data research ' safes'\" see also discussion ohm p \"broken promises privacy responding surprising failure anonymization\" ucla law review p gdpr art law enforcement directive art veale edwards l 'clarity surprises questions article working party draft guidance automated decision making profiling' computer law security review vol april pp article data protection working party guidelines automated individual decision making profiling purposes regulation adopted october last revised adopted february green b chen 'disparate interactions algorithm loop analysis fairness risk assessments' fat ' conference fairness accountability transparency fat ' january gonzalez fuster g artificial intelligence law enforcement - impact fundamental rights european parliament policy department citizens' rights constitutional affairs directorate general internal policies pe july p brkan ' algorithms rule world algorithmic decision making data protection framework gdpr beyond' international journal law information technology vol p article working party guidelines automated individual decision making profiling purposes regulation adopted october last revised adopted february wp251rev p misuraca g van noordt c overview use impact ai public services eu european commission joint research centre luxembourg council directive ec june implementing principle equal treatment persons irrespective racial ethnic origin oj l pp art council directive ec november establishing general framework equal treatment employment occupation oj l pp art fra coe handbook european non discrimination law edition luxembourg publications office june p cjeu c association belge des consommateurs test achats asbl others v conseil des ministres january european commission eu rules gender neutral pricing insurance industry enter force press release ip december elizabeth e joh ' new surveillance discretion automated suspicion big data policing' uc davis legal studies research paper pp ales zavrsnik 'algorithmic justice algorithms big data criminal justice settings' european journal criminology p doi see also european commission white paper artificial intelligence - european approach excellence trust com final brussels february p fra bigdata discrimination data supported decision making luxembourg publications office june p ibid fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office korff browne ' use internet related services private life data protection trends technologies threats implications' council europe pd see national non discrimination equality tribunal finland decision march see also syri case discussed uk court appeal r bridges v cc south wales ewca civ august see also equinet regulating equal ai new role equality bodies brussels report prepared allen r masters tolan miron gomez e castillo c ' machine learning may lead unfairness evidence risk assessment juvenile justice catalonia' best paper award international conference ai law richardson r schultz j crawford k dirty data bad predictions civil rights violations impact police data predictive policing systems justice n u l rev online available ssrn fra violence women eu wide survey main results report luxembourg publications office p fra second european union minorities discrimination survey main results luxembourg publications office p erik bakke \"predictive policing argument public transparency\" new york university annual survey american law vol pp andrew g ferguson 'policing predictive policing' washington university law review vol pp andcouncil europe committee experts internet intermediaries msi net algorithms human rights council europe dgi p elizabeth e joh ' new surveillance discretion automated suspicion big data policing' uc davis legal studies research paper p gstrein j bunnik zwitter 'ethical legal social challenges predictive policing' catolica law review pp albert meijer martijn wessels 'predictive policing review benefits drawbacks' international journal public administration p doi ales zavrsnik 'algorithmic justice algorithms big data criminal justice settings' european journal criminology pp doi wachter sandra 'affinity profiling discrimination association online behavioural advertising' berkeley technology law journal vol forthcoming available ssrn use ai financial industries leading unequal access financial services see legal literature e g boyd levy k marwick ' networked nature algorithmic discrimination' gangadharan p eubanks v barocas eds data discrimination collected essays open technology institute pp overview child rights issues see unicef innovation human rights center uc berkeley artificial intelligence children' rights eu network independent experts fundamental rights commentary charter fundamental rights european union june p see also fra coe handbook european law relating access justice luxembourg publications office june p cjeu c unibet london ltd unibet international ltd v justitiekanslern march para cjeu c et agrokonsulting velko stoyanov v izpalnitelen direktor na darzhaven fond 'zemedelie' - razplashtatelna agentsia june para cjeu c centre public 'action sociale 'ottignies louvain la neuve v moussa abdida december para law enforcement directive art gdpr art law enforcement directive art gdpr art law enforcement directive art gdpr art council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems adopted committee ministers april 1373rd meeting ministers' deputies appendix para b andrew g ferguson 'policing predictive policing' washington university law review pp gstrein j bunnik zwitter ethical legal social challenges predictive policing' catolica law review pp yeung k study implications advanced digital technologies including ai systems concept responsibility within human rights framework prepared council europe expert committee human rights dimensions automated data processing different forms artificial intelligence msi aut council europe algorithms human rights pp international technology law association 'responsible ai global policy framework' pp lack expertise ai also reflected survey among companies eu lack skills among existing staff difficulties hiring new staff prominent obstacle ai adoption european commission european enterprise survey use technologies based artificial intelligence luxembourg july p see detailed assessment impact predictive policing presumption innocence mendola marco one step 'surveillance society' case predictive policing see e g egorov wujczyk eds right social security constitutions world broadening moral legal space social justice geneva ilo global study vol europe pp xv xvii include arts tfeu arts european social charter well points community charter fundamental social rights workers see explanations relating charter fundamental rights oj c pp explanations relating charter fundamental rights oj c pp explanation article -- scope interpretation rights principles lukasz bojarski dieter schindlauer katrin wladasch european charter fundamental rights living instrument - manual rome warsaw vienna pp de becker e ' possible role right social security eu economic monitoring process' german law journal vol pp paju j european union social security law oxford hart publishing sub section ibid pp peers prechal 'scope interpretation rights principles' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp exception poland united kingdom see protocol application charter fundamental rights european union poland united kingdom oj c pp art christiaan van veen ben zevenbergen 'conference social protection artificial intelligence decoding human rights digital age' freedom tinker - research expert commentary digital technologies public life may art charter see also explanations relating charter fundamental rights oj c pp explanation article -- scope interpretation rights principles lukasz bojarski dieter schindlauer katrin wladasch european charter fundamental rights living instrument - manual rome warsaw vienna p sartor giovanni new aspects challenges consumer protection study committee internal market consumer protection policy department economic scientific quality life policies european parliament luxembourg european consumer organisation beuc digital health principles recommendations beuc artificial intelligence consumers say findings policy recommendations multi country survey ai recent case law see cjeu c h n v minister justice equality law reform ireland attorney general may para also confirmed cjeu joined cases c c ys v minister voor immigratie integratie en asiel minister voor immigratie integratie en asiel v july paras components initially developed cjeu case law codified article charter right leading academic literature see craig p 'article - right good administration' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp ibid p finck 'automated decision making administrative law' max planck institute innovation competition research paper p craig p 'article - right good administration' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp france code des relations entre le public et l'administration article l2111 panoptykon foundation profiling unemployed poland social political implications algorithmic decision making see also algorithm watch poland scrap controversial unemployment scoring system see decision k available constitutional tribunal' website fundamental rights impact assessment - practical tool protecting fundamental rights chapter illustrated extent using ai affects different fundamental rights chapter analyses fundamental rights impact assessments fria could reduce negative impacts using ai fundamental rights section provides brief overview current discussion need fundamental rights impact assessments field section analyses current practices addressing fundamental rights implications based interviews conducted report interviewees asked sort testing done system used controls tasks affected use technology chapter ends suggestions assess fundamental rights impact using ai related technologies calling fundamental rights impact assessment - available guidance tools international organisations academics2 civil society3 called fundamental rights impact assessments conducted using ai related technologies example committee ministers council europe' guidelines addressing human rights impacts algorithmic systems recommend states conduct \"impact assessments prior public procurement development regular milestones throughout context specific deployment order identify risks rights adverse outcomes\" need flexible impact assessments adapt different situations given fundamental rights violations always contextual scholars exemplify based eu anti discrimination law equality always contextual depends case hand fundamental rights compliance cannot automated hard coded computer software rather use case needs separate examination determine whether fundamental rights issue arises nevertheless assessments follow systematic approach provide similar information existing standards provide guidance fundamental rights impact assessment ai related technology include hard law soft law instruments recommendations declarations practical tools e g guidelines checklists beyond requirements flowing data protection legislation see box examples laws requiring mandatory assessments effects ai general view increasing uptake ai canadian government issued guidelines including mandatory requirements assessing ai use public administration applies system tool statistical model used recommend make administrative decision client european data protection law requires data protection impact assessment dpia learning coe modernised convention provides general obligation examine likely data impact data processing individuals' rights fundamental freedoms use protection following assessment controllers design processing manner impact prevent minimise identified risks b assessments eu law imposes similar detailed obligation gdpr foresees data protection impact assessment dpia data processing likely \" result high risk rights freedoms natural persons \"c therefore required law dpia ai technology could potentially also address broader fundamental rights implications besides impact right privacy used tool investigate algorithms impacts e however gdpr article dpia limited 'high risk' cases processing personal data therefore may miss high risk cases primarily obviously related protection personal data time gdpr delimited specific field application accompanying expertise field means potential extension scope dpia fundamental rights might limited gdpr also gives indications modalities undertake dpia first dpia conducted high risk processing f second dpia provide systematic description envisaged operations purpose legitimate interests pursued must also assess necessity proportionality processing possible risks rights individuals addition must contain planned security measures address risks identified g pointing different methodologies apply article working party wp proposes - check list form - minimum criteria controller use assess dpia comprehensively complies gdpr h finally gdpr foresees prior mandatory consultation relevant supervisory authority impact assessment indicates processing presents risks cannot mitigated gives crucial role dpas independent bodies established law j european data protection supervisor edps provides guidance carrying dpias k data protection authorities also discussed provide guidance assess ai technologies l information data protection impact assessment see fra council europe edps handbook european data protection law edition p b council europe modernised convention art c gdpr art gdpr recitals article working party guidelines data protection impact assessment dpia wp248rev october e edwards veale fra bigdata discrimination data supported decision making luxembourg publications office june f gdpr art wp29 specifies 'carrying dpia continual process one time exercise ' g gdpr art well recitals h article working party guidelines data protection impact assessment dpia wp248rev october annex gdpr art j gdpr art k edps accountability ground part ii data protection impact assessments prior consultation v july l see example declaration ethics data protection ai adopted 40th international conference data protection privacy commissioners icdppc many examples non binding guidelines global level united nations guiding principles business human rights recommend enterprises integrate findings human rights impact assessments across relevant internal functions processes take appropriate action although refer specifically ai guidelines relevant supporting development ai technology rights compliant manner eu level ethics guidelines trustworthy ai prepared european commission' high level group artificial intelligence9 also recommend performing fria system' development \" risks fundamental rights negatively affected technology\" also emphasise need put place mechanisms receive external feedback ai systems potentially infringe fundamental rights addition private companies associations private companies12 public private interests well ngos14 organisations15 developed different types guidance support ai impact assessments documents usually contain clear guidelines impact assessment instead highlight different aspects criteria taken account developing carrying impact assessment broad categories include purpose system description technology assessment impact targeted population individual evaluating fairness diversity description audits planned performed well accountability explicitly refer applicable international human rights law standards various codes ethics conducts standards well certification schemes also place several practical tools available assess impact ai technologies mitigate risks developed wide range actors include checklists lists questions online self evaluation tools risk management frameworks focus specifically assessing fundamental rights risks others focus ethical societal economic implications useful references performing thorough fundamental rights impact assessment ai technologies july example high level group artificial intelligence issued \"assessment list trustworthy ai\" altai six month pilot involving stakeholders altai helps organisations self evaluate - voluntary basis - reliability trustworthiness ai reduce potential risks users supports businesses public administrations ask right questions around seven requirements responsible ai identified ethics guidelines trustworthy ai altai specifically refers need perform fundamental rights impact assessment includes examples questions assess impact non discrimination equality right privacy rights child freedom expression well freedom information association several online assessment tools target use ai public authorities canadian government developed algorithmic impact assessment tool aia pursuant canadian directive automated decision making aia represents automated assessment consisting questions unfold requirements directive questions relate fundamental rights concerns - ai system' impact freedom movement likelihood incarceration individual legal status access funding benefits indigenous people score attributed reply final impact scoring provided made publically available government' website another example ethics toolkit31 freely accessible tool designed local governments based risk management approach supports fair automated decisions minimising unintentional harm individuals field criminal justice higher education social media areas among national human rights bodies danish institute human rights proposed human rights compliance \"quick check involves interactive online computer programme allows companies select modify information database suit type business area operations check rights compliance quick check based human rights compliance assessment tool runs database questions corresponding human rights indicators uses international human rights law standards benchmarks applying fields operations provide guidance developing impact assessment ai technology academic work also suggested operational frameworks assessing risks using ai technology focus specifically identifying addressing fundamental rights implications private sector focus developing ethical values oriented models analysing societal impact data used creation ad hoc expert review committee others developed guidance frameworks specific case studies example field criminal justice algo care framework36 introduced step step assessment evaluate key legal practical concerns considered relation police using algorithmic risk assessment tools argued participatory ways involve consider views affected rights holders stakeholders communities developing impact assessment publically engage start others joined cross discipline expertise science law design practical frameworks impact assessments testing practice virtually systems discussed interviews subject sort testing included elements impact assessment however mainly technical data protection impact assessments rarely address potential impacts fundamental rights interviewees argue conducting fundamental rights impact assessment view system negatively affect fundamental rights unsure example respondent working traffic management using cameras monitoring traffic indicated tested accuracy system fundamental rights apart respecting data protection rules respondents simply know fundamental rights assessed part general impact assessment carried testing stages development much testing done new ai system used respondents \" testing system highlighted moving ai system production challenging task really look legal aspects mentioned public administration well private companies usually looked whether system careful using ai many projects interviewees refer still profitable \" development pilot phase started concrete testing private company estonia testing done several stages include development stage called proof concept pilot stages deployment tests deployment possible live experimentation carried initial stages often involves staged deployment example organisation interviewed tests different applications support job seekers conducts continuous step step testing selected members organisation test tool real situations using check lists interviewee mentioned challenging move deployment stage planned supervise tool real time another example involving automated rule based granting social benefits different assessments carried implementation group lawyers data protection specialists compensation specialists accountants performed general impact assessment department responsible using system conducted tests decide whether system could used following system monitored implementation using step step approach first step half decisions taken system next step decisions taken automatically expanded negative decisions another area decisions added including decisions ending compensation payments time interviews conducted decisions automated interviewee indicated carrying tests feel sure system secure outstanding risks company working fraud detection system replaced rule based system machine learning tool changing system old new system run parallel see machine learning system performs better rule based one interviewee mentioned \" rigorous analysis behind direct feedback saw would impact losses versus many good customers impacting negatively\" interviewee added \" comfortable machine learning system better static rule system aspects deployed entirety\" use cases previous automated system existed tests reviewed humans example automated transcription service tested court hearings allowed judge included regular feedback correctness transcription services judges one interviewee law enforcement working tool detect domestic violence identifies issues precision accuracy using system police officer sufficient training knowledge system indicators required system cannot gather required information could lead miscalculation highlight robustness system tested annually assure quality two questionnaires used completeness data training police officers using ai system process also considers personal data protection laws protocols applied tests discussed focus strongly technical aspects general operations fundamental rights data protection impact assessments apart data protection respondents mentioned fundamental rights typically considered respondents reflected potential impacts fundamental rights mentioned aspects considered prompted interviewer many respondents generally aware discrimination issues - often discussed explicitly asked discrimination yet gave information formal depth tests discrimination generally respondents ruled possibility system discriminates based protected attributes example one interviewee states test system data protection laws specific applicable legal acts fundamental rights however interviewee consider potential discrimination ruled needs kept mind future technologies interviewee stated however cases non discrimination generally considered testing phase ai systems one respondent municipal authority mentioned cannot assess fairness model cannot access data needed due data protection reasons according interviewee \" huge tension surrounding gdpr want well might fact worse interpretation data turns impossible\" \"yes assess legality personal data protection respondents reported data protection impact assessment conformity specific legal required law conducted although took different forms bank acts \" tested tool analysing speech customer calls find public administration estonia reoccurring problems carried data protection impact assessment dpia specifically testing tool outcome system tested data used testing phase deleted certain period test access data employees restricted testing phase supervised deployment tool another dpia required case sometimes lack clarity extent use ai related technologies notably use algorithms belongs dpia area predictive policing instance dpias done underlying architecture system rather specific ai tool another interviewee using algorithms financial services also mentioned assessing machine learning tool within framework dpia belief apply machine learning system underlying data one interviewee felt data protection impact assessment crime heat map example sufficiently depth safeguard quality model system equipped deal cross sectoral use data different rules might apply indicated standards required respondent working migration management indicated data protection officers involved analysis legal service specialised quality control ai tool study data protection aspects system however respondent also mentioned guidance needed companies working targeted advertising looked data protection issues although respondents sure impact assessment conducted companies assessed example whether people consented approached targeted communication targeted ads assessed whether information possible identification deleted including whether cookies trackers anonymised respect dpias generally respondents know area responsibility others knew positive dpia aware details appears legal assessment sometimes detached technical side technical people knowing legal assessments one interviewee private company working credit risk scoring mentioned \" make suggestions system could developed compliance manager tells conformity laws\" audits working external oversight bodies public administrations private companies involved fra' research carry tests deploying ai often linked existing internal external oversight processes use ai frequently subjected internal review processes within companies public administration although necessarily formalised review processes interviewees mentioned working formalising existing internal review processes overseeing ai systems interviewees public sector say particularly cautious using ai support decisions representative working migration management public administration indicates \" n private sector wrong results might cause business related losses police impacts people' lives fundamental rights\" yet always clear public administration businesses responsible checking overseeing use ai public administrations appear stronger scrutiny comes oversight ai systems oversight often done regular audits example connected budgetary review interviewees public private organisations report ai systems currently checked framework existing review e g regular database checks absence review processes specifically look use ai addition interviewees report sector specific certification schemes also look use ai - example area health financial services several interviewees mentioned contact data protection authorities companies public administrations sought permission data protection authorities using ai system least generally contact example one company working targeted advertising mentioned discussing use personal data national data protection authority experts interviewed report highlighted relevance data protection authorities overseeing ai systems respect use personal data however experts strongly highlighted data protection authorities resourced task two reasons data protection authorities often relevant ai related expertise additionally budgets overstretched workload heavy experts' views differ respect need additional oversight bodies potential creation ai specific institution however agree existing bodies work topics linked ai within mandates equality bodies well human rights institutions mentioned interviewed experts providing oversight concerning possible discrimination using ai highlighted institutions need build expertise area better contribute oversight ai however similar data protection authorities challenging task equality bodies given lack resources \" proactive among several interviewees mentioned consumer protection authorities potentially mitigate risks providing relevant oversight use ai one respondent working also get additional audits also retail company would like advisory agency could consulted see sometimes regulatory possible use ai innovation without investigated right away audits quite sloppy us moment company prefers consult consumer authorities good lots data protection authorities potential future marketing campaigns customer data \" data protection authorities might start investigation private company estonia efforts discussing oversight developing using ai well experts repeatedly mention challenge really understand impact using ai despite need engage existing oversight bodies responsibilities oversee use ai fundamental rights perspective remain unclear fundamental rights impact assessment practice many key actors field fundamental rights called conducting fundamental rights impact assessments using ai driven systems section highlights elements could incorporated assessment fundamental rights impact assessments needed given contextualised assessment required uses ai vary considerably terms complexity level automation potential errors harm scale application well area use complex ai system difficult assess potential impact fundamental rights implicated vary depending area application full spectrum rights needs considered use ai however uses ai likely involve rights often affected ai systems discussion preceding chapter makes clear issues linked data protection non discrimination well access effective remedies fair trial relevant uses ai thus following horizontal points could basic starting point considering impact ai selected rights ---- legal processing data needs confirmed line data protection laws personal data used full data protection framework applies ensures processing legal violate person' rights respect private family life data protection ---- processing lead unfair treatment discrimination protected groups assessing non discrimination needs core assessing ai even apparently miniscule differences scale create risks contravening principle non discrimination disadvantage people depends nature kind harm severity strength harm significance many people put disadvantage compared another group people statistical assessments group differences important tool assess unfair discriminatory uses ai ----people subjected ai related technologies able complain receive effective remedies accessible ways people complain potential decisions made effectively access remedies includes availability information allows explanation decisions addition relevant rights charter apply public administrations using ai need consider good administration principles businesses take consumer protection account rights relevant depending area application examples include ---- right social protection working social benefits ---- right freedom expression information using ai support online content moderation ---- right assembly association considering use facial recognition technology public spaces ---- right education using ai education sector ---- right asylum using ai support migration management ---- right collective bargaining action using ai 'gig economy' ---- right fair working conditions using ai workplace ---- right access preventive health care using ai health services ---- right presumption innocence right defence using ai justice sector law enforcement purposes information needed assess potential impact fundamental rights implementing ai given variety tools purposes area application assessments contextual able meaningfully respond horizontal points raised assess specific rights linked different use cases least following information needs available ---- description purpose context system well legal basis ---- description possible harm using system including questions around false positives false negatives possible harm due automation scale use ---- description technology used includes information data used building system legal basis processing description relevant information include provided fra' paper data quality ai ---- evidence based description accuracy ai system terms outcomes based training data possible tests experiments real life situations appropriate false positives false negatives considered separately include breakdowns many groups possible allow checking potential discrimination e g differences accuracy women men ---- already available provision information compliance existing standards potential certifications obtained ex post assessments safeguards lastly envisaging ex post safeguards contributes fundamental rights compliant use ai could include ----regular repetition assessments deployment appropriate important learn potential feedback loops case rules updated also requires recording information use outcomes system extent data protection respected ----making people subjected ai systems aware subjected technology otherwise challenge decision affecting ----making available easily accessible channels effectively complaining decisions made based ai system engaging external experts stakeholders oversight bodies information could basis consultation different stakeholders experts particular ai system used depending nature application legal basis consultation relevant stakeholders would ensure potential harm omitted different perspectives brought assessment stakeholders could include civil society different public private organisations well experts different fields fundamental rights including data protection ten experts interviewed report highlighted existing oversight bodies also responsible ai oversight within mandates sector specific bodies certification schemes extent interviews suggest - example health care financial oversight monitor comprehend effectively respond potential impacts ai wide spectrum fundamental rights data protection authorities equality bodies ombuds institutions national human rights institutions could play important role providing input oversight various points expertise however interviews indicated extensive upskilling resource allocation needed underpin endnotes council europe commissioner human rights unboxing artificial intelligence steps protect human rights - recommendation council europe strasbourg may heleen l janssen ' approach fundamental rights impact assessment automated decision making international data privacy law' international data privacy law vol issue february pp alessandro mantelero 'ai big data blueprint human rights social ethical impact assessment' computer law security review vol issue august pp edwards lilian veale michael slave algorithm 'right explanation' probably remedy looking may duke law technology review accessnow access ' submission consultation \"white paper artificial intelligence european approach excellence trust\" council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems april para human rights impact assessment detailed discussion respect non discrimination see wachter mittelstatt b russel c fairness cannot automated bridging gap eu non discrimination law ai government canada directive automated decision making united nations un guiding principles business human rights endorsed human rights council resolution hrc res july principles heleen l janssen approach fundamental rights impact assessment automated decision making international data privacy law vol issue february pp high level expert group artificial intelligence ethics guidelines trustworthy ai april chapter iii ibid p see example ibm everyday ethics artificial intelligence sony sony group ai ethics guidelines vodaphone vodaphone' ai framework arborus international orange international charter inclusive ai april signed private companies including camfil danone edf l'oreal metro sodexo etc information technology industry council iti iti ai policy principles ecp platform information society artificial intelligence impact assessment netherlands november amnesty international access human rights watch wikimedia foundation toronto declaration protecting rights equality non discrimination machine learning systems may rightscon toronto university montreal montreal declaration responsible ai electrical electronics engineers ieee global initiative ethics autonomous intelligent systems ethically aligned design prioritizing human wellbeing autonomous intelligent systems future life institute asilomar ai principles conference outcome future life institute' second conference future artificial intelligence see example ecp platform information society artificial intelligence impact assessment netherlands november ieee initiative association computer machinery acm acm code ethics professional conduct june future humanity institute university oxford standards ai governance international standards enable global coordination ai research development april iso standards iso iec jtc sc artificial intelligence sstandard project direct responsibility iso iec jtc sc secretariat iso iso iec tr standard information technology -- artificial intelligence -- overview trustworthiness artificial intelligence may establishes among others \"approaches assess achieve availability resiliency reliability accuracy safety security privacy ai systems \" iso standards development september iso iec cd information technology -- artificial intelligence -- risk management iso iec awi tr information technology -- artificial intelligence ai -- bias ai systems ai aided decision making iso iec awi tr information technology -- artificial intelligence -- overview ethical societal concerns information available iso' website electrical electronics engineers ieee ieee p7003(tm) algorithmic bias considerations german ai federal association ki bundesverband german ai federal association seal quality ki bundesverband guetesiegel march article working party guidelines data protection impact assessment dpia wp248rev october annex - criteria acceptable dpia high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july government canada algorithmic impact assessment tool danish institute human rights human rights compliance assessment quick check june center government excellence johns hopkins university ethics algorithm toolkit government canada algorithmic impact assessment tool article working party guidelines data protection impact assessment dpia wp248rev october annex data protection focus danish institute human rights human rights impact assessment guidance toolbox ai pulse creating tool reproducibly estimate ethical impact artificial intelligence september fairness accountability transparency machine learning fat ml principles accountable algorithms social impact statement algorithms fat ml social impact statement algorithms high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july see e g human agency oversight technical robustness safety privacy data governance transparency diversity non discrimination fairness societal environmental well accountability high level expert group artificial intelligence ethics guidelines trustworthy ai april high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july p government canada algorithmic impact assessment tool government canada directive automated decision making article appendix c center government excellence johns hopkins university ethics algorithm toolkit danish institute human rights human rights compliance assessment quick check june danish institute human rights human rights impact assessment guidance toolbox heleen l janssen approach fundamental rights impact assessment automated decision making international data privacy law international data privacy law vol issue february alessandro mantelero ai big data blueprint human rights social ethical impact assessment computer law security review vol issue august pp ai pulse program understanding law science evidence pulse ucla school law creating tool reproducibly estimate ethical impact artificial intelligence september model includes series questions assessing human rights impact ai enabled projects marion oswald jamie grace sheena urwin geoffrey c barnes algorithmic risk assessment policing models lessons durham hart model 'experimental' proportionality journal vol - issue ainow algorithmic impact assessments practical framework public agency accountability april institute ethical ai machine learning ethical ml network beta machine learning maturity model brave europe' governments failing gdpr see wachter mittelstatt b russel c fairness cannot automated bridging gap eu non discrimination law ai fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office june moving forward challenges opportunities report published amidst ongoing european legislative policy developments artificial intelligence global fight coronavirus covid pandemic potentially quickened acceptance innovative technologies yet also shown ai panacea problems comes various challenges report clearly shows using ai systems engages wide range fundamental rights also shows many businesses public administrations already using planning use ai related technologies however technologies involve different levels complexity examples use relatively simple algorithms level automation also varies - - decision making subject human review applications currently used also often development stage eu national legislators policymakers keep reality mind - especially presented optimistic expectations ai' potential vis vis challenges related using new technologies need regulate \" try look future vast majority public administrations businesses interviewed plan automate \" keep working using ai two interviewees indicated private company estonia use develop ai another two interviewees cautious plan wait see others including lack resources work using ai \" next steps related transparency open data however said develop continue test tools say publish information data infrastructure respect use ai includes starting pdf also information new continuing ongoing pilots evaluating existing efforts sharing data reusable formatting could results others increasing data quality trying obtain reused internally data sources private sector \" public administration spain interviewees mentioned engaged ongoing debates expressed desire contribute development legislation still see current situation - absence harmonised law \"ai great thing must area - obstacle use ai addition respondents learn use \" said working issues linked interpretability ai private company spain means working methods enhance understanding explanation decisions based complex ai indicated desire look closely ethical legal matters figure shows correlations words interviewees often use talking future use ai figure indicates topics often raised example interviewees often used term 'data' discussing future developments figure correlations words respondents often mention discussing future plans use ai technologies systems plans data development company ai tool police management solutions companies level system project technology administration tax analysis organisation service processing lot translation process related customer easy national phase payment information services application time quality improve learning business human energy algorithms decisions support machine ml continue potential people develop future notes based text interview summaries respondents spoke future use ai including words mentioned least ten times lines connecting words indicate strength word correlations within text passages size dots indicate frequency words used source fra effectively adequately protecting fundamental rights eu key objective current efforts better regulate use ai context upcoming eu legislation ai european commission' white paper addresses current gaps helping mitigate uncertainty around use ai respect fundamental rights making use ai transparent accountable terms fundamental rights includes requirements ai use directly link information needed assess impact ai fundamental rights discussed requirements linked description training data data record keeping information provided subjected ai robustness accuracy well human oversight highly relevant assessing protecting fundamental rights respect body evidence presented report offers general insights different technologies affect fundamental rights safeguards needed ensure fully fundamental rights compliant use ai practice time research fundamental rights implications use ai specific areas support policy legislative efforts eu level aiming shape europe' digital future widely fra continue look fundamental implications ai carrying focussed analysis specific use cases increase knowledge potentially go wrong consequently help mitigate prevent fundamental rights violations fra look potential simulation studies showcase biased algorithms negatively affect fundamental rights use ai often involves automating tasks previously carried humans need acknowledge human behaviour sometimes line fundamental rights using ai using ai example police might engage unlawful profiling decisions public administration companies might sometimes driven negative stereotypes current developments use ai need acknowledge potential discrimination respect data ai system built respect underlying assumptions humans turn may feed development deployment system automating certain tasks without fully understanding automated could lead unlawful processing data use technology treats people unfairly might make impossible challenge certain outcomes - name challenges however increased availability data technological tools also used better understand unequal treatment occurs current technological developments increased availability data also provide unique opportunity better understand structures society used support fundamental rights compliance opportunities created ai also contribute better understanding consequently mitigation fundamental rights violations getting touch eu person european union hundreds europe direct information centres find address centre nearest https europa eu european union contact en phone email europe direct service answers questions european union contact service --b freephone certain operators may charge calls -- following standard number -- email via https europa eu european union contact en finding information eu online information european union official languages eu available europa website https europa eu european union index en eu publications download order free priced eu publications https op europa eu en publications multiple copies free publications may obtained contacting europe direct local information centre see https europa eu european union contact en eu law related documents access legal information eu including eu law since official language versions go eur lex http eur lex europa eu open data eu eu open data portal http data europa eu euodp en provides access datasets eu data downloaded reused free commercial non commercial purposes promoting protecting fundamental rights across eu -- artificial intelligence ai already plays role deciding unemployment benefits someone gets burglary likely take place whether someone risk cancer sees catchy advertisement low mortgage rates use keeps growing presenting seemingly endless possibilities need make sure fully uphold fundamental rights standards using ai report presents concrete examples companies public administrations eu using trying use ai focuses four core areas - social benefits predictive policing health services targeted advertising report discusses potential implications fundamental rights analyses rights taken account using developing ai applications aims help ensure future eu regulatory framework ai firmly grounded respect human fundamental rights eu charter fundamental rights access justice non discrimination information society fra - european union agency fundamental rights schwarzenbergplatz - vienna - austria tel - fax fra europa eu facebook com fundamentalrights twitter com eurightsagency linkedin com company eu fundamental rights agency","count":1},{"name":"3 others","count":3}]}},{"name":"text_word_count","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":2502,"max":49776,"histogram":[{"bin_start":2502,"bin_end":7229.4,"count":2},{"bin_start":7229.4,"bin_end":11956.8,"count":0},{"bin_start":11956.8,"bin_end":16684.199999999997,"count":1},{"bin_start":16684.199999999997,"bin_end":21411.6,"count":1},{"bin_start":21411.6,"bin_end":26139,"count":0},{"bin_start":26139,"bin_end":30866.399999999998,"count":0},{"bin_start":30866.399999999998,"bin_end":35593.799999999996,"count":0},{"bin_start":35593.799999999996,"bin_end":40321.2,"count":0},{"bin_start":40321.2,"bin_end":45048.6,"count":0},{"bin_start":45048.6,"bin_end":49776,"count":1}]}},{"name":"text_unique_words","dtype":"int64","stats":{"unique_count":5,"nan_count":0,"min":1006,"max":8464,"histogram":[{"bin_start":1006,"bin_end":1751.8,"count":2},{"bin_start":1751.8,"bin_end":2497.6,"count":0},{"bin_start":2497.6,"bin_end":3243.3999999999996,"count":0},{"bin_start":3243.3999999999996,"bin_end":3989.2,"count":1},{"bin_start":3989.2,"bin_end":4735,"count":0},{"bin_start":4735,"bin_end":5480.799999999999,"count":1},{"bin_start":5480.799999999999,"bin_end":6226.599999999999,"count":0},{"bin_start":6226.599999999999,"bin_end":6972.4,"count":0},{"bin_start":6972.4,"bin_end":7718.2,"count":0},{"bin_start":7718.2,"bin_end":8464,"count":1}]}},{"name":"_deepnote_index_column","dtype":"int64"}],"rows_top":[{"Name":"./sampledocs/Module-1-Lecture-Slides.pdf","Type":"pdf","Text":"Application of AI, Insurtech and Real Estate\nTechnology\nIntroduction to Insurtech\nProfessor Christopher Geczy, PhD\n  Introduction to Insurtech\n  • The insurance industry is global and diversified across applications and\n    subsegments\n  • Gross Written Premium (GWP) was $4.8 trillion in 2017*\n      • The amount of insurance written, not including commissions and costs\n  • Net income worldwide grew 23% last year, vs. actual growth of 14% in\n    2017*\n                                                       * Ernst & Young, “Global Insurance Trends Analysis 2018,” June 2018.\n5\n   Introduction to Insurtech\n   • The insurance industry needs to respond to technological change & disruption\n       • Wearables\n       • Driverless vehicles\n       • Internet of Things\n       • “Big Data”\n       • Natural language processing\n       • Blockchain\n       • Distributed ledger technologies\n       • Climate change\n       • Etc.\n   • New technologies offer opportunities to increase efficiency in the industry and\n     serve new markets\n                                                       * Ernst & Young, “Global Insurance Trends Analysis 2018,” June 2018.\n16\n   Introduction to Insurtech\n   • There is no standardized definition of insurtech\n   • It is said to be revolutionizing the insurance industry and changing the way\n     insurers do business\n                                             * Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.\n18\n“ InsurTech can be described as “an insurance company,\n  intermediary, or insurance value chain segment specialist that\n  utilizes technology to either compete or provide valued-added\n  benefits to the insurance industry.”\n                                               - SIA PARTNERS, 2016\n   Key Emerging Technologies Leveraged by Insurtech Companies\n                                                 Source: Capgemini World InsurTech Report 2018\n20\nApplication of AI, Insurtech and Real Estate\nTechnology\nEmerging Technologies: AI & Machine Learning\nProfessor Christopher Geczy, PhD\n   Key Emerging Technologies Leveraged by Insurtech Companies\n   • Artificial Intelligence\n      • A descriptor for software which can perform functions ordinarily\n         associated with human reasoning\n          • Iterative learning\n          • Self-awareness & emotions\n      • Insurers hope to exploit AI for chatbots\n      • Allstate’s Allstate Business Insurance\n         Expert (ABIE)\n          • Provides answers in real time to\n             customer owners' questions\n          • Other insurers have followed\n                                                  Source: “What Is Insurtech and How Are Insurers Using It?”\n                                                  https://www.thebalancesmb.com/what-is-insurtech-4584490.\n                                                  Graphic: Accenture presentation: “Accenture’s 2017 Technology Vision for Insurance.”\n37\n   Key Emerging Technologies Leveraged by Insurtech Companies\n   Machine Learning\n   • Enables computers to \"learn\" over time\n   • Using algorithms & mathematical models to simulate neural networks in the\n     human brain\n   • Allows computers to extract patterns from raw data rather than following\n     specific instructions\n   • Gives the appearance of being closer to the activities of the human brain\n   • Some insurance companies amass large amounts of data\n      • Yet, according to the National Association of Insurance Commissioners,\n         most insurers use only 10 to 15% of the data they collect\n                                                               Source: “What Is Insurtech and How Are Insurers Using It?”\n                                                               https://www.thebalancesmb.com/what-is-insurtech-4584490.\n44\n   Key Emerging Technologies Leveraged by Insurtech Companies\n   • Machine learning could allow insurers to mine their data more effectively\n     and extract valuable information\n      • Risk modeling: Analyze claims data to predict the risk of future losses\n      • Demand modeling: Predict demand for their products in the future and\n        to estimate premiums\n      • Detecting fraud: Identify patterns of behavior that aren't obvious to\n        human adjusters\n      • Processing claims: Automate claim reporting and processing\n      • Underwriting: Help underwriters analyze data collected from applicants.\n         • Computers can aid in the decision making process, flag risks or\n            inconsistencies in data that underwriters might not be able to see\n         • Can also check external sources such as social media to verify the\n            accuracy of the data                              Source: “What Is Insurtech and How Are Insurers Using It?”\n                                                              https://www.thebalancesmb.com/what-is-insurtech-4584490.\n52\nApplication of AI, Insurtech and Real Estate\nTechnology\nRedefining the Insurance Industry\nProfessor Christopher Geczy, PhD\n   Redefining the Insurance Industry\n   1. Product Design\n   2. Selling & Marketing / Front Office\n   3. Underwriting\n   4. Policy Administration\n   5. Claims Management\n58\n   Insurtech Are Redefining the Insurance Industry\n                                                                                 Policy                       Claims\n      Product Design           Front Office           Underwriting\n                                                                            Administration                Management\n                                Marketing,\n     Actuarial Models and                             Underwriting         Policy Acquisition            Claims Servicing\n                              Distribution and\n       Product Design                                 New Policies           and Servicing                  and Payout\n                          Channel Management\n    • 360° view of        • Extended multi-      • Real-time             • Segment the market         • Streamlined claims\n      customer’ needs       device & mobility      information             based on servicing           process with low\n    • More personalized     offering               capturing               desires                      waiting time\n      product designs     • Integrated           • Advanced risk         • Automated systems          • Instant notification of\n    • Design new            omnichannel            analytics enabling      with Straight                the claim &\n      products              offerings & touch      risk-based pricing      Through Processing           proactive status\n    • Adjust products in    points               • Automated workflow      (STP) capabilities           updates\n      real time           • Real-time updates &    management & rules    • Automated ,                • Real-time claims\n    • Disaggregate          interactions with      engines                 premium reminders            status monitoring\n      product mix           clients              • Customer value-led      and renewal notice         • Advanced analytics-\n      seamlessly          • Quick identification   promotions &          • Anytime access to            based fraud\n    • Design and deliver    of cross-selling &     discounts               policy details/view          detection\n      products to the end   up-selling           • Digitized systems\n      customer they want    opportunities          with less reliance on\n                          • Detecting client       data that a customer\n                            satisfaction           needs to provide\n                          • Information\n                            availability & price\n                            transparency\n                                                                                            Source: Capgemini World InsurTech Report 2018\n63\nApplication of AI, Insurtech and Real Estate\nTechnology\nClassification of Insurtech Companies\nProfessor Christopher Geczy, PhD\n   Segmentation of Insurtech Firms\n   • There are different classification methods for InsurTech firms and\n     initiatives, but they rotate to similar concepts:\n      • A “traditional” view: Full-stack / Agents / Brokers\n      • A nuanced view of sub-segments: Carriers / Enablers / Distributors\n67\n   Segmentation of Insurtech Firms\n   • Milken Institute has three main classifications:\n     • Full-stack Insurers: Platforms that underwrite policies, assume the risk,\n        and, in most cases, manage the process from beginning to end\n     • Agents: Platforms that act on behalf of a carrier, essentially acting as an\n        extension of an incumbent carrier\n     • Brokers: Platforms that provide customers with a variety of policies\n        offered by both incumbent carriers and insurgent InsurTech platforms\n         • May or may not be paid commission based on the policies sold\n            through their platform\n         • May require customers to scroll through policies offered or\n            automatically connect customers to a preferred policy through\n            algorithms employed & based on a user’s response to a set of\n            questions\n                                         Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.\n68\n   Segmentation of Insurtech Firms\n   • To make the classification:\n                                   Yes: It is an insurer\n         Is it a full insurance                                                   One: It is an agent\n               company?\n                                 No: Is it partnered with\n                                     one or multiple\n                                 insurance companies?\n                                                                               Multiple: It is a broker\n                                             Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.\n69\n   Classification of Insurtech Firms: Capgemnini\n   • Second classification system, used by Capgemini\n      • It categorizes InsurTech providers by their role in the distribution chain:\n         • Enablers\n         • Distributors\n         • Full Carriers\n                                                               Source: Capgemini World InsurTech Report 2018\n70\n   Classification of Insurtech Firms: Capgemnini\n                                                 Source: Capgemini World InsurTech Report 2018\n71\n     Segmentation of Insurtech Firms by Model\n                                                   InsurTech Platform Models\n                                     35\n                                     30\n                                                                                                 31\n              Number of InsurTechs\n                                     25\n                                     20\n                                     15\n                                                                 18\n                                     10       12\n                                     5\n                                     0\n                                          FUL L STACK          AGENT                          BROKER\n                                                    InsurTech Platform Models\n                                                              Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.\n72\n73\n                                                                                                          A                                      No. of product offerings\n                                                                           H                                  ut\n                                                                                                                    o\n                                                                               ea\n                                                                                                                                                                 4                   10                      18\n                                                                                     lth\n                                                                                                                         (I\n                                                                                                                              nc                             0   2           6        8          12   14     16\n                                                                                                (I                                   l.\n                                   Li                                                                nd                                   bi\n                                                     fe                                                                                        ke                            4\n                                                                                                          iv\n                                                                       (u                                      /f                                    )               2\n                                                                            ni                                      am\n                                                                                 ve                                         ily                                                                        13\n                                                                                      rs\n                                                                                            al                                       he\n                                     H                                                           ,                                         al\n                                                     om                                               te                                        th                           4\n                                                                                                           rm                                        ;…\n                                                                       e/\n                                                                                                                    ,                                                2\n                                                                            R                                           U\n                                                                                 en                                         ni                                                                                     16\n                                                                                      t                                          t\n                                                                                           (I                                        Li\n                                                                                                nc                                        nk\n                                                                                                      l.                                        ed               1\n                                                                                                                                                                                     6\n                                                                                                           tit                                           …\n                                                                           D                                     le\n                                                                               is                                                                                                            9\n                                                                                    ab                                   in\n                                                                                          ili                                 su\n                                                                                                ty                                   ra\n                                                          B                                          /P                                    nc\n                                                                      us                                  er                                    e)                           4\n                                                                           in                                  so                                                    2\n                                                                                es                                      na                                                               8\n                                                                                     s                                        lL\n                              E                                                            (I                                    ia\n                                             nd                                                 nc                                        bi\n                                                                      ow                              l.                                       lit                           4\n                                                                                                           sh                                        y…\n                                                                           m                                        ip                                                           5\n                                                                                en                                       pi\n                                                                                      t,                                      ng\n                                                                                            R                                         re\n                                                                                                 et                                        tu\n                                                                      Tr                              ir\n                                                                                                           em                                   rn                       3\n                                                                           av                                       en                                   …                   4\n                                                                                el\n     Product offering categories\n                                                                                     (i                                  t,\n                                                                                          nc                                     P\n                                                                                                 l.                                  en\n                                                         S                                            in                                   si\n                                                                                                                                                on\n                                                                      pe                                   -t\n                                                                                                                ri                                                               5\n                                                                           ci\n                                                                                al                                   p\n                                                                                     ty                                  in\n                                                                                                                              su\n                                                                                           (I                                         ra\n                                                                                                nc                                         nc\n                                                                                                      l.                                        e;                   2\n                                                                                                                                                                     2\n                                                                                                           bi                                            …\n                                                                                                                 cy\n                                                                                                                         cl                                                              8\n                                                                                                                                                                                                                                Type of Insurtech Platform, by Product Offering\n                                                                                                                              e,\n                                                                                                                                     m\n                                                                                                                                          ob\n                                                                                                                                                ile                  2\n                                                                                                                                                         …                           6\n                                                                                                                                                                                                                                                                                  Segmentation of Insurtech Firms by Model and Product\n                                                                                                                                                                                     6\n              Milken Institute, “InsurTech Rising: A Profile of the\n                                                                                                                                                                                                  Broker   Agent\n                                                                                                                                                                                                                   Full Stack\n              InsurTech Landscape,” December 2018.\n   Segmentation of Insurtech Firms\n   • A significant portion (approximately 40%) of Insurtech companies could\n     better be described as technology solution providers\n      • Human Resources and Earned Benefits Solution Providers\n      • Data Solution Providers\n      • Infrastructure Solution Providers\n                                                             Milken Institute, “InsurTech Rising: A Profile of the\n                                                             InsurTech Landscape,” December 2018.\n81\n   Segmentation of Insurtech Firms\n   • Human Resources and Earned Benefits Solution Providers\n      • Platforms using or deploying technology to help firms manage their\n         human capital more efficiently and cost effectively\n   • Data Solution Providers\n      • Platforms that specialize in collecting, aggregating, and/or analyzing\n         vast quantities of data to support (re)insurance carriers, startups, and\n         other stakeholders\n   • Infrastructure Solution Providers\n      • Platforms that focus on making back-end processes more efficient\n         through the use of application programming interfaces (APIs) or that\n         provide the means by which platforms can integrate and/or build\n         customizable insurance products and services          Milken Institute, “InsurTech Rising: A Profile of the\n                                                               InsurTech Landscape,” December 2018.\n87\n     Number of Technology Solution Providers in Insurtech Market\n                                                          Insurtech Platform Models\n                                15.5\n                                 15\n                                                 15                                                   15\n                                14.5\n          Number of Providers\n                                 14\n                                13.5\n                                 13\n                                                                            13\n                                12.5\n                                 12\n                                        Human Resources and       Data Solution Provider    Infrastructure Solution\n                                       Earned Benefits Solution                                     Provider\n                                        Provider - Small, Mild,\n                                          Large Businesses\n                                                                                                     Milken Institute, “InsurTech Rising: A Profile of the\n                                                          Type of Tech-Solution Providers            InsurTech Landscape,” December 2018.\n88\n     Examples of Full-Carrier Insurtech Firms\n                     Insurtech Firm\n                                         Products/Services Offered                                   Example\n                          Type\n                                                                                 ZhongAn is a Chinese property insurer that uses\n                                         Traditional insurance model\n                     Digital Carrier                                             the online channel to sell its products and handle\n                                         conducted online or on mobile\n                                                                                 claims\n                                         A risk-sharing network where a\n                                         group of associated individuals pools   insPeer, a France-based community insurance\n                     P2P Insurer         their premiums to insure against risk   platform, allows members to pool in money within\n     Full Carriers\n                                         and generally stands to benefit         a group for covering a group member’s deductible\n                                         regarding premium returns\n                                         Smaller insurance packages with         Leveraging mobile technology, BIMA offers\n                     Micro Insurer       lower premiums and typically lower      affordable insurance products to low-income\n                                         coverage                                populations in emerging markets\n                                         On-demand insurance coverage that       New York-based Sure offers on-demand personal\n                     On-Demand Insurer   can be purchased online as well as      or episodic policies that a user can buy either via\n                                         via mobile apps                         website or an app\n                                                                                 US-based Metromile offers auto insurance with\n                     Usage-Based         Premiums prices per usage or risky\n                                                                                 fees based on the number of miles the insured’s\n                     Insurer             behavior displayed by the customer\n                                                                                 car logs\n                                                                                                    Source: Capgemini World InsurTech Report 2018.\n99\n      Examples of Full-Distributor Insurtech Firms\n                                                                                  PolicyBazaar specializes in comparative analysis of\n                                          Online site enables individuals to\n                     Marketplace                                                  products from various insurers based on price,\n                                          compare plans from different insurers\n                                                                                  quality and key benefits\n                                                                                  Artificially intelligent insurance advisory application\n                                          One-stop app(s) allows customers to\n                                                                                  Brolly delivers contextually relevant insights\n                     Personal Financial   manage all their policies, obtain\n                                                                                  through web and mobile applications, so customers\n                     Assistant            coverage recommendations and\n                                                                                  can manage policies in one place and know where\n                                          compare and purchase plans\n      Distributors\n                                                                                  coverage may be duplicated or missing.\n                                          Online platform allows customers to     Licensed broker Coverfox offers insurance products\n                     Digital Broker\n                                          compare and purchase policies           for vehicles, home, health services and travel.\n                                          Online site enables commercial          CoverHound, a US-based InsurTech firm that offers\n                     B2B Digital\n                                          customers to compare plans from         a comparison platform for personal and small\n                     Distributor\n                                          different insurers                      commercial insurance products.\n                                                                                  London-based Bought By Many uses social media\n                                          Customized or flexible front-office     data to connect people with similar insurance needs\n                     Value-Adding\n                                          solutions via partnership with an       and then uses the group’s collective buying power to\n                     Intermediary\n                                          insurer/reinsurer for risk management   negotiate with insurers for deals that aren’t available\n                                                                                  for individuals.\n                                                                                                       Source: Capgemini World InsurTech Report 2018.\n109\n      Examples of Full-Enabled Insurtech Firms\n                                                                                   PremFina’s white label solution for brokers allows\n                 Front-Office Solution   Process-improvement solutions for the\n                                                                                   them to extend premium financing options to their\n                 Providers               front office\n                                                                                   customers and manage insurance policies.\n                                                                                   RiskGenius applies artificial intelligence to streamline\n                 Policy/Plan             Process-improvement solutions for         the work of insurance professionals by retrieving\n                 Management              underwriting and policy/plan              details on a specific coverage or exclusion, analyzing\n                 Solution Provider       administration                            policies and extracting relevant information such as a\n                                                                                   premium limit or deductible.\n      Enablers\n                                                                                   RightIndem has a white-label self-service insurance\n                 Claims Management       Process-improvement solutions\n                                                                                   claims platform for insurers that allows customers to\n                 Solution Provider       specifically for claims management\n                                                                                   interact with their claim in their own time.\n                                                                                   Carpe Data leverages fata from various channels\n                                                                                   such as online content, social media, connected\n                                         Data capture or analytics solutions for\n                 Data Specialist                                                   devices and offers predictive scoring and data\n                                         use cases or across the value chain\n                                                                                   products for the insurers, enabling them to predict risk\n                                                                                   better.\n                                         Solutions based on a specific\n                 Technology                                                        Betterview,     a drone-technology specialist, allows\n                                         technology such as blockchain or                Source: Capgemini World InsurTech Report 2018.\n                 Specialist                                                        drone-based inspection or property assessment.\n                                         drones\n                                                                                                         Source: Capgemini World InsurTech Report 2018.\n110\nApplication of AI, Insurtech and Real Estate\nTechnology\nInvestment & Market Size of the Insurtech Industry\nProfessor Christopher Geczy, PhD\n    Insurtech: Market Size\n    • What’s clear is that it’s large and growing\n    • Global Insurtech market revenue $532.7MM as of 2018*\n    • Market revenue expected to reach $1.2 billion by 2023 (+16% CAGR, 2018-\n      2023) *\n    • AsiaPacific will have highest regional CAGR, growing in financial hubs in\n      Hong Kong/Singapore/India\n       • Health insurance segment is expected to have the higher segment CAGR*\n    • Total Insurtech investments, 2017: $3.2 billion **\n    • Total deals, 2017: 202 deals for $2.2 billion\n    • 83% involved an insurer/reinsurer as investor **\n    • Estimated 5-year CAGR, 2012-2017: +45% **          *Orbis Research, “Global (Insurance Technology) InsurTech\n                                                         Market Size 2019,” Dec. 2018. **Ernst & Young, “Global\n                                                         Insurance Trends Analysis 2018,” June 2018.\n120\n    Size of Insurtech Market\n                                                                    258\n    Global private investment                         $12.3                               242\n    (VC, PE and M&A)\n                                                                 $10.3\n    in insurtech\n    2013-2018                                164       165\n                           114\n                   89                                                                   $5.7\n                                           $4.0\n                          $2.4\n                 $1.6\n                  2013     2014             2015       2016        2017                  2018\n                                Capital invested ($B)       Deal count\n                                                               Source: KPMG International: Pulse of Fintech 2018,\n                                                               Global Analysis of Investment in Fintech, January 4, 2019\n121\n    Insurtech: Startup Count\n            Number of Insurtech Startups Exploded Over Just 2 Years\n                  (Source: SMA – Strategy Meets Action, 2017)\n122\n    Insurtech: Disruptors\n                          The Top Insurtech Disruptors\n123\n    Insurtech: Key Techs\n    • More and more insurers are moving key business functions to the cloud*\n    • Tech research firm Ovum’s annual survey of penetration of Software as a\n      Service (SaaS) grew from 13% in 2016 to 26% last year\n                                                * Source: Ovum survey data cited in Deloitte, “2019 Insurance Industry Outlook”\n125\n    Insurtech: Key Techs\n                         * Source: Ovum survey data cited in Deloitte, “2019 Insurance Industry Outlook”\n126\n    Insurtech: Key Techs\n    • AIA Hong Kong has launched a blockchain app to share life policy data\n      with its bank distributors\n    • AXA Europe is offering flight delay insurance on a blockchain platform\n      featuring smart contracts\n    • Ovum’s annual survey of penetration of Software as a Service (SaaS) grew\n      from 13% in 2016 to 26% last year.\n    • Carriers and consortiums are expected to launch more impactful\n      blockchain initiatives due to concerns around data technology\n130\n    Profile of Insurtech Market\n    • In the U.S., 63 Insurtech deals, with a total value of $1.59 billion, were\n      announced in Q4 2018\n       • Compared with Q4 of 2017, deal count in Q4 increased by 24%, while\n          funding volume also increased by 155%\n       • 63 transactions in Q4 2018 – higher than Q3, but lower than Q1 & Q2\n    • Globally\n       • UK investment down 9% from last quarter\n       • China the second largest investor for Q4 after the U.S.\n       • UK has been responsible for 8% of total investment since 2012\n       • Investment from international markets remains strong; transactions\n          outside of the U.S. account for 43% of total transactions since 2012 and\n          57% in the 4th quarter of 2018\n                                                               *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n129\n    Profile of Insurtech Market\n    • While early-stage investments remain strong\n       • Seed and Series A account for 64% of total transactions since 2012 and\n          62% this quarter (up 4% from last quarter)\n    • Insurtech funding is maturing to mid – and later-stages – 45% of financings\n      in 2018 took place at the Series A, B, or C stages\n       • Could lead to consolidation further up\n                                                             *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n133\n    Profile of Insurtech Market\n    • Property and Casualty (P&C) funding volume increased by 57% from Q3\n      2018 and increased by 89% from Q4 2017\n       • 41 P&C transactions in the quarter was only marginally higher than the\n          40 transactions in Q3 but marks a 52%\n    • Life and Health (L&H) funding volume increased by 1% from Q3 2018 but\n      marked a 362% increase from Q4 2017\n       • 2018 hits record level of Insurtech investment, driven by large\n          investments; deal count increased by 10% from Q3 2018 and funding\n          volume increased by 26%\n                                                             *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n137\n    Size of Insurtech Market\n             Quarterly Insurtech funding volume – all stages\n             ($ in millions)\n         $2,000\n                                                                                                                    Property &\n         $1,800\n                                                                                                                    Casualty\n         $1,600\n         $1,400\n                                                                                                                     Life & Health\n         $1,200\n         $1,000\n           $800\n           $600\n           $400\n           $200\n              $0\n                   Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4\n                     2012         2013         2014        2015        2016          2017          2018\n           Deal Count\n            P&C   5  3   4   4  5  4   12 9 10    7 16  8 16  14 16 20  44  18 33 29   23   33 27 42  43  44   40   41\n            L&H   8  6   7   9 15  8    9 4  9   15 14 15 10  19 14 20  15  16  6 13   16   32 21  9  23  27   17   22\n                                                                                          *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n138\n      Size of Insurtech Market\n            Quarterly Insurtech transaction by target country\n                                          18%                        United States                                                 United States\n                                                                     United Kingdom                   24%                          China\n                   2012 – Q4 2018\n                                     2%                              China                                                         United Kingdom\n                                                                                      Q4 2018\n                                    4%                               Germany                                              43%      Germany\n                                    5%                               India                       5%                                South Africa\n                                                           57%\n                                    6%                               France                                                        Other\n                                                                                                  6%\n                                                                     Other\n                                          8%                                                           9%\n                                                                                                                13%\n                                                 2013-Q4 2018 Transactions: 972                               Q4 2018 Transactions: 63\n            Quarterly InsurTech transaction by investments stage\n                                           13%                       Seed/Angel                         16%\n                                                                                                                                     Seed/Angel\n                  2012 – Q4 2018\n                                      3%                             Series A\n                                     2%                                                                                 32%          Series A\n                                                                                                 0%3%\n                                                                                       Q4 2018\n                                    5%                    41%        Series B                                                        Series B\n                                                                                                  5%\n                                                                     Series C                                                        Series C\n                                    13%                              Series D                                                        Series D\n                                                                     Series E+                    14%                                Series E+\n                                                                     Other                                                           Other\n                                                23%\n                                                                                                                 30%\n                                           2013-Q4 2018 Transactions: 972                                     Q4 2018 Transactions: 63\n                                                                                                                       *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n139\n    Size of Private Technology Investment by Insurers/Reinsurers\n     Private technology investments by (re)insurers\n       140\n                                                                    118                118\n       120                                                105\n       100                                                            33                 31\n                                                            26\n        80                                       66                                      26\n                                                                      28\n                                                            28\n        60\n                                                  22\n        40                           29                     27\n                                                                      31                 34\n                                                  19\n        20      1          4                      14\n                                                            24        26                 27\n                                                  11\n         0\n               2012       2013       2014        2015      2016      2017               2018\n                                        Q1    Q2      Q3 Q4\n                                                                CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n144\nApplication of AI, Insurtech and Real Estate\nTechnology\nInsurtech, Fintech, & Financial Inclusion\nProfessor Christopher Geczy, PhD\n    Microinsurance\n    • Investment firm: Omidyar Network\n       • Started by eBay founder Pierre Omidyar in 2004\n       • Model: Combines for-profit LLC with grantmaking\n         501(c)(3)\n       • $1.5B committed since inception:                          Pierre & Pam Omidyar\n          • $676MM for-profit investments, $782MM grants\n                                                         Total Commitments by\n                                                          Year, 2004-YTD 2019\n                                                                          https://www.omidyar.com/financials\n150\n    Microinsurance\n    • Common feature of microinsurance products is the low-income, low-net-\n      worth population served which otherwise has very limited access to\n      insurance\n    • Microinsurance products by type (can be group or individual coverage)\n       • Term life\n       • Health / accident / disability\n       • Casualty (crop insurance, livestock, theft, fire, natural disaster)\n       • Certain forms of retirement savings plans\n       • Microinsurance by underwriter / delivery channel type\n          • Large multinational insurance companies\n          • Credit unions or mutual associations\n          • Government or NGOs\n          • Small community organizations\n161\n    Microinsurance\n    • Benefits of microinsurance\n       • Financial protection through risk pooling\n       • Insureds can assume more risk\n          • Crop insurance vs. drought enables small farmers to plant crops which\n             have higher yields in “good” years & poorer yields in drought years\n          • Safeguard vs. families falling back into poverty due to illness, death of\n             breadwinner, housing destroyed, etc.\n          • Indian ministry of health found one-quarter of all hospitalizations\n             pushed individual or family into poverty due to cost of treatment*\n                                              * Tina Rosenberg, “The Microinsurance Revolution,” New York Times Opinionator blog, June 6, 2012 at\n                                                http://opinionator.blogs.nytimes.com/2012/06/06/the-microinsurance-revolution/\n167\n    Microinsurance\n    • Benefits of microinsurance\n       • Can target specific at-risk populations\n          • HIV-positive, living in flood zone, microentrepreneurs\n       • Can complement social welfare programs, bolster other microfinance\n         initiatives\n          • Can assign term life policy to secure business, education or mortgage\n              loan\n                                              * Tina Rosenberg, “The Microinsurance Revolution,” New York Times Opinionator blog, June 6, 2012 at\n                                                http://opinionator.blogs.nytimes.com/2012/06/06/the-microinsurance-revolution/\n172\n    Microinsurance\n    • BIMA – insurance business disruptor,\n      “Insurtech” leader\n    • Swedish-founded mobile insurance & health\n      company that provides accident, life,\n      & health insurance products to 26 million low-income consumers in 15\n      countries (Africa, Asia, LatAm)\n       • Largest markets: Ghana, Sri Lanka, Bangladesh, Pakistan.\n    • Mobile technology lowered prices and brought affordable insurance to\n      the world’s poorest\n                                  TechCrunch https://techcrunch.com/2017/12/19/bima-raises-97m-from-allianz-for-microinsurance-aimed-at-emerging-markets/\n                                  BIMA website: http://www.bimamobile.com/about-bima/about-us-new/\n176\n    Microinsurance\n    • Business model: BIMA provides microinsurance subscription via basic\n      mobile phone service for as little as 60c/month on a “pay-as-you-go” rolling\n      monthly cover\n       • Offers payouts of up to $1,000 to the family if the insured person dies\n       • Sign-up takes only 3 minutes and payments are collected through basic\n          mobile phone service\n    • Customer growth: over 550 thousand new customers/month (BIMA, 2018)\n    • 93% of customers live on less than $10/day (BIMA, 2018)\n    • 75% of subscribers never had insurance before\n    • Educating customers is the top priority\n    • $300MM valuation at Dec-2017 sale of LeapFrog Investments’ stake to\n      Allianz for $97MM\n                                 TechCrunch https://techcrunch.com/2017/12/19/bima-raises-97m-from-allianz-for-microinsurance-aimed-at-emerging-markets/\n                                 BIMA website: http://www.bimamobile.com/about-bima/about-us-new/\n184\n      Size of Private Technology Investment by Insurers/Reinsurers\n            Private technology investments by (re)insurers by target country\n                                          16%                        United States                                                United States\n                                                                                                       20%\n                                                                     France\n                   2012 – Q4 2018\n                                                                                                                                  China\n                                     3%                              China                                               33%\n                                                                                      Q4 2018\n                                    5%                               United Kingdom                                               Germany\n                                    6%                      54%      Germany                     13%                              United Kingdom\n                                                                     Canada                                                       Other\n                                     8%                              Other\n                                           8%                                                          13%\n                                                                                                                   21%\n                                                 2013-Q4 2018 Transactions: 972                              Q4 2018 Transactions: 63\n            Private technology investments by (re)insurers by investment stage\n                                                                                                         12%      11%\n                                           11%       16%             Seed/Angel                                                         Seed\n                                      3%\n                                     4%                              Series A                     12%                                   Series A\n                  2012 – Q4 2018\n                                                                     Series B                                             19%           Series B\n                                                                                       Q4 2018\n                                    13%                              Series C                                                           Series C\n                                                                     Series D                                                           Series E+\n                                                           29%                                    11%\n                                                                     Series E+                                                          Other\n                                                                     Other\n                                          24%                                                                   35%\n                                           2013-Q4 2018 Transactions: 972                                    Q4 2018 Transactions: 63\n                                                                                                                    *CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018\n185\n","cleanText":"application ai insurtech real estate technology introduction insurtech professor christopher geczy phd introduction insurtech * insurance industry global diversified across applications subsegments * gross written premium gwp trillion * amount insurance written including commissions costs * net income worldwide grew last year vs actual growth ernst young \"global insurance trends analysis \" june introduction insurtech * insurance industry needs respond technological change disruption * wearables * driverless vehicles * internet things * \"big data\" * natural language processing * blockchain * distributed ledger technologies * climate change * etc * new technologies offer opportunities increase efficiency industry serve new markets ernst young \"global insurance trends analysis \" june introduction insurtech * standardized definition insurtech * said revolutionizing insurance industry changing way insurers business milken institute \"insurtech rising profile insurtech landscape \" december \" insurtech described \" insurance company intermediary insurance value chain segment specialist utilizes technology either compete provide valued added benefits insurance industry \" sia partners key emerging technologies leveraged insurtech companies source capgemini world insurtech report application ai insurtech real estate technology emerging technologies ai machine learning professor christopher geczy phd key emerging technologies leveraged insurtech companies * artificial intelligence * descriptor software perform functions ordinarily associated human reasoning * iterative learning * self awareness emotions * insurers hope exploit ai chatbots * allstate' allstate business insurance expert abie * provides answers real time customer owners questions * insurers followed source \" insurtech insurers using \" https www thebalancesmb com insurtech graphic accenture presentation \"accenture' technology vision insurance \" key emerging technologies leveraged insurtech companies machine learning * enables computers learn time * using algorithms mathematical models simulate neural networks human brain * allows computers extract patterns raw data rather following specific instructions * gives appearance closer activities human brain * insurance companies amass large amounts data * yet according national association insurance commissioners insurers use data collect source \" insurtech insurers using \" https www thebalancesmb com insurtech key emerging technologies leveraged insurtech companies * machine learning could allow insurers mine data effectively extract valuable information * risk modeling analyze claims data predict risk future losses * demand modeling predict demand products future estimate premiums * detecting fraud identify patterns behavior obvious human adjusters * processing claims automate claim reporting processing * underwriting help underwriters analyze data collected applicants * computers aid decision making process flag risks inconsistencies data underwriters might able see * also check external sources social media verify accuracy data source \" insurtech insurers using \" https www thebalancesmb com insurtech application ai insurtech real estate technology redefining insurance industry professor christopher geczy phd redefining insurance industry product design selling marketing front office underwriting policy administration claims management insurtech redefining insurance industry policy claims product design front office underwriting administration management marketing actuarial models underwriting policy acquisition claims servicing distribution product design new policies servicing payout channel management * deg view * extended multi * real time * segment market * streamlined claims customer' needs device mobility information based servicing process low * personalized offering capturing desires waiting time product designs * integrated * advanced risk * automated systems * instant notification * design new omnichannel analytics enabling straight claim products offerings touch risk based pricing processing proactive status * adjust products points * automated workflow stp capabilities updates real time * real time updates management rules * automated * real time claims * disaggregate interactions engines premium reminders status monitoring product mix clients * customer value led renewal notice * advanced analytics seamlessly * quick identification promotions * anytime access based fraud * design deliver cross selling discounts policy details view detection products end selling * digitized systems customer want opportunities less reliance * detecting client data customer satisfaction needs provide * information availability price transparency source capgemini world insurtech report application ai insurtech real estate technology classification insurtech companies professor christopher geczy phd segmentation insurtech firms * different classification methods insurtech firms initiatives rotate similar concepts * \"traditional\" view full stack agents brokers * nuanced view sub segments carriers enablers distributors segmentation insurtech firms * milken institute three main classifications * full stack insurers platforms underwrite policies assume risk cases manage process beginning end * agents platforms act behalf carrier essentially acting extension incumbent carrier * brokers platforms provide customers variety policies offered incumbent carriers insurgent insurtech platforms * may may paid commission based policies sold platform * may require customers scroll policies offered automatically connect customers preferred policy algorithms employed based user' response set questions milken institute \"insurtech rising profile insurtech landscape \" december segmentation insurtech firms * make classification yes insurer full insurance one agent company partnered one multiple insurance companies multiple broker milken institute \"insurtech rising profile insurtech landscape \" december classification insurtech firms capgemnini * second classification system used capgemini * categorizes insurtech providers role distribution chain * enablers * distributors * full carriers source capgemini world insurtech report classification insurtech firms capgemnini source capgemini world insurtech report segmentation insurtech firms model insurtech platform models number insurtechs ful l stack agent broker insurtech platform models milken institute \"insurtech rising profile insurtech landscape \" december product offerings h ut ea lth nc l li nd bi fe ke iv u f ni ily rs al h al om te th rm ... e r u en ni li nc nk l ed tit ... le ab ili su ty ra b p nc us er e es na e ia nd nc bi ow l lit sh ... ip en pi ng r et tu tr ir em rn av en ... el product offering categories nc p l en si pe ri ci al p ty su ra nc nc l e bi ... cy cl type insurtech platform product offering e ob ile ... segmentation insurtech firms model product milken institute \"insurtech rising profile broker agent full stack insurtech landscape \" december segmentation insurtech firms * significant portion approximately insurtech companies could better described technology solution providers * human resources earned benefits solution providers * data solution providers * infrastructure solution providers milken institute \"insurtech rising profile insurtech landscape \" december segmentation insurtech firms * human resources earned benefits solution providers * platforms using deploying technology help firms manage human capital efficiently cost effectively * data solution providers * platforms specialize collecting aggregating analyzing vast quantities data support insurance carriers startups stakeholders * infrastructure solution providers * platforms focus making back end processes efficient use application programming interfaces apis provide means platforms integrate build customizable insurance products services milken institute \"insurtech rising profile insurtech landscape \" december number technology solution providers insurtech market insurtech platform models number providers human resources data solution provider infrastructure solution earned benefits solution provider provider small mild large businesses milken institute \"insurtech rising profile type tech solution providers insurtech landscape \" december examples full carrier insurtech firms insurtech firm products services offered example type zhongan chinese property insurer uses traditional insurance model digital carrier online channel sell products handle conducted online mobile claims risk sharing network group associated individuals pools inspeer france based community insurance p2p insurer premiums insure risk platform allows members pool money within full carriers generally stands benefit group covering group member' deductible regarding premium returns smaller insurance packages leveraging mobile technology bima offers micro insurer lower premiums typically lower affordable insurance products low income coverage populations emerging markets demand insurance coverage new york based sure offers demand personal demand insurer purchased online well episodic policies user buy either via via mobile apps website app us based metromile offers auto insurance usage based premiums prices per usage risky fees based number miles insured' insurer behavior displayed customer car logs source capgemini world insurtech report examples full distributor insurtech firms policybazaar specializes comparative analysis online site enables individuals marketplace products various insurers based price compare plans different insurers quality key benefits artificially intelligent insurance advisory application one stop app allows customers brolly delivers contextually relevant insights personal financial manage policies obtain web mobile applications customers assistant coverage recommendations manage policies one place know compare purchase plans distributors coverage may duplicated missing online platform allows customers licensed broker coverfox offers insurance products digital broker compare purchase policies vehicles home health services travel online site enables commercial coverhound us based insurtech firm offers b2b digital customers compare plans comparison platform personal small distributor different insurers commercial insurance products london based bought many uses social media customized flexible front office data connect people similar insurance needs value adding solutions via partnership uses group' collective buying power intermediary insurer reinsurer risk management negotiate insurers deals ' available individuals source capgemini world insurtech report examples full enabled insurtech firms premfina' white label solution brokers allows front office solution process improvement solutions extend premium financing options providers front office customers manage insurance policies riskgenius applies artificial intelligence streamline policy plan process improvement solutions work insurance professionals retrieving management underwriting policy plan details specific coverage exclusion analyzing solution provider administration policies extracting relevant information premium limit deductible enablers rightindem white label self service insurance claims management process improvement solutions claims platform insurers allows customers solution provider specifically claims management interact claim time carpe data leverages fata various channels online content social media connected data capture analytics solutions data specialist devices offers predictive scoring data use cases across value chain products insurers enabling predict risk better solutions based specific technology betterview drone technology specialist allows technology blockchain source capgemini world insurtech report specialist drone based inspection property assessment drones source capgemini world insurtech report application ai insurtech real estate technology investment market size insurtech industry professor christopher geczy phd insurtech market size * ' clear ' large growing * global insurtech market revenue 7mm * market revenue expected reach billion cagr * asiapacific highest regional cagr growing financial hubs hong kong singapore india * health insurance segment expected higher segment cagr * total insurtech investments billion * total deals deals billion * involved insurer reinsurer investor * estimated year cagr orbis research \"global insurance technology insurtech market size \" dec ernst young \"global insurance trends analysis \" june size insurtech market global private investment vc pe insurtech capital invested b deal count source kpmg international pulse fintech global analysis investment fintech january insurtech startup count number insurtech startups exploded years source sma - strategy meets action insurtech disruptors top insurtech disruptors insurtech key techs * insurers moving key business functions cloud * tech research firm ovum' annual survey penetration software service saas grew last year source ovum survey data cited deloitte \" insurance industry outlook\" insurtech key techs source ovum survey data cited deloitte \" insurance industry outlook\" insurtech key techs * aia hong kong launched blockchain app share life policy data bank distributors * axa europe offering flight delay insurance blockchain platform featuring smart contracts * ovum' annual survey penetration software service saas grew last year * carriers consortiums expected launch impactful blockchain initiatives due concerns around data technology profile insurtech market * u insurtech deals total value billion announced q4 * compared q4 deal count q4 increased funding volume also increased * transactions q4 - higher q3 lower q1 q2 * globally * uk investment last quarter * china second largest investor q4 u * uk responsible total investment since * investment international markets remains strong transactions outside u account total transactions since 4th quarter cbinsights quarterly insurtech briefing q4 profile insurtech market * early stage investments remain strong * seed series account total transactions since quarter last quarter * insurtech funding maturing mid - later stages - financings took place series b c stages * could lead consolidation cbinsights quarterly insurtech briefing q4 profile insurtech market * property casualty p c funding volume increased q3 increased q4 * p c transactions quarter marginally higher transactions q3 marks * life health l h funding volume increased q3 marked increase q4 * hits record level insurtech investment driven large investments deal count increased q3 funding volume increased cbinsights quarterly insurtech briefing q4 size insurtech market quarterly insurtech funding volume - stages millions property casualty life health q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 deal count p c l h cbinsights quarterly insurtech briefing q4 size insurtech market quarterly insurtech transaction target country united states united states united kingdom china - q4 china united kingdom q4 germany germany india south africa france q4 transactions q4 transactions quarterly insurtech transaction investments stage seed angel seed angel - q4 series series q4 series b series b series c series c series series series e series e q4 transactions q4 transactions cbinsights quarterly insurtech briefing q4 size private technology investment insurers reinsurers private technology investments insurers q1 q2 q3 q4 cbinsights quarterly insurtech briefing q4 application ai insurtech real estate technology insurtech fintech financial inclusion professor christopher geczy phd microinsurance * investment firm omidyar network * started ebay founder pierre omidyar * model combines profit llc grantmaking c * 5b committed since inception pierre pam omidyar * 676mm profit investments 782mm grants total commitments year ytd https www omidyar com financials microinsurance * common feature microinsurance products low income low net worth population served otherwise limited access insurance * microinsurance products type group individual coverage * term life * health accident disability * casualty crop insurance livestock theft fire natural disaster * certain forms retirement savings plans * microinsurance underwriter delivery channel type * large multinational insurance companies * credit unions mutual associations * government ngos * small community organizations microinsurance * benefits microinsurance * financial protection risk pooling * insureds assume risk * crop insurance vs drought enables small farmers plant crops higher yields \"good\" years poorer yields drought years * safeguard vs families falling back poverty due illness death breadwinner housing destroyed etc * indian ministry health found one quarter hospitalizations pushed individual family poverty due cost treatment tina rosenberg \" microinsurance revolution \" new york times opinionator blog june http opinionator blogs nytimes com microinsurance revolution microinsurance * benefits microinsurance * target specific risk populations * hiv positive living flood zone microentrepreneurs * complement social welfare programs bolster microfinance initiatives * assign term life policy secure business education mortgage loan tina rosenberg \" microinsurance revolution \" new york times opinionator blog june http opinionator blogs nytimes com microinsurance revolution microinsurance * bima - insurance business disruptor \"insurtech\" leader * swedish founded mobile insurance health company provides accident life health insurance products million low income consumers countries africa asia latam * largest markets ghana sri lanka bangladesh pakistan * mobile technology lowered prices brought affordable insurance world' poorest techcrunch https techcrunch com bima raises 97m allianz microinsurance aimed emerging markets bima website http www bimamobile com bima us new microinsurance * business model bima provides microinsurance subscription via basic mobile phone service little 60c month \"pay go\" rolling monthly cover * offers payouts family insured person dies * sign takes minutes payments collected basic mobile phone service * customer growth thousand new customers month bima * customers live less day bima * subscribers never insurance * educating customers top priority * 300mm valuation dec sale leapfrog investments' stake allianz 97mm techcrunch https techcrunch com bima raises 97m allianz microinsurance aimed emerging markets bima website http www bimamobile com bima us new size private technology investment insurers reinsurers private technology investments insurers target country united states united states france - q4 china china q4 united kingdom germany germany united kingdom canada q4 transactions q4 transactions private technology investments insurers investment stage seed angel seed series series - q4 series b series b q4 series c series c series series e series e q4 transactions q4 transactions cbinsights quarterly insurtech briefing q4","NLP":"application ai insurtech real estate technology introduction insurtech professor christopher geczy phd introduction insurtech * insurance industry global diversified across applications subsegments * gross written premium gwp trillion * amount insurance written including commissions costs * net income worldwide grew last year vs actual growth ernst young \"global insurance trends analysis \" june introduction insurtech * insurance industry needs respond technological change disruption * wearables * driverless vehicles * internet things * \"big data\" * natural language processing * blockchain * distributed ledger technologies * climate change * etc * new technologies offer opportunities increase efficiency industry serve new markets ernst young \"global insurance trends analysis \" june introduction insurtech * standardized definition insurtech * said revolutionizing insurance industry changing way insurers business milken institute \"insurtech rising profile insurtech landscape \" december \" insurtech described \" insurance company intermediary insurance value chain segment specialist utilizes technology either compete provide valued added benefits insurance industry \" sia partners key emerging technologies leveraged insurtech companies source capgemini world insurtech report application ai insurtech real estate technology emerging technologies ai machine learning professor christopher geczy phd key emerging technologies leveraged insurtech companies * artificial intelligence * descriptor software perform functions ordinarily associated human reasoning * iterative learning * self awareness emotions * insurers hope exploit ai chatbots * allstate' allstate business insurance expert abie * provides answers real time customer owners questions * insurers followed source \" insurtech insurers using \" https www thebalancesmb com insurtech graphic accenture presentation \"accenture' technology vision insurance \" key emerging technologies leveraged insurtech companies machine learning * enables computers learn time * using algorithms mathematical models simulate neural networks human brain * allows computers extract patterns raw data rather following specific instructions * gives appearance closer activities human brain * insurance companies amass large amounts data * yet according national association insurance commissioners insurers use data collect source \" insurtech insurers using \" https www thebalancesmb com insurtech key emerging technologies leveraged insurtech companies * machine learning could allow insurers mine data effectively extract valuable information * risk modeling analyze claims data predict risk future losses * demand modeling predict demand products future estimate premiums * detecting fraud identify patterns behavior obvious human adjusters * processing claims automate claim reporting processing * underwriting help underwriters analyze data collected applicants * computers aid decision making process flag risks inconsistencies data underwriters might able see * also check external sources social media verify accuracy data source \" insurtech insurers using \" https www thebalancesmb com insurtech application ai insurtech real estate technology redefining insurance industry professor christopher geczy phd redefining insurance industry product design selling marketing front office underwriting policy administration claims management insurtech redefining insurance industry policy claims product design front office underwriting administration management marketing actuarial models underwriting policy acquisition claims servicing distribution product design new policies servicing payout channel management * deg view * extended multi * real time * segment market * streamlined claims customer' needs device mobility information based servicing process low * personalized offering capturing desires waiting time product designs * integrated * advanced risk * automated systems * instant notification * design new omnichannel analytics enabling straight claim products offerings touch risk based pricing processing proactive status * adjust products points * automated workflow stp capabilities updates real time * real time updates management rules * automated * real time claims * disaggregate interactions engines premium reminders status monitoring product mix clients * customer value led renewal notice * advanced analytics seamlessly * quick identification promotions * anytime access based fraud * design deliver cross selling discounts policy details view detection products end selling * digitized systems customer want opportunities less reliance * detecting client data customer satisfaction needs provide * information availability price transparency source capgemini world insurtech report application ai insurtech real estate technology classification insurtech companies professor christopher geczy phd segmentation insurtech firms * different classification methods insurtech firms initiatives rotate similar concepts * \"traditional\" view full stack agents brokers * nuanced view sub segments carriers enablers distributors segmentation insurtech firms * milken institute three main classifications * full stack insurers platforms underwrite policies assume risk cases manage process beginning end * agents platforms act behalf carrier essentially acting extension incumbent carrier * brokers platforms provide customers variety policies offered incumbent carriers insurgent insurtech platforms * may may paid commission based policies sold platform * may require customers scroll policies offered automatically connect customers preferred policy algorithms employed based user' response set questions milken institute \"insurtech rising profile insurtech landscape \" december segmentation insurtech firms * make classification yes insurer full insurance one agent company partnered one multiple insurance companies multiple broker milken institute \"insurtech rising profile insurtech landscape \" december classification insurtech firms capgemnini * second classification system used capgemini * categorizes insurtech providers role distribution chain * enablers * distributors * full carriers source capgemini world insurtech report classification insurtech firms capgemnini source capgemini world insurtech report segmentation insurtech firms model insurtech platform models number insurtechs ful l stack agent broker insurtech platform models milken institute \"insurtech rising profile insurtech landscape \" december product offerings h ut ea lth nc l li nd bi fe ke iv u f ni ily rs al h al om te th rm ... e r u en ni li nc nk l ed tit ... le ab ili su ty ra b p nc us er e es na e ia nd nc bi ow l lit sh ... ip en pi ng r et tu tr ir em rn av en ... el product offering categories nc p l en si pe ri ci al p ty su ra nc nc l e bi ... cy cl type insurtech platform product offering e ob ile ... segmentation insurtech firms model product milken institute \"insurtech rising profile broker agent full stack insurtech landscape \" december segmentation insurtech firms * significant portion approximately insurtech companies could better described technology solution providers * human resources earned benefits solution providers * data solution providers * infrastructure solution providers milken institute \"insurtech rising profile insurtech landscape \" december segmentation insurtech firms * human resources earned benefits solution providers * platforms using deploying technology help firms manage human capital efficiently cost effectively * data solution providers * platforms specialize collecting aggregating analyzing vast quantities data support insurance carriers startups stakeholders * infrastructure solution providers * platforms focus making back end processes efficient use application programming interfaces apis provide means platforms integrate build customizable insurance products services milken institute \"insurtech rising profile insurtech landscape \" december number technology solution providers insurtech market insurtech platform models number providers human resources data solution provider infrastructure solution earned benefits solution provider provider small mild large businesses milken institute \"insurtech rising profile type tech solution providers insurtech landscape \" december examples full carrier insurtech firms insurtech firm products services offered example type zhongan chinese property insurer uses traditional insurance model digital carrier online channel sell products handle conducted online mobile claims risk sharing network group associated individuals pools inspeer france based community insurance p2p insurer premiums insure risk platform allows members pool money within full carriers generally stands benefit group covering group member' deductible regarding premium returns smaller insurance packages leveraging mobile technology bima offers micro insurer lower premiums typically lower affordable insurance products low income coverage populations emerging markets demand insurance coverage new york based sure offers demand personal demand insurer purchased online well episodic policies user buy either via via mobile apps website app us based metromile offers auto insurance usage based premiums prices per usage risky fees based number miles insured' insurer behavior displayed customer car logs source capgemini world insurtech report examples full distributor insurtech firms policybazaar specializes comparative analysis online site enables individuals marketplace products various insurers based price compare plans different insurers quality key benefits artificially intelligent insurance advisory application one stop app allows customers brolly delivers contextually relevant insights personal financial manage policies obtain web mobile applications customers assistant coverage recommendations manage policies one place know compare purchase plans distributors coverage may duplicated missing online platform allows customers licensed broker coverfox offers insurance products digital broker compare purchase policies vehicles home health services travel online site enables commercial coverhound us based insurtech firm offers b2b digital customers compare plans comparison platform personal small distributor different insurers commercial insurance products london based bought many uses social media customized flexible front office data connect people similar insurance needs value adding solutions via partnership uses group' collective buying power intermediary insurer reinsurer risk management negotiate insurers deals ' available individuals source capgemini world insurtech report examples full enabled insurtech firms premfina' white label solution brokers allows front office solution process improvement solutions extend premium financing options providers front office customers manage insurance policies riskgenius applies artificial intelligence streamline policy plan process improvement solutions work insurance professionals retrieving management underwriting policy plan details specific coverage exclusion analyzing solution provider administration policies extracting relevant information premium limit deductible enablers rightindem white label self service insurance claims management process improvement solutions claims platform insurers allows customers solution provider specifically claims management interact claim time carpe data leverages fata various channels online content social media connected data capture analytics solutions data specialist devices offers predictive scoring data use cases across value chain products insurers enabling predict risk better solutions based specific technology betterview drone technology specialist allows technology blockchain source capgemini world insurtech report specialist drone based inspection property assessment drones source capgemini world insurtech report application ai insurtech real estate technology investment market size insurtech industry professor christopher geczy phd insurtech market size * ' clear ' large growing * global insurtech market revenue 7mm * market revenue expected reach billion cagr * asiapacific highest regional cagr growing financial hubs hong kong singapore india * health insurance segment expected higher segment cagr * total insurtech investments billion * total deals deals billion * involved insurer reinsurer investor * estimated year cagr orbis research \"global insurance technology insurtech market size \" dec ernst young \"global insurance trends analysis \" june size insurtech market global private investment vc pe insurtech capital invested b deal count source kpmg international pulse fintech global analysis investment fintech january insurtech startup count number insurtech startups exploded years source sma - strategy meets action insurtech disruptors top insurtech disruptors insurtech key techs * insurers moving key business functions cloud * tech research firm ovum' annual survey penetration software service saas grew last year source ovum survey data cited deloitte \" insurance industry outlook\" insurtech key techs source ovum survey data cited deloitte \" insurance industry outlook\" insurtech key techs * aia hong kong launched blockchain app share life policy data bank distributors * axa europe offering flight delay insurance blockchain platform featuring smart contracts * ovum' annual survey penetration software service saas grew last year * carriers consortiums expected launch impactful blockchain initiatives due concerns around data technology profile insurtech market * u insurtech deals total value billion announced q4 * compared q4 deal count q4 increased funding volume also increased * transactions q4 - higher q3 lower q1 q2 * globally * uk investment last quarter * china second largest investor q4 u * uk responsible total investment since * investment international markets remains strong transactions outside u account total transactions since 4th quarter cbinsights quarterly insurtech briefing q4 profile insurtech market * early stage investments remain strong * seed series account total transactions since quarter last quarter * insurtech funding maturing mid - later stages - financings took place series b c stages * could lead consolidation cbinsights quarterly insurtech briefing q4 profile insurtech market * property casualty p c funding volume increased q3 increased q4 * p c transactions quarter marginally higher transactions q3 marks * life health l h funding volume increased q3 marked increase q4 * hits record level insurtech investment driven large investments deal count increased q3 funding volume increased cbinsights quarterly insurtech briefing q4 size insurtech market quarterly insurtech funding volume - stages millions property casualty life health q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 q1 q2 q3 q4 deal count p c l h cbinsights quarterly insurtech briefing q4 size insurtech market quarterly insurtech transaction target country united states united states united kingdom china - q4 china united kingdom q4 germany germany india south africa france q4 transactions q4 transactions quarterly insurtech transaction investments stage seed angel seed angel - q4 series series q4 series b series b series c series c series series series e series e q4 transactions q4 transactions cbinsights quarterly insurtech briefing q4 size private technology investment insurers reinsurers private technology investments insurers q1 q2 q3 q4 cbinsights quarterly insurtech briefing q4 application ai insurtech real estate technology insurtech fintech financial inclusion professor christopher geczy phd microinsurance * investment firm omidyar network * started ebay founder pierre omidyar * model combines profit llc grantmaking c * 5b committed since inception pierre pam omidyar * 676mm profit investments 782mm grants total commitments year ytd https www omidyar com financials microinsurance * common feature microinsurance products low income low net worth population served otherwise limited access insurance * microinsurance products type group individual coverage * term life * health accident disability * casualty crop insurance livestock theft fire natural disaster * certain forms retirement savings plans * microinsurance underwriter delivery channel type * large multinational insurance companies * credit unions mutual associations * government ngos * small community organizations microinsurance * benefits microinsurance * financial protection risk pooling * insureds assume risk * crop insurance vs drought enables small farmers plant crops higher yields \"good\" years poorer yields drought years * safeguard vs families falling back poverty due illness death breadwinner housing destroyed etc * indian ministry health found one quarter hospitalizations pushed individual family poverty due cost treatment tina rosenberg \" microinsurance revolution \" new york times opinionator blog june http opinionator blogs nytimes com microinsurance revolution microinsurance * benefits microinsurance * target specific risk populations * hiv positive living flood zone microentrepreneurs * complement social welfare programs bolster microfinance initiatives * assign term life policy secure business education mortgage loan tina rosenberg \" microinsurance revolution \" new york times opinionator blog june http opinionator blogs nytimes com microinsurance revolution microinsurance * bima - insurance business disruptor \"insurtech\" leader * swedish founded mobile insurance health company provides accident life health insurance products million low income consumers countries africa asia latam * largest markets ghana sri lanka bangladesh pakistan * mobile technology lowered prices brought affordable insurance world' poorest techcrunch https techcrunch com bima raises 97m allianz microinsurance aimed emerging markets bima website http www bimamobile com bima us new microinsurance * business model bima provides microinsurance subscription via basic mobile phone service little 60c month \"pay go\" rolling monthly cover * offers payouts family insured person dies * sign takes minutes payments collected basic mobile phone service * customer growth thousand new customers month bima * customers live less day bima * subscribers never insurance * educating customers top priority * 300mm valuation dec sale leapfrog investments' stake allianz 97mm techcrunch https techcrunch com bima raises 97m allianz microinsurance aimed emerging markets bima website http www bimamobile com bima us new size private technology investment insurers reinsurers private technology investments insurers target country united states united states france - q4 china china q4 united kingdom germany germany united kingdom canada q4 transactions q4 transactions private technology investments insurers investment stage seed angel seed series series - q4 series b series b q4 series c series c series series e series e q4 transactions q4 transactions cbinsights quarterly insurtech briefing q4","text_word_count":3732,"text_unique_words":1509,"_deepnote_index_column":0},{"Name":"./sampledocs/fra-2020-artificial-intelligence_en.pdf","Type":"pdf","Text":"GETTING THE\nFUTURE RIGHT\n―\nARTIFICIAL\nINTELLIGENCE AND\nFUNDAMENTAL\nRIGHTS\n                   REPORT\n© European Union Agency for Fundamental Rights, 2020\nReproduction is authorised provided the source is acknowledged.\nFor any use or reproduction of photos or other material that is not under the European Union Agency for Fundamental Rights\ncopyright, permission must be sought directly from the copyright holders.\nNeither the European Union Agency for Fundamental Rights nor any person acting on behalf of the Agency is responsible\nfor the use that might be made of the following information.\nLuxembourg: Publications Office of the European Union, 2020\nPrint       ISBN 978-92-9474-861-4        doi:10.2811/58563        TK-03-20-119-EN-C\nPDF         ISBN 978-92-9474-860-7        doi:10.2811/774118       TK-03-20-119-EN-N\n© Photo credits:\nCover: HQUALITY/Adobe Stock                                            Page 61: Copyright © 2020 CODED BIAS - All Rights Reserved\nPage 5: Mimi Potter/Adobe Stock                                        Page 63: Siberian Art/Adobe Stock\nPage 8: monsitj/Adobe Stock                                            Page 68: Good Studio/Adobe Stock\nPage 14: Monsitj/Adobe Stock                                           Page 75: Sikov/Adobe Stock\nPage 16: Mykola Mazuryk/Adobe Stock                                    Page 79: robsonphoto/Adobe Stock\nPage 20: metamorworks/Adobe Stock                                      Page 82: thodonal/Adobe Stock\nPage 25: Gorodenkoff/Adobe Stock                                       Page 86: blackboard/Adobe Stock\nPage 28: Dimco/Adobe Stock                                             Page 88: blackboard/Adobe Stock\nPage 32: VideoFlow/Adobe Stock                                         Page 92: Monopoly919/Adobe Stock\nPage 37: zapp2photo/Adobe Stock                                        Page 95: Gorodenkoff/Adobe Stock\nPage 41: bestforbest/Adobe Stock                                       Page 96: Freedomz/Adobe Stock\nPage 44: zapp2photo/Adobe Stock                                        Page 100: Copyright © 2020 CODED BIAS - All Rights Reserved\nPage 47: European Communities                                          Page 103: Copyright © 2020 CODED BIAS - All Rights Reserved\nPage 52: blacksalmon/Adobe Stock\nForeword\n         Did you know that artificial intelligence already plays a role in deciding\n         what unemployment benefits someone gets, where a burglary is likely to\n         take place, whether someone is at risk of cancer, or who sees that catchy\n         advertisement for low mortgage rates?\n         We speak of artificial intelligence (AI) when machines do the kind of things\n         that only people used to be able to do. Today, AI is more present in our lives\n         than we realise – and its use keeps growing. The possibilities seem endless.\n         But how can we fully uphold fundamental rights standards when using AI?\n         This report presents concrete examples of how companies and public\n         administrations in the EU are using, or trying to use, AI. It discusses the\n         potential implications for fundamental rights and shows whether and how\n         those using AI are taking rights into account.\n         FRA interviewed just over a hundred public administration officials, private\n         company staff, as well as diverse experts – including from supervisory and\n         oversight authorities, non-governmental organisations and lawyers – who\n         variously work in the AI field.\n         Based on these interviews, the report analyses how fundamental rights are\n         taken into consideration when using or developing AI applications. It focuses\n         on four core areas – social benefits, predictive policing, health services and\n         targeted advertising. The AI uses differ in terms of how complex they are,\n         how much automation is involved, their potential impact on people, and how\n         widely they are being applied.\n         The findings underscore that a lot of work lies ahead – for everyone.\n         One way to foster rights protection is to ensure that people can seek remedies\n         when something goes awry. To do so, they need to know that AI is being\n         used. It also means that organisations using AI need to be able to explain\n         their AI systems and how they deliver decisions based on them.\n         Yet the systems at issue can be truly complex. Both those using AI systems,\n         and those responsible for regulating their use, acknowledge that they do not\n         always fully understand them. Hiring staff with technical expertise is key.\n         Awareness of potential rights implications is also lacking. Most know that\n         data protection can be a concern, and some refer to non-discrimination. They\n         are less aware that other rights – such as human dignity, access to justice and\n         consumer protection, among others – can also be at risk. Not surprisingly,\n         when developers review the potential impact of AI systems, they tend to\n         focus on technical aspects.\n         To tackle these challenges, let’s encourage those working on human rights\n         protection and those working on AI to cooperate and share much-needed\n         knowledge – about tech and about rights.\n                                                                                         1\n  Those who develop and use AI also need to have the right tools to assess\n  comprehensively its fundamental rights implications, many of which may not\n  be immediately obvious. Accessible fundamental rights impact assessments\n  can encourage such reflection and help ensure that AI uses comply with\n  legal standards.\n  The interviews suggest that AI use in the EU, while growing, is still in its\n  infancy. But technology moves quicker than the law. We need to seize the\n  chance now to ensure that the future EU regulatory framework for AI is firmly\n  grounded in respect for human and fundamental rights.\n  We hope the empirical evidence and analysis presented in this report spurs\n  policymakers to embrace that challenge.\n                                                          Michael O’Flaherty\n                                                                      Director\n2\nContents\n  Foreword \b��������������������������������������������������������������������������������������������������������������������������������������������������������������� 1\n  Key findings and FRA opinions \b��������������������������������������������������������������������������������������������������������������������������� 5\n  1     AI AND FUNDAMENTAL RIGHTS – WHY IT IS RELEVANT FOR POLICYMAKING \b������������������������������� 15\n        1.1.   WHY THIS REPORT? \b����������������������������������������������������������������������������������������������������������������������� 17\n        1.2.   WHAT DO WE MEAN BY ARTIFICIAL INTELLIGENCE? \b���������������������������������������������������������������� 19\n        1.3.\t\u0007 AI AND FUNDAMENTAL RIGHTS IN THE EU POLICY FRAMEWORK: MOVING\n               TOWARDS REGULATION \b��������������������������������������������������������������������������������������������������������������� 21\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 24\n  2     PUTTING FUNDAMENTAL RIGHTS IN CONTEXT – SELECTED USE CASES OF AI IN THE EU \b������������� 25\n        2.1.   EXAMPLES OF AI USE IN PUBLIC ADMINISTRATION \b���������������������������������������������������������������� 30\n        2.2. EXAMPLES OF AI USE IN THE PRIVATE SECTOR \b������������������������������������������������������������������������ 37\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 45\n  3     FUNDAMENTAL RIGHTS FRAMEWORK APPLICABLE TO AI \b������������������������������������������������������������ 47\n        3.1.   FUNDAMENTAL RIGHTS FRAMEWORK GOVERNING THE USE OF AI \b�������������������������������������� 47\n        3.2.   ‘USE CASE’ EXAMPLES \b���������������������������������������������������������������������������������������������������������������� 50\n        3.3.   REQUIREMENTS FOR JUSTIFIED INTERFERENCES WITH FUNDAMENTAL RIGHTS \b������������������ 52\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 54\n  4     IMPACT OF CURRENT USE OF AI ON SELECTED FUNDAMENTAL RIGHTS \b���������������������������������������� 57\n        4.1.   PERCEIVED RISKS \b�������������������������������������������������������������������������������������������������������������������������� 57\n        4.2.   GENERAL AWARENESS OF FUNDAMENTAL RIGHTS AND LEGAL FRAMEWORKS IN\n               THE AI CONTEXT \b�������������������������������������������������������������������������������������������������������������������������� 58\n        4.3.   HUMAN DIGNITY \b�������������������������������������������������������������������������������������������������������������������������� 60\n        4.4.   RIGHT TO PRIVACY AND DATA PROTECTION – SELECTED CHALLENGES \b��������������������������������� 61\n        4.5.   EQUALITY AND NON-DISCRIMINATION \b�������������������������������������������������������������������������������������� 68\n        4.6.   ACCESS TO JUSTICE \b���������������������������������������������������������������������������������������������������������������������� 75\n        4.7.   RIGHT TO SOCIAL SECURITY AND SOCIAL ASSISTANCE \b������������������������������������������������������������ 79\n        4.8.   CONSUMER PROTECTION \b������������������������������������������������������������������������������������������������������������ 79\n        4.9.   RIGHT TO GOOD ADMINISTRATION \b��������������������������������������������������������������������������������������������� 81\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 83\n  5     FUNDAMENTAL RIGHTS IMPACT ASSESSMENT – A PRACTICAL TOOL FOR PROTECTING\n        FUNDAMENTAL RIGHTS \b�������������������������������������������������������������������������������������������������������������������� 87\n        5.1.   CALLING FOR A FUNDAMENTAL RIGHTS IMPACT ASSESSMENT – AVAILABLE\n               GUIDANCE AND TOOLS \b���������������������������������������������������������������������������������������������������������������� 87\n        5.2.   IMPACT ASSESSMENTS AND TESTING IN PRACTICE \b���������������������������������������������������������������� 91\n        5.3.   FUNDAMENTAL RIGHTS IMPACT ASSESSMENT IN PRACTICE \b�������������������������������������������������� 96\n        ENDNOTES \b���������������������������������������������������������������������������������������������������������������������������������������������� 99\n  6     MOVING FORWARD: CHALLENGES AND OPPORTUNITIES \b������������������������������������������������������������� 101\n                                                                                                                                                                              3\n  Figures\n  Figure 1: Companies using AI in 2020, by Member State (%) \b������������������������������������������������������������������������������������������������������������� 26\n  Figure 2: Examples of different automation and complexity levels in use cases covered \b��������������������������������������������������������������� 27\n  Figure 3: Words interviewees most often used to describe the AI ‘use cases’ \b��������������������������������������������������������������������������������� 29\n  Figure 4: Awareness of GDPR right to opt out from direct marketing, in the EU and United Kingdom, by country and\n            region (%) \b��������������������������������������������������������������������������������������������������������������������������������������������������������������������������������� 65\n  Figure 5: Awareness of right to have a say when decisions are automated, by age, gender and difficulty in paying bills (%) \b������� 67\n  Figure 6: Awareness about the risks of discrimination when using AI, by country (%) \b������������������������������������������������������������������� 73\n  Figure 7: Correlations of words respondents most often mention when discussing future plans to use AI \b������������������������������� 102\n4\nKey findings and FRA opinions\nNew technologies have profoundly changed how we organise and live\nour lives. In particular, new data-driven technologies have spurred the\ndevelopment of artificial intelligence (AI), including increased automation of\ntasks usually carried out by humans. The COVID-19 health crisis has boosted\nAI adoption and data sharing – creating new opportunities, but also challenges\nand threats to human and fundamental rights.\nDevelopments in AI have received wide attention by the media, civil\nsociety, academia, human rights bodies and policymakers. Much of that\nattention focuses on its potential to support economic growth. How different\ntechnologies can affect fundamental rights has received less attention. To\ndate, we do not yet have a large body of empirical evidence about the wide\nrange of rights AI implicates, or about the safeguards needed to ensure that\nthe use of AI complies with fundamental rights in practice.\nOn 19 February 2020, the European Commission published a White Paper on\nArtificial Intelligence – A European approach to excellence and trust. It outlines\nthe main principles of a future EU regulatory framework for AI in Europe.\nThe White Paper notes that it is vital that such a framework is grounded in\nthe EU’s fundamental values, including respect for human rights – Article 2\nof the Treaty on European Union (TEU).\nThis report supports that goal by analysing fundamental rights implications\nwhen using artificial intelligence. Based on concrete ‘use cases’ of AI\nin selected areas, it focuses on the situation on the ground in terms of\nfundamental rights challenges and opportunities when using AI.\n                                                                                   5\n                   The overarching fundamental rights framework* that applies to the use of AI in the\n  Legal            EU consists of the Charter of Fundamental Rights of the EU (the Charter) as well as the\n  framework        European Convention on Human Rights.\n                   Multiple other Council of Europe and international human rights instruments are relevant.\n                   These include the 1948 Universal Declaration of Human Rights and the major UN human\n                   rights conventions.**\n                   In addition, sector-specific secondary EU law, notably the EU data protection acquis and\n                   EU non-discrimination legislation, helps safeguard fundamental rights in the context of AI.\n                   Finally, the national laws of EU Member States also apply.\n                   *   For more, see FRA (2012), Bringing rights to life: The fundamental rights landscape of the\n                       European Union, Luxembourg, Publications Office of the European Union.\n                   ** These major conventions include: the 1966 International Covenant on Civil and Political Rights;\n                       the 1966 International Covenant on Economic, Social and Cultural Rights; the 1965 International\n                       Convention on the Elimination of All Forms of Racial Discrimination; the 1979 Convention on the\n                       Elimination of All Forms of Discrimination against Women; the 1984 Convention against Torture;\n                       the 1989 Convention on the Rights of the Child; the 2006 Convention on the Rights of Persons with\n                       Disabilities; and the 2006 International Convention for the Protection of All Persons from Enforced\n                       Disappearance.\n                       For more on the universal international human rights law framework, including their enforcement\n                       mechanisms, see e.g. De Schutter, O. (2015), International Human Rights Law: Cases, Materials,\n                       Commentary, Cambridge, Cambridge University Press, 2nd edition.\n            The report is based on 91 interviews with officials in public administration\n            and staff in private companies, in selected EU Member States. They were\n            asked about their use of AI, their awareness of fundamental rights issues\n            involved, and practices in terms of assessing and mitigating risks linked to\n            the use of AI.\n            Moreover, 10 interviews were conducted with experts who deal, in various\n            ways, with the potential fundamental rights challenges of AI. This group\n            included public bodies (such as supervisory and oversight authorities), non-\n            governmental organisations and lawyers.\n6\n                     SAFEGUARDING FUNDAMENTAL RIGHTS – SCOPE,\n                     IMPACT ASSESSMENTS AND ACCOUNTABILITY\n                     Considering the full scope of fundamental rights\n                     with respect to AI\n                                                                                    FRA OPINION 1\nUsing AI systems engages a wide range of fundamental\n                                                                                    When introducing new policies and\n      rights, regardless of the field of application. These\n                                                                                    adopting new legislation on AI, the\n include – but also go beyond – privacy, data protection,                           EU legislator and the Member States,\n                 non-discrimination and access to justice.                          acting within the scope of EU law,\n                                                                                    must ensure that respect for the full\n                     The EU Charter of Fundamental Rights (the Charter)             spectrum of fundamental rights, as\n                     became legally binding in December 2009 and has the            enshrined in the Charter and the EU\n                     same legal value as the EU treaties. It brings together        Treaties, is taken into account. Specific\n                     civil, political, economic and social rights in a single text. fundamental rights safeguards need to\n                     Pursuant to Article 51 (1) of the Charter, the institutions,   accompany relevant policies and laws.\n                     bodies, offices and agencies of the Union have to respect\n                                                                                    In doing so, the EU and its Member\n                     all the rights as embodied in the Charter. EU Member\n                                                                                    States should rely on robust evidence\n                     States have to do so when they are implementing Union\n                                                                                    concerning AI’s impact on fundamental\n                     law. This applies equally to AI as to any other field.\n                                                                                    rights to ensure that any restrictions\n                     The fieldwork of this research shows that a large              of certain fundamental rights respect\n                     variety of systems are used under the heading of AI.           the principles of necessity and\n                     The technologies analysed entail different levels of           proportionality.\n                     automation and complexity. They also vary in terms of\n                                                                                    Relevant safeguards need to be\n                     the scale and potential impact on people.\n                                                                                    provided for by law to effectively\n                     FRA’s findings show that using AI systems implicate a          protect against arbitrary interference\n                     wide spectrum of fundamental rights, regardless of the         with fundamental rights and to give\n                     field of application. These include, but also go beyond,       legal certainty to both AI developers\n                     privacy and data protection, non-discrimination and            and users. Voluntary schemes\n                     access to justice. Yet, when addressing the impact of AI       for observing and safeguarding\n                     with respect to fundamental rights, the interviews show,       fundamental rights in the development\n                     the scope is often delimited to specific rights.               and use of AI can further help mitigate\n                                                                                    rights violations. In line with the\n                     A wider range of rights need to be considered when\n                                                                                    minimum requirements of legal clarity\n                     using AI, depending on the technology and area of use. In\n                                                                                    – as a basic principle of the rule of\n                     addition to rights concerning privacy and data protection,\n                                                                                    law and a prerequisite for securing\n                     equality and non-discrimination, and access to justice,\n                                                                                    fundamental rights – the legislator has\n                     other rights could be considered. These include, for\n                                                                                    to take due care when defining the\n                     example, human dignity, the right to social security and\n                                                                                    scope of any such AI law.\n                     social assistance, the right to good administration (mostly\n                     relevant for the public sector) and consumer protection        Given the variety of technology\n                     (particularly important for businesses). Depending on          subsumed under the term AI and\n                     the context of the AI use, any other right protected in        the lack of knowledge about the full\n                     the Charter needs consideration.                               scope of its potential fundamental\n                                                                                    rights impact, the legal definition of\n                                                                                    AI-related terms might need to be\n                                                                                    assessed on a regular basis.\n                                                                                                                              7\n                                                 Using effective impact assessments to prevent\n                                                 negative effects\n  FRA OPINION 2\n                                                   Prior impact assessments mainly focus on technical\n  The EU legislator should consider making         issues. They rarely address potential effects on\n  mandatory impact assessments that\n                                                   fundamental rights. This is because knowledge on how\n  cover the full spectrum of fundamental\n  rights. These should cover the private\n                                                   AI affects such rights is lacking.\n  and public sectors, and be applied\n                                                 Deploying AI systems engages a wide spectrum of\n  before any AI-system is used. The\n                                                 fundamental rights, regardless of the field of application.\n  impact assessments should take into\n                                                 Pursuant to Article 51 (1) of the Charter, EU Member States\n  account the varying nature and scope\n                                                 must respect all rights embodied in the Charter when\n  of AI technologies, including the level of\n                                                 they are implementing Union law. In line with existing\n  automation and complexity, as well as\n                                                 international standards – notably the United National\n  the potential harm. They should include\n                                                 Guiding Principles on Business and Human Rights (UNGPs)\n  basic screening requirements that can\n                                                 – businesses should have in place “a human rights due\n  also serve to raise awareness of potential\n                                                 diligence process to identify, prevent, mitigate and account\n  fundamental rights implications.\n                                                 for how they address their impacts on human rights”\n  Impact assessments should draw on              (Principles 15 and 17). This is irrespective of their size and\n  established good practice from other           sector, and encompasses businesses working with AI.\n  fields and be regularly repeated during\n                                                 While pursuing its commitments to the UNGPs, the EU\n  deployment, where appropriate. These\n                                                 has adopted several legislative acts addressing sector-\n  assessments should be conducted in a\n                                                 specific instruments, in particular in the context of due\n  transparent manner. Their outcomes\n                                                 diligence-related obligations for human rights. Discussions\n  and recommendations should be in the\n                                                 are currently underway on proposing new EU secondary\n  public domain, to the extent possible.\n                                                 law. Such law would require businesses to carry out due\n  To aid the impact assessment process,\n                                                 diligence of the potential human rights and environmental\n  companies and public administration\n                                                 impacts of their operations and supply chains. Such law\n  should be required to collect the\n                                                 would likely be cross-sectoral and provide for sanctions\n  information needed for thoroughly\n                                                 for non-compliance – which should encompass the use of\n  assessing the potential fundamental\n                                                 AI. See FRA’s recent report on Business and Human rights\n  rights impact.\n                                                 – access to remedy, which calls for improved horizontal\n  The EU and Member States should                human rights diligence rules for EU-based companies.\n  consider targeted actions to support\n                                                 Impact assessments are an important tool for businesses\n  those developing, using or planning\n                                                 and public administration alike to mitigate the potential\n  to use AI systems, to ensure effective\n                                                 negative impact of their activities on fundamental rights.\n  compliance with their fundamental\n                                                 EU law in specific sectors requires some forms of impact\n  rights impact assessment obligations.\n                                                 assessments, such as Data Protection Impact Assessments\n  Such actions could include funding,\n                                                 under the General Data Protection Regulation (GDPR).\n  guidelines, training or awareness\n                                                 Many interviewees reported that a data protection impact\n  raising. They should particularly – but\n                                                 assessment, as required by law, was conducted. However,\n  not exclusively – target the private\n                                                 these took different forms. Moreover, prior assessments,\n  sector.\n                                                 when conducted, focus mainly on technical aspects. They\n  The EU and Member States should                rarely address potential impacts on fundamental rights.\n  consider using existing tools, such as         According to some interviewees, fundamental rights impact\n  checklists or self-evaluation tools,           assessments are not carried out when an AI system does\n  developed at European and international        not, or appears not to, affect fundamental rights negatively.\n  level. These include those developed by\n                                                 The research shows that the interviewees’ knowledge on\n  the EU High-Level Group on Artificial\n                                                 fundamental rights – other than data protection and, to\n  Intelligence.\n                                                 some extent, non-discrimination – is limited. The majority\n                                                 acknowledge, however, that the use of AI has an impact\n                            on fundamental rights. Some interviewees indicate that their systems do not\n                            affect fundamental rights, which is to some extent linked to the tasks the AI\n                            systems are used for.\n                            All respondents are aware of data protection issues. Most respondents also\n                            realise that discrimination could – generally – be a problem when AI is used.\n8\n                      However, the exact meaning and applicability of rights related to data protection\n                      and non-discrimination remains unclear to many respondents.\n                      The research findings show differences between the private and public sector.\n                      Interviewees from the private sector are often less aware of the wider range of\n                      fundamental rights that could be affected. Data protection issues are known to\n                      the private sector. However, other rights, such as non-discrimination or access\n                      to justice-related rights, are less well known among business representatives\n                      who work with AI. Some were fully aware of potential problems. But others\n                      said that the responsibility for checking fundamental rights issues lies with\n                      their clients.\n                      Ensuring effective oversight and overall\n                      accountability\n                                                                                     FRA OPINION 3\n        Businesses and public administrations that are\n    developing and using AI are in contact with various                              The EU and Member States should ensure that\n  bodies that are responsible for overseeing AI-related                              effective accountability systems are in place\nsystems within their respective mandates and sectors.                                to monitor and, where needed, effectively\n                                                                                     address any negative impact of AI systems\n  These bodies include data protection authorities. But\n                                                                                     on fundamental rights. They should consider,\n   those using AI are not always sure which bodies are                               in addition to fundamental rights impact\n                responsible for overseeing AI systems.                               assessments (see FRA opinion 2), introducing\n                                                                                     specific safeguards to ensure that the\n                      In line with well-established international human rights       accountability regime is effective. This could\n                      standards – for example, Article 1 of the European             include a legal requirement to make available\n                      Convention on Human Rights (ECHR) and Article 51 of the        enough information to allow for an assessment\n                      Charter – states are obliged to secure people’s rights and     of the fundamental rights impact of AI systems.\n                      freedoms. To effectively comply, states have to – among        This would enable external monitoring and\n                      others – put in place effective monitoring and enforcement     human rights oversight by competent bodies.\n                      mechanisms. This applies equally with respect to AI.\n                                                                                     The EU and Member States should also\n                      At the level of monitoring, the findings point to the          make better use of existing oversight expert\n                      important role of specialised bodies established in specific   structures to protect fundamental rights\n                      sectors that are also responsible for AI oversight within      when using AI. These include data protection\n                      their mandates. These include, for example, oversight          authorities, equality bodies, national human\n                      in the area of banking, or data protection authorities.        rights institutions, ombuds institutions and\n                      A variety of such bodies are potentially relevant to the       consumer protection bodies.\n                      oversight of AI from a fundamental rights perspective.\n                                                                                     Additional resources should be earmarked to\n                      However, the responsibilities of bodies concerning\n                                                                                     establish effective accountability systems by\n                      the oversight of AI remains unclear to many of those\n                                                                                     ‘upskilling’ and diversifying staff working for\n                      interviewed from the private and the public sector.\n                                                                                     oversight bodies. This would allow them to\n                      Public administrations’ use of AI is sometimes audited,        deal with complex issues linked to developing\n                      as part of their regular audits. Private companies in          and using AI.\n                      specific sectors also have specialised oversight bodies,\n                                                                                     Similarly, the appropriate bodies should be\n                      for example in the area of health or financial services.\n                                                                                     equipped with sufficient resources, powers\n                      These also check the use of AI and related technologies,\n                                                                                     and – importantly – expertise to prevent and\n                      for example as part of their certification schemes. Private\n                                                                                     assess fundamental rights violations and\n                      sector interviewees expressed a wish for bodies that could\n                                                                                     effectively support those whose fundamental\n                      provide expert advice on the possibilities and legality of\n                                                                                     rights are affected by AI.\n                      potential AI uses.\n                                                                                     Facilitating cooperation between appropriate\n                      In the EU, there is a well-developed set of independent\n                                                                                     bodies at national and European level can help\n                      bodies with a mandate to protect and promote fundamental\n                                                                                     share expertise and experience. Engaging with\n                      rights. These include data protection authorities, equality\n                                                                                     other actors with relevant expertise – such\n                      bodies, national human rights institutions and ombuds\n                                                                                     as specialist civil society organisations – can\n                      institutions. The research shows that those using or\n                                                                                     also help. When implementing such actions at\n                      planning to use AI often contacted different bodies about\n                                                                                     national level, Member States should consider\n                      their use of AI, such as consumer protection bodies.\n                                                                                     using available EU funding mechanisms.\n                                                                                                                                     9\n                              Most often, users of AI contacted data protection authorities to seek guidance, input\n                              or approval where personal data processing was involved. Interviewed experts\n                              highlight the relevance of data protection authorities for overseeing AI systems with\n                              respect to the use of personal data. However, they also note that data protection\n                              authorities are under-resourced for this task and lack specific expertise on AI issues.\n                              Experts, including those working for oversight bodies such as equality bodies and\n                              data protection authorities, agree that the expertise of existing oversight bodies\n                              needs to be strengthened to allow them to provide effective oversight of AI related\n                              issues. According to the experts, this can be challenging given that these bodies’\n                              resources are already stretched. They also highlighted the important role of relevant\n                              civil society organisations specialised in the fields of technology, digital rights and\n                              algorithms. They can enhance accountability in the use of AI systems.\n                              NON-DISCRIMINATION, DATA PROTECTION AND ACCESS TO\n                              JUSTICE: THREE HORIZONTAL THEMES\n                              The research shows that the use of AI affects various fundamental rights.\n                              Apart from context-related specific aspects that affect different rights to a\n                              varying extent, the fundamental rights topics which emerged in the research to\n                              repeatedly apply to most AI cases include: the need to ensure non-discriminatory\n                              use of AI (right not to be discriminated); the requirement to process data legally\n                              (right to personal data protection); and the possibility to complain about AI-based\n                              decisions and seek redress (right to an effective remedy and to a fair trial).\n                              The two main fundamental rights highlighted in the interviews are data\n                              protection and non-discrimination. In addition, effective ways to complain\n                              about the use of AI came up repeatedly, linked to the right to a fair trial\n                              and effective remedy. The following three FRA opinions, which reflect these\n                              findings, should be read alongside the other opinions, which call for a more\n                                                     comprehensive recognition of, and response to, the full\n                                                     range of fundamental rights affected by AI.\n                                                     Specific safeguards to ensure non-discrimination\n                                                     when using AI\n   FRA OPINION 4                                        Interviewees rarely mentioned carrying out detailed\n                                                        assessments of potential discrimination when using AI.\n   EU Member States should consider\n   encouraging companies and public\n                                                        This suggests a lack of in-depth assessments of such\n   administration to assess any potentially\n                                                        discrimination in automated decision making.\n   discriminatory outcomes when using AI\n   systems.\n                                                     The obligation to respect the principle of non-\n   The European Commission and Member                discrimination is enshrined in Article 2 of the TEU,\n   States should consider providing funding          Article 10 of the TFEU (requiring the Union to combat\n   for targeted research on potentially              discrimination on a number of grounds), and Articles 20\n   discriminatory impacts of the use of AI           and 21 of the Charter (equality before the law and non-\n   and algorithms. Such research would               discrimination on a range of grounds). More specific and\n   benefit from the adaptation of established        detailed provisions in several EU directives also enshrine\n   research methodologies, from the social           this principle, with varying scopes of application.\n   sciences, that are employed to identify\n                                                     Automation and the use of AI can greatly increase\n   potential discrimination in different areas\n                                                     the efficiency of services and can scale up tasks that\n   – ranging from recruitment to customer\n                                                     humans would not be able to undertake. However, it is\n   profiling.\n                                                     necessary to ensure that services and decisions based on\n   Building on the results of such research,         AI are not discriminatory. Recognising this, the European\n   guidance and tools to support those               Commission recently highlighted the need for additional\n   using AI to detect possible discriminatory\n   outcomes should be developed.\n10\nlegislation to safeguard non-discrimination when using AI in the EU anti-\nracism action plan 2020-2025.\nMost interviewees are in principle aware that discrimination might happen.\nYet, they rarely raised this issue themselves. Only few believe their systems\ncould actually discriminate.\nInterviewees also rarely mentioned detailed assessments of potential\ndiscrimination, meaning that there is a lack of in-depth assessment of potential\ndiscrimination.\nA common perception is that omitting information about protected attributes,\nsuch as gender, age or ethnic origin, can guarantee that an AI system does\nnot discriminate. This is not necessarily true, however. Information potentially\nindicating protected characteristics (proxies), which can often be found in\ndatasets, could lead to discrimination.\nIn certain cases, AI systems can also be used to test for and detect discriminatory\nbehaviour, which can be encoded in datasets. However, very few interviewees\nmentioned the possibility of collecting such information about disadvantaged\ngroups to detect potential discrimination. In the absence of in-depth analysis\nof potential discrimination in the actual use of AI systems, there is also almost\nno discussion and analysis of the potential positive effect of using algorithms\nto make decisions fairer. Moreover, none of the interviewees working on AI\nmentioned using AI to detect possible discrimination as a positive outcome, in\nthe sense that discrimination can be better detected when data are analysed\nfor potential bias.\nSince detecting potential discrimination through the use of AI and algorithms\nremains challenging, and interviewees only briefly addressed the issue, different\nmeasures are needed to address this. These include the requirement to consider\nissues linked to discrimination when assessing the use of AI, and investment\ninto further studies of potential discrimination that use a diverse range of\nmethodologies.\nThis could involve, for example, discrimination testing. This could build on similar\nestablished methodologies for testing bias in everyday life, such as with respect\nto job applications, where the applicant’s name is changed to (indirectly) identify\nethnicity. In relation to AI applications, such tests could involve the possible\ncreation of fake profiles for online tools, which only differ with respect to\nprotected attributes. In this way, the outcomes can be checked with respect to\npotential discrimination. Research could also benefit from advanced statistical\nanalysis to detect differences in datasets concerning protected groups, and\ntherefore can be used as a basis for exploring potential discrimination.\nFinally, some research interviews underscored that results from complex\nmachine learning algorithms are often very difficult to understand and explain.\nThus, further research to better understand and explain such results (so-called\n‘explainable AI’) can also help to better detect discrimination when using AI.\n                                                                                     11\n                                                   More guidance on data protection\n                                                       More clarity is needed on the scope and meaning of\n   FRA OPINION 5                                       legal provisions regarding automated decision making.\n   The European Data Protection Board\n                                                   Data protection is critical in the development and use of\n   (EDPB) and the European Data\n                                                   AI. Article 8 (1) of the Charter and Article 16 (1) of the TFEU\n   Protection Supervisor (EDPS) should\n                                                   provide that everyone has the right to the protection of\n   consider providing further guidance\n                                                   their personal data. The GDPR and the Law Enforcement\n   and support to effectively implement\n                                                   Directive (Directive (EU) 2018/680) further elaborate\n   GDPR provisions that directly apply\n                                                   on this right, and include many provisions applicable to\n   to the use of AI for safeguarding\n                                                   the use of AI.\n   fundamental rights, in particular as\n   regards the meaning of personal data            The interviewees indicated that most of the AI systems\n   and its use in AI, including in AI training     they employ use personal data, meaning data protection\n   datasets.                                       is affected in many different ways. However, a few\n                                                   applications – according to the interviewees – do not\n   There is a high level of uncertainty\n                                                   use personal data, or only use anonymised data, and\n   concerning the meaning of automated\n                                                   hence data protection law would not apply. If personal\n   decision making and the right to\n                                                   data are used, all data protection related principles and\n   human review linked to the use of AI\n                                                   provisions apply.\n   and automated decision making. Thus,\n   the EDPB and the EDPS should also               This report highlights an important issue linked to data\n   consider further clarifying the concepts        protection, which is also relevant for other fundamental\n   of ‘automated decision making’ and              rights with respect to automated decision making.\n   ‘human review’, where they are                  According to a Eurobarometer survey, only 40 % of\n   mentioned in EU law.                            Europeans know that they can have a say when decisions\n                                                   are automated. Knowledge about this right is considerably\n   In addition, national data protection\n                                                   higher among those working with AI – the majority of\n   bodies should provide practical\n                                                   interviewees raised this issue. However, many of the\n   guidance on how data protection\n                                                   interviewees, including experts, argued that more clarity\n   provisions apply to the use of\n                                                   is needed on the scope and meaning of legal provisions\n   AI. Such guidance could include\n                                                   on automated decision making.\n   recommendations and checklists,\n   based on concrete use cases of AI,              In the area of social benefits, interviewees mentioned only\n   to support compliance with data                 one example of fully automated, rule-based decisions.\n   protection provisions.                          All other applications they mentioned are reviewed by\n                                                   humans. Interviewees in public administration stressed\n                                                   the importance of human review of any decisions.\n                                                   However, they rarely described what such human review\n                                                   actually involves and how other information was used\n                                                   when reviewing output from AI systems.\n                              While interviewees disagree as to whether or not the existing legislation is\n                              sufficient, many called for more concrete interpretation of the existing data\n                              protection rules with respect to automated decision making, as enshrined\n                              in Article 22 of the GDPR.\n12\n                      Effective access to justice in cases involving\n                      AI-based decisions\n                                                                                      FRA OPINION 6\n  To effectively contest decisions based on the use of AI,\npeople need to know that AI is used, and how and where                                The EU legislator and Member States\n   to complain. Organisations using AI need to be able to                             should ensure effective access to\n                                                                                      justice for individuals in cases involving\n       explain their AI system and decisions based on AI.\n                                                                                      AI-based decisions.\n                      Access to justice is both a process and a goal, and is crucial  To ensure that available remedies are\n                      for individuals seeking to benefit from other procedural        accessible in practice, the EU legislator\n                      and substantive rights. It encompasses a number of core         and Member States could consider\n                      human rights. These include the right to a fair trial and to    introducing a legal duty for public\n                      an effective remedy under Article 6 and 13 of the ECHR          administration and private companies\n                      and Article 47 of the EU Charter of Fundamental Rights.         using AI systems to provide those\n                      Accordingly, the notion of access to justice obliges states     seeking redress information about\n                      to guarantee each individual’s right to go to court – or,       the operation of their AI systems.\n                      in some circumstances, an alternative dispute resolution        This includes information on how\n                      body – to obtain a remedy if it is found that the individual’s  these AI systems arrive at automated\n                      rights have been violated.                                      decisions. This obligation would help\n                                                                                      achieve equality of arms in cases of\n                      In accordance with these standards, a victim of a human\n                                                                                      individuals seeking justice. It would\n                      rights violation arising from the development or use of an\n                                                                                      also support the effectiveness of\n                      AI system by a public or private entity has to be provided\n                                                                                      external monitoring and human\n                      with access to remedy before a national authority. In line\n                                                                                      rights oversight of AI systems (see\n                      with relevant case law under Article 47 of the Charter and\n                                                                                      FRA opinion 3).\n                      Article 13 of the ECHR, the remedy must be “effective in\n                      practice as well as in law”.                                    In view of the difficulty of explaining\n                                                                                      complex AI systems, the EU, jointly\n                      The research findings identify the following preconditions\n                                                                                      with the Member States, should\n                      for the remedy to be effective in practice in cases\n                                                                                      consider developing guidelines to\n                      involving AI systems and their impact on fundamental\n                                                                                      support transparency efforts in this\n                      rights: everyone needs to be aware when AI is used and\n                                                                                      area. In so doing, they should draw on\n                      informed of how and where to complain. Organisations\n                                                                                      the expertise of national human rights\n                      using AI must ensure that the public is informed about\n                                                                                      bodies and civil society organisations\n                      their AI system and the decisions based on them.\n                                                                                      active in this field.\n                      The findings show that explaining AI systems and how\n                      they make decisions in layman terms can be challenging.\n                      Intellectual property rights can hamper the provision of detailed information\n                      about how an algorithm works. In addition, certain AI systems are complex.\n                      This makes it difficult to provide meaningful information about the way a\n                      system works, and on related decisions.\n                      To tackle this problem, some companies interviewed avoid using complex\n                      methods for certain decision making altogether, because they would not be\n                      able to explain the decisions. Alternatively, they use simpler data analysis\n                      methods for the same problem to obtain some understanding about the main\n                      factors influencing certain outcomes. Some of the private sector interviewees\n                      pointed to efforts made to gradually improve their understanding of AI\n                      technology.\n                                                                                                                                 13\n141\nAI AND FUNDAMENTAL RIGHTS – WHY\nIT IS RELEVANT FOR POLICYMAKING\n               Artificial intelligence (AI) is increasingly used in the private and public sectors,\n               affecting daily life. Some see AI as the end of human control over machines.\n               Others view it as the technology that will help humanity address some of its\n               most pressing challenges. While neither portrayal may be accurate, concerns\n               about AI’s fundamental rights impact are clearly mounting, meriting scrutiny\n               of its use by human rights actors.\n               Examples of potential problems with using AI-related technologies in relation\n               to fundamental rights have increasingly emerged. These include:\n               ――an algorithm used to recruit human resources was found to generally\n                    prefer men over women;1\n               ――an online chatbot2 became ‘racist’ within a couple of hours;3\n               ――machine translations showed gender bias;4\n               ――facial recognition systems detect gender well for white men, but not for\n                    black women;5\n               ――a public administration’s use of algorithms to categorise unemployed\n                    people did not comply with the law;6\n               ――and a court stopped an algorithmic system supporting social benefit\n                    decisions for breaching data protection laws.7\n               These examples raise profound questions about whether modern AI systems\n               are fit for purpose and how fundamental rights standards can be upheld\n               when using or considering using AI systems.\n               This report addresses these questions by providing a snapshot of the current\n               use of AI-related technologies in the EU – based on selected use cases – and\n               its implications on fundamental rights.\n                                                                                                    15\n               This report is the main publication stemming from FRA’s project on Artificial intelligence,\n   FRA’s work  big data and fundamental rights. The project aims to assess the positive and negative\n   on AI, big  fundamental rights implications of new technologies, including AI and big data.\n   data and\n   fundamental The current report builds on the findings of a number of earlier papers:\n   rights      •    Facial recognition technology: fundamental rights considerations in the context of law\n                    enforcement (2019): this paper outlines and analyses fundamental rights challenges\n                    triggered when public authorities deploy live FRT for law enforcement purposes. It also\n                    briefly presents steps to take to help avoid rights violations.\n               •    Data quality and artificial intelligence – mitigating bias and error to protect\n                    fundamental rights (2019): this paper highlights the importance of awareness and\n                    avoidance of poor data quality.\n               •    #BigData: Discrimination in data-supported decision making (2018): this focus paper\n                    discusses how such discrimination can occur and suggests possible solutions.\n               As part of the project, FRA is also exploring the feasibility of studying concrete examples of\n               fundamental rights challenges when using algorithms for decision making through either\n               online experiments or simulation studies.\n               Several other FRA publications address relevant issues:\n               •    The Guide on Preventing unlawful profiling today and in the future (2018) illustrates\n                    what profiling is, the legal frameworks that regulate it, and why conducting profiling\n                    lawfully is both necessary to comply with fundamental rights and crucial for effective\n                    policing and border management.\n               •    The Handbook on European data protection law (2018 edition) is designed to\n                    familiarise legal practitioners not specialised in data protection with this area of law.\n               •    Data from FRA’s Fundamental Rights Survey. It surveyed a random sample of 35,000\n                    people across the EU, including findings on people’s opinions and experiences linked to\n                    data protection and technology (2020) and security (2020).\n               •    FRA’s report on Business and human rights – access to remedy analyses obstacles and\n                    promising practices in relation to access to remedies for victims of business-related\n                    human rights abuses. By analysing complaints mechanisms in EU Member States, the\n                    research maps what hinders and what facilitates access to remedies.\n16\n1.1.\t WHY THIS REPORT?\nThe growing attention to AI and its potential to drive economic growth has not\nbeen matched by a body of evidence about how different technologies can\naffect fundamental rights – positively or negatively. Only concrete examples\nallow for a thorough examination of whether, and to what extent, applying\na technology interferes with various fundamental rights – and whether any\nsuch interference can be justified, in line with the principles of necessity\nand proportionality.\nThis report provides a fundamental rights-based analysis of concrete ‘use\ncases’ – or case studies. ‘Use case’ is a term in software engineering. This\nreport loosely defines it as the specific application of a technology for a\ncertain goal used by a specified actor.\nThe report illustrates some of the ways that companies and the public sector\nin the EU are looking to use AI to support their work, and whether – and\nhow – they are taking fundamental rights considerations into account. In\nthis way, it contributes empirical evidence, analysed from a fundamental\nrights perspective, that can inform EU and national policymaking efforts to\nregulate the use of AI tools.\nWhat did the research cover?\nFRA conducted fieldwork research in five EU Member States: Estonia, Finland,\nFrance, the Netherlands and Spain. It collected information from those involved\nin designing and using AI systems in key private and public sectors on how\nthey address relevant fundamental rights issues.\nThe research – based on 91 personal interviews – gathered information on:\n――the purpose and practical application of AI technologies;\n――the assessments conducted when using AI and the applicable legal\n    framework and oversight mechanisms;\n――the awareness of fundamental rights issues and potential safeguards\n    in place; and\n――future plans.\nIn addition, 10 experts involved in monitoring or observing potential\nfundamental rights violations concerning the use of AI, including civil society,\nlawyers and oversight bodies, were interviewed.\nPresenting the main findings\nThis report presents the main findings of the fieldwork. In particular, the\nreport includes:\n――An overview of the use of AI in the EU across a range of sectors, with a\n    focus on: (1) social benefits, (2) predictive policing, (3) healthcare, and\n    (4) targeted advertising.\n――An analysis of the awareness of fundamental rights and further implications\n    on selected rights, with a focus on the four use cases.\n――A discussion of measures to assess and mitigate the impact of AI-related\n    technologies on people’s fundamental rights.\nTwo annexes, available on FRA’s website, supplement the report:\n――Annex 1 gives a detailed description of the research methodology and\n    the questions asked in the interviews.\n――Annex 2 provides examples of potential errors when using AI in selected\n    areas.\n                                                                                 17\n   In addition, country-specific information on each of the five Member States\n   covered complements the fieldwork. This research, delivered by the contractor,\n   is also available on FRA’s website. It maps policy developments on AI and\n   the legal framework governing its use in different sectors.\n   Supporting rights-compliant policymaking\n   This report provides evidence on the extent to which fundamental rights\n   considerations are brought into discussions and activities to develop, test, employ\n   and monitor AI systems in the EU. It also highlights how different technologies\n   can affect some of the rights set out in the Charter, and reflects on how to protect\n   these rights as AI becomes both more widespread and more sophisticated.\n   The analysis of selected fundamental rights challenges can help the EU and\n   its Member States, as well as other stakeholders, assess the fundamental\n   rights compatibility of AI systems in different contexts. The findings in the\n   report about current views and practices among those using AI supports\n   policymakers in identifying where further actions are needed.\n   The report does not aim to provide a comprehensive mapping of the use of\n   different AI systems in the five EU Member States covered by the research,\n   or to provide in-depth technical information about how the different systems\n   mentioned by the interviewees work.\n   Conducting\n   the                               Who?\n   interviews                        This report is based on 91 semi-structured interviews with representatives from public\n                                     administration and private companies who are involved in the use of AI for their services and\n                                     businesses. FRA intentionally provided a very general definition of AI to those interviewed as\n                                     part of the research, based on existing definitions.\n                                     The organisations interviewed were active in public administration in general, with some\n                                     working in law enforcement.\n                                     The private companies include those working in health, retail, pricing and marketing,\n                                     financial services, insurance, employment, transport and energy. Importantly, except for two\n                                     interviewees, the research did not include companies that sell AI to other companies. Instead,\n                                     the entities use AI to support their own operations.\n                                     In addition, ten interviews were conducted with experts dealing with potential challenges of\n                                     AI in public administration (e.g. supervisory authorities), in non-governmental organisations\n                                     or as lawyers working in this field.\n                                     Where?\n                                     Interviews were carried out in five EU Member States (Estonia, Finland, France, the\n                                     Netherlands and Spain). These countries were selected based on their different levels\n                                     of uptake of AI technology and of policy development in the area of AI, as well as to\n                                     incorporate experience from across different parts of the EU.\n                                     How?\n                                     FRA outsourced the fieldwork to Ecorys. FRA staff supervised the work, and developed\n                                     the research questions and methodology. Interviewers received dedicated training before\n                                     conducting the fieldwork.\n                                     Interviews were carried out anonymously. As a consequence, no information identifying the\n                                     organisation concerned is provided in the report. In addition, certain details of the applications\n                                     described – most notably the country – are omitted to protect respondents’ anonymity. This\n                                     was communicated to interviewees, increasing their level of trust and allowing them to speak\n                                     more freely about their work. It also proved useful for recruiting respondents.\n18\n                                                           1.2.\t WHAT DO WE MEAN BY ARTIFICIAL INTELLIGENCE?\n                                                           There is no universally accepted definition of AI. Rather than referring to\n                                                           concrete applications, it reflects recent technological developments that\n                                                           encompass a variety of technologies. Although AI is usually defined very\n                                                           widely, a survey conducted in 2020 on behalf of the European Commission\n                                                           among companies in the EU showed that eight in ten people working at\n                                                           companies in the EU say they know what AI is. Slightly more than two in\n                                                           10 respondents from companies in the EU-27 do not know (7 %) or are not\n                                                           sure about (14 %) what AI is.8\n                                                                           FRA’s research did not apply a strict definition of AI on the use\nHigh-level                                                                 cases it presents. For the interviews, AI was defined broadly,\n                                                                           with reference to the definition provided by the High-Level\nexpert group                                                               Expert Group on Artificial Intelligence (AI HLEG ).\non artificial                                                              The interviewees also expressed a variety of ways to think\n                                                                           about AI. When identifying use cases to explore in the research,\nintelligence                                                               the project focused on applications that support decision\n                                                                           making based on data and machine learning, and applications\n      “Artificial intelligence (AI) refers to systems that\n                                                                           and systems that contribute to automating tasks that are usually\n      display intelligent behaviour by analysing their\n      environment and taking actions – with some degree                    undertaken by humans or which cannot be undertaken by\n      of autonomy – to achieve specific goals. AI-based                    humans due to their large scale. As such, the use cases in this\n      systems can be purely software-based, acting                         report provide insight into the different technologies that are\n      in the virtual world (e.g. voice assistants, image                   used and discussed in selected areas under the broad heading\n      analysis software, search engines, speech and                        of AI. As there may be some contention concerning whether\n      face recognition systems) or AI can be embedded                      certain use cases constitute AI at the current level of use, the\n      in hardware devices (e.g. advanced robots,                           report often refers to ‘AI and related technologies’.\n      autonomous cars, drones or Internet of Things\n      applications).”                                                      The past years have seen an enormous increase in computing\n      This initial definition of AI HLEG was subject to                    power, increased availability of data and the development of\n      further discussion in the groups. See AI HLEG (2019), A              new technologies for analysing data. The increased amount and\n      definition of AI: Main capabilities and disciplines.                 variety of data, sometimes available almost in real time over\n                                                                           the internet, is often referred to as big data. Machine learning\n                                                                           technologies and related algorithms, including deep learning,\n                                                                           benefit enormously from this increased computing power and\n                                                                           data availability, and their development and use is flourishing.\n                                                           The use of these terms is, however, of limited use. It can even prove\n                                                           counterproductive, as it triggers ideas linked to science fiction rather than\n                                                           any real application of AI. A variety of myths exist about what AI is and\n                                                           can do,9 often spread via (social) media. For example, some claim that AI\n                                                           can act on its own, being some form of entity. This distracts from the fact\n                                                           that all AI systems are made by humans and that computers only follow\n                                                           instructions made and given by humans. For a human-centric approach to\n                                                           AI, it is important to note that AI can never do anything on its own – it is\n                                                           human beings who use technology to achieve certain goals. However, the\n                                                           human work and decision making behind the AI systems is often not visible\n                                                           or the centre of attention.\n“Currently, there is no lawyer\n                                                           Entire studies and many discussions have explored possible AI definitions.\nwho can tell the definition of AI\n                                                           The European Commission’s Joint Research Centre analysed AI definitions.\nand we’ve asked around pretty\n                                                           It highlights that they often refer to issues linked to the perception of the\nthoroughly. No one can tell.”\n                                                           environment (i.e. the way a system receives input/data from its environment,\n(Public administration, Netherlands)\n                                                           e.g. through sensors), information processing, decision making and the\n                                                           achievement of specific goals. Definitions frequently refer to machines\n                                                           behaving like humans or taking over tasks associated with human intelligence.\n                                                           Given the difficulty of defining intelligence, many definitions remain vague.\n                                                           This makes the use of AI hard to measure in practice10 and, equally, challenging\n                                                           to define in law.11\n                                                                                                                                             19\n   This report discusses the use of AI based on concrete applications. These\n   differ in terms of their complexity, level of automation, potential impact on\n   individuals, and the scale of application.\n   Most of the discussion around, and the actual use of AI, involves deploying\n   machine learning technologies. These can be seen as a sub-domain of AI.\n   There is also some confusion around the term “learning”, which implies that\n   machines learn like humans. In reality, much of current machine learning\n   is based on statistical learning methodologies.12 Machine learning uses\n   statistical methods to find rules in the form of correlations that can help to\n   predict certain outcomes.\n   This is different from traditional statistical analysis, because it does not involve\n   detailed checks of how these predictions were produced (often referred to as\n   ‘black boxes’13). Traditional statistical analysis is based on specific theoretical\n   assumptions about the data generation processes and the correlations used.14\n   Machine learning is geared towards producing accurate outcomes, and can\n   be used for automating workflows or decisions, if an acceptable level of\n   accuracy can be obtained.\n   The usual example is an email spam filter, which uses statistical methods to\n   predict if an email is spam. As it is not important to know why a certain email\n   was blocked and because spam can be predicted with very high accuracy,\n   we do not really need to understand how the algorithm works (i.e. based on\n   what rules emails get blocked). However, depending on the complexity of\n   the task, prediction is not always possible with high accuracy. Moreover, as\n   this report highlights, not understanding why certain outcomes are predicted\n   is not acceptable for certain tasks.\n   The area of machine learning incorporates several approaches. Most often,\n   machine learning refers to finding rules that link data to a certain outcome\n   based on a dataset that includes outcomes (supervised learning). For example,\n   a dataset of emails, which are labelled as spam or not (‘ham’), is used to find\n   correlations and rules that are associated with spam emails in this dataset.\n   These rules are then used to ‘predict’ with some degree of likelihood if any\n   future email is spam or not.\n   Sometimes, machine learning is used to find hidden groups in datasets\n   without defining a certain outcome (unsupervised learning) – for example,\n   segmenting people into groups based on similarities in their demographics.\n20\nFinally, rules and correlations can be found through trial and error\n(reinforcement learning). These systems try to optimise a certain goal\nthrough experimentation, and update their rules automatically to have the\nbest possible output. Such systems need enormous amounts of data and\ncan hardly be used on humans, as it involves experimentation. They were\nmainly responsible for the success of winning board games against humans,\nwhich were often sensationalised by media.\n1.3.\t\u0007AI AND FUNDAMENTAL RIGHTS IN THE EU POLICY\n           FRAMEWORK: MOVING TOWARDS REGULATION\nPolicymakers have for some time highlighted the potential for AI and related\ntechnologies to improve efficiency and drive economic growth. Yet public\nauthorities and international organisations have only recently reflected on\nthe fundamental rights challenges associated with such technologies. Coupled\nwith the growing use and accuracy of AI systems, this has turned attention\nto whether and how to regulate their use.\nA 2017 European Parliament resolution marked a milestone in the EU’s\nrecognition of the fundamental rights implications of AI. The resolution\nstressed that “prospects and opportunities of big data can only be fully tapped\ninto by citizens, the public and private sectors, academia and the scientific\ncommunity when public trust in these technologies is ensured by a strong\nenforcement of fundamental rights”.15 It called on the European Commission,\nthe Member States, and data protection authorities “to develop a strong and\ncommon ethical framework for the transparent processing of personal data\nand automated decision-making that may guide data usage and the ongoing\nenforcement of Union law”. 16\nLater that year, the European Council called for a “sense of urgency to address\nemerging trends” including “issues such as artificial intelligence […], while\nat the same time ensuring a high level of data protection, digital rights and\nethical standards”.17 The European Council invited the European Commission\nto put forward a European approach to AI.\nResponding to these calls, the European Commission published in 2018 its\nCommunication on AI for Europe18 and set up a High Level Expert Group on\nAI.19 Both initiatives include a strong reference to fundamental rights.\nThe Commission-facilitated High Level Expert Group was made up of 52\nindependent experts from academia, civil society and industry (including a\nrepresentative from FRA). It published ‘Ethics Guidelines for Trustworthy AI’\nand ‘Policy and investment recommendations for trustworthy AI’ in 2019.\nThese were developed further in 2020.20 Its work triggered further discussion\non the importance of framing AI in human rights terms, alongside ethical\nconsiderations. This led to the development of Ethics Guidelines that refer\nto the Charter and place fundamental rights consideration with respect to AI.\nThe Ethics Guidelines include an assessment list for trustworthy AI, which has\nbeen translated into a checklist to guide those who develop and deploy AI.21\nIndicating political support at the highest level, the European Council calls\nin its Strategic Guidelines for 2019-2024 to “ensure that Europe is digitally\nsovereign” and for policy to be “shaped in a way that embodies our societal\nvalues”.22 Similarly, Commission President Von der Leyen committed to “put\nforward legislation for a coordinated European approach on the human and\nethical implications of [AI]”.23 This prompted significant moves towards setting\nout an EU legal framework to govern the development and use of AI and\nrelated technologies, including with respect to their impact on fundamental\nrights.\n                                                                                 21\n   In February 2020, the European Commission published a White Paper on\n   artificial intelligence. It sets out policy options for meeting the twin objectives\n   of “promoting the uptake of AI and addressing the risks associated with certain\n   uses of this new technology”. The paper promotes a common European\n   approach to AI. It deems this necessary “to reach sufficient scale and avoid\n   the fragmentation of the single market”. As it notes, “[t]he introduction of\n   national initiatives risks to endanger legal certainty, to weaken citizens’ trust\n   and to prevent the emergence of a dynamic European industry”.24 Legal\n   uncertainty is also a concern of companies planning to use AI.\n   The Commission White Paper on AI highlights risks to fundamental rights as\n   one of the main concerns associated with AI. It acknowledges that “the use\n   of AI can affect the values on which the EU is founded and lead to breaches\n   of fundamental rights, be it as a result from flaws in the overall design of\n   AI systems, or from the use of data without correcting possible bias”. It also\n   lists some of the wide range of rights that can be affected.25\n   The White Paper on AI indicates the Commission’s preference for the\n   possible new regulatory framework to follow a risk-based approach, in\n   which mandatory requirements would, in principle, only apply to high-risk\n   applications. These would be determined on the basis of two cumulative\n   criteria: if it is employed in a sector, such as healthcare, transport or parts\n   of the public sector, where significant risks can be expected to occur; and if\n   it is used in a manner where significant risks are likely to arise. This latter\n   risk could be assessed based on the impact on the affected parties, adding\n   a harm-based element.\n   The White Paper also highlights some instances where AI use for certain\n   purposes should be considered high-risk, irrespective of the sector. These\n   include the use of AI applications in recruitment processes or for remote\n   biometric identification, including facial recognition technologies.\n   Following a public consultation, which ran from February to June 2020,26\n   the Commission is expected to propose legislation on AI in the first quarter\n   of 2021.27\n   Ahead of the proposal, the EU’s co-legislators have considered various aspects\n   of the potential legal framework. In October 2020, the European Parliament\n   adopted resolutions with recommendations to the European Commission on\n   a framework of ethical aspects of AI, robotics and related technologies,28\n   and a civil liability regime for AI.29 It also adopted a resolution on intellectual\n   property rights for the development of artificial intelligence technologies,30\n   and continues to work on resolutions on AI in criminal law and its use by\n   the police and judicial authorities in criminal matters,31 and AI in education,\n   culture and the audio-visual sector.32 It also established a special committee\n   on artificial intelligence in the digital age.33\n   Following their meeting on 1-2 October 2020, the heads of state and\n   government of the EU Member States declared that the “EU needs to be a\n   global leader in the development of secure, trustworthy and ethical Artificial\n   Intelligence” and invited the Commission to “provide a clear, objective definition\n   of high-risk Artificial Intelligence systems.34 In addition, the Council of the EU\n   adopted Conclusions on shaping Europe’s digital future35 and on seizing the\n   opportunities of digitalisation for access to justice, which included a dedicated\n   section on deploying AI systems in the justice sector.36 The German Presidency\n   of the Council of the EU published conclusions on the Charter of Fundamental\n   Rights in the context of artificial intelligence and digital change; the text was\n   supported, or not objected to, by 26 Member States.37\n22\nThe growing reference to fundamental rights in these discussions indicates\nthat a fundamental rights framework alongside other legal frameworks38 is\nnecessary for an effective and human rights compliant evaluation of the many\nopportunities and challenges brought by new technologies. Many existing AI\ninitiatives are guided by ethical frameworks, which are typically voluntary.\nA fundamental rights-centred approach to AI is underpinned by legal regulation,\nwhere the responsibility for respecting, protecting and fulfilling rights rests\nwith the State. This should guarantee a high level of legal protection against\npossible misuse of new technologies. It also provides a clear legal basis\nfrom which to develop AI, where reference to fundamental rights – and their\napplication in practice – is fully embedded.39\nIn addition to steps towards legal regulation, the EU is taking significant\npolicy and financial actions to support the development of AI and related\ntechnologies. Alongside the White Paper, the Commission published the\nEuropean Data Strategy.40 It aims to set up a single market for data, including\nnine common European data spaces, covering areas such as health data\nand financial data. The proposal for the 2021-2027 Multiannual Financial\nFramework would create a Digital Europe Programme worth € 6.8 billion\nto invest in the EU’s “strategic digital capacities”, including AI, in addition to\nfunding through Horizon Europe and the Connecting Europe Facility.41\nOther international actors are also considering steps to regulate AI. Most\nnotably, the Council of Europe is an active player in the field of AI and related\ntechnologies. In September 2019, the Committee of Ministers of the Council\nof Europe set up the Ad Hoc Committee on Artificial Intelligence (CAHAI). It\naims to examine “the feasibility and potential elements of a legal framework\nfor the development, design and application of AI, based on the Council of\nEurope’s standards on human rights, democracy and the rule of law”.42 In\nApril 2020, the Committee of Ministers of the Council of Europe adopted\nrecommendations on the human rights impact of algorithmic systems.43\nIn addition, the Organisation for Economic Cooperation and Development\n(OECD) adopted AI principles and created an AI policy observatory.44 At global\nlevel, UNESCO is starting to develop a global standard setting instrument\non AI.45 These are selected examples of the wide range of legal and policy\ninitiatives aiming to contribute to standard setting in the area of AI. This\nincludes, amongst others, actual (draft) legislation, soft-law, guidelines and\nrecommendations on the use of AI, or reports with recommendations for\nlaw and policy.\nFRA put together a (non-exhaustive) list of initiatives linked to AI policymaking.46\nWhile these also include legislative initiatives in EU Member States, many\norganisations and businesses launched initiatives to tackle ethical concerns\nof AI. However, while useful to tackle potential problems with AI, ethical\napproaches often rely on voluntary action. This does not sufficiently address\nthe obligation to respect fundamental rights.\nAs FRA pointed out in its Fundamental Rights Report 2019: “only a rights-based\napproach guarantees a high level of protection against possible misuse of new\ntechnologies and wrongdoings using them.”47 The European Commission’s\ninitiative on regulating AI helps to avoid disjointed responses to AI across\nMember States, which can undermine businesses across the EU and with\nentities outside the EU.\n                                                                                     23\n   Endnotes\n   1  Reuters (2018), ‘Amazon scraps secret AI recruiting tool that showed bias against women’, 10 October 2018.\n   2  Chatbot or chatterbot is a common AI feature embedded in messaging applications to simulate human conversation through voice or text.\n   3  Independent (2017), ‘AI robots learning racism, sexism and other prejudices from humans, study finds’, 17 April 2017.\n   4  Prates, M., Avelar, P. and Lamb, L. (2019) ‘Assessing Gender Bias in Machine Translation – A Case Study with Google Translate’, 11 March\n      2019.\n   5  The Gender Shades project evaluating the accuracy of AI powered gender classification products.\n   6  See for example: Der Standard (2020), Datenschutzbehörde kippt umstrittenen AMS-Algorithmus, or AlgorithmWatch (2019), Poland:\n      Government to scrap controversial unemployment scoring system.\n   7  Privacy First (2020), Dutch risk profiling system SyRI banned following court decision.\n   8  European Commission (2020), European enterprise survey on the use of technologies based on artificial intelligence, Luxembourg, July\n      2020.\n   9  See, for example, the website “AI myths”.\n   10 Samoili, S., López Cobo, M., Gómez, E., De Prato, G., Martínez-Plumed, F., and Delipetrev, B. (2020), AI Watch. Defining Artificial\n      Intelligence. Towards an operational definition and taxonomy of artificial intelligence, Luxembourg.\n   11 Schuett, J. (2019), A legal definition of AI, arXiv: 1909.01095\n   12 Hastie, T., Tibshirani R., and Friedman, J. (2009), The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.\n   13 See, for example: Pasquale, F. (2015), The Black Box Society. The Secret Algorithms That Control Money and Information, Harvard\n      University Press, Cambrigde and London; and Rai, A. (2020), ‘Explainable AI: from black box to glass box’, Journal of the Academy of\n      Marketing Science, Vol. 48, pp. 137-141.\n   14 A seminal paper describing this difference is: Breiman, L. (2001), ‘Statistical Modeling: The Two Cultures’, Statistical Science, 2001, Vol. 16,\n      No. 3, pp. 199-231.\n   15 European Parliament resolution of 14 March 2017 on fundamental rights implications of big data: privacy, data protection, non-\n      discrimination, security and law-enforcement (2016/2225(INI)), para. 1.\n   16 Ibid., para. 20.\n   17 European Council (2017), European Council meeting (19 October 2017) – Conclusions, EUCO 14/17, Brussels, 19 October 2017, p. 8.\n   18 European Commission (2018), Communication from the Commission to the European Parliament, the European Council, the Council, the\n      European Economic and Social Committee and the Committee of the Regions on Artificial Intelligence for Europe, COM(2018) 237 final,\n      25 April 2018.\n   19 More information is available on the webpage of the High level expert group.\n   20 High-Level Expert Group on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy Artificial Intelligence; Policy and investment\n      recommendations for trustworthy AI.\n   21 High-Level Expert Group on Artificial Intelligence (2020), Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-\n      assessment.\n   22 European Council, A New Strategic Agenda 2019-2014, p. 4.\n   23 Vonder Leyen, Ursula, A Union that strives for more: My agenda for Europe, p. 13.\n   24 European Commission, White Paper On Artificial Intelligence – A European approach to excellence and trust, COM(2020) 65 final, Brussels,\n      19 February 2020, p. 2.\n   25 Ibid., p. 12.\n   26 European Commission (2020), White paper on Artificial Intelligence: Public consultation towards a European approach for excellence and\n      trust,17 July 2020.\n   27 European Commission (2020), Adjusted Commission Work Programme 2020, Annex I: New initiatives, 27 May 2020.\n   28 European Parliament, Legislative Observatory, Framework of ethical aspects of artificial intelligence, robotics and related technologies,\n      2020/2012 (INL).\n   29 European Parliament resolution of 20 October 2020 with recommendations to the Commission on a civil liability regime for artificial\n      intelligence, 2020/2014 (INL).\n   30 European Parliament resolution of 20 October 2020 on intellectual property rights for the development of artificial intelligence\n      technologies, 2020/2015 (INI).\n   31 European Parliament, Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters,\n      2020/2016 (INI).\n   32 European Parliament, Legislative Observatory, Artificial intelligence in education, culture and the audiovisual sector, 2020/2017 (INI).\n   33 European Parliament decision of 18 June 2020 on setting up a special committee on artificial intelligence in a digital age, and defining\n      its responsibilities, numerical strength and term of office, 2020/2684 (RSO).\n   34 European Council (2020), Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13/20, 2 October 2020.\n   35 Council of the European Union (2020), Shaping Europe’s Digital Future – Council Conclusions, 9 June 2020.\n   36 Council of the European Union, Council Conclusions “Access to Justice – Seizing the Opportunities of Digitalisation”, 13 October 2020.\n   37 Council of the European Union, Presidency Conclusions – the Charter of Fundamental Rights in the context of Artificial Intelligence and\n      Digital Change, 21 October 2020.\n   38 See e.g. Pagallo, U., Casanovas, P. & Madelin, R. (2019), ‘The middle-out approach: assessing models of legal governance in data\n      protection, artificial intelligence, and the Web of Data’, The Theory and Practice of Legislation 7 (1), pp. 1-25.\n   39 See FRA (2019), Fundamental Rights Report 2019, Luxembourg, Publications Office, Chapter 7.\n   40 Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the\n      Committee of the Regions, A European strategy for data, COM/2020/66 final.\n   41 European Council, Conclusions from Special meeting of the European Council (17, 18, 19, 20 and 21 July 2020), EUCO 10/20, 21 July 2020.\n   42 Council of Europe, Ad Hoc Committee on Artificial Intelligence (CAHAI), Factsheet: Governance for digital transformation.\n   43 Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights impacts of\n      algorithmic systems (adopted by the Committee of Ministers on 8 April 2020 at the 1373rd meeting of the Ministers’ Deputies).\n   44 See the dedicated OECD website.\n   45 See the dedicated UNESCO website.\n   46 See, for an overview by FRA, AI Policy Initiatives, or at the Council of Europe website.\n   47 FRA (2019), Fundamental Rights Report, Luxembourg, Publications Office, p. 166.\n24\n2\nPUTTING FUNDAMENTAL RIGHTS IN\nCONTEXT – SELECTED USE CASES OF AI\nIN THE EU\n                                 In the EU, the use of AI-related technologies is relatively wide-spread.\n                                 A recent survey shows that 42 % of companies use AI-related technologies\n                                 – and that 18 % plan to do so.\n                                                              This chapter presents selected cases of AI use\nNote on                                                       – typically referred to as ‘use cases’ in the AI\n                                                              field. FRA collected information on such cases\ninterviewees The use cases presented in this\n                                                              from five EU Member States: Estonia, France,\n                                                              Finland, the Netherlands and Spain. They involve\n             chapter are based on information\n                                                              different areas of application across public\n             obtained in interviews with\n             both public and private sector                   administration and private companies. Special\n             representatives.                                 focus is put on the use of AI in the areas of social\n                                                              benefits; predictive policing; health services;\n             The interviewed representatives                  and targeted advertising.\n             from public administration work\n             in the areas of health services,                 The chapter provides information on the current\n             infrastructure and energy, the\n                                                              use of AI, as well as basic information on EU\n             judiciary, law enforcement,\n                                                              competence, in these select areas. The use cases\n             migration and border management,\n             social benefits, tax, as well as                 provide a good sense of what kind of AI and\n             transportation and traffic control.              related technologies are currently being used.\n             Interviewees from private companies              The examples also offer context for the\n             mainly work in retail, marketing                 fundamental rights analysis. Looking at a\n             and pricing, the health sector, in               broad variety of use cases provides important\n             financial services, energy, insurance,           insights on how the actual use of AI can affect\n             employment and transport, as well\n                                                              people’s fundamental rights. Chapter 4 includes\n             as in cross-cutting areas with a focus\n                                                              a discussion of fundamental rights implications,\n             on AI development for different\n             sectors.                                         and makes reference to the cases described in\n                                                              this chapter.\n                                                                                                                   25\n                                    According to the European Enterprise Survey, at the beginning of 2020, 42 % of companies\n     Use of AI by                   in the EU said they use technologies that depend on AI. This percentage ranges from 27 %\n     companies in                   in Estonia and Cyprus to 61 % in Czechia (see Figure 1). Another 18 % of companies are\n     the EU in 2020                 planning to use AI in the future.\n                                    The survey indicates that AI is used mostly in the IT sector (63 %). The technologies used\n                                    comprise a variety of IT applications aiming at process or equipment optimisation, anomaly\n                                    detection, process automation, and forecasting, price optimisation and decision making.\n                                    FIGURE 1:       COMPANIES USING AI IN 2020, BY MEMBER STATE (%)\n                                     100\n                                     90\n                                     80\n                                      70           26\n                                                        20                               1\n                                                             18 21 27\n                                     60                                   11   12 19\n                                                                                     61      8    16 31 21 21\n                                            18                                                                23 14 25 22\n                                      50                                                                                                 9    22 19\n                                                                          54                 54                                                     16                  20\n                                                        51 50                  51                                                                      10 13\n                                                   48           46                                                                       48                  20 20 16\n                                      40                                            43\n                                                                                                  46                      44\n                                            42                       40                                     40 40                                      40\n                                      30                                                                             36             36              36\n                                                                                                                               33             34 35       35            34\n                                                                                                       31                                                          29\n                                      20                                                                                                                     27 27\n                                      10\n                                       0\n                                           EU-27\n                                                   RO\n                                                                                                                                                                        UK\n                                                   AT\n                                                   SE\n                                                    SI\n                                                   NL\n                                                   BG\n                                                   LU\n                                                   EL\n                                                   CZ\n                                                    LT\n                                                   BE\n                                                   MT\n                                                   LV\n                                                   DK\n                                                   HR\n                                                   DE\n                                                   HU\n                                                   FR\n                                                   PT\n                                                   PL\n                                                    IT\n                                                    FI\n                                                   ES\n                                                    IE\n                                                   EE\n                                                   CY\n                                                   SK\n                                                                               Currently using AI                   Planning to use AI\n                                    Notes:\t\u0007The survey asked about the use or plans for use of ten different AI related technologies, such\n                                            as speech recognition, visual diagnostics, fraud detection, analysis of emotions, forecasting\n                                            based on machine learning and more. Includes the percentage of companies using at least one\n                                            AI technologies. N = 9,640.\n                                    Source: FRA, 2020 [based on data extracted from European Commission, European enterprise\n                                            survey on the use of technologies based on artificial intelligence, Luxembourg, July\n                                            2020]\n     As noted, this report focuses on four broad AI ‘use cases’:\n     ――social benefits,\n     ――predictive policing,\n     ――health services, and\n     ――targeted advertising.\n     These areas are particularly sensitive as regards fundamental rights. Two                                                  “AI and machine learning are\n     cover mainly the public administration’s use of AI (social benefits allocation                                             different concepts. AI is an umbrella\n     and predictive policing). The other two concern private companies (health                                                  term.”\n     services and targeted advertising). These use cases provide the basis for                                                  (Private company, Estonia)\n     the report’s fundamental rights analysis by offering the necessary context.\n     Where appropriate, the report also highlights findings from interviews that\n     cover areas other than these four areas.\n     Detailed studies on the taxonomy of AI are available,1 providing further                                                   “What you see now is that\n     categorisations of the technology. As noted in the introduction, interviewees                                              everyone doing something with\n     had different views about what AI is and some also stated that there is no                                                 machine learning is labelling this\n     clear definition of AI.                                                                                                    as ‘AI’.”\n                                                                                                                                (Public administration, Netherlands)\n     This report discusses specific use cases without further classifying the\n     technology applied. Yet the use of AI in the cases examined differed: the\n26\n                                                            use of technology described by the interviewees involved both varying levels\n                                                            of complexity and varying levels of automation.\n                                                            Figure 2 provides an overview of different examples of use that interviewees\n                                                            discussed under the heading of AI. Some applications are relatively\n                                                            straightforward to understand. In rule-based decision making, algorithms\n                                                            are defined based on ‘if-then-rules’ (for example, if a person has an income\n                                                            below a certain threshold, then they will be eligible for certain benefits).\n                                                            Such algorithms were used in the area of social benefits at different levels\n                                                            of automation, with examples of full, partial or no human review involved.\n                                                            Other applications used more traditional statistical methods to inform\n                                                            decisions. These include, for example, regression analysis. This is a classical\n                                                            statistical method that analyses correlation between several pieces of\n                                                            information (‘variables’) and an outcome, which is a credit score in this\n                                                            example. Others used more complex machine learning methodologies to\n                                                            feed into the production of forecasts and statistics for government reports.\n                                                            There are also algorithms with much higher levels of complexity, such as\n                                                            deep learning for diagnosis support in the area of health. Such tools still\n                                                            include a high level of human review, and hence do not include a high level\n                                                            of automation.\n                                                            By contrast, targeted advertising is an example of potentially using highly\n                                                            complex algorithms without human review of each output and decision, also\n                                                            using highly complex algorithms including deep learning and reinforcement\n                                                            learning. (See Chapter 1 for descriptions of these terms.) Human review would\n                                                            also not be possible in this area due to the scale at which such algorithms\n                                                            operate.\nFIGURE 2:             EXAMPLES OF DIFFERENT AUTOMATION AND COMPLEXITY LEVELS IN USE CASES COVERED\nhigh          rule-based           regression analysis     deep learning and\n              automated            predictions for fully   reinforcement                            Areas of examples\n              decision making      automated credit        learning for\n              on social benefits   scoring                 advertising                               Social welfare\n              rule-based           regression analysis     facial recognition                        Marketing\nAutomation\n              decision for         predictions for         technology for\n              positive             human-reviewed          identification                            Law enforcement\n              outcomes of          credit scoring          with human\n              social benefits                              review                                    Health services\n                                                                                                     Financial services\n              rule-based           machine learning        medical images\n              human decisions      supported               analysis for\n              for social           production of           diagnosis\n              benefits             forecasts\nlow\n             low                        Complexity                                high\nSource: FRA, 2020\n                                                           AI systems also vary according to the potential harm that could result\nNotes: The examples from financial                          from an erroneous decision based on the use of AI. Depending on the area\nservices and the use of facial recognition                  of application, wrong decisions – based on erroneous outputs from the\ntechnology are not covered in the                           system – can have different impacts. When using AI for decision making,\ndetailed use case descriptions, but were                    the consequences are different if a decision is affirmative but wrong (false\nmentioned in other interviews. The\n                                                            positive) or negative but wrong (false negative).\nexamples illustrate different levels of\ncomplexity and automation, as used in\npractice.                                                   These issues are particularly important when machine learning is used, as\n                                                            it is based on statistical calculations, which always come with some degree\n                                                                                                                                              27\n   of error. While rule-based algorithms can also make mistakes (especially\n   if they grow more complex), risks are lower because of the deterministic\n   nature of the rules developed.\n   For example, when using AI to make decisions on social benefits, a false\n   positive means that a person may erroneously receive benefits. This does not\n   necessarily have a negative impact on the person concerned (unless the error\n   is found out later and the money needs to be\n   paid back). However, it negatively impacts on the\n   public administration, as money is paid not in line\n   with good administration practices. In contrast, a\n   false negative would have a negative impact on\n   the individual, because they would not receive\n   benefits to which they are entitled. Annex 2,\n   available on FRA’s website, provides further\n   hypothetical examples of effects of wrong\n   decisions based on the use cases discussed.\n   Importantly, when automating tasks, the impact\n   could also scale up, potentially exacerbating\n   the negative effect on society as a whole. The\n   severity and scale of potential harm is one aspect\n   that needs to be taken into consideration when\n   analysing potential limitations on fundamental\n   rights with respect to the use of AI.\n   For example, small error rates when using facial recognition technology\n   used by law enforcement might still lead to flagging many innocent people,\n   if the technology is used at places where many people are analysed. This\n   might apply to airports or train stations, where thousands of people could\n   be scanned on a daily basis.2 A potential bias in error rates could then lead\n   to disproportionally targeting certain groups in society.\n                                     Interviewees mostly mention ‘machine learning’, including the use of neural networks\n   Technologies                      and its extensions (see Chapter 1 for a description of machine learning). Respondents\n   used across                       either directly mentioned this, or mentioned subfields of machine learning, such as image\n   all cases                         recognition or facial recognition technology (FRT).\n   identified in\n   the research                      Most often, interviewees mentioned the use of ‘supervised machine learning’ as mainly\n                                     used to optimise for a specifically defined outcome. Yet sometimes ‘unsupervised machine\n                                     learning’ was also used to categorise or cluster data. Only one case referred to the use of\n                                     ‘reinforcement learning’, without going much into detail.\n                                     Several respondents used ‘natural language processing (NLP)’. This is a technology to\n                                     analyse text and speech, and is sometimes combined with machine learning algorithms.\n                                     Few mention examples that involve rule-based algorithms, meaning that the rules for the\n                                     algorithm to follow are directly encoded (i.e. based on ‘if-then-rules’).\n                                     In some cases, interviewees did not disclose or could not provide detailed information about\n                                     the technology used.\n   Generally, the interviewees referred to more than one use case, but were\n   asked to focus on one application during interviews.\n   ――Importantly, the fieldwork shows that companies and public administrations\n        are often still at the beginning of looking into the use of AI. Only about\n        two thirds of the use cases are actually in use and deployed in practice.\n        Many of the use cases described by interviewees are at pilot stage, under\n        development, or still in the research phase.\n   ――Two AI-driven applications were halted after tests.\n28\n                                      FIGURE 3:     WORDS INTERVIEWEES MOST OFTEN USED TO DESCRIBE THE AI\n                                                    ‘USE CASES’\n                                      Notes:\t\u0007FRA visualisation of the words most frequently used in descriptions\n                                                of use cases. The bigger the size of the word, the more often the\n                                                interviewees mentioned the terms.\n                                      Source: FRA, 2020\n                                      Figure 3 shows the most frequently used words to describe the use cases\n                                      covered in this report. It highlights the importance of data when using AI\n                                      systems as well as its relevance to supporting decision making.\n                                      FRA has previously highlighted that a thorough description of the data used by\n                                      AI applications is essential for identifying and mitigating potential fundamental\n                                      rights challenges.3 A variety of data were used for the AI systems covered\n                                      in this report. However, it was difficult to obtain detailed information about\n                                      the data used, because most respondents remained rather vague about\n                                      their data sources.\n                                      Rather generically, many respondents mentioned using ‘open data’, ‘historical\n                                      data’ or ‘metadata’. More concretely, respondents mentioned using customer\n                                      data, e.g. about purchases or browsing behaviour, or administrative records,\n“It is mostly used to save time […]\n                                      such as data on social benefits and taxes. Interviewees also mentioned\nwhen you have to go through a lot\n                                      medical records, police records, court records, as well as social media and\nof material.”\n                                      traffic data. Data included text data (e.g. e-mails), audio recordings, video,\n(Public administration, Netherlands)\n                                      and geolocation data. Data come from internal databases of companies and\n                                      public administration, but also from external sources.\n “The most important is to deal with\ncases more efficiently. It’s about    The single most important reason for using AI is increased efficiency. The\nmaking use of your workforce,         vast majority of respondents, across the public and private sector, mentioned\nthe people who handle cases, as       using AI for greater speed, fewer errors and cost reduction, as fewer human\neffectively as possible.”             resources are needed. Some interviewees from law enforcement also said\n (Public administration, Netherlands) they use AI for safety and security, as well as crime prevention.\n                                      Humans previously performed many of the use cases. Some respondents said\n                                      they use AI because it entails fewer mistakes than having humans carry out\n                                                                                                                        29\n   certain tasks. Some respondents also use AI for tasks that humans did not\n   previously carry out, as the quantity of information could not be processed by\n   humans – for example, in the area of genome analysis or traffic predictions.\n   Importantly, for about half of the respondents interviewed, the use of AI is\n   relevant for decision making. However, AI is mainly used to support decision\n   making, and the final decisions remain largely in the hands of humans.\n   Interviewees pointed out that, while enthusiastic, public administration and\n   companies are still cautious when deploying AI. Many of the use cases are\n   still in the testing phase. And some, as described below, were stopped during\n   this phase. Nevertheless, almost no interviewees were aware of any plans\n   to reduce the level of technology used. In fact, most expressed intentions to\n   invest in innovation or new ways to employ currently available AI systems.\n   2.1.\t EXAMPLES OF AI USE IN PUBLIC ADMINISTRATION\n   [Use case 1]\n   Automating social welfare systems – using algorithms in the area of\n   social benefits\n   Background and EU legal framework\n   The United Nations Special Rapporteur on extreme poverty and human rights,\n   Philip Alston, warned in his October 2019 report that introducing a ‘digital\n   welfare’ state, including the use of AI, can lead to a “digital welfare dystopia”.\n   Digitalisation of welfare systems is often accompanied with reductions of\n   overall welfare budgets, narrowing the beneficiary pool, and other measures\n   that reduce the availability of welfare. Digitalisation also increases the power\n   of states by offering opportunities to control people. This is particularly\n   worrying in countries with significant rule of law deficits.4\n   The use of algorithms by public administration in welfare raises major concerns\n   with respect to its potentially negative impact on poverty and inequality, if\n   applied erroneously in the area of social benefits.5 This includes areas such\n   as child welfare services6 and unemployment benefits.7\n   Yet public authorities are keen to use new technologies to make decision\n   making on social security and other benefits more efficient and potentially\n   fairer. Globally, new technologies are used in many ways to administer\n   welfare systems. These include identity verification, eligibility assessments,\n   benefit calculations, fraud prevention and detection, risk scoring and need\n   classification, as well as communication between authorities and beneficiaries.\n   The OECD defines social benefits as transfers made to households in need\n   after certain events or particular circumstances have arisen, including sickness,\n   unemployment, retirement, housing, education or family circumstances.8\n   However, there is no commonly agreed definition of social benefits. Social\n   benefits, in particular social insurance, systems are different from private\n   insurance schemes, as they involve compulsory contributions made by both\n   employees and employers, sometimes in the form of taxation.9\n   Social policy, including social security and social protection, is an area of shared\n   competence between the EU and the Member States (Article 4 (2) (b) of the\n   TFEU). Pursuant to Article 151 of the TFEU, the EU pursues the objectives,\n   among other things, to promote “improved living and working conditions”\n   and “proper social protection”. To this end, the EU supports and complements\n30\n                                    the activities of the Member States in a number of fields, including social\n                                    security and social protection of workers and combating social exclusion\n                                    (Article 153 (1) of the TFEU). EU actions can encourage cooperation between\n                                    Member States and adopt directives with minimum requirements. Moreover,\n                                    decisions on social security and social protection can only be adopted through\n                                    special legislative procedure by a unanimous vote in the Council.10\n                                    Against this backdrop, EU Member States are mostly free to shape their social\n                                    security and social protection policies. Since there is virtually no harmonisation,\n                                    social security systems differ significantly across the EU in terms of what\n                                    benefits are provided, conditions for eligibility, how benefits are calculated,\n                                    what contributions need to be paid and by whom, etc.\n                                    Public administrations in EU Member States are working on implementing AI\n                                    and related technologies in the area of public welfare. However, information\n                                    about its applications is limited. FRA collected information about use cases\n                                    linked to:\n                                    ――using algorithms when it comes to compensating job seekers,\n                                    ――processing social benefits applications, and\n                                    ――machine learning-supported data analysis on the use of pensions.\n           Several private insurance companies interviewed for this research use AI and related\nPrivate    technologies. This includes handling requests of customers for complementary health\ninsurance  insurance, insurance compensation decision support, evaluating the credit risk of\ncompanies’ individuals, insurance pricing, insurance claims management, and decision-making support\nuse of AI  related to management functions and credit decisions.\n           Private insurance companies generally embrace AI-related technologies, as these help make\n           their business more profitable. An OECD report highlights the importance of technology\n           for this sector. But it also argues that risk classification could lead to the exclusion of those\n           belonging to certain vulnerable groups in ways that are undesirable from a societal and\n           political perspective.*\n           * OECD (2020), The Impact of Big Data and Artificial Intelligence (AI) in the Insurance Sector.\n                                    Use in practice\n                                    The use cases outlined below exemplify some of the challenges when using\n                                    or planning to use AI in the area of social benefits, linked to algorithmic\n                                    decision making.\n                                    Experimenting with new technologies to support jobseekers\n                                    Over the course of a three-year project, a public organisation experimented\n                                    with several AI-related technologies concerning all of their work related to\n                                    processing benefits for job seekers and assisting them to return to work. The\n                                    representative interviewed states that the tested technologies can improve\n                                    and foster the relationship with job seekers and improve the advice given to\n                                    both job seekers and companies. After testing is completed, the organisation\n                                    will decide if and how it will apply these technologies in its day-to-day work.\n                                    Tests include machine learning-based detection of the attractiveness of\n                                    job offers and a system for detecting whether job seekers are still actively\n                                    looking for a job.\n                                    The tests are also looking into profiling job seekers to provide advice to them.\n                                    This would include calculating the probability of someone being offered an\n                                    available job within a given time, and identifying parameters that make job\n                                    offers relevant. This may also be reflected in advice to companies on best\n                                    practices for formulating job offers. The profiling would allow the organisation\n                                                                                                                        31\n   to determine appropriate services according to the profile and background\n   of the job seeker, rather than having an analysis and advice drawn up by\n   employees. Practically, this would be done by requiring job seekers to complete\n   a monthly diary on their job search. However, it is still under consideration\n   whether the programme should be limited to providing descriptive analyses, or\n   whether it should go further and provide recommendations. The organisation\n   is hesitant about the latter aspect.\n   Additionally, a natural language processing system is being tested for analysing\n   the content of job seekers’ e-mails. Here, e-mails are categorised, relevant\n   data is extracted, and the urgency and relevance of e-mails is identified.\n   Using a chatbot, and using automatic replies to emails, is being considered.\n   The data used for the systems come from several sources from within\n   the organisation. The data on job seekers and their background, including\n   personal tax data, as well as data on salaries and social security allowances,\n   are used under very strict conditions. This is because they are derived from\n   highly regulated data sources (e.g. salary statements cannot be accessed).\n   Other data, such as job offers from companies, are also used to generate\n   knowledge about the job market. The organisation currently does not use\n   external data, such as from (professional) social media networks, because\n   no legal provisions are in place for using such data.\n   Processing housing benefits – failure and success\n   A public body responsible for processing social benefits piloted an AI tool\n   to process applications and subsequently support their staff in making\n   decisions on housing benefits. The system selected cases from new benefit\n   applications that were relatively straightforward to calculate. These include\n   new applications for housing benefits submitted by an individual living alone\n   or with children, and by an individual who does not have any other income\n   than government benefits. Overall, these cases were deemed simple, with\n   the result always being that the individual receives the benefits.\n   The technological solution was based on a decision-tree model following the\n   rules for housing benefits. Calculating general housing benefits requires income\n   estimates in advance. The data used during the testing stemmed from an\n   internal database, which contains data on benefit application processes. The\n   data was pseudonymised as there was no need to use personal information.\n   A simple statistical model (linear regression) was used, where the input is\n   the income and the cost limits, and the output is the amount of benefit.\n   However, even in such simplified cases, they\n   found it too difficult to use AI in practice\n   because of the frequent changes in the\n   legislation. The test was terminated. According\n   to the interviewee, the lack of a legal basis for\n   using machine learning does not allow using\n   it for administrative decisions. There are no\n   further plans to use AI to support decision\n   making on social benefits.\n   While the organisation is not pursuing this\n   particular project due to the aforementioned\n   legal challenges, the interviewee noted\n   potential for further applications and solutions\n   in this area in the future. It was noted that\n   AI or related technologies that can support\n   operations without having a legal impact were\n   particularly good for the organisation.\n32\nThe SyRI case In the Netherlands, the so-called\n                                                                  At the same time, the organisation is using\n                                                                  image processing for social benefits applications.\n              ‘System Risk Indication’ (SyRI)* was                Generally, benefit applicants have to complete\n              developed as a government tool to                   several forms and attachments, which are often\n              alert the Dutch public administration               submitted in paper format. For more efficient\n              about fraud risk of citizens, by                    and time-saving handling of those documents\n              processing and linking large amounts                by the agency’s staff, the hard copies received\n              of their personal data from public\n                                                                  are scanned and then classified by an automated\n              authorities.\n                                                                  system.\n              A broad coalition of civil society\n              organisations dealing with privacy                  A first step is to turn images the right way\n              issues initiated a lawsuit, prompting               round. Algorithms re-align documents that\n              the District Court of The Hague to                  were not aligned properly when they were\n              scrutinise the algorithm-based SyRI.**              scanned, remove spots and clean up and\n                                                                  edit the colouring of the document, identify\n              The court ruled that SyRI impinges\n              disproportionately on the private                   columns, paragraphs, tables, and other elements\n              life of citizens. The court found that              as distinctive blocks, recognise the script, etc.\n              everyone whose data was analysed                    Then, the application checks if the received\n              by SyRI was exposed to this risk.                   application form and attachment are marked\n              In addition, due to the opacity of                  correctly (e.g. if a document is marked as an\n              the algorithm used, citizens could                  invoice, the system determines whether this\n              “neither anticipate the intrusion into              is correct).\n              their private life nor can they guard\n              themselves against it.” ***                         The turning and the classification of the images\n              *A good description of SyRI can be                  are done by image recognition and Optical\n              found in Ilja Braun (2018), High risk               Character Recognition (OCR) technologies. They\n              citizens, in: Algorithm Watch.                      recognise text stemming from images, including\n                                                                  from photographs and scans of documents\n              ** The ruling of 5 February 2020 (in                or handwritten notes. OCR technology then\n              Dutch) is available online.                         converts the recognised text into text data that is\n              *** Privacy First (2020), Dutch                     machine-readable. Here, in a pattern recognition\n              risk profiling system SyRI banned                   process, input from the scanned images is\n              following court decision.                           first isolated, then compared to ‘glyphs’ (i.e.\n                                                                  variations of letters) stored by the system on\n                                                                  a pixel to pixel basis.\n                                   The agency will continue processing images and further develop it, for\n                                   example, by potentially making it possible to scan bar codes from attachments.\n                                   This would help to speed up the confirmation of the correctness of documents\n                                   and attachments. There will also be more solutions related to natural language\n                                   processing.\n                                   Automating unemployment benefits\n                                   In one of the countries selected, most decisions on unemployment benefits\n                                   are fully automated. The national institution responsible for unemployment\n                                   insurance benefits updated its system in 2019 to fully automate most of the\n                                   processing of benefit applications and decisions. This was done after the\n                                   relevant legislation was adapted to allow automated decisions.\n                                   If a person registers as unemployed and lodges an application for benefits,\n                                   the system draws on information about the applicant from various other\n                                   databases. This includes, for example, the population register, and tax\n                                   authorities’ databases containing information about salaries and work\n                                   experience, etc. If all conditions for receiving unemployment benefits are\n                                   fulfilled, the system calculates the period of payments, based on the length\n                                   the person has contributed to the insurance system, and the amount of\n                                   benefits, based on the average daily salary.\n                                                                                                                      33\n   The procedure is fully automated. However, an employee of the institution\n   must intervene if necessary information cannot be extracted from the\n   databases, if there is contradictory information in the databases or if the\n   decision on a case involves a level of discretion (i.e. the decision cannot be\n   definitively determined based on the data available and a human has some\n   leeway in deciding on the case).\n   The main reason for using this system is improved efficiency. In addition, the\n   system is believed to achieve consistency in the processes. This is because\n   every application, not subject to discretion, is handled in the same way.\n   [Use case 2]\n   Predictive policing – trying to anticipate crime in advance\n                                                                                     FRA ACTIVITY\n   Background and EU legal framework\n   AI technologies are used in law enforcement, particularly in predictive policing.\n                                                                                     Preventing\n   Existing research into how such tools can affect fundamental rights has           unlawful profiling\n   highlighted particular issues concerning discrimination, among other rights.\n   One recurrent concern is the potential for predictive policing to reproduce\n                                                                                     today and in the\n   and entrench existing discriminatory practices, particularly through reliance     future: a guide\n   on historical crime data that may be biased or incomplete. This is because\n                                                                                     In developing and using algorithmic\n   many crimes – such as domestic violence or hate crime – remain largely\n                                                                                     profiling, bias may be introduced at\n   unreported and therefore are under-counted in official police statistics.11\n                                                                                     each step of the process. To avoid this\n                                                                                     and subsequent potential violations\n   A focus on certain crimes, such as violence and drug-related crime in public\n                                                                                     of fundamental rights, both IT experts\n   places – rather than on business fraud and non-payment of taxes, for example –\n                                                                                     and officers interpreting the data\n   can also make law enforcement responses less equitable.12 This is because the\n                                                                                     should have a clear understanding of\n   former are often associated with certain demographics and neighbourhoods.\n                                                                                     fundamental rights.\n   Ultimately, this can undermine police relations with particular communities.\n                                                                                     This FRA guide explains what profiling\n   Criminological research on crime ‘hotspots’ has been around for several           is, the legal frameworks that regulate\n   decades – notably in the UK and USA.13 It uses police data to map certain         it, and why conducting profiling\n   crimes and undertakes statistical tests to explore crime probabilities. Various   lawfully is both necessary to comply\n   police forces have used and developed them to address different types of          with fundamental rights and crucial\n   crime concentrations or clusters (‘hotspots’).                                    for effective policing and border\n                                                                                     management.\n   More recently, adaptations of this area of applied research have used AI as a\n                                                                                     For more information, see FRA (2018),\n   tool to enhance its effectiveness, with some suggesting that using algorithmic\n                                                                                     Preventing unlawful profiling today\n   tools could reduce the police’s reliance on subjective human judgments\n                                                                                     and in the future: a guide.\n   that may reflect biases or stereotypes.14 Some studies have also indicated\n   that predictive policing could potentially reduce unnecessary surveillance,\n   questioning, and physical checks and searches,15 reducing the humiliation\n   and harassment of individuals that may occur during these activities.\n   Predictive policing aims to forecast the probability of crime and anticipate\n   emerging trends and patterns to inform crime prevention and intervention\n   strategies.16 It may also be a part of an investigation into a crime that has\n   already taken place. While there is no authoritative definition of predictive\n   policing,17 it is typically characterised by analysing data to identify common\n   patterns and trends in crime by using algorithms to create models based\n   on the analysis. This is used to forecast criminal activity that may occur in\n   the future.\n   AI technologies in this area generally either aim to ‘predict’ crimes or to\n   ‘predict’ which individuals will either commit or be victims of crimes. Tools\n   aiming to predict crimes are generally fed with historical data – largely from\n   official sources – on the time, place and type of crimes committed. This can\n   be complemented by environmental variables, such as population density,\n34\n                                        presence of certain public places or services, and major events or holidays.\n                                        They generally do not use personal data when applied.18\nFRA ACTIVITY                            In contrast, AI systems focused on predicting potential perpetrators or victims\nFacial recognition                      of crime employ both historical and real-time personal data. This could include\n                                        criminal records data, addresses, phone numbers, location data, data extracted\ntechnology                              from social media, information about known associates and health or income\non the rise:                            data. This is then combined with other criminal and environmental data.19\nfundamental rights                      The EU and its Member States have shared competence in the area of freedom,\nconsiderations in                       security, and justice (Article 4 (2) ( j) of the TFEU). This includes judicial\n                                        cooperation in criminal matters and police cooperation (Articles 82-89 of\nlaw enforcement                         the TFEU). Already when the Treaty of Lisbon was adopted, an annexed\n                                        declaration on the protection of personal data in judicial cooperation in\nEU law recognises as ‘sensitive data’\n                                        criminal matters and police cooperation observed that “specific rules on\npeople’s facial images, which are a\n                                        the protection of personal data and the free movement of such data in the\nform of biometric data if processed by\n                                        fields of […] police cooperation based on Article 16 of the [TFEU] […] prove\nfacial recognition software. But such\n                                        necessary because of the specific nature of these fields.”20\nimages are also quite easy to capture\nin public places. Although the accuracy\n                                        Within the framework of predictive policing, the collection, storage, processing,\nof matches is improving, the risk of\n                                        analysis and exchange of information is particularly relevant. The processing\nerrors remains real – particularly for\n                                        of personal data in the context of law enforcement operations is regulated at\ncertain minority groups. People whose\n                                        EU level by the Law Enforcement Directive (Directive (EU) 2016/680). 21 It sets\nimages are captured and processed\n                                        out comprehensive standards and safeguards for such processing, including\nmight not know this is happening\n                                        the safeguarding against and the prevention of threats to public security.\n– and so cannot challenge possible\nmisuses.\n                                        Use in practice\nThe FRA paper outlines and analyses\n                                        The use cases collected by FRA signal the variety of ways in which law\nthese and other fundamental rights\n                                        enforcement authorities already use, or plan to use, AI and related technologies\nchallenges that are triggered when\n                                        to support their work.\npublic authorities deploy live FRT for\nlaw enforcement purposes. It also\n                                        Examples mentioned by interviewees range from data mining systems\nbriefly presents steps to take to help\n                                        designed to map crime patterns, detecting online hate speech and making\navoid rights violations.\n                                        risk assessments on gender-based violence, to automating certain prison\nFor more information, see FRA (2019),   guard duties. Other use cases include detecting illicit objects from satellite\nFacial recognition technology:          images, and, more generally, recognising objects in images. In addition, a tool\nfundamental rights considerations in    was mentioned in the research used in the private sector for fraud prevention\nthe context of law enforcement.         and crime detection in money transfers.\n                                        Interviewees emphasised that AI or related technology systems are used to\n                                        automate and speed up tasks previously done by humans, thus freeing up\n                                        and/or better distributing resources.\n                                        Mapping crime to support the efficient allocation of investigation capacity\n                                        A national intelligence agency and public prosecutor’s office employ a data-\n                                        driven system to help their employees make choices on how, where and\n                                        when to use the available investigation capacity. The aim is to improve the\n                                        allocation of human resources, ensuring that officers can be present at the\n                                        right time and place.\n                                        The interviewees suggest that this system could make more precise\n                                        assessments compared to humans, who often rely on their gut feeling for\n                                        decisions. Still, the system is always used in combination with human appraisal\n                                        and other non-AI systems to make operational decisions.\n                                        Based on system-generated outcomes, analysts create a ‘heat map’. This\n                                        outlines the prevalence of certain crimes in certain areas. This replicates a\n                                        long-standing manual version of this crime anticipation system, whereby\n                                                                                                                          35\n   police officers put pins on a map to indicate specific risk areas. Using AI to\n   increase the speed of this process also makes it more reliable, users believe,\n   because it can analyse more data.\n   The system is based on data mining and machine learning processes. It is\n   primarily built on unique police data contained in crime reports, witness\n   statements, and suspect declarations. Gaps are, to the extent possible,\n   addressed by using other data sources, such as criminology research, and\n   social and demographic information obtained from the national office of\n   statistics. The system also uses data from open sources.\n   The specific parameters for calculation depend on the type of crime, as\n   predictive factors vary in relevance across crime areas. For example, in the\n   case of burglaries, data on burglaries is collected and combined with data\n   on the place of residence of known criminals and their distance to burgled         FRA ACTIVITY\n   houses. The relevant criteria are preselected to allow the system to produce\n   the heat map.                                                                      Detecting hate\n   Location-based predictions are made for the next six months, and indicate the\n                                                                                      speech online\n   time and location where a burglary may occur. The result is a map of small         A public agency combatting hate\n   squares where the risk of crime occurring is indicated in different shades.        crime uses an AI-based tool to detect\n   The interviewees indicated that this visualisation helps officers to analyse       online hate speech by analysing\n   neighbourhoods and observe correlations between different locations.               patterns of speech online. On the\n                                                                                      basis of the processing, the system\n   Assessing the risk of gender-based domestic violence                               determines which social groups are\n                                                                                      targeted. This helps law enforcement\n   A national police force uses an internal system to track cases of gender-\n                                                                                      adopt measures to protect them\n   based domestic violence. The system helps police officers take decisions and\n                                                                                      before threats are realised.\n   distribute resources across domestic violence cases. The system categorises\n   cases on the basis of the assessed risk of relapse and repetition, in order to     Although the tool aims to identify\n   focus on the ‘riskiest’ cases.                                                     potential victims, rather than\n                                                                                      perpetrators, law enforcement can\n   A specialist team could complete the risk analysis without using AI. However,      use the information generated by the\n   the system is able to compute a large amount of data in a short amount of          system to ask social media providers\n   time and assist untrained or non-specialist police officers in risk analysis.      for information on users to pursue\n                                                                                      criminal investigations.\n   When a case of alleged gender-based domestic violence is reported, the police\n                                                                                      One particular challenge is\n   officer starts an initial investigation. This includes collecting evidence, taking\n                                                                                      understanding the context in which\n   witness statements and – potentially – making an arrest. Using information\n                                                                                      statements are made. For example,\n   gathered from this process, the officer fills out two detailed questionnaires\n                                                                                      journalists or academics may use\n   to assess the complaints, evaluate the probability of reoffending, examine\n                                                                                      words associated with hate speech to\n   the evolution of the case and assess the behaviour of the perpetrator and\n                                                                                      report on or analyse its occurrence.\n   the victim. Police officers also indicate the level of gravity, the nature of\n   threats faced and attitudes concerning the victim.                                 In 2021, FRA plans to initiate research\n                                                                                      on online hate present on social\n   The system then produces a risk ‘score’ on a three point scale. The police         media. This will allow FRA to provide\n   officer can raise the level of risk manually, but cannot lower the risk level      input to policy developments in the\n   below that indicated by the system. Once the level is confirmed, specific          area of online content moderation,\n   measures are applied in line with established police protocols. The system         which uses AI.\n   also informs a judge about potentially ‘severe cases’ through an automated\n   system.\n36\n2.2.\t EXAMPLES OF AI USE IN THE PRIVATE SECTOR\n[Use case 3]\nAI and health – analysing medical records to save lives\nBackground and EU legal framework\nHealthcare is particularly prominent in discussions about the use of AI. Medical\ndata and online applications have the potential to support improved health\noutcomes and – as a result – wider socio-economic benefits. The COVID-19\npandemic has further increased focus and interest in the area, particularly\nin terms of the potential for (online) data and applications to enhance the\nability of governments and health services to track the spread of disease.\nHealth is also prominent in the general population’s views on uses of AI. A\n2019 Eurobarometer survey found that every second European thinks that\nAI can be best used to improve medical diagnostics, develop personalised\nmedicine, or improve surgery.22\nThis use case covers applications of AI or related technologies by public\nand private sector stakeholders in the area of medical records and disease\nprediction. Feeding data from electronic medical records (EMR) and electronic\nhealth records (EHR) into AI systems and related technologies can support\nthe development of preventative medicine that recognises early risks of\ndisease and designs appropriate interventions. Researchers can predict\nclinical events such as mortality, hospitalisation, readmissions and length\nof stay in the hospital.\nBeyond disease prediction, medical record data can be analysed to predict\npatients’ adherence to treatment and their keeping of medical appointments.\nThese technologies have the potential to support improved health outcomes,\nas well as increase the efficiency of the healthcare system.\nUnder Article 6 of the TFEU, the EU has supporting competence in protecting\nand improving human health. Member States retain full responsibility for\ndefining their health policies, organising and managing their health systems,\nand for delivering health services (Article 168 (7) of the TFEU).\nWithin the EU competence, Union action, which has to complement national\npolicies, is directed towards improving public health, preventing physical and\nmental illness and diseases, and obviating sources of danger to physical and\n                                                                                 37\n   mental health. Such action can cover health information and education, as\n   well as monitoring, early warning of and combating serious cross-border\n   threats to health (Article 168 (1) of the TFEU). In the latter areas, the EU can\n   adopt incentive measures, excluding any harmonisation of the laws and\n   regulations of the Member States.\n   Other rules and policies adopted at the EU level aim to ensure free movement\n   of citizens, their equal treatment and non-discrimination abroad, as well as\n   availability and safety of medical products and services in the single market.\n   Considering the development of technologies and their application in health\n   care, exchange of medical records, patients’ rights in cross-border situations\n   and disease prediction as a matter of public health are particularly relevant.\n   Under the GDPR, health and genetic data are considered as a special category\n   of data (Article 9) called ‘sensitive data’.23 These require specific protection as\n   their processing could create significant risks. Data subjects’ health and genetic\n   data can only be shared in specific circumstances under Article 9 (2) of the\n   GDPR. The GDPR provides an exemption to the purpose limitation principle if\n   data are used for research purposes, in line with its Article 89 (1). Researchers\n   are required to ensure that technical and organisational safeguards – such\n   as pseudonymisation and anonymity – are in place when using patient data.\n   The EU has also taken action regarding the exchange of medical records.\n   European Commission Recommendation C(2019)800 on a European Electronic\n   Health Record exchange format24 “seeks to facilitate the cross-border\n   interoperability of EHRs in the EU by supporting Members States in their\n   efforts to ensure that citizens can securely access and exchange their health\n   data wherever they are in the EU.”25 The recommendation lays out technical\n   specifications for the exchange of such data between EU Member States.\n   The European Data Strategy (February 2020) also has a strong focus on health\n   data.26 A ‘Common European health data space’ is one of the nine common\n   European data spaces whose establishment the European Commission will\n   support.\n   The Early Warning and Response System (EWRS) is owned by the European\n   Commission and operated by the European Centre for Disease Prevention\n   and Control. It aims at “notifying at EU level on serious cross-border threats\n   to health”27 and enabling “the European Commission and EU countries to be\n   in permanent communication for the purposes of alerting, assessing public\n   health risks and determining the measures that may be required to protect\n   public health.”28\n   EMR, which is a computerised medical record created for patients of a\n   healthcare organisation,29 and EHR, which contains a patient’s medical history\n   beyond one organisation and involve sharing data across the healthcare\n   system, can include a large amount of personal data. This can encompass,\n   among others: the name and contact details of the individual and their next\n   of kin; demographic information, diagnoses and test results; and medication\n   and treatment.30 They may also include patient-generated data from wearable\n   devices.31\n   There is no uniform EMR/EHR system operating across all EU Member States.32\n   Some, such as Germany, do not have a national EMR/EHR system. Others –\n   including Belgium and Denmark – have different EMR/EHR systems at the\n   regional level. The systems differ considerably depending on what data is\n   recorded and by whom and who has access to what data.33 The European\n   Commission and other stakeholders have highlighted the diversity of country-\n38\nlevel EMR/EHR systems and their lack of interoperability as a major barrier\nto the digital single market in health.34\nStudies highlight the potential for AI or related technologies to enable earlier\ndiagnosis, widen possibilities for disease prevention and improve patient\nsafety,35 strengthening the right to access preventive healthcare and benefit\nfrom medical treatment. EMR/EHR may also help to make healthcare more\npersonalised,36 while the possibility for rapid sharing of data can facilitate\nmore coordinated and timely treatment.\nHowever, use of EMR/EHR presents significant data protection risks. The\nhealthcare sector leads in terms of personal data breaches.37 The amount of\nthe personal data stored, the highest among all industries, combined with\nthe large data-sharing network and number of access points, makes the\nhealthcare sector an attractive target for hackers.38\nThe quality of data in EMR/EHR also raises some concern. Studies where\npatients were shown their medical files and asked about their accuracy\nfound that up to 50 % of information was incomplete or erroneous.39 A lot\nof important data in EMR/EHR is unstructured in the form of free text, which\nfurther reduces data quality.40 Low levels of accuracy, completeness and\noverall data quality increases the risk of medical error.41\nUse in practice\nThe applications described in the interviews include both simple and more\nadvanced models employed in the public and private sectors. The largest\nnumber of use cases refer to image-based diagnosis tools. However,\ninterviewees also discussed tools to automate various working procedures,\nsuch as the mapping of text data, filing of medical records, and analyses and\nmeasurements of body tissues and nerve fibres.\nA smaller number of examples touched on more advanced projects, such as\nsystems to monitor remotely certain health indicators, such as heart rate. In\neach case, systems complement the expertise of health professionals. The\nnext sections present examples of diagnostic and remote monitoring tools.\nImage-based tools to help detect and diagnose disease\nThe tools used to support the detection and diagnosis of diseases described\nby interviewees work in similar ways. For example, a privately owned hospital\nuses an AI system to interpret images from CT-scans of stroke patients. After\na stroke, imaging is used to detect where damage to the brain has occurred\nand where there may be blockages in the blood supply to the brain. It can\nalso generate measures that can be compared to particular values by a\nmedical specialist.\nThe interviewee feels that the application helps to determine such\ncharacteristics in images more quickly, potentially – depending on who uses\nthe tool – improving the quality of the diagnosis. However, they highlight\nthat it is not necessarily more efficient to rely on the AI application, since a\nmedical professional must be present and they could examine the image.\nRather, the tool can offer some support – for example, if the specialist finds\nit difficult to interpret a certain image or find abnormalities.\nThe system was built, trained and validated using a dataset partially based on a\nlarge scientific study to which the hospital contributed. This was supplemented\nby purchasing foreign datasets. The algorithm will not be further trained or\nadapted in the future based on new data. No new versions will be released.\n                                                                                 39\n   The developers feel that allowing the system\n   to continue to learn would make it difficult to\n                                                       Using AI to                A public authority responsible for\n   validate its operation.\n                                                       target health              inspecting food safety standards in\n                                                                                  restaurants uses machine learning to\n   A private company developed an algorithm\n   that supports the detection of breast cancer\n                                                       inspections                process customer review data from\n                                                                                  major online platforms. This helps to\n   from mammography exams. The tool gives a                                       decide where and when to conduct\n                                                                                  inspections. Previously, this process\n   probability and degree of certainty, which can\n                                                                                  was based on complaints the\n   help radiologists to speed up their analysis\n                                                                                  authority received and on previous\n   of the results and decide whether additional                                   reports. Since the introduction of\n   tests are warranted. The algorithm detects and                                 the tool, the rate of non-compliant\n   characterises anomalies in a mammography                                       restaurants identified doubled from\n   as cancerous or not.                                                           around 18 % to 36 %.\n   While the interviewee indicates that the                                       The first step involves text mining.\n                                                                                  The algorithm identifies reviews\n   system now has a very low rate of false\n                                                                                  containing key words that may\n   negatives or false positives, they note that in\n                                                                                  indicate health and safety issues,\n   many cases it does not deliver a clear outcome.                                such as ‘sick, ‘nausea’ or ‘rodents’.\n   The system was trained on radiography and                                      For the second step, the authority\n   mammography data from Europe and the EU,                                       compared results coming from\n   with written reports and past biopsies acting                                  customer reviews with previous\n   as control data.                                                               inspection reports to improve the\n                                                                                  algorithm’s accuracy and reliability.\n   Monitoring patients’ vital statistics remotely\n   A hospital is piloting a system to support\n   early detection of potential illness. Monitoring\n   patients’ health indicators – for example, blood pressure or heart rate –\n   typically takes place manually and captures the situation at a specific moment\n   in time. Constantly monitoring such indicators has the potential to identify\n   trends that doctors may otherwise not recognise and detect health issues\n   early to prevent illness. The system uses a biosensor – a kind of plaster –\n   which gathers hemodynamic data from patients continuously by constantly\n   monitoring heart pulsation and respiration.\n   The data used by the system come from the hospital and the patient. These\n   data are anonymised before being shared with the third-party provider.\n   No other information besides that gathered through the monitoring of the\n   plaster is used to build and train the system. Data on environmental factors\n   were not incorporated in the pilot because, the interviewee pointed out,\n   they could contain biases.\n   In the future, the system will combine the information gathered by the\n   biosensor with separate information from patients’ EMR to draw conclusions\n   from trends observed in the monitoring.\n40\n[Use case 4]\nTargeted advertising – profiling consumers to boost profit\nBackground and EU legal framework\nThe internet has transformed the way we live. Many people make use of\ninternet services, often offered for free, on a daily basis. Companies offering\ntheir services for free mainly generate revenue through advertising, with\nadverts automatically targeted to individual consumers based on information\nabout them.\nThe availability of data about online individual behaviour combined with\nmachine learning technologies have considerably improved the ability of\ncommercial enterprises to target individuals. This could even go as far as\nmanipulating consumers by predicting their reactions based on irrational\naspects of psychology and not reasoned choice.42\n                                   The Cambridge Analytica scandal underscored\n                                   the particularly negative impact of such uses\n                                   for political purposes. In that case, a company\n                                   illegally obtained personal data on millions of\n                                   social media users to target political adverts\n                                   to different social groups based on certain\n                                   psychological profiles.43\n                                   A recent declaration of the Committee\n                                   of Ministers of the Council of Europe\n                                   highlights the lack of knowledge about the\n                                   manipulative power of algorithms. “The\n                                   effects of the targeted use of constantly\n                                   expanding volumes of aggregated data on\n                                   the exercise of human rights in a broader\n                                   sense, significantly beyond the current\n                                   notions of personal data protection and\n                                   privacy, remain understudied and require\n                                   serious consideration.”44 Concerns have also\nbeen raised about how online advertising, powered by AI technologies, can\naffect data protection and privacy,45 consumer protection,46 the right to non-\ndiscrimination,47 and even the way democracies work.48\nThe word ‘advertising’ is associated with messages designed to influence\nconsumer behaviour. Advertising in one form or another has always targeted\nspecific groups based on their characteristics and behaviour.49\nThe growth of social media, however, has taken targeted advertising to\nanother level, using direct access to consumer data. Micro-targeting is directed\ntowards very specific groups – and the more data that is gathered through\nonline activities, the more targeted these activities can be. As social media\nproviders and platforms like Google or Amazon gather comprehensive user\ndata by monitoring the various activities of their users, advertisers can access\nmore detailed and specific information.50\nThe area of targeted advertising and systems that recommend content (e.g.\nnews or movies) is one of the few real life examples that also involves so-\ncalled reinforcement learning. This is a technology that is based on optimising\na certain goal through experimenting and updating its rules automatically\nto have the best possible output. This means a systems tries out different\nad placements through trial and error and so finds the best way to optimise\nrevenue – including an element of self-learning.\n                                                                                   41\n   While very little knowledge on the actual use of reinforcement learning is\n   available for European countries, major companies working in the area are\n   researching the issue.51\n   Issues related to targeted advertising fall under consumer protection. This\n   falls under shared EU competence with Member States under Article 4 (2) (f)\n   of the TFEU. EU consumer protection measures seek to protect the health,\n   safety and economic interests of consumers; and promote their right to\n   information, education and to organise themselves to safeguard their interests\n   (Article 169 (1) of the TFEU). The EU can adopt minimum harmonisation\n   measures to achieve a high level of consumer protection (Article 114 (3) of\n   the TFEU), yet allowing EU Member States to introduce even more stringent\n   measures nationally.\n   In secondary EU legislation, rules on advertising are covered by\n   Directive 2006/114/EC concerning misleading and comparative advertising.52\n   This directive provides a minimum level of protection from misleading\n   advertising. It also harmonises rules on comparative advertising across the\n   EU. The provisions of Directive 2006/114/EC apply to both consumer-to-\n   business and business-to-business relations. However, they are practically\n   only applied to the latter53 after Directive 2005/29/EC on unfair business-to-\n   consumer commercial practices in the internal market practices54 took effect.\n   Further, Directive 2006/123/EC on services in the internal market55 covers\n   services that include advertising. Additionally, Directive 2000/31/EC on\n   certain legal aspects of information society services, in particular electronic\n   commerce, in the internal market (E-Commerce Directive) also applies. This\n   directive forms part of the legal framework for digital services in the EU.\n   To meet significant developments in the area of new online services and\n   practices, the E-Commerce Directive is currently being revised as part of the\n   Digital Services Act package. That package aims to “strengthen the Single\n   Market for digital services and foster innovation and competitiveness of the\n   European online environment”.56\n   FRA collected information about actual use cases from six European companies\n   engaged in placing online ads, content recommendation, and personalised\n   marketing.\n   Use in practice\n   The examples covered include:\n   ――placing ads online based on click predictions (i.e. learning about the\n        likelihood that online users click on certain links or adverts) and automated\n        bidding at auctions for online advertisement space\n   ――personalised and targeted marketing and communication via email.\n   Most tasks were fully automated. The examples concern analyses of user\n   preferences and activity and calculations of probabilities of clicks and\n   purchases, including a measurement of the effectiveness of previously made\n   recommendations. This also includes methods of targeted communication\n   on the basis of identified target groups to build (long term) trust between\n   clients and service providers.\n   Targeted online ads based on click predictions\n   Business models working with click predictions and targeted advertisements\n   often follow a ‘click and buy’ policy. Companies purchase advertising space\n   on media platforms, and optimise the display of adverts by analysing the\n   interests and preferences of website users and showing them advertisements\n42\nthat interest them. The purpose is to increase the relevance of advertisements\nshown by better matching them to the interests of those who see them.\nIn the present example, the company only gets paid if people click on an\nadvertisement and buy something. Additionally, the company uses AI to\ndetect inappropriate content in advertising, such as advertisements for\nalcohol, firearms or political content.\nThe company uses a range of machine learning techniques in the field of\ncomputational advertising. To estimate the probability of a user clicking on\nan advertisement displayed in a specific context (optimising the so-called\nclick-through-rate), customers’ interests and the relevance of products are\nmeasured via a mapping of individuals’ browsing histories and transaction\npatterns. Further, information is derived from individuals’ navigation on\nmerchant websites worldwide, with whom the advertising company works.\nThis is done via anonymised third party cookies and trackers.These are placed\non these merchant websites and outline individuals’ navigation across them,\nand also list the products seen and purchased.\nThe profiles of individuals are linked to devices used by them, although\nIP addresses are anonymised. Once a product has been purchased, a\nrecommender system algorithm tries to determine other products that\nthe customer could also buy. In this case, ‘fresh’ data is valued higher than\nolder data. Browsing histories are stored for a maximum of one year, as\ninterests change and purchases older than a year are no longer necessarily\nconsidered relevant.\nAdvertisements shown to the respective person are immediately adapted\naccordingly, and they vary across websites, to also match the content of\nthe latter. Once an advertisement is posted, it is continuously analysed.\nThe combination of elements taken into account on an individual’s interest\nis confirmed when a purchase is made. Data is shared across platforms,\nwhich includes informing others once a purchase has been made, to stop\nadvertisements of that particular item. If no purchase is made, the formula\nis reviewed and the algorithm is further adapted on individuals’ continuous\nonline behaviour.\nIn the future, the company covered in this example expects to work more\non optimising its timing in terms of when it places advertisements, within\ntheir given budget for a certain time frame. It also expects to focus more on\ndisplayed ads that have an impact on consumers.\nAnother example is based on a European online market place, which links\nbuyers and sellers on a range of specialised products. Here, AI is used\nto optimise advertising campaigns, to categorise products based on the\nadvertisements that are shown on the website of the market place, to\nimprove the search engine experience by predicting complementary and\nsubstitutable products, and to detect fraud attempts.\nThe company uses machine learning to predict the value of clicks of customers\nto buy advertisement space, which is offered in real-time auctions. With\nthese examples, the company indicates that AI enables it to make decisions\nthat otherwise would not be possible without AI, or which would have to be\nsignificantly scaled down.\nTargeted communication with customers and clients\nIn the case of a retail company focusing on specialised supplies sold across\nphysical stores and online, direct marketing or personalised advertising is\n                                                                               43\n   used to increase appeal to customers, and at the same time measure the\n   efficiency of a particular instance of marketing or advertising.\n   According to the company at issue in this example, marketing emails are\n   opened at an average of 20-30 %, and particularly so when customers\n   recognise relevant and favourite products being offered. Marketing emails\n   are sent to around 250,000 registered individuals, and a system is used to\n   establish what may be considered relevant by each of these individuals.\n   This is done by analysing purchases made by the respective individuals in\n   the previous six months. 80 % of the offers displayed are directly based on\n   previous purchases. Meanwhile, 20 % are new suggestions, i.e. alternative\n   products in the same category as the previous purchases.\n   A similar approach is used by a bank that sends emails to clients. Messages\n   offering specific services or products are sent only to certain clients. Data\n   analysts calculate the probability of clients being interested in a service or\n   product. If this probability is above a certain threshold, the client will receive\n   the message. The system used does not yet include machine learning models\n   and is not fully automated. These points will be taken on when they further\n   develop the system.\n   In a third example, a grocery retailer uses loyalty cards both to increase the\n   customers’ interaction and to personalise offers. Loyalty card systems can\n   predict how many customers are likely to engage with a product offering.\n   The system covered in this example also suggests new products to customers\n   and tracks the results of these suggestions. It groups buyers with similar\n   behavioural patterns into segments to make more personalised suggestions.\n   Every week, the company’s loyalty card owners receive personalised offers\n   by email, website or mobile application, and they can access offers through\n   in-store terminals. The AI system selects the offerings based on the individual\n   purchase history, and it recommends new items that might catch the buyer’s\n   interest and prompt a purchase.\n44\nEndnotes\n1  See, for instance, Samoili et al. (2020), AI Watch. Defining Artificial Intelligence. Towards an operational definition and taxonomy\n   of artificial intelligence, Luxembourg; Karanasiou A. and Pinotsis D. (2017), ‘A study into the layers of automated decision-making:\n   emergent normative and legal aspects of deep learning’, International Review of Law, Computers & Technology, 2017, pp. 170-187.\n2  See FRA (2019), Facial recognition technology: fundamental rights considerations in the context of law enforcement, Luxembourg,\n   Publications Office, p. 9 and 22.\n3  FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications\n   Office, June 2019.\n4  UN, Human Rights Council (2019), Report of the Special rapporteur on extreme poverty and human rights, Philip Alston, A/74/48037.\n5  Eubanks, V. (2018), Automating Inequality. How hightech tools profile, police, and punish the poor, St. Martin’s Press.\n6  Redden, Joanna, Dencik, Lina and Warne, Harry (2020), Datafied child welfare services: unpacking politics, economics and power, Policy\n   Studies.\n7  Panoptykon Foundation (2015), Profiling the unemployed in poland: Social and political implications of algorithmic decision making; see\n   also Algorithm Watch (2019), Poland: Government to scrap controversial unemployment scoring system.\n8  OECD, Glossary of Statistical Terms - Social Benefits Definition, accessed 5 August 2020.\n9  J. Henry Richardson, CHAPTER IV, SOCIAL INSURANCE, Economic and Financial Aspects of Social Security (University of Toronto Press, 1960).\n   Pieters, Social Security.\n10 For an overview of the EU competence in this domain and Regulation (EC) No. 883/2004, see Paju, J. (2017), The European Union and\n   Social Security Law, Oxford, Hart Publishing, Ch. 2.\n11 Erik Bakke (2018), ‘Predictive policing: The argument for public transparency’, New York University Annual Survey of American Law, Vol.\n   74, pp. 139-140; Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, Washington University Law Review, Vol. 94, pp. 1146-1150. For\n   example, only one in five women who experienced violence brought the most serious incident to the attention of the police. See FRA\n   (2014), Violence against women: an EU-wide survey. Main results report, Luxembourg, Publications Office, p. 61.\n12 Elizabeth E. Joh (2015), The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing, UC Davis Legal Studies Research\n   Paper No. 473, p.18.\n13 Braga A., et al (2019), Hot spots policing of small geographic areas effects on crime, Campbell Systematic Reviews, Vol. 15 (3).\n14 Elizabeth E. Joh (2015). The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing, UC Davis Legal Studies Research\n   Paper No. 473, pp. 17-18. Available at: SSRN.\n15 Erik Bakke (2018), ‘Predictive policing: The argument for public transparency’, New York University Annual Survey of American Law, Vol.\n   74, pp. 137-138.\n16 Wim Hardyns and Anneleen Rummens (2017), ‘Predictive Policing as a New Tool for Law Enforcement? Recent Developments and\n   Challenges’, Eur J Crim Policy Res, p. 3, DOI: 10.1007/s10610-017-9361-2.\n17 Albert Meijer & Martijn Wessels (2019), ‘Predictive Policing: Review of Benefits and Drawbacks’, International Journal of Public\n   Administration 42:12, p. 1032, DOI: 10.1080/01900692.2019.1575664.\n18 The Law Society Commission on the Use of Algorithm in the Justice System (2019), Algorithms in the criminal justice system, p. 36.\n19 Newbold, J. (N.D.), ‘Predictive Policing’, ‘Preventative Policing’ or ‘Intelligence Led Policing’. What is the future?\n20 Declaration No. 21 Annexed to the Final Act of the Intergovernmental Conference which adopted the Treaty of Lisbon, signed on 13\n   December 2007.\n21 Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard\n   to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution\n   of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework\n   Decision 2008/977/JHA, OJ L 119, 4.5.2016, pp. 89-131.\n22 European Commission (2019), Standard Eurobarometer 92, Report, Europeans and Artificial Intelligence, p. 10.\n23 European Patients Forum (n.d.), The new EU Regulation on the protection of personal data: what does it mean for patients? A guide for\n   patients and patients’ organisations.\n24 Commission Recommendation (EU) 2019/243 of 6 February 2019 on a European Electronic Health Record exchange format, OJ L 39,\n   11.2.2019, pp. 18-27.\n25 Digital Health Society, Exchange of electronic health records across the EU, 19 February 2020.\n26 Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the\n   Committee of the Regions, A European Strategy for data, COM(2020)66 final, Brussels, 19 February 2020.\n27 Decision No. 1082/2013/EU of the European Parliament and of the Council of 22 October 2013 on serious cross-border threats to health\n   and repealing Decision No. 2119/98/EC, OJ L 293, 5.11.2013, pp. 1-15.\n28 See Commission webpage on Communicable diseases.\n29 OECD and European Union (2018), Healthcare at a glance: Europe 2018, p. 192.\n30 Vera Ehrenstein, Hadi Kharrazi, Harold Lehmann and Casey Overby Taylor (2019), ‘Obtaining Data From Electronic Health Records’, in:\n   Gliklich RE, Leavy MB, Dreyer NA (eds.), Tools and Technologies for Registry Interoperability, Registries for Evaluating Patient Outcomes:\n   A User’s Guide, 3rd ed., Addendum 2.\n31 On the use of these data in the insurance industry currently and their potential see, for example, A. Spender, C. Bullen, L. Altmann-Richer,\n   J. Cripps, R. Duffy, C. Falkous, M. Farrell, T. Horn, J. Wigzell and W. Yeap (2019), ‘Wearables and the internet of things: considerations for\n   the life and health insurance industry’, British Actuarial Journal 24:22, pp. 1-31.\n32 See WHO visualisation.\n33 See a short overview of different EHR systems in Europe from nurses’ perspective in HealthEurope (2019), The world of cloud-based\n   services: storing health data in the cloud.\n34 College of Europe (2018), Transformation Health and Care in the Digital Single Market. Synopsis report of the public consultation.\n35 European Commission (2016), Study on Big Data in public health, telemedine and healthcare; Roberta Pastorino, Corrado De Vito,\n   Giuseppe Migliara, Katrin Glocker, Ilona Binenbaum, Walter Ricciardi, Stefania Boccia (2019), Benefits and challenges of Big Data in\n   healthcare: an overview of the European initiatives, European Journal of Public Health, Vol. 29, Issue Supplement 3, pp. 23–27.\n36 Ministry of Health, Welfare and Sport of The Netherlands (2016), Digitalization in health care and benefits for patient safety: Literature\n   and web reports (2015-2016).\n37 This is according to multiple reports by different cybersecurity companies and over time. See, for example, SC Magazine (2019),\n   Healthcare leads in cost of data breaches; Shannon Williams (2020), New report reveals ‘wall of shame’ in health care data breaches;\n   Tammy Lovell (2019), Statistics reveal healthcare is the sector most affected by personal data breaches.\n                                                                                                                                                  45\n   38 SC Magazine (2019), Healthcare leads in cost of data breaches.\n   39 Annet Sollie (2016), Reuse and Sharing of Electronic Health Record Data with a focus on Primary Care and Disease Coding, Doctoral\n      dissertation at the Vrije Univesiteit Amsterdam, pp. 28-30.\n   40 Vera Ehrenstein, Hadi Kharrazi, Harold Lehmann and Casey Overby Taylor (2019), ‘Obtaining Data From Electronic Health Records’, in:\n      Gliklich RE, Leavy MB, Dreyer NA (eds.), Tools and Technologies for Registry Interoperability, Registries for Evaluating Patient Outcomes:\n      A User’s Guide, 3rd ed., Addendum 2.\n   41 Mowafa Househ, Bakheet Aldosari, Abdullah Alanazi Show, Andre Kushniruk and Elizabeth M Borycki (2017), ‘Big Data, Big Problems: A\n      Healthcare Perspective’, Studies in health technology and informatics 238, p. 38.\n   42 Sartor, Giovanni (2020), New aspects and challenges in consumer protection, study for the committee on the Internal Market and\n      Consumer Protection, Policy Department for Economic, Scientific and Quality of Life Policies, European Parliament, Luxembourg.\n   43 Neudert, Lisa and Marchal Nahema (2019), Polarisation and the use of technology in in political campaigns and communication, study\n      at the request of the Panel for the Future of Science and Technology (STOA) and managed by the Scientific Foresight Unit, within the\n      Directorate-General for Parliamentary Research Services (EPRS) of the Secretariat of the European Parliament; Information Commissioner’s\n      Office (ICO) (2018), Investigation into the use of data analytics in political campaigns.\n   44 Council of Europe (2019), Declaration by the Committee of Ministers on the manipulative capabilities of algorithmic processes,\n      Decl(13/02/2019)1.\n   45 For example, Costello, Róisín Áine (2020), The Impacts of AdTech on Privacy Rights and the Rule of Law, Technology and Regulation,\n      11–23; EDPS (2018), Opinion 3/2018, EDPS Opinion on online manipulation and personal data.\n   46 Sartor, Giovanni (2020), New aspects and challenges in consumer protection; Jabłonowska, Agnieszka et al. (2018), ‘Consumer law and\n      artificial intelligence. Challenges to the EU consumer law and policy stemming from the business’ use of artificial intelligence’ EUI\n      Working Papers, LAW 2018/11.\n   47 Wachter, Sandra (2020), ‘Affinity Profiling and Discrimination by Association in Online Behavioural Advertising’, Berkeley Technology Law\n      Journal, Vol. 35, No. 2, 2020, (forthcoming), available at SSRN.\n   48 Zuboff, Shoshana (2018), The Age of Surveillance Capitalism, London; EDPS (2018), Opinion 3/2018, EDPS Opinion on online manipulation\n      and personal data.\n   49 Martin, Gillian (2011), The importance of marketing segmentation, American Journal of Business Education, Vol. 4, No. 6.\n   50 Kaili Lambe and Becca Ricks (2020),The basics on microtargeting and political ads on Facebook.\n   51 See for example, information on the RecSys2020 Workshop on REVEAL 2020: Bandit and Reinforcement Learning from User Interactions\n      (accessed on 7 August 2020).\n   52 Directive 2006/114/EC of the European Parliament and of the Council of 12 December 2006 concerning misleading and comparative\n      advertising, OJ L 376, 27.12.2006, pp. 21-27.\n   53 European Commission, Misleading and comparative advertising directive: Objective of the directive.\n   54 Directive 2005/29/EC of the European Parliament and of the Council of 11 May 2005 concerning unfair business-to-consumer commercial\n      practices in the internal market and amending Council Directive 84/450/EEC, Directives 97/7/EC, 98/27/EC and 2002/65/EC of the\n      European Parliament and of the Council and Regulation (EC) No 2006/2004 of the European Parliament and of the Council (‘Unfair\n      Commercial Practices Directive’), OJ L 149, 11.6.2005, pp. 22-39.\n   55 Directive 2006/114/EC of the European Parliament and of the Council of 12 December 2006 concerning misleading and comparative\n      advertising, OJ L 376, 27.12.2006.\n   56 See the European Commission’s webpage on The Digital Services Act package.\n46\n3\nFUNDAMENTAL RIGHTS FRAMEWORK\nAPPLICABLE TO AI\n              The use of AI – as presented in the four use cases discussed in Chapter 2 – can\n              affect specific fundamental rights (as outlined in Chapter 4). Full compliance\n              with fundamental rights is a prerequisite for using AI-driven technologies,\n              irrespective of the area concerned.\n              This chapter introduces the general fundamental rights framework in the\n              EU that governs the use of AI, including selected secondary EU legislation\n              and national law (Section 3.1). This fundamental rights framework provides\n              the normative basis and benchmarks for the design, development and\n              deployment of AI tools.1 It helps determine whether or not a specific use of AI\n              is fundamental rights compliant. The requirements for justified interferences\n              with fundamental rights are outlined in Section 3.3.\n              3.1.\t FUNDAMENTAL RIGHTS FRAMEWORK GOVERNING\n                      THE USE OF AI\n              The cornerstone instrument of the EU fundamental rights framework applicable\n              to the use of AI is the Charter. Together with the unwritten general principles\n              of EU law, it is the main source of fundamental rights in the EU. The Charter\n              enshrines a wide array of fundamental rights and has the same legal value\n              as the EU Treaties. All EU institutions and bodies are bound by the Charter, as\n              are Member States when they act within the scope of EU law (Article 51 (1)\n              of the Charter).2\n                                                                                              47\n   Many Charter rights are the same as those set out in the European Convention\n   on Human Rights (ECHR).3 Their meaning and scope must be the same as\n   the corresponding ECHR rights (Article 52 (3) of the Charter). However, this\n   cannot prevent Union law from providing more extensive protection.\n   Fundamental rights can also be found in provisions of the Treaties (see e.g.\n   Article 6 (2) of the TEU and Titles V and X of the TFEU), and in EU secondary\n   law.4 These rights are further safeguarded in different pieces of secondary\n   EU law.\n   A central piece of EU secondary law in the context of AI is the General\n   Data Protection Regulation (GDPR – Regulation (EU) 2016/679).5 It governs\n   automated processing of personal data in the European Economic Area and\n   processing of personal data by any other means which form part of a filing\n   system – within the scope of EU law. (As a result, the GDPR does not apply\n   to national security-related data processing.)\n   The GDPR is coupled with the Law Enforcement Directive, which applies to\n   police and judicial cooperation in criminal matters. Both EU instruments include\n   numerous provisions on the protection of personal data, determining the key\n   principles of data processing, such as lawfulness, fairness and transparency.6\n   Whether EU data protection legislation applies depends on whether personal\n   data are processed. Some AI-driven applications do not use personal data\n   (for example, traffic data). Others use anonymised data. In these cases, data\n   protection laws do not apply, or their applicability is not entirely clear.7 The line\n   between personal and non-personal data is blurred, because there is some\n   risk that anonymised data can be ‘re-identified’ – ie, the anonymisation can\n   be undone. However, re-identification is usually illegal. In addition, persons\n   re-identifying the data usually have to put in major efforts and potentially need\n   access to additional information about individuals who might be included in\n   an anonymised dataset for re-identification. Section 4.2 discusses the topic in\n   more detail, linked to the results of the interviews carried out for this report.\n   In addition to the EU data protection acquis, European non-discrimination law\n   is key for safeguarding fundamental rights in the context of the use of AI and\n   related technologies. Article 2 of the TEU provides that non-discrimination is\n   one of the fundamental values of the EU, and Article 10 of the TFEU requires\n   the Union to combat discrimination on a number of grounds. Moreover,\n   Articles 20 and 21 of the Charter provide for equality before the law and\n   non-discrimination.\n   Beyond this, several EU non-discrimination directives enshrine more specific\n   and detailed provisions. They have varying scopes of application.8 These\n   include the Employment Equality Directive (2000/78/EC),9 the Racial Equality\n   Directive (2000/43/EC),10 the Gender Goods and Services Directive (2004/113/\n   EC),11 and the recast Gender Equality Directive (2006/54/EC).12\n   EU Member States are also party to other international human rights\n   conventions (see the list of conventions in the Key Findings and FRA opinions\n   section). These contain legally binding standards and safeguards to comply\n   with when they act in areas that do not fall within the scope of EU competence.\n   The main such instrument is the ECHR, ratified by all EU Member States. It is\n   accompanied by additional protocols, to which a great majority of EU Member\n   States are parties. The ECHR has a wide reach: it also applies to areas not\n   covered by EU law.\n   In addition, the Council of Europe Convention for the Protection of Individuals\n   with regard to Automatic Processing of Personal Data13 is another source of\n48\npan-European data protection obligations binding on all EU Member States.\nIt was recently modernised.14\nSector-specific EU and national legislation also enshrines safeguards for\nthe protection of fundamental rights. An overview of such more technical\nlegislation is beyond the scope of this report. However, this chapter provides\na few examples relevant to the use cases discussed in the report. This is\ncomplemented by a couple of examples of national laws from the five EU\nMember States covered.\nNone of the five EU Member States covered currently have horizontal AI-\nspecific laws, although the countries are looking into the potential need for\nregulation. Some EU countries, such as Finland, issued recommendations\nfor self-regulation and the development of responsibility standards for the\nprivate sector.15 In Estonia, an assessment concluded that a separate AI-\nspecific law will not be required in the foreseeable future, since the current\nlegal framework is sufficient.16 According to the relevant Estonian long-\nterm strategy, however, the legal environment must be adapted to avoid\nunnecessary hindrances to implementing AI.17\nThe situation concerning sectoral legislation relevant to the use of AI in different\nsectors varies across EU Member States. However, active policymaking on AI\nhas recently emerged at the national level. National action plans on AI have\nappeared and remain the core policy development in Member States. Some\ncountries are working on growing entrepreneurship.18 Others are focused on\nenacting market-oriented policies compatible with the UN 2030 Agenda for\nSustainable Development.19 Educational activities to promote AI and increasing\npublic use of AI are often identified as AI-related strategy goals. Investment\nin research and development is also frequently outlined as a relevant goal.20\nWhile domestic AI discussions on potential legislative reforms remain attentive\nto European initiatives, national, sector-specific fundamental rights safeguards\nare also being enacted. For instance, Finland began considering an overhaul of\ndomestic human rights safeguards in the public sector by proposing a broader,\nacross-the-board legislative update as opposed to individual AI laws.\nIn specific reference to the processing of personal data under immigration\nlaw, the Finnish Constitutional Law Committee has put forward a proposal to\nstrengthen the safeguards of the Finnish Constitution, overriding constitutional\nlaw shortcomings in relation to, among others, protection under the law,\naccountability, as well the ambiguity of algorithms in automated decision\nmaking. Whenever public authorities automate their decision-making\nprocesses, these processes must adhere to the constitutional principle of rule\nof law, and may not endanger the observance of rules on good administration\nand due process.21 This proposal articulated a vision on what requirements\nthe Finnish Constitution sets for AI use and automated decision making within\npublic administration.\nThe research identified other initiatives and policies linked to AI and\nfundamental rights in the five Member States examined. For example, the\nEstonian e-State charter includes a summary of citizens’ rights for better\ncommunicating with agencies electronically. It also targets AI in relation to\nthe right to know what data is collected by public authorities.22\nSimilarly, the Ministry of the Interior of the Netherlands presented a policy\nbrief to parliament on AI, public values and fundamental rights.23 The brief\nstresses a human-centric approach, where AI-applications have a strong\ninfluence on human beings or on society as a whole. It also lists the most\nimportant risks of AI for fundamental rights, such as discrimination as a result\n                                                                                     49\n   of biased data, or reduced interpersonal relations if AI takes over certain\n   forms of interaction.\n   3.2.\t ‘USE CASE’ EXAMPLES\n   Social welfare (Use case 1)\n   When regulating social welfare, EU Member States enacted rules aiming to\n   protect fundamental rights specifically in this area in addition to existing\n   horizontal EU regulations (see Section 2.1). These mostly define rules for\n   the processing and protection of personal data for the purpose of social\n   benefits and insurance.\n   In Estonia, for example, the Insurance Activities Act, applicable to all types\n   and forms of insurance, regulates the processing and transmission of personal\n   data in this context. It states that public authorities, health care providers,\n   insurance undertakings and other third parties may transmit personal data\n   at the request of an insurance undertaking if the personal health or court\n   data are necessary for the insurance undertaking to perform an insurance\n   contract or if the right and obligation to disclose such data derives from law.\n   The scope of this Act also includes data transfers for the purpose of data\n   processing within AI systems.\n   The Social Welfare Act contains more specific provisions on data protection\n   of persons in need of social assistance. They have to be notified of the\n   processing of their data and should provide consent for further processing.\n   Any person in the established target group has the right to opt out of data\n   processing. The Social Welfare Act also allows local authorities to process\n   (including using algorithms) personal data of youth between 16 and 26 years\n   of age stored in state registries to identify the youth not in employment,\n   education or training.\n   In Finland, Act No. 552/2019 on Secondary Use of Health and Social Data\n   applies to using AI in social care and healthcare. This Act is based on the norms\n   for securing and protecting sensitive personal data as outlined in the GDPR. It\n   aims to establish conditions for the effective and secure “processing of, and\n   access to, personal health and social data for certain secondary purposes,\n   such as research and statistics, innovation and development, knowledge\n   management, teaching and authority planning.”24 The Act regulates the\n   manner in which registered health data can and cannot be processed.\n   Several other laws apply to various types of social benefits. In France, the\n   2015 Code of relations between the public and the administration applies\n   for the purpose of processing or accessing personal data related to social\n   benefits with minor amendments after the entry into force of the GDPR.\n   This code states “that algorithms used by public administrations must be\n   published” and “the person subject to automated decision making has a right\n   to be informed”.25\n   Predictive policing (Use case 2)\n   In the context of predictive policing, the EU’s Law Enforcement Directive\n   contains key fundamental rights safeguards. These stipulate how law\n   enforcement authorities should apply some of the main data protection\n   principles set out in the GDPR.26 These include the requirement for data\n   controllers (i.e. the competent law enforcement authorities) to provide data\n   subjects with information on the controller’s data processing activities, such\n   as the identity and contact details of the data controller, the purposes of the\n   processing and information about the right to lodge a complaint (Article 13).\n50\nIn specific cases, data controllers shall provide further information – for\nexample, the legal basis for processing – to enable data subjects to exercise\ntheir rights. The right of access (Article 14) requires the data controller to\nconfirm, upon request of the data subject, whether there are processing\noperations related to them. If this is the case, the data subject shall be able\nto access this data and also to request additional information, including the\npurposes and legal basis of the processing and the categories of personal\ndata processed. Both the right to information and the right to access can be\nrestricted in a number of cases, including to avoid obstructing or prejudicing\nthe prevention, detection, investigation or prosecution of criminal offences;\nor to protect public security and national security.27\nIn addition, Article 11 of the Law Enforcement Directive explicitly prohibits\nautomated decision making.28 This prohibition is limited if authorised by\nEU or national law which safeguards the data subject’s rights, including “at\nleast the right to obtain human intervention on the part of the controller”\n(for more, see Section 4.2).\nIn some cases, the scope of implementing national legislation is broader than\nthe directive. For example, the Finnish Act on the Processing of Personal Data\nin Criminal Matters and in Connection with Maintaining National Security\nstrengthens the right to information by not distinguishing between the\ninformation provided in general and in special circumstances.29\nHealthcare (Use case 3)\nAs regards EU-level fundamental rights safeguards when using AI in healthcare,\nthe GDPR empowers patients with rights to be informed, in part by granting\nthem more control of their personal health data. Such data qualifies as\n‘sensitive data’30, as found, for example, in their medical records.31 The rights\ninclude the rights to access one’s own personal (health) data; to object to\nthe processing of own personal data; rectification and erasure of data, as\nwell as rights in case of breach..32\nUnder the GDPR, administrative fines for breaches of processing data, including\nhealth data, are not allowed. However, in Estonia, for instance, domestic\nlaw allows for a maximum penalty of EUR 400,000 in application of the\nmisdemeanour procedure in such cases. The Data Protection Inspectorate\ncan also impose similar fines in the misdemeanour procedure.33\nIn France, the Data Protection Act and the Public Health Code impose stricter\nrequirements than those set out in the GDPR regarding health data processing.\nThe French Data Protection Act has been amended through the Law for the\nModernisation of the Health System, to allow for the processing of personal\nhealth data for various purposes, provided they fall within the scope of one\nof the exceptions to the general principle of prohibition of sensitive data\nprocessing under Article 9 of the GDPR.34\nTargeted advertising (Use case 4)\nWhen considering fundamental rights safeguards in relation to targeted\nadvertising and the underlying mechanisms regarding profiling in particular,\nthe EU legal framework on privacy and data protection provides the most\nrelevant fundamental rights provisions. The protection of privacy and personal\ndata holds a status that takes precedence over economic benefits. Hence,\nrules on processing of (special categories of) personal data are relevant for\ncompanies operating in the area of or applying targeted advertising in that\nthey place companies under certain obligations.\n                                                                                  51\n   The main legal provisions setting out rules on protecting personal data in the\n   EU are the GDPR and the Directive on privacy and electronic communications\n   (e-Privacy Directive), which is a lex specialis to the GDPR. The GDPR is directly\n   applicable in all EU Member States whenever a company is based in the EU\n   and processes personal data, and if a company is based outside of the EU,\n   but processes data relating to individuals in the EU.\n   The e-Privacy Directive, with a strong focus on fundamental rights, concerns\n   the processing of personal data and the protection of privacy in the electronic\n   communications sector (e.g. when individuals use their computer, smartphone\n   or tablet). In 2017, the European Commission proposed an e-Privacy Regulation,\n   which would replace the current e-Privacy Directive.35 The legislative proposal\n   would broaden the scope of the directive, and include specific provisions\n   concerning unsolicited marketing, cookies and confidentiality.\n   3.3.\t REQUIREMENTS FOR JUSTIFIED INTERFERENCES\n            WITH FUNDAMENTAL RIGHTS\n   Chapter 4 highlights selected fundamental rights – as covered by the Charter –\n   that are particularly affected by AI, taking into account the four use cases\n   discussed in Chapter 2. Most of these rights are not absolute rights, so can\n   be subject to limitations in line with Article 52 (1) of the Charter. Accordingly,\n   before analysing to what extent the different fundamental rights are impacted\n   by the use of AI, this section presents the general steps that need to be\n   followed to determine whether or not a Charter right can be limited.\n   Fundamental rights affected by AI that are not absolute can be subject to\n   limitations. Interferences with such fundamental rights can only be justified\n   if they respect the requirements of the Charter and of the ECHR, in case of\n   Charter rights corresponding to rights guaranteed in the ECHR (Article 52 (3)\n   of the Charter).36\n   Pursuant to Article 52 (1) of the Charter, any limitation on fundamental\n   rights must:\n   ――be provided for by law,\n   ――genuinely meet objectives of general interest recognised by the Union\n        or the need to protect the rights and freedoms of others,\n   ――respect the essence of the right,\n   ――be necessary, and\n   ――be proportionate.37\n   The Court of Justice of the EU (CJEU) has also emphasised that any limitation\n   on the exercise of the rights and freedoms recognised by in the Charter\n   must respect “the essence” of those rights and freedoms.38 This means that\n   fundamental rights can be limited to a certain extent, but not completely\n   disregarded.\n   Once it has been established that the inalienable, essential core of a right\n   is not violated by a measure, the next step is to conduct the necessity and\n   proportionality test outlined in the Charter in respect of non-core aspects of\n   that right.39 Any interference with a Charter right needs to be examined as\n   to whether the given legitimate aim could not be obtained by other means\n   that interfere less with the right guaranteed.40 Similar requirements are\n   also imposed by the ECHR, as interpreted by the European Court of Human\n   Rights (ECtHR).41 These include the ‘essence of a right’ concept, which can be\n   derived from the object and purpose of the ECHR as a whole.42 In respect to\n   the use of new technologies, the ECtHR observed in S. and Marper v. the UK\n   that States should “strike a right balance” between protecting fundamental\n   rights and developing new technologies.43\n52\nGiven the wide range of applications of AI systems in everyday life as\npresented in the four selected use cases, a wide range fundamental rights may\nhave to be assessed, taking into account a variety of elements, depending on\nthe context and the particular area of use. Most notably, the specific purpose\nfor which AI is used, its functionality, complexity, and the scale at which it\nis deployed, are relevant for assessing fundamental rights implications.44\n                                                                               53\n   Endnotes\n   1  See also van Veen, C. (2018), ‘Artificial Intelligence; What’s Human Rights Got to Do with It?’ Data & Society: Points – blog of Data\n      & Society Research Institute, 14 May 2018; Barfield, W. & Pagallo, U. (2020), Advanced Introduction to Law and Artificial Intelligence,\n      Cheltenham/Northhampton, MA, Edward Elgar, 2020, pp. 19-20.\n   2  See also CJEU, Åklagaren v. Hans Åkerberg Fransson [GC], 26 February 2013, paras. 17, 20.\n   3  European Convention for the Protection of Human Rights and Fundamental Freedoms, as amended by Protocols Nos. 11 and 14, 4\n      November 1950, ETS 5.\n   4  For an overview of the application of the Charter, see FRA (2018a), Applying the Charter of Fundamental Rights of the European Union in\n      law and policy making at national level, Luxembourg, Publications Office.\n   5  Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard\n      to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection\n      Regulation), OJ L 119, 4.5.2016, pp. 1-88.\n   6  For more, see FRA (2018), Handbook on European Data Protection Law. 2018 Edition, Luxembourg, Publications Office.\n   7  See for example, Hacker, P. (2020), A Legal Framework for AI Training Data. Law, Innovation and Technology (forthcoming), available at\n      SSRN.\n   8  For an overview of European non-discrimination law, see FRA (2018), Handbook on European non-discrimination law. 2018 Edition,\n      Luxembourg, Publications Office.\n   9  Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation,\n      OJ L 303, 2.12.2000, pp. 16-22.\n   10 Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or\n      ethnic origin, OJ L 180, 19.7.2000, pp. 22-26.\n   11 Council Directive 2004/113/EC of 13 December 2004 implementing the principle of equal treatment between men and women in the\n      access to and supply of goods and services, OJ L 373, 21.12.2004, pp. 37-43.\n   12 Directive 2006/54/EC of the European Parliament and of the Council of 5 July 2006 on the implementation of the principle of equal\n      opportunities and equal treatment of men and women in matters of employment and occupation (recast), OJ L 204, 26.7.2006, pp. 23-36.\n   13 Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data, Strasbourg, 28 January 1981 (ETS No.\n      108).\n   14 Protocol amending the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data, Strasbourg, 10\n      October 2018 (CETS No. 223).\n   15 The AI Finland Project’s ethics working group and the Ethics Challenge added emphasis on companies and self-regulation. AI Finland,\n      ‘Etiikkahaaste (Ethics Challenge)’, Tekoäly on uusi sähkö (in Finnish).\n   16 Republic of Estonia (2019), Report of Estonia’s AI Taskforce, p. 38.\n   17 The Estonian Government launched the preparation for a long-term strategy.\n   18 For example, see the Netherlands, Ministry of Economic Affairs and Climate Policy (2019), Strategic Action on AI Strategic Action Plan AI\n      (Strategisch Actieplan AI – SAPAI).\n   19 For an example of an effort to adapt goals to the development of a sustainable market, see Spain, Ministry of Science, Innovation and\n      Universities (2019), National AI Strategy (in Spanish).\n   20 For a more comprehensive overview, see European Commission (2019), National strategies on Artificial Intelligence; or the OECD AI\n      policy observatory.\n   21 Finnish Constitutional Law Committee (2019), ‘Committee Opinion PeVL 7/2019 Vp ─ HE 18/2019 vp: Draft Proposal to Parliament for the\n      Law on the Processing of Personal Data in the Immigration Administration and for Related Laws’.\n   22 Estonia, National Audit Office and Chancellor of Justice (2018), Everyone’s Rights in e-State: The e-State Charter.\n   23 Netherlands, Ministry of the Interior and Kingdom Relations (2019) AI, public values ​​and fundamental rights (in Dutch).\n   24 Elina Saxlin-Hautamäki and Johanna Lilja (2019), Secondary use of health data – the new Finnish Act.\n   25 de Donno, M. (2017), The French Code “Des Relations Entre Le Public Et L’Administration”. A New European Era For Administrative\n      Procedure?, Italian Journal of Public Law 2, pp. 220-260.\n   26 See FRA (2018), Preventing unlawful profiling today and in the future: a guide, Luxembourg, Publications Office, Tables 2 and 4.\n   27 Sajfert, J. and Quintel, T. (2017), Data Protection Directive (EU) 2016/680 for Police and Criminal Justice Authorities, available at SSRN.\n   28 Note that Article 11 of the Law Enforcement Directive seems to apply to automated decisions taken solely through automated processing.\n      This means that this safeguard will not apply if human agency is involved. Orla Lynskey (2019), Criminal justice profiling and EU data\n      protection law: Precarious protection from predictive policing, p. 21.\n   29 The English translation is available via the Finlex website.\n   30 GDPR, recital (10) and Art. 9 (1).\n   31 European Patients Forum (n.d.), The new EU Regulation on the protection of personal data: what does it mean for patients? A guide for\n      patients and patients’ organisations.\n   32 GDPR, Arts. 15-17, 20-21 and 34.\n   33 White&Case (2019), GDPR Guide to National Implementation: Estonia.\n   34 Merav Griguer (2019), Processing health data in France: What to look out for after GDPR?\n   35 European Commission, Proposal for a regulation of the European Parliament and of the Council concerning the respect for private life\n      and the protection of personal data in electronic communications and repealing Directive 2002/58/EC (Regulation on Privacy and\n      Electronic Communications), COM(2017) 10 final, Brussels, 10.1.2017.\n   36 Charter, Art. 52 (3): “In so far as this Charter contains rights which correspond to rights guaranteed by the Convention for the Protection\n      of Human Rights and Fundamental Freedoms, the meaning and scope of those rights shall be the same as those laid down by the said\n      Convention.”\n   37 As also reiterated and explained by the CJEU. See, for example, C-73/07, Satakunnan Markkinapörssi and Satamedia, 16 December 2008,\n      para. 56; Joined cases C-92/09 and C-93/09, Volker und Markus Schecke and Eifert GbR and Hartmut Eifert, 9 November 2010, para.\n      77; Joined cases C-293/12 and C-594/12, Digital Rights Ireland Ltd v. Minister for Communications, Marine and Natural Resources and\n      Others and Kärntner Landesregierung and Others, 8 April 2014, para. 52; C-362/14, Maximillian Schrems v. Data Protection Commissioner,\n      6 October 2015, para. 92; and C-419/14, WebMindLicenses Kft. v. Nemzeti Adó-es Vámhivatal Kiemelt Adó- és Vám Főigazgatóság,\n      17 December 2015, paras. 69 and 80-82.\n   38 See CJEU, C-362/14, Maximillian Schrems v. Data Protection Commissioner, 6 October 2015, paras. 94-95, which refer to Article 52 (3) of\n      the Charter. See also Scheinin, Martin and Sorell, Tom (2015), SURVEILLE Deliverable D4.10 – Synthesis report from WP4, merging the\n54\n   ethics and law analysis and discussing their outcomes, 7 April 2015, p. 9.\n39 See e.g. Brkan, M. (2019), ‘The Essence of the Fundamental Rights to Privacy and Data Protection: Finding the Way Through the Maze\n   of the CJEU’s Constitutional Reasoning’, German Law Journal 20 (2019), p. 867; Lenaerts, K. (2019), ‘Limits on Limitations: The Essence of\n   Fundamental Rights in the EU’, German Law Journal 20 (2019), pp. 779-794.\n40 CJEU, Joined cases C-293/12 and C-594/12, Digital Rights Ireland Ltd v. Minister for Communications, Marine and Natural Resources and\n   Others and Kärntner Landesregierung and Others, 8 April 2014.\n41 See, for instance, Khelili v. Switzerland, No. 16188/07, 18 October 2011; ECtHR, S. and Marper v. the United Kingdom [GC], Nos. 30562/04\n   and 30566/04, 4 December 2008; ECtHR, K & T v. Finland, No. 25702/94, 12 July 2001; ECtHR, Z v. Finland, No. 22009/93, 25 February\n   1997; ECtHR, Huvig v. France, No. 11105/84, 24 April 1990; ECtHR, Leander v. Sweden, No. 9248/81, 26 March 1987.\n42 Scheinin, Martin and Sorell, Tom (2015), SURVEILLE Deliverable D4.10 – Synthesis report from WP4, merging the ethics and law analysis\n   and discussing their outcomes, 7 April 2015, p. 9.\n43 ECtHR, S. and Marper v. the United Kingdom [GC], Nos. 30562/04 and 30566/04, 4 December 2008, para. 112.\n44 See also Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights\n   impacts of algorithmic systems, Appendix, para. A.8.\n                                                                                                                                              55\n564\nIMPACT OF CURRENT USE OF AI ON\nSELECTED FUNDAMENTAL RIGHTS\n             Deploying AI systems engages a wide range of fundamental rights. As\n             seen in Chapter 2, the use cases presented in this report involve a range\n             of technologies of varying levels of complexity and automation. They are\n             in different phases of development and applied in different contexts, for\n             different purposes and at different scale. While the rights affected depend\n             on these factors, a number of horizontal and sector-specific fundamental\n             issues emerge.\n             The chapter begins with a general overview of risks perceived by interviewees,\n             and their general awareness of fundamental rights implications when using\n             AI. The chapter then highlights selected fundamental rights affected by AI-\n             related technologies, with reference to the four use cases analysed.\n             The analysis takes into account and presents the views, practices and\n             awareness of these issues expressed in the interviews conducted for this\n             report. Interviewees were first asked about the general risks they see when\n             using AI. They were then asked about general fundamental rights awareness\n             when using AI and about more concrete fundamental rights implications, which\n             were mostly linked to data protection, non-discrimination and availability of\n             complaints mechanisms.\n             4.1.\t PERCEIVED RISKS\n             It is important to recognise that many issues cut across different rights. For\n             example, a potentially biased decision made by an algorithm could involve\n             the right to non-discrimination, protection of personal data, and the right to an\n             effective remedy. Similarly, a particular issue can be seen from the perspective\n             of different rights. For instance, a good explanation of a decision made by\n             an algorithm is required under the right to protection of personal data, right\n             to good administration, and the right to an effective remedy and a fair trial.\n             When asked about general risks when using AI, the interviewees did not always\n             mention fundamental rights as the main risks, although some highlighted\n             related topics. Private sector representatives most often mentioned inaccuracy\n             as a risk of using AI, followed by potential bias and the proper legal basis\n             for processing personal data. One respondent from an international retail\n             company stated that one business risk is linked to European customers being\n             extremely knowledgeable about their rights; namely, people do not hesitate to\n             ask about data storage and automated decision making. If customers are not\n             properly informed, they might complain and the company may lose a client.\n             In addition, the interviewee continued, breaching the law, and possible fines\n             linked to a breach, is another major business risk.\n             With respect to public administration, bias was most often highlighted as\n             a risk associated with using AI. In addition, public authorities often discussed\n                                                                                               57\n   inaccuracy and data re-identification as risks of using AI. For example,\n   interviewees working on social benefits algorithms stated that incorrect\n   results in general are a risk. This can occur potentially due to rare cases, which\n   are not well identified by the algorithm, or due to errors in the input data.\n   They also highlighted the difficulties associated with moving from testing to\n   deploying a system, including technical challenges, resources required and\n   potential different results when deployed.\n   Respondents working on targeted advertising also highlighted business\n   risks – for example, when offering irrelevant or inappropriate content. One\n   respondent mentioned potentially losing control over automated systems.\n   In addition, interviewees indicate challenges linked to the difficulty of\n   interpreting results and outputs from AI systems. One interviewee from\n   the consultancy sector fears that the risk related to the lack or absence of\n   sufficient AI knowledge and understanding can cause ongoing projects to\n   be halted, due to a company’s inability to explain clearly what an algorithms\n   will perform, and for what purpose.\n   Another interviewee from the law enforcement sector, looking into the\n   possible use of AI to support decisions about licence applications, explains that\n   there are inherent risks on how and why such a system proposes a certain\n   response. For example, when potentially using AI to support decisions about\n   license applications for firearms, the respondent asserts that it would not\n   only be critical to understand the reasoning behind negative decisions, but\n   also positive decisions. Several interviews showed that a major concern is\n   to assign properly trained staff with sufficient expertise to trace, explain and\n   interact with the AI system.\n   This finding is also corroborated by the results of the European Commission\n   survey among companies in the EU. In that survey, 85 % indicate as an\n   obstacle to adopting AI technologies the difficulty to hire new staff with\n   the right skills; 80 % mention the complexity of algorithms as an obstacle.1\n   With respect to the ability to explain decisions based on algorithms, an\n   interviewee working in public administration mentioned that there are no\n   alternatives to being completely transparent when making decisions. There\n   should not be any room for doubt. In a similar vein, a respondent working\n   in the area of health for the private sector mentions that ‘self-learning’\n   algorithms are forbidden in their area of work, because only fixed algorithms\n   can be traced.                                                                     “The use of AI can bring many\n                                                                                      benefits, but also risks, it is like\n   Other risks reported without providing much additional information include         nuclear energy.”\n   cyber-security, data quality, excessive monitoring of people due to the use        (Interviewee working in private sector,\n   of data and algorithms, job loss due to automation, and profiling.                 Spain)\n   4.2.\t GENERAL AWARENESS OF FUNDAMENTAL RIGHTS\n            AND LEGAL FRAMEWORKS IN THE AI CONTEXT\n   Not everyone in the EU is aware about their fundamental rights. FRA’s\n   Fundamental Rights Survey shows that slightly more than every second\n   person in the EU (aged 16 or older) has heard about the Charter. Slightly\n   more people, two out of three, have heard about the ECHR and the Universal         “[Our use of AI] does not impact\n   Declaration of Human Rights. This might be because the ECHR is older and           [human rights] in any way. In terms\n   more established in people’s common knowledge.2                                    of the decision process, it does not\n                                                                                      matter whether the decision is made\n   The majority of people interviewed for this project acknowledge that using         by machine or a human.”\n   AI can generally affect fundamental rights. Only very few mention that             (Interviewee working for public\n   their use of AI does not have a potential impact on fundamental rights or          administration, Estonia)\n58\n                                     that they were not aware of any such implications. Their responses are\n                                     influenced by the different ways they use AI, but also their understanding\n                                     of what fundamental rights are.\n                                     For example, one respondent working on the production of pension forecasts\n                                     based on machine learning says that producing statistics does not have an\n                                     impact on fundamental rights, apart from data protection issues, which need\n                                     to be addressed. Another respondent working on social benefits algorithms\n                                     argues that the impact depends on “how widely human rights are defined” –\n                                     for example, the right to receive a correct pension.\n“Once all the rights related to data\n                                     None of the interviewees working on targeted advertising believe that their\nprotection are ensured, I do not see\n                                     use of AI affects fundamental rights negatively. One respondent working\nhow human rights are of relevance\n                                     on targeted communication with customers stated that one reason for such\nhere.”\n                                     a response relates to a lack of knowledge about what exactly fundamental\n(Private company, Spain)\n                                     rights are.\n                                     Practically all interviewees showed awareness about the rights to privacy\n“We did not touch the topic because\n                                     and data protection as well as to non-discrimination. Other rights, such as\nwe assume that there are no\n                                     human dignity, the right to a fair trial and to effective remedy were also\nhuman rights issues involved: all\n                                     mentioned, albeit very briefly.\nthe activities are within the legal\nframework, all the activities are\n                                     A closer look at interviewees’ responses indicates diverging views across\ncompliant with data protection and\n                                     respondent groups. Most respondents working for private companies discuss\ngood practices, and therefore we\n                                     data protection and non-discrimination, but rarely mention other rights\nassume that there are no human\n                                     challenges. A company working on targeted advertising mentions that they\nrights issues related to these\n                                     are attentive to issues linked to freedom of speech and the right to information\nsystems.”\n                                     in the sense that their company promotes these rights. This is because\n(Public administration, Spain)\n                                     posting adverts helps news and other websites obtain funding to continue\n                                     their work, one interviewee notes.\n                                     The range of rights awareness is much broader among public sector\n                                     representatives working on AI, who referred to other rights such as human\n                                     dignity and the presumption of innocence.\n                                     Those working on AI systems in different fields of application also highlight that\n                                     the use of the systems is also covered by sector-specific laws. For example,\n                                     the system making decisions about unemployment benefits is regulated by\n                                     national legislation on unemployment insurance, on administrative procedures\n                                     and on data protection. However, some respondents are not aware of any\n                                     legal standards that apply to their use of AI or are unsure about it.\n                                     In the absence of AI-specific regulation, several respondents mention ethics\n                                     guidelines and certification schemes. Some work with existing guidelines\n                                     and standards, not necessarily specifically aimed at AI. This is the case, for\n                                     example, with the IT security system ‘ISKE’ in Estonia3 or in the area of financial\n                                     services, with the Payment Card Industry Data Security Standard.4 Respondents\n                                     also refer to the standards developed by the International Organization for\n                                     Standardization (ISO), the Institute of Electrical and Electronics Engineers\n                                     (IEEE) or the European Committee for Standardization (CEN).\n                                     A respondent working on targeted advertising argues that certification is not\n                                     needed in their field because posting ads is not the same as issues linked\n                                     to the health sector or the work of banks. Several interviewees noted that\n“I do not think that we should\n                                     their organisations are developing (internal) guidelines.\nregulate specific technology like\nAI. It is sufficient to have general\n                                     Some respondents mention the guidelines developed at the EU and\nprinciples and technology-neutral\n                                     international level, such as the guidelines from the European Commission’s\nrules.”\n(Private sector, Estonia)\n                                                                                                                         59\n   High-Level Expert Group on AI, the OECD’s guidelines, or the UNESCO standards.     “Yes, there are codes, and yes, there\n   Some are aware of ongoing developments at EU and Council of Europe level.          are procedures, but both these codes\n                                                                                      and procedures are out of date,\n   Some refer to the need to update sector-specific regulations to be able to         because we are using something we\n   innovate on AI – for example, in the area of health. Yet one interviewee           created for the analog world in the\n   states that existing standards are sufficient and that AI does not need to be      digital world.”\n   regulated separately.                                                              (Private sector, Spain)\n   4.3.\t HUMAN DIGNITY\n   Using AI-driven technologies broadly implicates the duty to respect human\n   dignity, the foundation of all fundamental rights guaranteed by the Charter.5\n   Article 1 of the Charter states that human dignity is inviolable and that it must\n   be respected and protected at all times. The CJEU has confirmed in its case\n   law that the fundamental right to dignity is part of EU law.6\n   AI-driven processing of personal data must be carried out in a manner that\n   respects human dignity. This puts the human at the centre of all discussions and\n   actions related to AI. Rather than the technology, the ‘human being’ creating\n   and affected by the new technology needs to be the focus. Taking human\n   dignity as the starting point can help to ensure that the use of AI benefits\n   everyone – for example, by supporting ageing and access to healthcare in\n   a dignified manner.\n   The use of AI also risks infringing on other closely connected Charter rights,\n   such as the right to life (Article 2) and the right to integrity of the person\n   (Article 3). In this context, it is important to consider how to avoid the harmful\n   use of AI to prevent violations of these rights, for example when it comes\n   to the use of AI by people engaging in criminal activities or when AI is used\n   for weapons.7\n   Apart from such extreme cases, preserving dignity includes avoiding subjecting\n   people to AI without their knowledge and/or informed consent, which is\n   strongly linked to privacy and data protection. For example, when people’s\n   applications for social benefits are decided upon through the use of AI, people\n   need to be made aware (and consent to the use when automated decisions\n   are taken). To give another example, a certain proportion of the population\n   does not feel comfortable being subjected to biometric identification systems.\n   Hence, using it without allowing them to opt out could potentially violate\n   their dignity.8\n   Only very few respondents from public administration referred to the right\n   to dignity when discussing fundamental rights. One respondent, considering\n   the use of AI in prisons, mentions that in this particular context it first needs\n   to be assessed whether the risk of violating fundamental rights would be\n   too high, such as the right to human dignity. Other interviewees made only\n   general references to this right, without discussing it in relation to a concrete\n   use of AI.\n60\n4.4.\t RIGHT TO PRIVACY AND DATA PROTECTION –\n        SELECTED CHALLENGES\nThe right to respect for private life and the protection of personal data\n(Articles 7 and 8 of the Charter) are at the core of fundamental rights\ndiscussions around the use of AI. While closely related, the rights to respect\nfor private life and the protection of personal data are distinct, self-standing\nrights. They have been described as the “classic” right to the protection of\nprivacy and a more “modern” right, the right to data protection.9\nBoth strive to protect similar values, i.e. the autonomy and human dignity\nof individuals, by granting them a personal sphere in which they can freely\ndevelop their personalities, think and shape their opinions. They thus form\nan essential prerequisite for the exercise of other fundamental rights, such\nas the freedom of thought, conscience and religion (Article 10 of the Charter),\nfreedom of expression and information (Article 11 of the Charter), and freedom\nof assembly and of association (Article 12 of the Charter).10\nGiven that these two rights are not absolute rights, they can be subject to\nlimitations. However, any interference needs to be adequately justified11\nand cannot compromise the essential, inalienable core of that right,12 as\nexplained in Section 3.3.\n                                      The concept of “private life” or “privacy”\n                                      is complex and broad, and not susceptible\n                                      to an exhaustive definition. It covers the\n                                      physical and psychological integrity of\n                                      a person, and can, therefore, embrace\n                                      multiple aspects of the person’s physical\n                                      and social identity.13 There is also a zone of\n                                      interaction of a person with others, even in\n                                      a public context, which may fall within the\n                                      scope of “privacy”. In other contexts, the\n                                      ECtHR has used the concept of “reasonable\n                                      expectation of privacy” – referring to the\n                                      extent to which people can expect privacy\n                                      in public spaces without being subjected\n                                      to surveillance – as one of the factors,\n                                      albeit not necessarily a conclusive one,\n                                      to decide on a violation of the right to\n                                      respect for private life. Its relevance and\n                                      scope of application, however, appears\n                                      to be limited.14 Similarly, according to the\nUN Human Rights Committee, the mere fact that participants in assemblies\nare out in public does not mean that their privacy cannot be infringed. The\nsame applies to the monitoring of social media to glean information about\nparticipation in peaceful assemblies.15\nThe widespread use of AI-technologies may, as the technologies continue\nto develop, raise unchartered issues and novel concerns about the right to\nrespect for private life. AI-driven technologies may change the way we\nthink about privacy. Algorithmic tools can predict, and reveal information\nabout, people’s behaviour in unprecedented ways – without people even\nrealizing that they are giving away such information. Personal data obtained\nfrom the internet may, for instance, then be used for targeted advertising,\nraising many fundamental rights concerns.16 Issues linked to personal data\nsharing via smart-phone apps particularly raises significant concerns, including\na variety of potential harmful effects, such as manipulation and exploitation\nof vulnerabilities, discrimination, security issues and fraud (e.g. identity theft)\nand reduced trust in the digital economy.17\n                                                                                     61\n   Using AI-driven technologies often implies computerised processing of large\n   amounts of personal data. This constitutes an interference with the right to\n   protection of personal data set out in Article 8 of the Charter (embodying\n   pre-existing EU data protection law), as well as the right to private life under\n   Article 7 of the Charter and Article 8 of the ECHR.\n   Awareness of data protection issues and use of personal data\n   In the EU, 69 % of people have heard about the GDPR.18 By contrast, virtually    “We were a little anxious when the\n   all interviewees are aware of the GDPR and discussed data protection issues.     GDPR was implemented, but in the\n   Data protection rules deriving from the GDPR and national law are clearly        end it meant managing datasets\n   the most well-known and applied rights in the area of AI. Other fundamental      and access rights […] It is a good\n   rights are less known.                                                           reminder that not everything can be\n                                                                                    or should be done.”\n   When discussing the legal framework governing the use of AI, most                (Public administration, Finland)\n   respondents only mentioned data protection rules, as well as some sectoral\n   laws. Some clearly say that there is no other legal framework apart from\n   the data protection laws. An interviewee working for a Spanish public            “Actually, I’m concerned that the\n   administration notes: “we rely on the data protection regulation and norms,      GDPR might hinder AI research. I’m\n   which is all that is available at the moment”.                                   afraid that some large databases\n                                                                                    that we have used previously cannot\n   One interviewee, reflecting on an image-based diagnostic tool, expressed         be used for our research anymore.”\n   the view that the GDPR could hinder research. The hospital using the tool        (Private company, Netherlands)\n   to support diagnosis after strokes had clear rules on data protection, the\n   interviewee indicated, although they did not know whether data protection\n   certification was requested.                                                     “There is the GDPR but it does not\n                                                                                    give you specific rules. It gives\n   Others referred to more general data protection guidelines or indicated that     principles but it comes down to\n   they were not aware of such documents.                                           ethical issues and interpretation.”\n                                                                                    (Private company, Estonia)\n   All respondents working in target advertising are aware of privacy and\n   data protection issues. Although not all are responsible for data protection\n   issues in their companies, they are all aware of efforts to protect the data\n   and privacy. One interviewee mentioned that, contrary to earlier years,\n   personal data are now stored much more securely and handled with more\n   care. Attention was given to properly handling consent for data processing.\n   As a consequence, there is a high level of awareness about data protection\n   and privacy issues linked to AI use.\n   However, data protection law only applies when personal data are processed.\n   For example, using anonymised data to develop AI tools (i.e. as training data)\n   is most likely permissible in many instances and would not trigger the GDPR.\n   Research shows that data can often be de-anonymised.19 However, such\n   efforts often require expert knowledge and potentially additional information,\n   and are illegal. While the illegality of de-anonymisation does not necessarily\n   preclude the applicability of the GDPR, it is more important to consider if re-\n   identification of anonymised data is reasonably likely.20 Anonymising data is\n   only one aspect of protecting the privacy of data subjects. When assessing\n   risks of re-identification, other aspects are also important to consider when\n   disseminating anonymised data. These include who will use the data, for\n   what purpose, and what outputs will be produced.21\n   In the interviews, respondents were not always entirely clear about their use\n   of personal data. They often only superficially described the data used, as\n   mentioned in Chapter 2. In several instances, interviewees indicated that they\n   use non-personal data or anonymised data, arguing that data protection was\n   not relevant in such cases. For example, a semi-public organisation working\n   on environmental management uses aggregated data on water consumption\n62\n                                      for machine learning-based predictions of water consumption. These data\n                                      are not available at individual level.\n                                      Other interviewees said they did not use personal data, although the data\n                                      originally stem from individuals. The tool supporting restaurant inspection\n                                      by collecting data from online sources does not use any personal data – the\n                                      interviewee indicated. However, they indicated the need to be careful when\n                                      mining data online, because, even if publicly available, it might include\n                                      personal data, such as usernames.\n“It would be great to retrieve some   In another example, an insurance company is using a chatbot to make client\ndata from another service so that     contact more effective. The data used to train the system are chat protocols\nthe client wouldn’t have to repeat    (conversation logs), which are not linked to any personal data. However, in\nit all, but where does the line go in this example, linking these data to personal data might be possible in the\nreusing data?”                        future, according to the respondent.\n(Public administration, Finland)\n                                      Companies working on targeted online advertising indicate using (pseudo-)\n                                      anonymised data. This is done, for example, after excluding names and\n                                      social security keys and encrypting data. The identity of the consumers is\n                                      not relevant to the company, an interviewee mentioned.\n                                      While some indicate that they use non-personal or anonymised data, for\n                                      others this is not possible because the data are used to make predictions\n                                      or decisions about specific individuals. For example, an interviewee from\n                                      a company working on credit rating mentioned they need to know the identity\n                                      of consumers for their assessments. In this case, this is even more important\n                                      than the right to be forgotten, according to an interviewee.\n                                      An exhaustive discussion of data protection issues is not possible in this report.\n                                      However, two aspects clearly emerged during the interviews: automated\n                                      decision making linked to the right to human review, and the right to obtain\n                                      meaningful information when decisions are automated.\n                                      Automated decision making\n                                      Article 22 of the GDPR and Article 11 of the Law Enforcement Directive\n                                      generally forbid automated decision making, meaning any “decision based\n                                      solely on automated processing, including profiling, which produces legal\n                                      effects concerning him or her or similarly significantly affects him or her.”\n                                                                         Under Article 22 of the GDPR, explicit\n                                                                         consent is needed when decisions are\n                                                                         solely automated and have a legal or\n                                                                         similarly significant effect on people and\n                                                                         if such automated decision making is\n                                                                         not authorised by law. The authorisation\n                                                                         by Union or national law is the sole\n                                                                         precondition under the Law Enforcement\n                                                                         Directive (Article 11) for such processing.\n                                                                         For a decision not to be considered fully\n                                                                         automated, both instruments require human\n                                                                         review by the controller.22\n                                                                                                                         63\n   However, the concept of ‘automated’ decision making is elusive and requires\n   further discussion and research. For example, in some cases, human\n   intervention might be limited to ‘signing-off’ on the outcomes of the AI\n   system, rendering it virtually automated.23 Importantly, human review must\n   not mean a human just signing off the recommendations or outputs from\n   an algorithm. It must be done by someone who has the “authority and\n   competence to change the decision”, considering all relevant data at hand.24\n   If humans review and potentially override outcomes of the system, this\n   must also be evaluated.\n   Research indicates that humans overrule outcomes from algorithms mainly\n   when the result from the algorithm is not in line with their stereotypes. 25\n   This behaviour threatens the possible added value of automated processing\n   in being potentially more accurate or even fairer than humans. It may also\n   put minority groups at a disadvantage, and is therefore also relevant for\n   non-discrimination issues (discussed below).\n   Overall, there is disagreement about the exact scope of these provisions of\n   the EU data protection acquis, and whether they impose a general ban on\n   certain types of automated decisions, or provide data subjects with some\n   rights in the context of certain types of AI-driven decision making.26\n   Using algorithms in the area of social benefits, health and predictive policing\n   clearly have potential legal or other significant consequences. The interviews\n   suggest that those working in these areas are well aware of the concept of\n   human review before decisions are taken with the support of AI.\n   Many interviewees indicate that no automated decisions are taken. One\n   exception is an automation of unemployment benefits, which is, based on\n   national law, fully automated for decisions that do not involve any discretion.\n   In another example, from another country, only positive decisions, based on\n   pre-defined rules, are automated for student benefits. In this case all negative\n   decisions are made by humans. Both cases refer to rule-based decisions, not\n   involving the use of statistics or machine learning.\n   Another respondent, testing the use of AI systems, including machine learning\n   in the area of social benefits, mentions that equality could be negatively\n   impacted. This is because automation makes human behaviour visible,\n   including existing biased practices. This makes precautions necessary and, as\n   a consequence, the organisation would only allow decisions made by humans.\n   Interviewees working in health highlighted risks linked to the automation\n   of decisions. An interviewee discussing the tool to support stroke diagnosis\n   feels it is important not to rely on the system to avoid the risk of automation\n   or confirmation bias. They caution that early positive experiences with\n   the application could prompt users to rely on it too easily and devote less\n   attention to their own assessment of the images. Other interviewees raised\n   similar concerns. One interviewee, discussing a tool that analyses images to\n   provide a probability for the presence of a certain type of lesion, notes that\n   the technology supports the diagnosis of simple cases, but that the expertise\n   of doctors is particularly important – and trusted – in more complex cases.\n   Targeted advertising is often considered not to have a significant effect\n   on people. However, this may be the case if, for example, an individual’s\n   vulnerabilities are used for successful advertising. Considering vulnerabilities\n   is particularly important for people from disadvantaged groups, who may\n   not be aware that they can opt out of direct marketing (see box) or of their\n   right to have a say when decisions are automated.\n64\n                                                       In the absence of case law in this area, more information and research\n                                                       is needed to identify the impact of such automated decisions (i.e. which\n                                                       advertisement will be delivered to whom, when, how and why). Answering\n                                                       these questions is challenging, as targeted advertising is based on highly\n                                                       complex technology and at scale.\n                In 2019, a Eurobarometer survey asked people in the EU if they are aware of their right to\nAwareness of    opt out from direct marketing. Overall, only 59 % of EU citizens have heard about this right\nright to opt    (with 24 % having exercised it). But people can only exercise their right if they are aware\nout from direct of it – which becomes even more important when direct marketing is made much more\nmarketing       efficient through machine learning.\namong general\n                Awareness levels strongly vary across the EU. The percentage of people who know about\npopulation      their right to opt out from direct marketing ranges from 38 % in Bulgaria to 81 % in the\n                Netherlands. Figure 4 shows the percentages. It also highlights – based on FRA’s analysis of\n                Eurobarometer data – that there is a strong variation within countries, when broken down\n                by regions.\n                In some regions, fewer than one in four have heard about their right. These are areas with\n                higher shares of people at risk of poverty. This indicates the general problem that people\n                who are more disadvantaged in society tend to be less aware of this right. The data show\n                that people who are not working, who more often struggle to pay their bills, who are living\n                in rural areas, or who are older, are less aware of this right.\n                FIGURE 4:          AWARENESS OF GDPR RIGHT TO OPT OUT FROM DIRECT MARKETING, IN THE EU\n                                   AND UNITED KINGDOM, BY COUNTRY AND REGION (%)\n                        75\n                        50\n                                                                                                                                     FI−80\n                        25\n                                                                                                               SE−65\n                                                                                                                                         EE−62\n                        Insufficient or no data available\n                                                                                                                                        LV−60\n                                                                                          DK−67\n                                                                        UK−63                                                       LT−54\n                                                                 IE−74\n                                                                                      NL−81\n                                                                                                                         PL−64\n                                                                                                  DE−69\n                                                                                    BE−59\n                                                                                            LU−62                CZ−55\n                                                                                                                       SK−61\n                                                                                                              AT−62\n                                                                                                                       HU−56\n                                                                              FR−47                            SI−57\n                                                                                                                                      RO−50\n                                                                                                            HR−50\n                                                                                                        IT−56\n                                                                                                                                      BG−38\n                                                                PT−51  ES−51\n                                                                                                                              EL−53\n                                                                                                          MT−46\n                                                                                                                                               CY−59\n                Note:\t\u0007     Map does not show non-EU countries other than the UK. Light shading = more aware of right.\n                            Dark shading = less aware of right. Results for regions within countries represented by light\n                            grey spaces were excluded because there were fewer than 20 respondents, meaning the\n                            numbers of observations were too low for reliable results. N = 26,503. Question: “The General\n                            Data Protection Regulation (GDPR) guarantees a number of rights. Have you heard of each of\n                            the following rights? […] 18.2 The right to object to receiving direct marketing.”\n                Source: FRA, 2020 [Calculations and presentation based on European Commission (2019),\n                            Eurobarometer, 91.2]\n                                                                                                                                                     65\n   Experiences from use cases\n   In general, interviewed experts highlighted that data protection law is difficult\n   to interpret and lacks clarity when it comes to the meaning of automated\n   decision making. One expert from France felt that, because automated\n   decision making is so difficult to explain, all automated decision making\n   should be banned, meaning the exceptions in the GDPR that allow for some\n   automated decision making should be removed. They pointed out that AI\n   can only be used as a decision support tool.\n   Another expert, an independent lawyer from the Netherlands, views current\n   laws and standards as sufficient, but says they need to be concretised per\n   sector. Particularly, the expert mentions that the scope of the existing rules\n   on permissible automated decision making are not clear and that it remains\n   unclear to him what a comprehensive assessment, or a ‘human in the loop’\n   means. This was also raised in relation to the SyRI case, where it remained\n   unclear to what extent the decisions were reviewed.\n   Another expert working at a supervisory authority, more generally, sees\n   no need for adapting data protection laws as “the legislation is quite\n   comprehensive. It is more about the organisation of the supervision thereof,\n   and also the political will behind it”.\n   These concerns reflect the findings of other research, which also raise serious   “There is a risk of having too much\n   issues concerning the right to human review. For example, the responsible         trust in the machine.”\n   officers questioned the results of an algorithmic system built to profile         (Public administration, France)\n   unemployed people in Poland in less than one percent of cases. This essentially\n   makes a supporting tool an automated decision making tool.27\n                                                                                     “There is a huge tension surrounding\n   Linked to the question of reviewing decisions or outputs from AI systems is       the GDPR. So we want to do well,\n   the challenge of a clear lack of knowledge about how AI works. Interviewees       but might in fact be worse off,\n   often could not explain in detail how the system they use works or which          because interpretation of the data\n   data it uses, be it due to the lack of knowledge or lack of transparency.         then turns out to be impossible.”\n   Meaningful information about the logic involved, or explaining outcomes           (Public administration, Netherlands)\n   from algorithms, is essential for several fundamental rights. It is crucial\n   not only for the processing of personal data, but also for ensuring that the\n   algorithms are fair and do not discriminate. It is also necessary to enable       “If we had to explain the model, we\n   people to properly challenge decisions and AI systems.                            wouldn’t be able to. The model is\n                                                                                     statistical and not very explainable.”\n   One interviewee working for public administration explains that the complexity    (Public administration, France)\n   differs depending on the tasks. Licence administration systems can be\n   relatively straightforward. Crime prevention analysis uses more data sources,\n   which makes it harder to understand. Another interviewee working in law           “Internally we can explain the\n   enforcement says that the current AI used by police organisations is not yet      decisions of the machine learning\n   so complex that it would make explanations difficult, but that this might be      models and we have several means\n   the case in the future.                                                           to do that.”\n                                                                                     (Private sector, Estonia)\n   A respondent working on financial data transactions indicates that traditional\n   models were straightforward to understand. However, new methodologies\n   are more difficult to explain, and the company has to invest resources into       “If the systems do not have black\n   making these models more explainable. Still, the level of explainability          boxes of information or processes,\n   required by the GDPR is not clear to the respondent.                              we already take a step forward in\n                                                                                     the defence of human rights.”\n                                                                                     (Public administration, Spain)\n                                                                                     “We are strongly attached to the\n                                                                                     idea that AI has to be explainable.”\n                                                                                     (Public administration, France)\n66\n                Most people are not aware that they have the right to have a say when decisions are\nAwareness of    automated, evidence suggests. A Eurobarometer survey showed that 40 % of Europeans\nright to have   know about their data protection rights.\na say when\ndecisions are   FRA’s analysis of the Eurobarometer survey shows that this figure drops considerably\nautomated       among people with lower socio-economic status. Only 26 % of EU citizens who report\n                that they are struggling to pay their bills most of the time know about this right. This\n                lack of rights awareness among those socially disadvantaged could contribute to further\n                social exclusion if those already disadvantaged are less aware that they can challenge\n                (automated) decisions about them (see Figure 5).\n                Gender differences are small, yet women are even less aware of this right (38 % of women\n                and 43 % of men). Older people are considerably less aware (31 % among those aged 55\n                and older).\n                FIGURE 5:                        AWARENESS OF RIGHT TO HAVE A SAY WHEN DECISIONS ARE AUTOMATED, BY\n                                                 AGE, GENDER AND DIFFICULTY IN PAYING BILLS (%)\n                                           55 years and older                                 31\n                                                 40-54 years                                                       46\n                Age\n                                                 25-39 years                                                             50\n                                                 15-24 years                                                  44\n                                                     Women                                         38\n                Gender\n                                                        Men                                               43\n                                                      Refusal                                       39\n                Difficulty paying bills\n                                          Almost never/never                                             42\n                                           From time to time                                       38\n                                             Most of the time                       26\n                Total                                                                               40\n                                                                0   10       20          30        40                   50    60\n                Notes:\t\u0007N = 26,503. Question: “The General Data Protection Regulation (GDPR) guarantees a number\n                        of rights. Have you heard of each of the following rights? […] 18.5 The right to have a say\n                        when decisions are automated (e.g. an algorithm decides if you will be granted a loan or not).”\n                Source: FRA, 2020 [calculations and presentation based on European Commission (2019),\n                        Eurobarometer, 91.2]\n                                                                                                                                   67\n   4.5.\t EQUALITY AND NON-DISCRIMINATION\n   Equality before the law and non-discrimination are enshrined in Articles 20\n   and 21 of the Charter. Discrimination is “where one person is treated less\n   favourably than another is, has been or would be, treated in a comparable\n   situation” based on a perceived or real personal characteristic28 (called\n   ‘protected grounds/characteristics’). Article 21 of the Charter prohibits any\n   discrimination based on any ground such as sex, race, colour, ethnic or social\n   origin, genetic features, language, religion or belief, political or any other\n   opinion, membership of a national minority, property, birth, disability, age\n   or sexual orientation.\n   The Charter prohibition reflects corresponding rights in the ECHR (Article 14)\n   and in Protocol No. 12 to the ECHR (Article 12), but is even broader, as it\n   establishes a non-exhaustive, open list extending protection to a wide range\n   of new grounds. Unlike Article 14 of the ECHR, the Charter right to non-\n   discrimination is a freestanding right that applies to situations that do not\n   need to be covered by any other Charter provision.29\n   Main challenges\n   Discrimination is a crucial topic when it comes to the use of AI, because the\n   very purpose of machine learning algorithms is to categorise, classify and\n   separate. As one interviewed expert points out, making differences is not\n   per se a bad thing. According to this expert, when deciding to grant a loan,\n   credit history can be used to differentiate between individuals, but not on\n   the basis of protected attributes, such as gender or religion. However, many\n   personal attributes or life experiences are often strongly correlated with\n   protected attributes. The credit history might be systematically different for\n   men and women due to differences in earnings and job histories.\n   Interviewees often mention efficiency as the main purpose for using AI-\n   related technologies. Yet it is important to note that this cannot not justify\n   unfair, differential treatment.\n   Often, protected attributes might be highly correlated with risks. For example,\n   differences in life situations among men and women might often be linked to\n68\n                                       different insurance risks. This is, however, not acceptable, as the Test-Achats\n                                       ruling, 30 shows. In that case, the CJEU put an end to gender discrimination\n                                       in insurance pricing.31\n                                       Under certain circumstances and in some areas, using algorithms could\n                                       positively contribute by reducing bias and stereotyping. Algorithmic data\n                                       analysis may produce results that could dispel prejudicial attitudes. For\n                                       example, predictive policing might, in some contexts, lead to more equitable\n                                       and non-discriminatory policing by reducing reliance on subjective human\n                                       judgments.32 Predictive techniques may be used to identify so-called ‘white-\n                                       collar crimes’, such as financial crimes that are historically under-policed.33\n                                       Nevertheless, direct or indirect discrimination34 through the use of algorithms\n                                       that involve big data is considered as one of the most pressing challenges\n                                       in the use of AI-driven technologies.35 Bias and discrimination, including\n                                       gender-based discrimination, in data-supported algorithmic decision making\n                                       can occur for several reasons and at many levels in AI systems. They are\n                                       difficult to detect and mitigate.36 Often, the quality of the data and biases\n                                       within it are the source of potential discrimination and unfair treatment.37\n                                       The discriminatory effects generated on certain groups are, in practice, very\n                                       difficult for individuals to challenge.38 So far, only a limited number of court\n                                       cases have dealt with discrimination relating to AI systems.39\n               A first instance decision of the Divisional Court of Cardiff in 2019 dismissed a claim\nUK Court of    concerning the lawfulness of the South Wales Police’s use of the “AFR Locate” face\nAppeal: police recognition system. The Court of Appeal overturned that decision.\nuse of facial\nrecognition    It found that the facial recognition programme used by the police was unlawful. The Court of\nviolates       Appeal ruled that “too much discretion is currently left to individual police officers”. It added\n               that “[i]t is not clear who can be placed on the watch list, nor is it clear that there are any\nhuman rights   criteria for determining where [the technology] can be deployed”.*\n               The court also held that the police did not sufficiently investigate if the software in use\n               exhibited race or gender bias.\n               This judgment is the first in-merit specifically on this matter in Europe. It considerably\n               narrows the scope of what is permissible and what law enforcement agencies need to do to\n               fully comply with human rights law.**\n               * UK, Court of Appeal, R (Bridges) v. CC South Wales, [2020] EWCA Civ 1058, 11 August 2020.\n               ** Ars Technica, ‘Police use of facial recognition violates human rights, UK court rules’, 11 August\n               2020.\n                                       Studies have highlighted the potential for discrimination prompted by the\n                                       use of AI-systems across the areas covered by the report.40 In the area of\n                                       predictive policing, for example, a particular risk relates to the potential\n                                       for automated decision making tools to reproduce and entrench existing\n                                       discriminatory practices that undermine equality before the law (Article 20\n                                       of the Charter). The historical crime data that underpins predictive policing\n                                       may be biased,41 reflecting inherent data gaps (e.g. chronic underreporting\n                                       for certain types of crime), alongside issues with how data is recorded (e.g.\n                                       human error, but also bias by individual officers).\n                                       Crime victimisation surveys consistently show that a large proportion of crime\n                                       is never reported to the police by the public – particularly crimes involving\n                                       physical and/or sexual violence, and hate crimes. For example, FRA’s survey\n                                       on violence against women – with 42,000 respondents – showed that only\n                                       one in five women who experienced violence, by their partner or anyone\n                                       else, brought the most serious incident to the attention of the police.42 FRA’s\n                                                                                                                        69\n   EU-MIDIS II survey of 25,500 respondents across the EU showed that only\n   three in ten reported incidents of racially motivated hate crime to the police\n   or any other organisation.43\n   Compared with violent crime and hate crime, property crime – such as\n   burglary – has a higher rate of reporting to the police, particularly in developed\n   countries. This may be because this is a requirement when claiming on an\n   insurance policy.\n   In sum – relying on official crime statistics (that are based on reported crime)\n   when looking to develop AI models in the field of predictive policing is\n   particularly problematic when it comes to specific crimes and specific groups.\n   Some variables used in AI modelling can be proxies for race, ethnicity, gender\n   and other protected categories. The complexity of the algorithms makes it\n   harder to identify and remove such biases. Instead of providing objective\n   analysis, predictive policing software may turn into an ‘echo chamber’\n   cementing existing systemic flaws and injustices with the ‘stamp’ of what\n   appears to be scientific legitimacy.44\n   The use of predictive policing may also make law enforcement responses\n   less equitable by focusing on certain crimes or areas.45 Predictive policing\n   is currently focused on property crimes such as theft and burglaries, which\n   are often associated with certain demographics and neighbourhoods. This\n   can result in certain demographics and neighbourhoods – and the individuals\n   living in them – being further stigmatised.46 Meanwhile, white-collar crime –\n   typically committed by different demographics – is less prioritised.47 These\n   patterns of policing – whereby certain neighbourhoods or communities are\n   disproportionately policed – predates the use of AI. However, the ‘promise’ that\n   AI is more ‘objective’ and can, in turn, be used to counteract discriminatory\n   policing, needs to be verified in practice.\n   Oxford University researcher Sandra Wachter highlights that discrimination may\n   occur due to information linked to protected attributes in targeted advertising.\n   Newly created profiles for the purpose of advertising might amount to indirect\n   discrimination and potentially even require new characteristics to be added\n   to non-discrimination legislation, and extend for its scope to be expanded\n   to other areas.48\n   Experiences from use cases\n                                                                                      “If you want the machine not to\n   Many interviewees noted that the use of AI, in general, can discriminate, but\n                                                                                      discriminate on the basis of sex,\n   that the systems they are working with do not. Many indicated a belief that\n                                                                                      do not put the variable of sex, as\n   excluding information on protected attributes is sufficient protection against\n                                                                                      easy as that, or make the examples\n   discrimination. However, discrimination can occur due to other information\n                                                                                      symmetrical if you notice that sex\n   contained in datasets that may indicate protected attributes. Traces of\n                                                                                      has certain relevance.”\n   protected groups are often hidden in other information.\n                                                                                      (Public administration, Spain)\n   An example from a public authority, which uses AI in tax and customs,\n   shows the challenges linked to identifying possible bias and potential\n   discrimination when using algorithms. When scrutinising their algorithms,\n   a public administration body found a higher degree of errors in tax declarations\n   among recently issued national identification numbers, which have almost\n   always been attributed to immigrants. This prompted further research into the\n   correlation. It turned out that the outputs of people with recent identification\n   numbers more often contained errors because they had never filed their\n   taxes before, and did not know how to do so (which was also the case for\n   non-migrants). This is also an example of proxy information, where parts of\n   a number could indicate immigrant status.\n70\n                                        Another interviewee working on the potential use of AI for detecting benefits\n                                        fraud mentioned in this respect: “If you want to prevent discrimination based\n                                        on ethnicity, for instance, it does not suffice to just remove the ‘ethnicity\n                                        label’, because the neighbourhood composition is often also determined by\n                                        ethnicity, or ethnicity plays a role in it. So [preventing discrimination] often\n                                        goes beyond the ‘direct’ characteristics”.\n“[I]f you do not have access            Even if most of the respondents were aware of the general potential\nto sensitive personal data, it is       for discrimination when using AI, they often ruled out that their system\nimpossible to check if you are          discriminates against people based on protected characteristics. Some\nprofiling on that basis.”               respondents also believe that their tools have a positive impact in terms of\n(Public administration, Netherlands)    non-discrimination. One respondent, testing AI for social benefits decisions,\n                                        regrets not being able to use AI for data protection reasons, even though, in\n                                        the respondent’s view, automation could process big datasets effectively and\n                                        without discrimination. While noting that protection of personal data needs\n                                        to be observed, the respondent feels it hinders prompt decision making\n                                        and non-discrimination – “if it can be automated, it should be automated”.\n                                        Some respondents were not clear or not sure about whether their use of AI\n                                        could discriminate. Respondents repeatedly stated that their system cannot\n                                        discriminate because it does not include data on protected characteristics.\n                                        For example, several interviewees working in predictive policing and law\n                                        enforcement felt that there was no potential for discrimination, as the AI\n                                        systems did not use data on, or return outcomes related to, protected grounds,\n                                        or because the system does not aim to identify people.\n                                        Others working on predictive policing felt that discrimination could occur, in\n                                        particular because of issues in the training data. In relation to the predictive\n                                        policing ‘heat map’ case, for example, one interviewee noted that – because\n                                        the dataset is never fully neutral, representative or complete – there is\n                                        a strong risk of bias and possible discrimination towards particular groups.\n                                        They identified sharing datasets to increase the amount of data available\n                                        as one way to mitigate this risk, but felt that this was impeded by data\n                                        protection regulations. They also indicated that multi-level teams with the\n                                        task to travel to different police authorities and check on the quality of the\n                                        systems used are being set up.\n                                        In the area of targeted advertising, some interviewees mentioned discrimination\n                                        as a potential problem, mainly after being asked directly about it. Overall\n                                        respondents do not think that their systems discriminate. Three respondents\n                                        mention that information on gender and age is not used and consequently\n                                        no discrimination in this respect can occur. Another interviewee is not sure\n                                        if this information is included or not.\n“For discrimination, it’s complicated   A respondent working on a breast cancer detection tool highlighted that\nbecause some diseases are more          age, gender and ethnicity are relevant factors as some population groups\npresent in certain ethnic groups.       are more likely to develop certain types of cancer. Respondents working in\nPredictions take into account the       health highlighted that the potential for discrimination is also linked to who\nsexual, ethnic, genetic character. But  uses the system, suggesting that this could become a greater challenge if\nit is not discriminatory or a violation the system were used by non-medical staff.\nof human rights.”\n(Private sector, France)                A different, but related, example comes from a respondent working on credit\n                                        rating for a private company, selling credit scores of individuals created by an\n                                        algorithm. The company uses information about gender, age and citizenship\n                                        in its credit risk models. This information has some impact on the outcome\n                                        of the credit scores. For example, younger people or non-citizens have\n                                        a higher credit risk score, but the influence of demographics is much smaller\n                                        compared to credit history data. According to the interviewee, their system\n                                        “certainly does not impact on the right to non-discrimination, because we\n                                                                                                                         71\n   do not make any decisions, we sell data and data analytics. Creditors have\n   to monitor that they do not discriminate”.\n   Another interviewee working on the data strategy for a financial institution in\n   the private sector, using AI to analyse financial transactions, clearly mentions\n   the challenges of understanding what non-discrimination constitutes for their\n   work. The interviewee mentions, for example, that it is not clear to what\n   extent it is illegal to exclude older people from receiving credit if their life\n   expectancy is expected to be lower than the mortgage repayment period\n   they asked for.\n   These findings point to uncertainty and ambiguity in the financial sector with\n   respect to how Article 21 of the Charter – on non-discrimination – translates\n   into real life situations.49\n   Vulnerable groups\n   Much of the discussion and research about discrimination when using AI is\n   linked to biased results with respect to ethnic origin, gender, and to some\n   extent, age. Although it is important to analyse potential discrimination against\n   these groups, the Charter covers several other grounds of discrimination,\n   which are less often part of discussions or research.\n   These other grounds include, for example, political opinion, sexual orientation,\n   and disability. The Charter provides particular rights to some special groups\n   (beyond Articles 20 and 21), including the rights of the child (Article 24), the\n   rights of the elderly (Article 25), and the rights of persons with disabilities\n   (Article 26).\n   The question of age – with respect to older age groups and younger adults –\n   came up during the interviews, notably when it comes to insurance and\n   credit (see above).\n   However, none of the interviewees or experts directly mentioned the rights\n   of the child. This might be linked to some extent to the nature of the use\n   cases investigated, but it clearly reflects the fact that this topic is not high\n   on the agenda of many of those working in AI.\n   Article 24 of the Charter emphasises that the best interests of the child\n   must be the primary consideration in all activities of public authorities and\n   private actors that concern children, which applies of course – equally – to\n   the field of AI.50\n   Only two respondents from public administration mentioned possible use of\n   AI in the area of child custody and the distribution of children in schools. But\n   they did not address this in consideration of the rights of the child. These\n   respondents did not wish to go into more detail concerning these use cases –\n   potentially reflecting the sensitivity of this topic.\n   Finally, issues linked to the integration of people with disabilities were not\n   raised in any of the interviews.\n72\n                 A Eurobarometer survey that included questions on AI asked respondents about the areas\nAwareness        they are mostly concerned about when it comes to the use of AI, including discrimination in\namong general    decision making, unclear responsibility, and that there is nobody to complain to.\npopulation of\npotential for    Only around 40 % of EU citizens indicated that they are concerned that using AI could\nAI to lead to    lead to discrimination in terms of age, gender, race or nationality – for example, in taking\n                 decisions on recruitment, credit worthiness, etc.\ndiscrimination\n                 Results vary across countries. Higher proportions of people are concerned about\n                 discrimination in the Netherlands (58 %), Luxembourg (48 %) and Sweden (47 %). Lower\n                 proportions expressed concern in Estonia (25 %), Hungary (24 %) and Lithuania (23 %)\n                 (see Figure 6).\n                 However, from this question it is not clear if people do not know that discrimination can\n                 happen, or if they are aware that it can happen but do not think it is a problem.\n                 FIGURE 6:     AWARENESS ABOUT THE RISKS OF DISCRIMINATION WHEN USING AI, BY COUNTRY\n                               (%)\n                 100\n                  90\n                  80\n                  70\n                  60           58\n                  50                48 47 46\n                                             45 44 43\n                       40                             41 41 40 40 40 39                                               41\n                  40                                                    38 37 36\n                                                                                 35 34 34 34\n                                                                                             32 31\n                                                                                                   29 28\n                  30                                                                                       25 24 23\n                  20\n                  10\n                   0\n                       EU-27\n                               NL\n                                                                                                                      UK\n                               LU\n                               SE\n                               FR\n                               EL\n                                SI\n                               DE\n                               ES\n                               CY\n                               BE\n                                IE\n                               HR\n                               AT\n                               DK\n                                IT\n                               CZ\n                                FI\n                               BG\n                               PT\n                               SK\n                               MT\n                               LV\n                               RO\n                               PL\n                               EE\n                               HU\n                                LT\n                 Notes:\t\u0007Includes people who indicated that they are concerned that AI could lead to discrimination\n                         among three possible issues, or all of the three issues.\n                 Source: FRA calculations based on European Commission (2019), Eurobarometer, 92.3\n                                                                                                                           73\n   Tackling\n   gender                          The Charter stipulates that equality\n   inequality in between                      women and men must\n                                   be ensured in all areas, including\n   the design and employment,                      work and pay\n                                   (Article 23). Gender discrimination\n   use of AI\n                                   is a major concern when it comes to\n                                   the design and use of AI and related\n                                   technologies.*\n                                   On the development side, the\n                                   European Economic and Social\n                                   Committee notes that the\n                                   development of AI is taking place\n                                   within a homogenous environment\n                                   principally consisting of young white\n                                   men. This results in cultural and\n                                   gender disparities, which are being\n                                   embedded in AI technologies. For\n                                   example, training data are prone to\n                                   manipulation, may be biased, reflect\n                                   cultural, gender and other prejudices\n                                   or preferences, and contain errors.**\n      * See also European          This is also reflected in this research,\n         Commission, White\n                                   where, despite efforts to achieve\n         Paper On Artificial\n         Intelligence –\n                                   gender balance, the majority of\n         A European approach       interviewees were men.\n         to excellence and trust,\n         COM(2020) 65 final,       Disparities at the design and\n         Brussels, 19 February     deployment stage are linked to the\n         2020, p. 1.               systematic disadvantages affecting\n      ** European Economic         women in the labour market and\n         and Social Committee,     the potential lack of awareness\n         Artificial intelligence – of gender biases. A recent study\n         The consequences of       showed that the increased use\n         artificial intelligence   of industrial robots could widen\n         on the (digital) single   the gender gap, despite both\n         market, production,       genders benefitting from increased\n         consumption,              automation, as the analysis indicated\n         employment and society\n                                   that men in medium- and high-\n         (own-initiative opinion),\n                                   skill occupations would benefit\n         31 May 2017, JO C 288,\n         p. 43.                    disproportionally.***\n      *** Aksoy, C., Özcan, B.     Looking ahead, using data and\n         and Philipp, J. (2020),\n                                   algorithms could help to better\n         Robots and the Gender\n                                   mainstream gender equality into\n         Pay Gap in Europe, IZA\n         Discussion Paper No.      policies and processes by paying\n         13482.                    attention to gendered datasets.\n                                   Drawing on discussions around\n      ****       See the webpage\n         on data feminism on the   gender inequalities and the use\n         datasociety’s website.    of data (‘data feminism’)****\n                                   could help to raise awareness that\n      ***** Criado Perez,\n         C. (2020), Invisible      the male point of view should\n         Women. Exposing data      not be taken as the default view,\n         bias in a world designed  which also then finds its way into\n         for men, London.          datasets.*****\n74\n4.6.\t ACCESS TO JUSTICE\nThe right to an effective remedy before a tribunal and to a fair trial (Article 47\nof the Charter) is one of the most often used Charter right in legal proceedings.\nThis highlights its importance in upholding fundamental rights and the rule\nof law. This right of horizontal character empowers individuals to challenge\na measure affecting any right conferred to them by EU law, not only in respect\nof those guaranteed in the Charter.51 The CJEU has underlined that Article 47\nof the Charter constitutes a reaffirmation of the principle of effective judicial\nprotection and that the characteristics of a remedy must be determined in\na manner that is consistent with this principle.52\nThe right to an effective remedy also covers decisions taken with the support\nof AI technologies. EU data protection law reconfirms that the right to an\neffective judicial remedy must be provided in relation to decisions by the\ncontroller or the processor53 as well as the supervisory authority.54 Data\nprocessed by AI-driven technologies is no exception.\nIt is crucial to note that the possibility to lodge an administrative complaint\nbefore a supervisory authority as provided for by the GDPR and the Law\nEnforcement Directive55 is not considered an effective judicial remedy under\nArticle 47 of the Charter. This is because no court is involved in such a review.\nJudicial review should always remain available and accessible, when internal\nand alternative dispute settlement mechanisms prove insufficient or when\nthe person concerned opts for judicial review.56\nUsing AI can challenge the right to an effective remedy in different ways.\nOne prominent concern is the lack of transparency in the use and operation\nof new technologies. Algorithmic decision making is notoriously opaque:\ndata collection, algorithm training, selection of data for modelling or profiling,\nthe situation around individual consent, effectiveness and error rates of the\nalgorithm and other aspects are often not transparently reported.57\n                                                                                   75\n   Without access to this information, individuals may not be able to defend\n   themselves, assign responsibility for the decisions affecting them,58 appeal\n   any decision negatively affecting them or have a fair trial, which includes the\n   principle of equality of arms and adversarial proceedings as established by\n   the ECtHR.59 These requirements also form part of the corresponding Charter\n   right (Article 47) in view of Article 52 (3) of the Charter.\n   Main challenges\n   These issues are reflected in the specific challenges to the right to an effective\n   remedy and a fair trial that the interviewed experts outlined. Generally, experts\n   indicate a difference in accessing remedies at private companies and public\n   administration. Public authorities are more often forced to be transparent\n   about their use of AI. Meanwhile, companies appear to be more secretive,\n   the assessment of several experts suggests. However, an expert from the\n   Netherlands said that people might more readily complain to companies, but\n   be reluctant to complain to public authorities. This is because public services\n   often concern vulnerable people, in need of social benefits, who would be\n   less inclined to complain about any decisions.\n   Opportunities to successfully complain about the use of AI and challenge\n   decisions based on AI are essential for providing access to justice. The\n   interviews emphasised the following as important in this respect:\n   ――Making people aware that AI is used\n   ――Making people aware of how and where to complain\n   ――Making sure that the AI system and decisions based on AI can be explained\n   First, everyone needs to know if they are dealing with an AI system. If\n   a taken decision affects people, e.g. on social benefits, people concerned\n   might complain in general – but they will not be able to complain about the\n   use of AI if they do not know AI is involved.\n   An expert explained that, while there is general willingness to complain,\n   the biggest problem is that people often do not know that AI is being used,\n   because organisations are not transparent about this, even though this is\n   required by the GDPR. Several interviewees indicate that informing people\n   that any decision made about them is based on (partly) automated tools is\n   the very first step for providing access to complaints.\n   Second, everyone needs to know how and where to complain. It may be\n   difficult for people to know which body deals with what type of complaints.\n   One expert pointed out that consumers often do not know how to complain –\n   for example, to a bank that might use algorithms for deciding on financial\n   matters. A public administration that issues automated decisions decided\n   to add names of employees to the decisions to provide contact persons to\n   those potentially challenging the (automated) decision. Most interviewees\n   indicated that there are ways and procedures for complaints in place, which\n   are the same procedures as those for any other complaints not linked to the\n   use of AI. Only few companies or organisations that use AI on anonymised or\n   aggregated data indicate that they do not have any complaint mechanisms\n   in place.\n   Finally, those complaining need enough information to challenge the underlying\n   decision. Only thorough information about the AI systems provides equality of\n   arms to meaningfully challenge decisions. However, this is not straightforward\n   when it comes to the use of AI, particularly because of:\n   ――potential intellectual property rights issues, and\n   ――because complex systems are difficult to explain.\n76\n                                     Intellectual property rights form one hurdle to providing enough information\n                                     about how a decision was made, or how a system works. Algorithms can be\n                                     part of an implemented software, or technical invention, that may be subject\n                                     to intellectual property rights – a right protected under Article 17 (2) of the\n                                     Charter. Actors often seek out copyright, patent and trade secret protection\n                                     to safeguard their knowledge on AI.60\n                                     One interviewee from the insurance sector claims that, due to the highly\n                                     competitive market, “one may not share too much about the workings of\n                                     a used technology” as to, for instance, why a particular price was given to\n                                     a customer. This is essentially because competitors could benefit from this\n                                     knowledge if the underlying software were subject to scrutiny. Another\n                                     respondent using AI to handle visa applications notes that using systems\n                                     developed by external providers whose algorithms are covered by intellectual\n                                     property rights can hinder the necessary transparency at a later stage.\n                                     Another challenge for successfully complaining about automated decisions\n                                     or the use of AI in general is the challenge to explain the decisions based\n                                     on complex systems. The interviewees working for public administration\n                                     suggest that there is usually clear guidance on how to complain against an\n                                     administrative decision, an area where interviewees highlight the importance\n                                     of detailed explanations. For example, for the systems that automatically\n                                     provide unemployment benefits for cases that do not involve discretion,\n                                     clients can ask for the reasoning behind automated administrative acts. An\n                                     interviewee indicates that if clients wish to see the calculations behind financial\n                                     decisions, they may do so in the self-service system on the organisation’s\n                                     website or in their publications, which contain detailed descriptions of the\n                                     calculations used.\n                                     Interviewees recognise that an open and transparent logic is essential for\n                                     providing explanations regarding AI-supported decisions, but that this is\n                                     often challenging or impossible to achieve. One interviewee working for\n                                     a bank mentions that more complex machine learning solutions cannot\n                                     be used for certain decision making, because the reasoning of the system\n                                     cannot be explained easily, and this is why such systems are only used for\n                                     other purposes. However, an interviewee working for another bank indicates\n                                     that such systems are used, but they use simpler methods in addition to\n                                     the complex ones to get an idea of the probable reasons for the decisions.\n                                     One expert raised the problem that companies internally might not have\n                                     enough information about the way algorithms work themselves. The lack of\n                                     expertise and knowledge appears to be a major hindrance in practice when\n                                     seeking access to effective remedy.61\n                                     Experiences from use cases\n“The topic of transparency is very\n                                     Respondents discussing predictive policing tools highlighted transparency\nimportant nowadays, there are\n                                     as important.\nmany procedures on how to publish\nthe information, many automatic\n                                     In the gender-based violence use case, they felt that sending both the police\nmeans that help to upload the\n                                     file and the outcome of the AI system to the judge, and informing the victim\ninformation on the portals, and\n                                     of the level of risk attributed to the case and the police measures that will\nthere has been a lot of work done in\n                                     apply as a result, enhances transparency.\nterms of transparency.”\n(Public administration, Spain)\n                                     Interviewees discussing the heat map example referred to numerous requests\n                                     to the police to explain the system’s purpose and how it works, and highlighted\n                                     transparency as a way to reduce public anxiety.\n                                     A number of interviewees pointed to the possibility for individuals\n                                     affected by the system to make complaints to the police, the courts or the\n                                                                                                                         77\n   Ombudsinstitution. With reference to the domestic violence case, however,\n   the interviewee indicated that there is no procedure in place to question\n   a system of police protocol.\n   In terms of measures to protect fundamental rights in the health services use\n   cases, several interviewees referred to ethics committees, as well as general\n   legal safeguards and data protection rules. Checks and controls were primarily\n   mentioned to take place through external actors. No specific complaints\n   procedures were in place in the organisations of those interviewees who\n   responded to this question.\n   Some interviewees highlighted that doctors ultimately take responsibility for\n   decisions, and that patients often do not know about the use of an AI tool\n   in the first place. For example, in the breast cancer detection example, the\n   interviewee indicated that there is no possibility for legal recourse against\n   the developer of the tool, as the radiologist makes the decision on diagnosis\n   and is liable for any errors.\n   The safeguards in place for the targeted advertising cases mainly follow\n   data protection requirements, such as ensuring that consent is obtained and\n   respected. One company makes sure not to have clients engaged in illicit\n   practices and rejects clients from certain sectors, such as political advertising.\n   Complaints received\n   Few of the organisations interviewed received any complaints challenging\n   their use of AI. In some cases, interviewees claim to have received complaints\n   by complainants not aware that AI was used, who noticed incorrect outputs\n   in decision making.\n   For example, individuals lodged complaints regarding traffic fines, whereby\n   a police officer stopped a car driver, and upon hearing the car driver’s\n   explanation as to why the fine was wrongfully administered, proceeded\n   to manually correct the information in the system, without being able to\n   update the system’s historical data. In these cases, such fines will remain\n   visible throughout the system, and this particular person would continue to\n   be profiled as a high risk on each occasion.\n   Even though organisations rarely received any formal complaints with respect       “The number of the complaints\n   to their use of AI, interviewees often state that this is due to the early stages  about data use is miniscule, rather\n   of their AI implementation. Nonetheless, interviewees reported repeated            people may have asked to delete\n   requests for access to or rectification of personal data, and some people          some information about them.”\n   requested their information to be removed, as well as explanations as to           (Private company, Estonia)\n   why a certain recommendation was made.\n   The majority of interviewees claim that procedures are the same as to if\n   a decision had been processed or undertaken by a human. On the other\n   hand, a few other interviewees showed interest in opening new channels\n   to analyse, explain and redress decisions involving their AI solutions.\n   Other rights linked to access to justice set out in the Charter are also impacted,\n   most notably by the use of AI in law enforcement. These include, for example,\n   the presumption of innocence (Article 48 of the Charter).\n   When identifying people who are suspected of having committed a crime,\n   the police may target their activities specifically against one person or put\n   them under suspicion based on flawed and fragmented data and algorithmic\n   profiling.62 Uncritical reliance on automated tools, without proper human review\n   that takes into account other information, might contribute to discrimination\n   in decision making.\n78\n4.7.\t RIGHT TO SOCIAL SECURITY AND SOCIAL\n         ASSISTANCE\n                                 The right to social security and assistance\n                                 enshrined in Article 34 of the Charter is a classic\n                                 social right,63 inspired by various international\n                                 and European legal standards.64 This provision,\n                                 combining both elements of a right and of\n                                 a principle,65 has a great significance in the\n                                 EU in view of the free movement of people\n                                 within the Union.\n                                 Instead of tying issues of social protection\n                                 to the labour market, this Charter right takes\n                                 a new, communitarian approach when broadly\n                                 referring to “providing [social] protection in\n                                 cases such as maternity, illness, industrial\n                                 accidents, dependency or old age, and in the\n                                 case of loss of employment” (Article 34 (1)).66\nIt is, however, a primarily programmatic statement that does not prescribe\nany minimum standard of protection. It is in principle up to EU Member States\nto determine the conditions of entitlement and access to social benefits, with\nfurther clarification needed from the CJEU.67 Yet, Article 34 (1) of the Charter\nprovides protection against measures restricting or abolishing existing social\nsecurity rights.68\nIn addition, access to social rights is guaranteed to all individuals legally\nresiding within the EU who exercise their right to free movement, regardless\nof their nationality, subject to EU and national laws (Article 34 (2)). This thus\ncreates justiciable rights before national courts and the CJEU.69\nIt is becoming increasingly apparent that the impact of AI technologies on\nsocial protection systems and the lives of the many individuals who rely upon\nthem can be far-reaching and – potentially – very problematic. Introducing\nAI-driven technologies in social welfare systems risks creating barriers to\naccess to this right.70\nFor example, using AI in social security needs to account for potential\nnegative – and discriminatory – effects on non-nationals (both EU citizens\nand third-country nationals) exercising their right to freedom of movement\nin the EU. They could be negatively affected, for example, if a system relies\non data about job histories, which are not available for those moving from\nother EU Member States.\nOnly one respondent addressed the ‘right to receive a correct pension’ as\nan aspect of a wider definition of human rights. Meanwhile, none of those\ninterviewed referred to the fundamental right to social security and social\nassistance. This could partly reflect the nature of the use cases. However,\nthe lack of references to social rights among public sector interviewees was\nnotable.\n4.8.\t CONSUMER PROTECTION\nThe Charter stipulates that EU policies must ensure a high level of consumer\nprotection, which is based on Article 169 of the TFEU. EU institutions and\nother bodies needs to observe this principle, as do Member State authorities\nwhen implementing EU law.71\n                                                                                     79\n   This Charter principle provides only for the guarantee of a particular goal (“a\n   high level of consumer protection”). Article 169 of the TFEU is more concrete,\n   as it also determines the means of how to achieve the stated aim – for\n   example, protecting the health, safety and economic interest of consumers,\n   as well as promoting their right to information and education. 72\n   Among the use cases, the use of AI for targeted advertising, and the use of\n   medical records by companies, are of particular importance.\n   When it comes to targeted advertising, consumers need to be aware that\n   they can opt out from being targeted. If they are not aware, they might be\n   subjected to advertising they do not want. This is particularly problematic in\n   combination with highly sophisticated AI systems for advertising, which can\n   amount to some sort of manipulation of consumer preferences.73\n   Consumer protection is also of major relevance for the use of health data\n   (EHRs). The European Consumer Organisation (BEUC) noted that AI in the\n   area of health brings challenges for consumers. It recommends that AI\n   technologies must fully respect data protection rules, be transparent to\n   the consumer, and avoid discrimination. BEUC has also called for updated\n   regulation and legislative measures for market surveillance, law enforcement,\n   and efficient redress concerning digital health products and services to fully\n   protect EU consumers.74\n   BEUC carried out a survey among consumers about their views on AI in\n   selected EU Member States. It shows that more than one in two respondents\n   agree that companies are using AI to manipulate consumer decisions. In\n   addition, almost half of respondents believe that personalised content and\n   adverts on e-commerce platforms do not have an added value (44 %).\n   Slightly more than half of the survey respondents expressed low trust that\n   governments effectively control AI.75\n   In the interviews conducted for this study, consumer protection was only\n   mentioned at the margins, when discussing risks of using AI and fundamental\n   rights. However, some respondents from businesses refer to consumer\n   protection legislation as a relevant framework also applying to their use\n   of AI. Moreover, some respondents deem consumer protection authorities\n   potentially relevant oversight bodies when AI is used.\n   In general terms, many interviewees in the business sector stress the\n   importance of consumer satisfaction. For example, a company using video\n   surveillance for the security of customers at their premises mention that\n   consumer protection regulations are relevant for such technical solutions,\n   and that the use of the systems should aim to improve the situation of\n   consumers while also preserving their rights. Several AI tools are built to\n   understand and profile consumers to enable businesses to improve their\n   services and marketing.\n   Data protection is an important aspect for business. This is also linked to the\n   fact that breaching data protection rules is considered a business risk, as\n   mentioned above. One major concern of companies is obtaining and managing\n   consent from consumers and customers to process their data, when using\n   AI tools for marketing purposes. Interviewees report that the GDPR has had\n   an impact, improving their systems to handle consent.\n80\n4.9.\t RIGHT TO GOOD ADMINISTRATION\nThe right to good administration is a well-established general principle of\nEU law elaborated by the CJEU. As such, it is binding on all EU Member States.76\nIt is also a fundamental right enshrined in Article 41 of the Charter, although\nonly for actions of EU institutions, bodies and agencies.77\nAs a general principle of EU law, it requires EU Member States to apply the\nrequirements of the right to good administration in all public action. This right\nincludes, but is not limited to, the right of an individual to have access to\ntheir file and the obligation of any public authority to give sufficient reasons\nfor its decisions.78\nAccess to the file facilitates understanding of the evidentiary basis on which\na decision has been made, and/or of the reasons underlying it. This places\nthe individual in a better position to put forward counter-arguments when\nexercising the right to be heard and the right to an effective remedy.79\nThe obligation to give reasons makes, from the perspective of the individuals\naffected, the decision-making process more transparent, so that the person\nconcerned can know and understand why a measure or action has been\ntaken. Transparency is also an enabling principle that provides foundations\nfor other rights,80 including the exercise of the right to an effective remedy.\nAccording to the CJEU, the context in which individual decisions are made is\nimportant in determining the extent of the duty to give reasons.81 In France, for\ninstance, the Code on the Relations between the Public and the Administration\nrequires written explanations of the factual and legal considerations on which\na decision has been based.82\nThe right to good administration also applies when AI systems process\npersonal data and support decision making by public authorities. Although\nthe right to good administration may be subjected to certain limitations,\nthe question arises of how to ensure that the potentially huge number of\nindividuals all have access to their files (personal data used in AI systems).\nAnother question is how to make sure that public authorities always give\nsufficient reasons when the operation of AI-driven technologies cannot be\nfully explained due to their inherent opacity and complexity.\nThe use of a system to categorise unemployed people, set up in Poland,\nhighlighted problems linked to public administration and the use of algorithms.\nBased on questions answered by unemployed people, a categorisation was\ndeveloped through a statistical algorithm. The system received a lot of criticism\nfrom civil society with respect to the lack of opportunities to complain and\npotential discrimination.83 In the end, a complaint by the Ombudsinstitution –\nbased on administrative grounds – led to a constitutional court ruling that\nput an end to the system’s use.84\nThe intent to increase efficiency drives the use of AI in the public sector – an\naim that directly speaks to improving administration and benefiting citizens.\nRespondents in public administration by far most often indicate efficiency\nas the reason for considering the use of AI or for presently using AI. One\nrespondent, who advises ministries on digital strategies and their use of\nAI, said that the main reasons for adopting AI are to improve the service to\ncitizens and to reduce the costs of these services for public administration.\nInterviewees also indicate that public administration has particular\nrequirements, meaning AI cannot be used for all purposes and needs particular\nattention when it comes to decision making. However, the efficiency of\na system is also considered an important added value.\n                                                                                  81\n   In this sense, a respondent working on the digitalisation of migration\n   management indicates that building too complex AI systems is a risk,\n   because afterwards it would require a lot of work to understand the system\n   in retrospect. The interviewee indicates that their team needs to be careful\n   not to allow AI to make final decisions, which have to be taken by a human –\n   because society and clients are not ready for this, according to the interviewee.\n   Although some systems are appealing, they do not work effectively, and this\n   could result in extra work and negative results. However, the interviewee\n   also indicates that the dimension of efficiency “is often side-lined when\n   discussing data protection”.\n   The requirements for good administration also directly link the issues raised\n   above with respect to data protection, non-discrimination and the right to an\n   effective remedy and fair trial. Public administration can only process data\n   on a legal basis. Decisions need to be fair and transparent and pathways\n   to challenge decisions need to be available and accessible. As a result, the\n   requirements for good administration are directly linked to the discussion\n   and analysis above with respect to the legal processing of data (under data\n   protection), fair decisions (linked to the discussion about non-discrimination),\n   alongside transparency and ways to challenge and explain decisions (with\n   respect to access to justice).\n82\nEndnotes\n1   See European Commission, European enterprise survey on the use of technologies based on artificial intelligence, Luxembourg, July\n    2020.\n2 FRA (2020), What do fundamental rights mean for people in the EU, Luxembourg, Publications Office, p. 28.\n3 See webpage on Three-level IT Baseline Security System ISKE on ther website of Estonia’s Information System Authority.\n4 See the website of the PCI Security Standards Council.\n5 Barak, A. (2019), ‘Human dignity as a framework right (motherright)’, in Barak, A., Human Dignity: The Constitutional Value and the\n    Constitutional Right, Cambridge, Cambridge University Press, 2015, Ch. 9 (pp. 156-169).\n6 CJEU, C-377/98, Netherlands v. European Parliament and Council, 9 October 2001, paras. 70-77.\n7 For a discussion on the malicious use of AI, see for example, Brundage, M. et al. (2018), The Malicious Use of Artificial Intelligence:\n    Forecasting, Prevention, and Mitigation.\n8 FRA (2019), Facial recognition technology: fundamental rights considerations in the context of law enforcement, Luxembourg,\n    Publications Office, November 2019.\n9 CJEU, Joined Cases C-92/09 and C-93/09, Volker und Markus Schecke and Eifert GbR and Hartmut Eifert, Opinion of Advocate General\n    Sharpston, 17 June 2010, para. 71.\n10 FRA, Council of Europe and EDPS (2018), Handbook on European data protection law. 2018 Edition, Luxembourg, Publications Office, June\n    2018, p. 19.\n11 See also Ibid., pp. 35-52.\n12 ECtHR (2019), Guide on Article 8 of the European Convention on Human Rights – Right to respect for private and family life, home and\n    correspondence, Strasbourg, Council of Europe, updated on 31 August 2019, paras. 133 and 136.\n13 ECtHR, López Ribalda and Others v. Spain, Nos. 1874/13 and 8567/13, 17 October 2019, para. 87. For a comprehensive legal analysis of\n    the meaning and content of ‘privacy’, see also Koops, B.-J. et al. (2017), ‘A Typology of Privacy’, University of Pennsylvania Journal of\n    International Law, Vol. 38, Issue 2, pp. 483-575.\n14 Vermeulen, M. (2015), SURVEILLE Deliverable D4.7 – The scope of the right to private life in public places, July 2014, p. 2.\n15 UN, Human Rights Committee, General Comment No. 37 (2020) on the right of peaceful assembly (article 21), CCPR/C/GC/37, 17\n    September 2020, para. 62.\n16 Costello, Róisín Áine (2020), The Impacts of AdTech on Privacy Rights and the Rule of Law,\nTechnology and Regulation.\n17 Norwegian Consumer Council (2020), Out of Control. How consumers are exploited by the online advertising industry.\n18 FRA (2020), Your rights matter: Data protection and privacy – Fundamental Rights Survey, Luxembourg, Publications Office.\n19 Rocher, L., Hendrickx, J. M. and de Montjoye Y. (2019), Estimating the success of re-identifications in incomplete datasets using\n    generative models, Nature Communications 10, No. 3069.\n20 Hacker, P. (2020), A Legal Framework for AI Training Data. Law, Innovation and Technology (forthcoming), available at SSRN; Article\n    29 Data Protection Working Party (2014), Opinion 05/2014 on Anonymisation Techniques; see also Finck, Michèle and Pallas, Frank,\n    They Who Must Not Be Identified - Distinguishing Personal from Non-Personal Data Under the GDPR (October 1, 2019), Forthcoming,\n    International Data Privacy Law, 2020, Max Planck Institute for Innovation & Competition Research Paper No. 19-14, available at SSRN; and\n    Sartor G. and Lagioia F. (2020), The impact of the General Data Protection Regulation (GDPR) on artificial intelligence, study prepared for\n    the panel for the Future of Science and Technology (STOA) of the European Parliament.\n21 See, for example, the UK Data Service’s blog on “Access to sensitive data for research: ‘The 5 Safes’”; see also the discussion in Ohm, P.\n    (2010), “Broken promises of privacy: responding to the surprising failure of anonymization”, UCLA Law Review, p. 1701.\n22 GDPR, Art. 22 (3); and Law Enforcement Directive, Art. 11 (1).\n23 Veale, M. and Edwards, L. (2018), ‘Clarity, surprises, and further questions in the Article 29 Working Party draft guidance on automated\n    decision-making and profiling’, Computer Law & Security Review, Vol. 34 (2), April 2018, pp. 398-404.\n24 Article 29 Data Protection Working Party (2018), Guidelines on Automated individual decision-making and Profiling for the purposes of\n    Regulation 2016/679, Adopted on 3 October 2017, as last Revised and Adopted on 6 February 2018.\n25 Green, B. And Chen, Y. (2019), ‘Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments’, In FAT* ’19:\n    Conference on Fairness, Accountability, and Transparency (FAT* ’19), January 29-31, 2019.\n26 González Fuster, G. (2020), Artificial Intelligence and Law Enforcement – Impact on Fundamental Rights, European Parliament, Policy\n    Department for Citizens’ Rights and Constitutional Affairs, Directorate-General for Internal Policies, PE 656.295, July 2020, p. 17; Brkan,\n    M. (2019), ‘Do algorithms rule the world? Algorithmic decision-making and data protection in the framework of the GDPR and beyond’,\n    International Journal of Law and Information Technology, Vol. 27 (2), p. 98; Article 29 Working Party, Guidelines on Automated individual\n    decision-making and Profiling for the purposes of Regulation 2016/679, Adopted on 3 October 2017, as last Revised and Adopted on 6\n    February 2018, WP251rev.0, p. 19.\n27 Misuraca, G., and van Noordt, C. (2020), Overview of the use and impact of AI in public services in the EU, European Commission Joint\n    Research Centre, Luxembourg.\n28 Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial\n    or ethnic origin, OJ L 180, 19.7.2000, pp. 22-26, Art. 2; and Council Directive 2000/78/EC of 27 November 2000 establishing a general\n    framework for equal treatment in employment and occupation, OJ L 303, 2.12.2000, pp. 16-22, Art. 2.\n29 FRA and CoE (2018), Handbook on European non-discrimination law. 2018 edition, Luxembourg, Publications Office, June 2018, p. 35.\n30 CJEU, C-236/09, Association Belge des Consommateurs Test-Achats ASBL and Others v. Conseil des ministres, 14 January 2011.\n31 European Commission (2012), EU rules on gender-neutral pricing in insurance industry enter into force, Press release, IP/11/1581, 20\n    December 2012.\n32 Elizabeth E. Joh (2015), ‘The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing’, UC Davis Legal Studies Research\n    Paper No. 473, pp. 17-18.\n33 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, p. 14. DOI:\n    10.1177/1477370819876762.\n34 See also European Commission, White Paper On Artificial Intelligence – A European approach to excellence and trust, COM(2020) 65 final,\n    Brussels, 19 February 2020, p. 1.\n35 FRA (2018), #BigData: Discrimination in data-supported decision making, Luxembourg, Publications Office, June 2018, p. 3.\n36 Ibid.\n37 FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications\n    Office.\n                                                                                                                                                   83\n   38 Korff, D. and Browne, I. (2013) ‘The use of the Internet & related services, private life & data protection: trends, technologies, threats\n      and implications’, Council of Europe, T-PD(2013)07.\n   39 See National Non-discrimination and Equality Tribunal of Finland, decision no. 216/2017 from 21 March 2018. See also the SyRI case\n      discussed above and UK, Court of Appeal, R (Bridges) v. CC South Wales, [2020] EWCA Civ 1058, 11 August 2020.\n   40 See also Equinet (2020), Regulating for an equal AI: A new role for equality bodies, Brussels, report prepared by Allen R. and Masters D.\n   41 Tolan S., Miron M., Gomez E. and Castillo C (2019), ‘Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for\n      Juvenile Justice in Catalonia’, Best Paper Award, International Conference on AI and Law, 2019; Richardson R., Schultz J. and Crawford K.\n      (2019), Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice, 94 N.Y.U. L.\n      REV. ONLINE 192 (2019), available at SSRN.\n   42 FRA (2014), Violence against women: an EU-wide survey. Main results report, Luxembourg, Publications Office, p. 61.\n   43 FRA (2018), Second European Union Minorities and Discrimination Survey. Main results, Luxembourg, Publications Office, p. 66.\n   44 Erik Bakke (2018), “Predictive policing: The argument for public transparency”, New York University Annual Survey of American Law, Vol.\n      74, pp. 139-140; Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, Washington University Law Review, Vol. 94, pp. 1146-1150;\n      andCouncil of Europe Committee of experts on internet intermediaries (MSI-NET) (2017), Algorithms and Human Rights, Council of Europe\n      DGI(2017)12, p. 11.\n   45 Elizabeth E. Joh (2015), ‘The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing’, UC Davis Legal Studies Research\n      Paper No. 473, p. 18.\n   46 Gstrein, O. J., Bunnik, A., and Zwitter, A. (2019), ‘Ethical, Legal and Social Challenges of Predictive Policing’, Católica Law Review 3:3, pp.\n      77-98; Albert Meijer & Martijn Wessels (2019), ‘Predictive Policing: Review of Benefits and Drawbacks’, International Journal of Public\n      Administration 42:12, p. 1036, DOI: 10.1080/01900692.2019.1575664.\n   47 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, pp. 8-9.\n      DOI: 10.1177/1477370819876762.\n   48 Wachter, Sandra (2020), ‘Affinity Profiling and Discrimination by Association in Online Behavioural Advertising’, Berkeley Technology Law\n      Journal, Vol. 35, No. 2, 2020, (forthcoming), available at SSRN.\n   49 On the use of AI in financial industries leading to unequal access to financial services, see in the legal literature e.g. Boyd, D., Levy K. &\n      Marwick, A. (2014), ‘The Networked Nature of Algorithmic Discrimination’ in Gangadharan, S. P., Eubanks, V. & Barocas, S. (eds), Data and\n      Discrimination: Collected Essays, Open Technology Institute, pp. 53-62.\n   50 For an overview of child rights issues, see UNICEF Innovation, Human Rights Center, UC Berkeley (2019), Artificial Intelligence and\n      Children’s Rights.\n   51 EU Network of Independent Experts on Fundamental Rights, Commentary on the Charter on Fundamental Rights of the European Union,\n      June 2006, p. 360. See also: FRA and CoE (2016), Handbook on European law relating to access to justice, Luxembourg, Publications\n      Office, June 2016, p. 92.\n   52 CJEU, C-432/05, Unibet (London) Ltd, Unibet (International) Ltd v. Justitiekanslern, 13 March 2007, para. 37; CJEU, C-93/12, ET\n      Agrokonsulting-04-Velko Stoyanov v. Izpalnitelen direktor na Darzhaven fond ‘Zemedelie’ – Razplashtatelna agentsia, 27 June 2013, para.\n      59; CJEU, C-562/13, Centre public d’action sociale d’Ottignies-Louvain-la-Neuve v. Moussa Abdida, 18 December 2014, para. 45.\n   53 Law Enforcement Directive, Art. 54; and GDPR, Art. 79.\n   54 Law Enforcement Directive, Art. 53; and GDPR, Art. 78.\n   55 Law Enforcement Directive, Art. 52; and GDPR, Art. 77.\n   56 Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights impacts of\n      algorithmic systems (adopted by the Committee of Ministers on 8 April 2020 at the 1373rd meeting of the Ministers’ Deputies), Appendix,\n      para. B.4.5.\n   57 Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, 94 Washington University Law Review, pp. 1165-1167.\n   58 Gstrein, O. J., Bunnik, A., & Zwitter, A. (2019), Ethical, Legal and Social Challenges of Predictive Policing’, Católica Law Review, 3:3, pp. 80-\n      81; Yeung K. (2019), A study of the implications of advanced digital technologies (including AI systems) for the concept of responsibility\n      within a human rights framework, Prepared by the Council of Europe Expert Committee on human rights dimensions of automated data\n      processing and different forms of artificial intelligence (MSI-AUT).\n   59 Council of Europe, Algorithms and human rights, pp.11 and 24.\n   60 International Technology Law Association (2019), ‘Responsible AI: A Global Policy Framework’, pp. 258-282.\n   61 The lack of expertise on AI is also reflected in the survey among companies in the EU, where the lack of skills among existing staff and\n      difficulties in hiring new staff are the most prominent obstacle for further AI adoption (European Commission (2020), European enterprise\n      survey on the use of technologies based on artificial intelligence, Luxembourg, July 2020, p. 11).\n   62 See a detailed assessment of the impact of predictive policing on the presumption of innocence in Mendola, Marco (2016), One Step\n      Further in the ‘Surveillance Society’: The Case of Predictive Policing.\n   63 See e.g. Egorov, A. and Wujczyk, M. (eds.) (2016), The Right to Social Security in the Constitutions of the World: Broadening the moral\n      and legal space for social justice, Geneva, ILO Global Study, Vol. 1: Europe, pp. xv-xvii and 1-6.\n   64 These include Arts. 153 and 156 of the TFEU; Arts. 12 and 13 of the 1961 European Social Charter; as well as points 2 and 10 of the 1989\n      Community Charter on the Fundamental Social Rights of Workers (see Explanations relating to the Charter of Fundamental Rights, OJ\n      C 303, 14.12.2007, pp. 17-35).\n   65 Explanations relating to the Charter of Fundamental Rights (OJ C 303, 14.12.2007, pp. 17-35), Explanation on Article 52 — Scope and\n      interpretation of rights and principles.\n   66 Łukasz Bojarski, Dieter Schindlauer and Katrin Wladasch (2014), The European Charter of Fundamental Rights as a Living Instrument –\n      Manual, Rome/Warsaw/Vienna, pp. 61-62.\n   67 De Becker, E. (2016), ‘The (Possible) Role of the Right to Social Security in the EU Economic Monitoring Process’, German Law Journal, Vol.\n      17, No. 3, pp. 297, 304; Paju, J. (2017), The European Union and Social Security Law, Oxford, Hart Publishing, sub-section 7.5.2.\n   68 Ibid., pp. 297-298; Peers, S. & Prechal, S. (2014), ‘Scope and Interpretation of Rights and Principles’, in Hervey, T., Kenner, J., Peers, S. and\n      Ward, A. (eds.), The EU Charter of Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1455, 1508.\n   69 With the exception of Poland and the United Kingdom, see Protocol (No. 30) on the application of the Charter of Fundamental Rights of\n      the European Union to Poland and to the United Kingdom (OJ C 115, 9.5.2008, pp. 313-314), Art. 1 (2).\n   70 Christiaan van Veen and Ben Zevenbergen, ‘Conference on Social Protection by Artificial Intelligence: Decoding Human Rights in a Digital\n      Age’, Freedom to Tinker – Research and Expert Commentary on Digital Technologies in Public Life, 29 May 2019.\n   71 Art. 51 (1) of the Charter; see also Explanations relating to the Charter of Fundamental Rights (OJ C 303, 14.12.2007, pp. 17-35),\n      Explanation on Article 52 — Scope and interpretation of rights and principles.\n   72 Łukasz Bojarski, Dieter Schindlauer and Katrin Wladasch (2014), The European Charter of Fundamental Rights as a Living Instrument –\n      Manual, Rome/Warsaw/Vienna, p. 67.\n84\n73 Sartor, Giovanni (2020), New aspects and challenges in consumer protection, study for the committee on the Internal Market and\n   Consumer Protection, Policy Department for Economic, Scientific and Quality of Life Policies, European Parliament, Luxembourg.\n74 The European Consumer Organisation (BEUC) (2018), Digital Health, Principles and Recommendations.\n75 BEUC (2020), Artificial Intelligence: what consumers say. Findings and policy recommendations of a multi-country survey on AI.\n76 In recent case law, see CJEU, C-604/12, H. N. v. Minister for Justice, Equality and Law Reform, Ireland, Attorney General, 8 May 2014, para.\n   49.\n77 Also confirmed by the CJEU, Joined Cases C-141/12 and C-372/12, YS v. Minister voor Immigratie, Integratie en Asiel, and Minister voor\n   Immigratie, Integratie en Asiel v. M, S, 17 July 2014, paras. 66-70).\n78 These components, initially developed by the CJEU case law, have been codified in Article 41 (2) of the Charter. For more on this right in\n   leading academic literature, see Craig, P. (2014), ‘Article 41 – Right to Good Administration’, in Hervey, T., Kenner, J., Peers, S. and Ward, A.\n   (eds.), The EU Charter of Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1069-1098.\n79 Ibid., p. 1082.\n80 Finck, M. (2019), ‘Automated Decision-Making and Administrative Law’, Max Planck Institute for Innovation & Competition Research\n   Paper No. 19-10, p. 8.\n81 Craig, P. (2014), ‘Article 41 – Right to Good Administration’, in Hervey, T., Kenner, J., Peers, S. and Ward, A. (eds.), The EU Charter of\n   Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1086-1087.\n82 France, Code des relations entre le public et l’administration, Article L2111-5.\n83 Panoptykon Foundation (2015), Profiling the unemployed in Poland: Social and political implications of algorithmic decision making; see\n   also Algorithm Watch (2019), Poland to scrap controversial unemployment scoring system.\n84 See Decision K 53/16, available on the Constitutional Tribunal’s website.\n                                                                                                                                                     85\n865\nFUNDAMENTAL RIGHTS IMPACT\nASSESSMENT – A PRACTICAL TOOL\nFOR PROTECTING FUNDAMENTAL\nRIGHTS\n             Chapter 4 illustrated the extent to which using AI affects different fundamental\n             rights. This chapter analyses how fundamental rights impact assessments\n             (FRIA) could reduce the negative impacts that using AI can have on\n             fundamental rights.\n             Section 5.1 provides a brief overview of the current discussion on the need\n             for fundamental rights impact assessments in this field. Section 5.2 analyses\n             current practices in addressing fundamental rights implications, based on the\n             interviews conducted for this report. Interviewees were asked about what\n             sort of testing was done before the system was used, and who controls the\n             tasks affected by the use of the technology.\n             The chapter ends with suggestions on how to assess the fundamental rights\n             impact when using AI and related technologies.\n             5.1.\t CALLING FOR A FUNDAMENTAL RIGHTS IMPACT\n                     ASSESSMENT – AVAILABLE GUIDANCE AND TOOLS\n             International organisations,1 academics2 and civil society3 have called for\n             fundamental rights impact assessments to be conducted when using AI or\n             related technologies.\n             For example, the Committee of Ministers of the Council of Europe’s guidelines\n             on addressing the human rights impacts of algorithmic systems recommend\n             that states should conduct “impact assessments prior to public procurement,\n             during development, at regular milestones, and throughout their context-\n             specific deployment in order to identify the risks of rights-adverse outcomes”.4\n             There is a need for flexible impact assessments that can adapt to different\n             situations given that fundamental rights violations are always contextual.\n             Scholars exemplify this based on EU anti-discrimination law, where equality\n             is always contextual and depends on the case at hand.5\n             Fundamental rights compliance cannot be automated and hard-coded into\n             computer software. Rather, each use case needs separate examination\n             to determine whether any fundamental rights issue arises. Nevertheless,\n             assessments can follow a systematic approach and provide similar information.\n             Existing standards provide guidance on how to do a fundamental rights impact\n             assessment of AI and related technology. These include hard law, soft law\n                                                                                              87\n   instruments (such as recommendations or declarations), and practical tools\n   (e.g. guidelines and checklists).\n   Beyond the requirements flowing from data protection legislation (see box),\n   there are few examples of laws requiring mandatory assessments of the\n   effects of AI in general. In view of the increasing uptake of AI, the Canadian\n   government has issued guidelines, including mandatory requirements for\n   assessing AI for use by public administration. It applies to any system, tool,\n   or statistical model used to recommend or make an administrative decision\n   about a client.6\n88\n            European data protection law requires a data protection impact assessment (DPIA).a The\nLearning    CoE Modernised Convention No. 108 provides for a general obligation to examine the likely\nfrom data   impact of data processing on individuals’ rights and fundamental freedoms before their use.\nprotection  Following the assessment, controllers should design the processing in such a manner to\nimpact      prevent or minimise identified risks.b\nassessments\n            EU law imposes a similar, more detailed, obligation. The GDPR foresees a Data Protection\n            Impact Assessment (DPIA) for data processing that is likely “to result in a high risk to the\n            rights and freedoms of natural persons.”c Therefore, where required by law, a DPIA for an\n            AI technology could potentially also address the broader fundamental rights implications,\n            besides the impact on the right to privacy,d and be used as a tool to further investigate\n            algorithms and their impacts.e\n            However, under the GDPR (Article 35), the DPIA is limited to ‘high risk’ cases processing\n            personal data. It therefore may miss other high risk cases that are not primarily or obviously\n            related to protection of personal data. At the same time, the GDPR is delimited to its specific\n            field of application, with accompanying expertise in this field. This means that the potential\n            extension of the scope of a DPIA to other fundamental rights might be limited.\n            The GDPR also gives some indications about the modalities to undertake a DPIA. First,\n            a DPIA should be conducted before any high risk processing.f Second, a DPIA should provide\n            for a systematic description of envisaged operations, the purpose and the legitimate\n            interests pursued. It must also assess the necessity and proportionality of the processing\n            and the possible risks to the rights of individuals. In addition, it must contain the planned\n            security measures to address the risks identified.g\n            While pointing out that different methodologies can apply, the Article 29 Working Party (WP\n            29) proposes – in a check list form – minimum criteria that a controller should use to assess\n            if the DPIA comprehensively complies with the GDPR.h\n            Finally, the GDPR foresees prior mandatory consultation of the relevant supervisory\n            authority, if the impact assessment indicates that processing presents risks that cannot be\n            mitigated. i This gives a crucial role to DPAs, as independent bodies established by law.j\n            The European Data Protection Supervisor (EDPS) provides guidance on carrying out DPIAs.k\n            Data protection authorities have also discussed, and provide guidance on, how to assess AI\n            technologies.l\n            a\n              For more information on Data Protection Impact Assessment, see: FRA, Council of Europe and\n            EDPS (2018), Handbook on European data protection law. 2018 edition, p. 179-181.\n            b\n              Council of Europe Modernised Convention No. 108, Art. 10 (2).\n            c\n              GDPR, Art. 35 (1).\n            d\n              GDPR, Recitals (2) and (75); Article 29 Working Party, Guidelines on Data Protection Impact\n            Assessment (DPIA), wp248rev.01, 13 October 2017.\n            e\n              Edwards and Veale (2018); FRA (2018), #BigData: Discrimination in data-supported decision\n            making, Luxembourg, Publications Office, June 2018.\n            f\n              GDPR, Art. 35 (1). The WP29 specifies that ‘carrying out a DPIA is a continual process, not a one-\n            time exercise.’\n            g\n              GDPR, Art. 35 (7), as well as recitals (84) and (90).\n            h\n              Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13\n            October 2017, Annex 2.\n            i\n              GDPR, Art. 36.\n            j\n              GDPR, Art. 35.\n            k\n              EDPS (2019), Accountability on the ground Part II: Data Protection Impact Assessments & Prior\n            Consultation, v.1.3, July 2019.\n            l\n              See, for example, the Declaration on ethics and data protection in AI, adopted by the 40th\n            International Conference of Data Protection and Privacy Commissioners (ICDPPC), in 2018.\n                                                                                                                 89\n   There are many more examples of non-binding guidelines. At the global\n   level, the United Nations Guiding Principles on Business and Human Rights\n   recommend that enterprises integrate the findings from human rights impact\n   assessments across relevant internal functions and processes, and take\n   appropriate action.7 Although they do not refer specifically to AI, the guidelines\n   are relevant in supporting the development of AI technology in a rights\n   compliant manner.8\n   At the EU level, the Ethics Guidelines for Trustworthy AI prepared by the\n   European Commission’s High-Level Group on Artificial Intelligence9 also\n   recommend performing a FRIA, before a system’s development, when\n   “there are risks that fundamental rights can negatively be affected by the\n   technology”.10 They also emphasise the need to put in place mechanisms\n   to receive external feedback on AI systems that potentially infringe on\n   fundamental rights.\n   In addition, private companies, 11 associations of private companies12 or of both\n   public and private interests,13 as well as NGOs14 and other organisations15 have\n   developed different types of guidance to support AI impact assessments. These\n   documents do not usually contain clear guidelines on impact assessment.\n   Instead, they highlight the different aspects and criteria that should be taken\n   into account when developing and carrying out an impact assessment.\n   Broad categories include the purpose of the system, the description of the\n   technology, the assessment of the impact and targeted population/individual,\n   evaluating fairness and diversity, the description of the audits planned or\n   performed, as well as accountability. Some explicitly refer to applicable\n   international human rights law standards.16\n   Various codes of ethics or conducts,17 standards,18 as well as certification\n   schemes are also in place.19\n   Several practical tools are available to assess the impact of AI technologies and\n   mitigate risks, developed by a wide range of actors. These include checklists,20\n   lists of questions,21 online self-evaluation tools,22 and risk management\n   frameworks.23\n   Some focus specifically on assessing fundamental rights risks.24 Others focus\n   on ethical, societal or economic implications.25 These can be useful references\n   when performing a thorough fundamental rights impact assessment of AI\n   technologies.\n   In July 2020, for example, the High-Level Group on Artificial Intelligence\n   issued an “Assessment List for Trustworthy AI” (ALTAI),26 after a six month\n   pilot involving more than 350 stakeholders. ALTAI helps organisations\n   self-evaluate – on a voluntary basis – the reliability and trustworthiness of\n   AI and reduce potential risks for users. It supports businesses and public\n   administrations to ask the right questions around the seven requirements\n   for responsible AI identified in the Ethics Guidelines for Trustworthy AI.27\n   ALTAI specifically refers to the need to perform a fundamental rights impact\n   assessment. It includes examples of questions to assess impact on non-\n   discrimination/equality; the right to privacy; the rights of the child; the\n   freedom of expression; as well as the freedom of information and association.28\n   Several online assessment tools target the use of AI by public authorities.\n   The Canadian Government developed an Algorithmic Impact Assessment tool\n   (AIA),29 pursuant to the Canadian Directive on Automated Decision-Making.30\n   The AIA represents an automated assessment consisting of more than 50\n   questions that unfold the requirements of the directive. Questions relate to\n90\nfundamental rights concerns – such as an AI system’s impact on the freedom\nof movement, on the likelihood of incarceration of an individual, on the legal\nstatus, on access to funding or benefits, or on indigenous people. A score\nis attributed to each reply and a final impact scoring is provided and made\npublically available on the government’s website.\nAs another example, the Ethics Toolkit31 is a freely accessible tool designed\nfor local governments. Based on a risk management approach, it supports fair\nautomated decisions and minimising unintentional harm to individuals in the\nfield of the criminal justice, higher-education, social media and other areas.\nAmong national human rights bodies, the Danish Institute for Human\nRights proposed a human rights compliance “quick check.32 This involves\nan interactive online computer programme that allows companies to select\nand modify the information in a database to suit their type of business and\narea of operations to check rights compliance. The quick check is based on\nthe Human Rights Compliance Assessment tool,33 which runs on a database\nof over 350 questions and 1,000 corresponding human rights indicators. It\nuses international human rights law standards as benchmarks. Applying to\nall fields of operations, it can provide guidance when developing impact\nassessment for AI technology.\nAcademic work has also suggested operational frameworks for assessing\nrisks in using AI technology. Some focus specifically on identifying and\naddressing fundamental rights implications by the private sector.34 Some\nfocus on developing ethical and values-oriented models (analysing the\nsocietal impact of the data used) with the creation of ad hoc expert (review)\ncommittee.35\nOthers have developed guidance frameworks for specific case studies.\nFor example, in the field of criminal justice, the ALGO CARE framework36\nintroduced a step-by-step assessment to evaluate the key legal and practical\nconcerns that should be considered in relation to police using algorithmic\nrisk assessment tools.\nSome have argued for participatory ways to involve and consider the views\nof the affected rights-holders and other stakeholders communities when\ndeveloping an impact assessment and publically engage with them from\nthe start.37 Others have joined cross-discipline expertise of science and law\nto design practical frameworks.38\n5.2.\t IMPACT ASSESSMENTS AND TESTING IN PRACTICE\nVirtually all the systems discussed in the interviews were subject to some\nsort of testing, which included elements of impact assessment. However,\nthese were mainly technical and data protection (impact) assessments. These\nrarely address potential impacts on other fundamental rights.\n                                                                               91\n   Some interviewees argue against conducting a fundamental rights impact\n   assessment, because in their view the system does not negatively affect\n   fundamental rights or because they are unsure about it. For example,\n   a respondent working on traffic management, using cameras for monitoring\n   traffic, indicated that they only tested for accuracy of the system, but not\n   fundamental rights, apart from respecting data protection rules.\n   Some respondents simply did not know if fundamental rights were assessed\n   as part of a general impact assessment that was carried out.\n   Testing and stages of development\n   Much testing is done before any new AI system is used. As respondents            “When testing the system, we did\n   highlighted, moving an AI system into production is a very challenging task.     not really look at the legal aspects,\n   As mentioned, public administration as well as private companies are usually     we looked at whether the system is\n   careful when using AI. Many projects that interviewees refer to are still in     profitable.”\n   development or in the pilot phase, and some had not started concrete testing.    (Private company, Estonia)\n   Testing can be done in several stages. These include the development stage\n   (so-called proof-of-concept), pilot stages before deployment, and tests during\n   and after deployment. If possible, live experimentation is carried out at the\n   initial stages, which often involves staged deployment.\n   For example, the organisation interviewed that tests different applications\n   to support job seekers conducts continuous, step-by-step testing. Selected\n   members of the organisation test the tool in real situations, using check lists.\n   The interviewee mentioned that it is challenging to move to the deployment\n   stage and it is planned to supervise the tool in real time.\n   In another example, involving automated rule-based granting of social\n   benefits, different assessments were carried out. Before implementation,\n   a group of lawyers, data protection specialists, compensation specialists\n   and accountants performed a general impact assessment. After this, the\n   department responsible for using the system conducted tests to decide\n   whether the system could be used.\n   Following this, the system was monitored in its implementation, using a step-\n   by-step approach. In a first step, about half of the decisions were taken by\n   the system. In a next step, the decisions taken automatically were expanded\n   to all negative decisions. After this, another area of decisions was added,\n92\n                                     including all decisions on ending compensation payments. At the time the\n                                     interviews were conducted, about 95 % of decisions were automated. The\n                                     interviewee indicated that, after carrying out these tests, they feel sure that\n                                     the system is secure, and that there are no outstanding risks.\n                                     A company working on a fraud detection system replaced their rule-based\n                                     system with a machine learning tool. Before changing the system, the old\n                                     and new system were run in parallel to see if the machine learning system\n                                     performs better than the rule-based one. The interviewee mentioned that\n                                     “[there] was rigorous analysis behind it and direct feedback where we saw\n                                     what would be the impact on losses versus how many good customers we\n                                     were impacting negatively”. The interviewee added that, when they “were\n                                     comfortable that [the machine learning system] was better [than the static\n                                     rule system] in all aspects, we deployed it in its entirety”.\n                                     In other use cases, no previous automated system existed and tests were\n                                     reviewed by humans. For example, an automated transcription service was\n                                     tested during court hearings, when allowed by the judge. This included\n                                     regular feedback on the correctness of the transcription services from judges.\n                                     One interviewee from law enforcement, working on a tool to detect domestic\n                                     violence, identifies issues with precision and accuracy when using the system.\n                                     If a police officer does not have sufficient training and knowledge about the\n                                     system, the indicators required by the system cannot gather the required\n                                     information, which could lead to miscalculation. They highlight that the\n                                     robustness of the system is tested annually to assure the quality of the two\n                                     questionnaires used, the completeness of the data, and the training of the\n                                     police officers using the AI system. This process also considers how personal\n                                     data protection laws and protocols are applied. The tests discussed focus\n                                     strongly on technical aspects and general operations.\n                                     Fundamental rights and data protection impact assessments\n                                     Apart from data protection, which all respondents mentioned, other\n                                     fundamental rights were typically not considered. Respondents only reflected\n                                     about other potential impacts on fundamental rights, or mentioned that these\n                                     aspects were considered, when prompted by the interviewer.\n                                     Many respondents are generally aware of discrimination issues – but often\n                                     discussed this only after being explicitly asked about discrimination. Yet they\n                                     gave no information about any formal, in-depth tests for discrimination.\n                                     Generally, respondents ruled out the possibility that their system discriminates\n                                     based on protected attributes. For example, one interviewee states that they\n                                     test the system against data protection laws and specific applicable legal\n                                     acts, but not fundamental rights. However, the interviewee did consider\n                                     potential discrimination, but ruled it out. It needs to be kept in mind for future\n                                     technologies, the interviewee stated.\n                                     However, there are cases where non-discrimination was generally considered\n                                     during the testing phase of AI systems. One respondent from a municipal\n                                     authority mentioned that they cannot assess the fairness of a model, because\n                                     they cannot access data needed due to data protection reasons. According\n                                     to the interviewee, “there is a huge tension surrounding the GDPR. So we\n                                     want to do well, but might in fact be worse off, because interpretation of\n                                     the data then turns out to be impossible”.\n“Yes, we assess the legality of\npersonal data protection and the     Most respondents reported that a data protection impact assessment, as\nconformity with their specific legal required by law, was conducted, although these took different forms. A bank\nacts.”                               tested a tool for analysing speech from customer calls to find out about\n(Public administration, Estonia)     reoccurring problems, and carried out a data protection impact assessment\n                                                                                                                        93\n   (DPIA) specifically for testing the tool. The outcome was that the system\n   can be tested if data were only used for the testing phase and are deleted\n   after a certain period after the test, and if access to the data by employees\n   is restricted to the testing phase and supervised. For the deployment of the\n   tool, another DPIA is required in this case.\n   There is sometimes a lack of clarity as to what extent the use of AI and\n   related technologies, most notably the use of algorithms, belongs to an DPIA.\n   In the area of predictive policing, for instance, some DPIAs were done for\n   the underlying architecture of the system, rather than the specific AI tool.\n   Another interviewee using algorithms in financial services also mentioned\n   not assessing the machine learning tool as such within the framework of\n   a DPIA, because of the belief that it does not apply to the machine learning\n   system (but the underlying data).\n   One interviewee felt that the data protection impact assessment for the crime\n   heat map example was not sufficiently in depth to safeguard the quality of\n   the model, and that the system was not equipped to deal with cross-sectoral\n   use of data, where different rules might apply. They indicated that further\n   standards were required.\n   A respondent working on migration management indicated having data\n   protection officers involved in their analysis. The legal service has a specialised\n   quality control AI tool to study the data protection aspects of their system.\n   However, the respondent also mentioned that more guidance is needed.\n   The companies working on targeted advertising all looked into data protection\n   issues, although not all respondents were sure if an impact assessment was\n   conducted. The companies assessed, for example, whether only people who\n   consented are approached in targeted communication. For targeted ads,\n   they assessed whether information on possible re-identification is deleted,\n   including whether cookies and trackers are anonymised.\n   With respect to DPIAs generally, some respondents did not know, as this\n   was not their area of responsibility. Others knew they had a positive DPIA,\n   but were not aware of any details. It appears that the legal assessment\n   is sometimes detached from the technical side, with technical people not\n   knowing about legal assessments. One interviewee from a private company\n   working on credit risk scoring mentioned: “I make suggestions how some\n   system could be developed and then the compliance manager tells me if it\n   is in conformity with the laws”.\n   Audits and working with external (oversight) bodies\n   The public administrations and private companies involved in FRA’s research\n   all carry out tests before deploying any AI. These are often linked to existing\n   internal and external oversight processes. The use of AI is frequently subjected\n   to internal review processes within companies and public administration,\n   although these are not necessarily formalised review processes. Some\n   interviewees mentioned that they are working on formalising existing internal\n   review processes for overseeing AI systems.\n94\n                                                                      Interviewees from the public sector say\n                                                                      that they have to be particularly cautious\n                                                                      before using any AI to support decisions.\n                                                                      A representative working on migration\n                                                                      management at a public administration\n                                                                      indicates that “[i]n the private sector, [wrong\n                                                                      results] might cause business-related losses,\n                                                                      in the police it impacts people’s lives and their\n                                                                      fundamental rights”.\n                                                                      Yet it is not always clear to public administration,\n                                                                      or to businesses, who is responsible for\n                                                                      checking and overseeing the use of AI. Public\n                                                                      administrations appear to be under stronger\n                                                                      scrutiny when it comes to oversight of their AI\n                                                                      systems. Such oversight is often done through\n                                                                      regular audits, for example connected to\n                                                                      budgetary review.\n                                     Some interviewees, from public and private organisations, report that their\n                                     AI systems are currently checked in the framework of an existing IT review\n                                     (e.g. regular database checks), in the absence of review processes that\n                                     specifically look into the use of AI. In addition, interviewees report about\n                                     sector-specific certification schemes that also look into the use of AI – for\n                                     example, in the area of health or financial services.\n                                     Several interviewees mentioned that they were in contact with data protection\n                                     authorities. Some companies and public administrations sought permission\n                                     from the data protection authorities before using their AI system or at least\n                                     were generally in contact with them. For example, one company working on\n                                     targeted advertising mentioned discussing their use of personal data with\n                                     the national data protection authority.\n                                     Experts interviewed for this report further highlighted the relevance of data\n                                     protection authorities for overseeing AI systems with respect to the use of\n                                     personal data. However, experts strongly highlighted that data protection\n                                     authorities are under-resourced for this task for two reasons. Data protection\n                                     authorities often do not have relevant AI-related expertise.39 Additionally,\n                                     their budgets are overstretched and their workload heavy.\n                                     Experts’ views differ with respect to the need of additional oversight bodies,\n                                     and the potential creation of an AI specific institution. However, they agree that\n                                     existing bodies all have to work on topics linked to AI within their mandates.\n                                     Equality bodies, as well as other human rights institutions, are mentioned\n                                     by some interviewed experts as providing oversight concerning possible\n                                     discrimination when using AI. They highlighted that these institutions need\n                                     to build up expertise in this area to better contribute to the oversight of AI.\n                                     However, similar to data protection authorities, this is a challenging task for\n                                     equality bodies given their lack of resources.\n“We are proactive not only among\n                                     Several interviewees mentioned consumer protection authorities as potentially\nourselves to mitigate risks, but we\n                                     providing relevant oversight on the use of AI. One respondent, working for\nalso get additional audits. We also\n                                     a retail company, would like to have an advisory agency that could be consulted\nsee sometimes that some regulatory\n                                     about possible use of AI for innovation without being investigated right away.\naudits are quite sloppy. For us that\n                                     At the moment, the company prefers to consult consumer authorities over\nis not good because we have lots of\n                                     data protection authorities about potential future marketing campaigns.\ncustomer data.”\n                                     This is because data protection authorities might start an investigation into\n(Private company, Estonia)\n                                     their efforts.\n                                                                                                                           95\n   When discussing oversight, those developing and using AI, as well as experts,\n   repeatedly mention the challenge to really understand the impact when using\n   AI. Despite the need to engage existing oversight bodies, responsibilities to\n   oversee the use of AI from a fundamental rights perspective remain unclear.\n   5.3.\t FUNDAMENTAL RIGHTS IMPACT ASSESSMENT IN\n            PRACTICE\n   Many key actors in the field of fundamental rights have called for conducting\n   fundamental rights impact assessments before using any AI-driven systems.\n   This section highlights some of the elements that could be incorporated into\n   such an assessment.\n   Fundamental rights impact assessments are needed given that a contextualised\n   assessment is required. This is because uses of AI vary considerably in terms of\n   complexity, level of automation, potential errors and harm, scale of application,\n   as well as area of use. The more complex an AI system is, the more difficult\n   it is to assess its potential impact.\n   While the fundamental rights implicated will vary depending on the area of\n   application, the full spectrum of rights needs to be considered for each use\n   of AI. However, uses of AI are likely to involve some of the rights most often\n   affected by AI systems. The discussion in the preceding chapter makes clear\n   that issues linked to data protection, non-discrimination, as well as access to\n   effective remedies and a fair trial, are relevant for all uses of AI.\n   Thus, the following horizontal points could be a basic starting point for\n   considering the impact of AI on selected rights:\n   ――The legal processing of data needs to be confirmed in line with data\n        protection laws.\n   If personal data are used, the full data protection framework applies. This\n   ensures that processing is legal and does not violate a person’s rights to\n   respect for a private and family life, and data protection.\n   ――The processing should not lead to unfair treatment or discrimination of\n        protected groups.\n   Assessing non-discrimination needs to be at the core of assessing AI. Even\n   apparently miniscule differences can scale up and create risks contravening\n   the principle of non-discrimination. The disadvantage to people depends\n   on the nature (kind of harm), severity (strength of harm) and significance\n96\n(how many people are put at a disadvantage compared to another group of\npeople). Statistical assessments on group differences are an important tool\nto assess unfair and discriminatory uses of AI.40\n――People subjected to AI and related technologies should be able to complain\n    and receive effective remedies.\nThere should be accessible ways for people to complain about potential\ndecisions being made and to effectively access remedies. This includes\navailability of information that allows the explanation of decisions.\nIn addition, other relevant rights in the Charter apply. Public administrations\nusing AI need to consider good administration principles. Businesses have\nto take consumer protection into account.\nOther rights are relevant depending on the area of application. Some examples\ninclude:\n――the right to social protection, when working with social benefits;\n――the right to freedom of expression and information, when using AI to\n    support online content moderation;\n――the right of assembly and of association, when considering the use of\n    facial recognition technology in public spaces;\n――the right to education, when using AI in the education sector;\n――the right to asylum, when using AI to support migration management;\n――the right of collective bargaining and action, when using AI in the ‘gig-\n    economy’;\n――the right to fair and just working conditions, when using AI at the workplace;\n――the right to access preventive health care, when using AI in health services;\n――and the right to the presumption of innocence and the right to defence,\n    when using AI in the justice sector or for law-enforcement purposes.\nInformation needed to assess the potential impact on fundamental\nrights before implementing AI\nGiven the variety of tools, purposes and area of application, assessments\nare contextual. To be able to meaningfully respond to the horizontal points\nraised above, and to assess specific rights linked to different use cases, at\nleast the following information needs to be available:\n――A description of the purpose and context of the system, as well as the\n    legal basis.\n――A description of possible harm of using the system, including questions\n    around false positives, false negatives, and other possible harm due to\n    the automation and scale of use.\n――A description of the technology used. This includes information on the\n    data used for building the system and its legal basis for processing.\n    A description of relevant information to include is provided in FRA’s paper\n    on data quality and AI.41\n――An evidence-based description of the accuracy of the AI system in terms\n    of outcomes based on training data and possible tests and experiments in\n    real life situations, if appropriate. Here, false positives and false negatives\n    should be considered separately. These should include breakdowns for\n    as many groups as possible to allow for checking potential discrimination\n    (e.g. differences in the accuracy between women and men).\n――Where already available, the provision of information about compliance\n    with existing standards and potential certifications obtained.\nEx-post assessments and safeguards\nLastly, envisaging ex-post safeguards further contributes to the fundamental\nrights compliant use of AI. These could include:\n                                                                                    97\n   ――Regular repetition of assessments after deployment, where appropriate.\n       This is important to learn about potential feedback loops and in case\n       rules are updated. This also requires recording information on the use\n       and outcomes of the system to the extent data protection is respected.\n   ――Making people subjected to AI systems aware that they are subjected\n       to this technology, as they can otherwise not challenge any decision\n       affecting them.\n   ――Making available easily accessible channels for effectively complaining\n       about decisions made based on the AI system.\n   Engaging external experts, stakeholders and oversight bodies\n   The above information could be the basis for consultation with different\n   stakeholders and experts before a particular AI system is used. Depending on\n   the nature of the application and its legal basis, a consultation with relevant\n   stakeholders would ensure that no potential harm has been omitted and\n   different perspectives are brought into the assessment. Stakeholders could\n   include civil society; different public and private organisations; as well as\n   experts from different fields of fundamental rights, including data protection.\n   As the ten experts interviewed for this report highlighted, existing oversight\n   bodies are also responsible for AI oversight within their mandates. Sector-\n   specific bodies and certification schemes are doing this to some extent, the\n   interviews suggest – for example, in health care and financial oversight.\n   To monitor, comprehend and effectively respond to the potential impacts\n   of AI on a wide spectrum of fundamental rights, data protection authorities,\n   equality bodies, ombuds institutions and national human rights institutions\n   could play an important role, providing input and oversight from their various\n   points of expertise. However, as interviews indicated, extensive upskilling\n   and resource allocation is needed to underpin this.\n98\nEndnotes\n1  Council of Europe, Commissioner for Human Rights (2019), Unboxing Artificial Intelligence: 10 steps to protect Human Rights –\n   Recommendation, Council of Europe, Strasbourg, May 2019.\n2  Heleen L Janssen (2020), ‘An approach for a fundamental rights impact assessment to automated decision-making, International\n   Data Privacy Law’, International Data Privacy Law,Vol. 10, Issue 1, February 2020, pp. 76-106; Alessandro Mantelero, ‘AI and Big Data:\n   A blueprint for a human rights, social and ethical impact assessment’, Computer Law & Security Review Vol. 34, Issue 4, August 2018, pp.\n   754-772; Edwards, Lilian and Veale, Michael (2017), Slave to the Algorithm? Why a ‘Right to an Explanation’ Is Probably Not the Remedy\n   You Are Looking For (May 23, 2017), 16 Duke Law & Technology Review 18.\n3  AccessNow (2020), Access Now’s submission to the Consultation on the “White Paper on Artificial Intelligence - a European approach\n   to excellence and trust”.\n4  Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States\n   on the human rights impacts of algorithmic systems, 8 April 2020, para 5.2. (human rights impact assessment).\n5  For a detailed discussion with respect to non-discrimination, see: Wachter S., Mittelstatt, B., and Russel C. (2020), Why fairness cannot be\n   automated: bridging the gap between EU non-discrimination law and AI.\n6  Government of Canada, (2019), Directive on Automated Decision-Making.\n7  United Nations, UN Guiding principles on Business and Human Rights, endorsed by Human Rights Council Resolution 17/4, A/HRC/\n   RES/17/4, 6 July 2011, Principles 18, 19, 20.\n8  Heleen L Janssen, An approach for a fundamental rights impact assessment to automated decision-making, International Data Privacy\n   Law, Vol. 10, Issue 1, February 2020, pp. 76-106.\n9  High-Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI, 8 April 2019, Chapter III.\n10 Ibid, p. 15.\n11 See for example: IBM, Everyday Ethics for Artificial Intelligence, 2019; Sony, Sony Group AI Ethics Guidelines, 2019; Vodaphone,\n   Vodaphone’s AI framework, 2019; Arborus International and Orange, International Charter for Inclusive AI, 21 April 2020, signed by more\n   than 40 private companies, including Camfil, Danone, EDF, L’Oréal, Metro, Sodexo, etc.\n12 Information Technology Industry Council (ITI), ITI AI Policy Principles, 2017.\n13 ECP Platform for the Information Society, Artificial Intelligence Impact Assessment, The Netherlands, 14 November 2019.\n14 Amnesty International, Access Now, Human Rights Watch, Wikimedia Foundation, The Toronto Declaration: Protecting the rights to\n   equality and non-discrimination in machine learning systems, 16 May 2018 at RightsCon Toronto; University of Montreal, Montreal\n   Declaration Responsible AI, 2018.\n15 Electrical and Electronics Engineers (IEEE), Global Initiative on Ethics of Autonomous and Intelligent Systems, Ethically Aligned Design:\n   Prioritizing Human Wellbeing with Autonomous and Intelligent Systems, 2019; Future of Life Institute, Asilomar AI Principles, Conference\n   outcome of the Future of Life Institute’s second conference on the future of artificial intelligence, 2017.\n16 See for example: ECP Platform for the Information Society, Artificial Intelligence Impact Assessment, The Netherlands, 14 November\n   2019; IEEE Initiative.\n17 Association for Computer Machinery (ACM), ACM Code of Ethics and Professional Conduct, 22 June 2018.\n18 Future of Humanity Institute, University of Oxford, Standards for AI Governance: International Standards to Enable Global Coordination in\n   AI Research & Development, April 2019.\n19 ISO, Standards by iso/iec jtc 1/sc 42, Artificial intelligence,Sstandard and/or project under the direct responsibility of iso/iec jtc 1/sc\n   42 secretariat, ISO, ISO/IEC TR 24028:2020 standard Information technology — Artificial intelligence — Overview of trustworthiness in\n   artificial intelligence, May 2020. It establishes, among others, the “approaches to assess and achieve availability, resiliency, reliability,\n   accuracy, safety, security and privacy of AI systems.” Other ISO standards under development as of September 2020: ISO/IEC CD\n   23894 Information Technology — Artificial Intelligence — Risk Management, ISO/IEC AWI TR 24027 Information technology — Artificial\n   Intelligence (AI) — Bias in AI systems and AI aided decision making, or ISO/IEC AWI TR 24368 Information technology — Artificial\n   intelligence — Overview of ethical and societal concerns, more information available on ISO’s website; Electrical and Electronics Engineers\n   (IEEE), IEEE P7003™ Algorithmic Bias Considerations; German AI Federal Association (KI Bundesverband), German AI Federal Association:\n   seal of quality (KI Bundesverband Guetesiegel), 22 March 2019.\n20 Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13 October 2017, Annex 2 – Criteria for\n   an acceptable DPIA.\n21 High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17\n   July 2020.\n22 Government of Canada, Algorithmic Impact Assessment Tool, 2019; Danish Institute for Human Rights, Human rights compliance\n   assessment quick check, 7 June 2016.\n23 Center of Government Excellence, Johns Hopkins University, Ethics & Algorithm toolkit, 2018; Government of Canada, Algorithmic Impact\n   Assessment Tool, 2019.\n24 Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13 October 2017, Annex 2 (data\n   protection focus); Danish Institute for Human Rights, Human Rights Impact Assessment Guidance and Toolbox, 2016; AI Pulse, Creating\n   a Tool to Reproducibly Estimate the Ethical Impact of Artificial Intelligence, 26 September 2019.\n25 Fairness, Accountability, and Transparency in Machine Learning (FAT/ML), Principles for Accountable Algorithms and a Social Impact\n   Statement for Algorithms, 2019; FAT/ML, Social Impact Statement for Algorithms, 2019.\n26 High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17\n   July 2020.\n27 See e.g. human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, non-\n   discrimination and fairness; societal and environmental well-being; accountability. High-Level Expert Group on Artificial Intelligence, Ethics\n   Guidelines for Trustworthy AI, 8 April 2019.\n28 High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17\n   July 2020, p.5.\n29 Government of Canada, Algorithmic Impact Assessment tool, 2019.\n30 Government of Canada, Directive on Automated Decision-Making, 2019, Article 6 an Appendix C.\n31 Center of Government Excellence, Johns Hopkins University, Ethics & Algorithm toolkit, 2018.\n32 Danish Institute for Human Rights, Human rights compliance assessment quick check, 7 June 2016.\n33 Danish Institute for Human Rights, Human Rights Impact Assessment Guidance and Toolbox, 2016.\n34 Heleen L Janssen, An approach for a fundamental rights impact assessment to automated decision-making, International Data Privacy\n                                                                                                                                                  99\n       Law, International Data Privacy Law,Vol. 10, Issue 1, February 2020.\n    35 Alessandro Mantelero, AI and Big Data: A blueprint for a human rights, social and ethical impact assessment, Computer Law & Security\n       Review, Vol. 34, Issue 4, August 2018, pp. 754-772; AI Pulse - Program on Understanding Law, Science, and Evidence (PULSE), UCLA School\n       of Law, Creating a Tool to Reproducibly Estimate the Ethical Impact of Artificial Intelligence, 26 September 2019. This model includes\n       a series of questions for Assessing the Human Rights Impact of AI-Enabled Projects.\n    36 Marion Oswald, Jamie Grace, Sheena Urwin & Geoffrey C. Barnes, Algorithmic risk assessment policing models: lessons from the Durham\n       HART model and ‘Experimental’ proportionality, Journal, Vol. 27, 2018 – Issue 2.\n    37 AINOW, Algorithmic Impact Assessments: a Practical Framework for Public Agency Accountability, April 2018.\n    38 The Institute for Ethical AI & Machine Learning (Ethical ML Network (BETA)), The Machine Learning Maturity Model, 2019.\n    39 Brave (2020), Europe’s governments are failing the GDPR.\n    40 See Wachter S., Mittelstatt, B., and Russel C. (2020), Why fairness cannot be automated: bridging the gap between EU non-\n       discrimination law and AI.\n    41 FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications\n       Office, June 2019.\n100\n6\nMOVING FORWARD: CHALLENGES\nAND OPPORTUNITIES\n                                     This report is published amidst ongoing European legislative and policy\n                                     developments on artificial intelligence and the global fight against the\n                                     coronavirus. The Covid-19 pandemic has potentially quickened acceptance\n                                     of innovative technologies. Yet it has also shown that AI is not the panacea\n                                     to all problems, and comes with various challenges.\n                                     This report clearly shows that using AI systems engages a wide range\n                                     of fundamental rights. It also shows that many businesses and public\n                                     administrations are already using or planning to use AI and related technologies.\n                                     However, these technologies involve different levels of complexity. Most\n                                     examples use relatively simple algorithms. The level of automation also\n                                     varies. Most – but not all – decision making is subject to human review.\n                                     The applications currently used are also often only in the development\n                                     stage. EU and national legislators and policymakers should keep this reality\n                                     in mind – especially when presented with optimistic expectations of AI’s\n                                     potential vis-à-vis the challenges related to using new technologies and the\n                                     need to regulate them.\n“We try to look into the future. We  The vast majority of public administrations and businesses interviewed plan\nwill automate more and more.”        to keep working on or using AI. Only two interviewees indicated that they\n(Private company, Estonia)           will not further use or develop AI. Another two interviewees are cautious.\n                                     They plan to wait and see what others are doing, including because of a lack\n                                     of resources for further work on using AI.\n“The next steps are related to\ntransparency and open data: that is  However, most said that they will further develop or continue to test tools\nto say, publish not only information and (data) infrastructure with respect to the use of AI. This includes starting\nin pdf, but also information in      new or continuing ongoing pilots, evaluating existing efforts, sharing data\nreusable formatting so that it could and results with others, increasing data quality, or trying to obtain other\nbe reused internally and by the      data sources.\nprivate sector.”\n (Public administration, Spain)      Some interviewees mentioned that they are engaged in ongoing debates and\n                                     expressed the desire to contribute to the further development of legislation.\n                                     They still see the current situation – the absence of harmonised law in the\n“AI is a great thing but we must\n                                     area – as an obstacle to the further use of AI. In addition, some respondents\nlearn to use it.”\n                                     said they are working on issues linked to the interpretability of AI. This\n(Private company, Spain)\n                                     means that they are working on methods that enhance understanding and\n                                     explanation of decisions based on more complex AI. Some indicated a desire\n                                     to look more closely into ethical and legal matters.\n                                     Figure 7 shows correlations of words interviewees often use when talking\n                                     about their future use of AI. The figure indicates topics that are often raised.\n                                     For example, interviewees often used the term ‘data’ when discussing future\n                                     developments.\n                                                                                                                       101\n    FIGURE 7:     CORRELATIONS OF WORDS RESPONDENTS MOST OFTEN\n                  MENTION WHEN DISCUSSING FUTURE PLANS TO USE AI\n                                                                   technologies\n                                                                                         systems\n                                                    plans                                                    data\n                                  development                            company\n                                ai\n                                                                                                     tool             police\n                                   management\n                                                             solutions\n                                                                                   companies\n               level\n                              system                                                                                  project\n                                                                                                                                    technology\n                                                                                              administration\n                                                                               tax                            analysis\n             organisation                                    service\n                                                                              processing                                        lot\n                                        translation    process\n                     related\n                                                                  customer\n                                            easy                                                                   national          phase\n                                                   payment\n                     information\n                                                                services    application                                             time\n                              quality                                                                              improve\n                                              learning\n               business                                                  human                   energy\n                               algorithms                              decisions                                        support\n                                             machine\n                                                                 ml                                             continue\n                                                     potential\n                                                                                   people      develop\n                                                               future\n    Notes:\t\u0007Based on text from interview summaries, when respondents spoke about\n              their future use of AI, including words mentioned at least ten times. The\n              lines connecting words indicate the strength of word correlations within\n              text passages. The size of the dots indicate the frequency of the words\n              used.\n    Source: FRA, 2020\n    Effectively and adequately protecting fundamental rights in the EU is a key\n    objective of the current efforts to better regulate the use of AI. In the context\n    of upcoming EU legislation on AI, the European Commission’s White Paper\n    addresses current gaps, helping to mitigate the uncertainty around the use\n    of AI with respect to fundamental rights, and making the use of AI more\n    transparent and accountable in terms of fundamental rights. It includes\n    requirements for AI use that directly link to the information needed to assess\n    the impact of AI on fundamental rights, as discussed above.\n    Requirements linked to the description of training data, data and record\n    keeping, information to be provided to those subjected to AI, robustness and\n    accuracy, as well as human oversight are all highly relevant when assessing\n    and protecting fundamental rights. In this respect, the body of evidence\n    presented in this report offers general insights into how different technologies\n    can affect fundamental rights and what safeguards are needed to ensure\n    fully fundamental rights-compliant use of AI in practice.\n    At the same time, further research into the fundamental rights implications\n    of the use of AI in specific areas will further support policy and legislative\n    efforts at the EU level aiming to shape Europe’s digital future more widely.\n102\nFRA will continue to look into the fundamental implications of AI by carrying\nout more focussed analysis of specific use cases. To increase knowledge\non what can potentially go wrong and consequently help mitigate and\nprevent fundamental rights violations, FRA will look into potential simulation\nstudies. These can showcase how biased algorithms can negatively affect\nfundamental rights.\nThe use of AI often involves automating tasks that were previously carried\nout by humans. Here we need to acknowledge that human behaviour is\nsometimes not in line with fundamental rights, both when using AI and when\nnot using AI. For example, the police might engage in unlawful profiling.\nDecisions by public administration or companies might sometimes be driven\nby negative stereotypes.\nCurrent developments in the use of AI need to acknowledge the potential for\ndiscrimination with respect to the data on which an AI system is built, and\nwith respect to the underlying assumptions that humans in turn may feed\ninto the development and deployment of a system. Automating certain tasks\nwithout fully understanding what is being automated could lead to unlawful\nprocessing of data, the use of technology that treats people unfairly, and might\nmake it impossible to challenge certain outcomes – to name some challenges.\nHowever, the increased availability of data and technological tools can also\nbe used to better understand where and how unequal treatment occurs.\nCurrent technological developments and the increased availability of data\nalso provide a unique opportunity to better understand the structures of\nsociety, which can be used to support fundamental rights compliance. The\nopportunities created by AI can also contribute to better understanding and\nconsequently mitigation of fundamental rights violations.\n                                                                                 103\nGetting in touch with the EU\nIn person\nAll over the European Union there are hundreds of Europe Direct information centres.\nYou can find the address of the centre nearest you at:\nhttps://europa.eu/european-union/contact_en\nOn the phone or by email\nEurope Direct is a service that answers your questions about\nthe European Union. You can contact this service:\n—b  \u0007 y freephone: 00 800 6 7 8 9 10 11\n    (certain operators may charge for these calls),\n— at the following standard number: +32 22999696 or\n— by email via: https://europa.eu/european-union/contact_en\nFinding information about the EU\nOnline\nInformation about the European Union in all the official languages of the EU is available\non the Europa website at: https:// europa.eu/european-union/index_en\nEU publications\nYou can download or order free and priced EU publications at: https://op.europa.eu/\nen/publications. Multiple copies of free publications may be obtained by contacting\nEurope Direct or your local information centre (see https://europa.eu/european-union/\ncontact_en).\nEU law and related documents\nFor access to legal information from the EU, including all EU law since 1952 in all the\nofficial language versions, go to EUR- Lex at:\nhttp://eur-lex.europa.eu\nOpen data from the EU\nThe EU Open Data Portal (http://data.europa.eu/euodp/en) provides access to datasets\nfrom the EU. Data can be downloaded and reused for free, for both commercial and\nnon-commercial purposes.\n\nPROMOTING AND PROTECTING\nYOUR FUNDAMENTAL RIGHTS\nACROSS THE EU\n―\nArtificial intelligence (AI) already plays a role in deciding what\nunemployment benefits someone gets, where a burglary is likely to\ntake place, whether someone is at risk of cancer, or who sees that\ncatchy advertisement for low mortgage rates. Its use keeps growing,\npresenting seemingly endless possibilities. But we need to make sure\nto fully uphold fundamental rights standards when using AI.\nThis report presents concrete examples of how companies and public\nadministrations in the EU are using, or trying to use, AI. It focuses on\nfour core areas – social benefits, predictive policing, health services\nand targeted advertising.\nThe report discusses the potential implications for fundamental rights\nand analyses how such rights are taken into account when using\nor developing AI applications. In so doing, it aims to help ensure\nthat the future EU regulatory framework for AI is firmly grounded in\nrespect for human and fundamental rights.\n   EU Charter of\nFundamental Rights Access to justice Non-discrimination Information society\nFRA – EUROPEAN UNION AGENCY FOR FUNDAMENTAL RIGHTS\nSchwarzenbergplatz 11 – 1040 Vienna – Austria\nTEL. +43 158030-0 – FAX +43 158030-699\nfra.europa.eu\n       facebook.com/fundamentalrights\n       twitter.com/EURightsAgency\n       linkedin.com/company/eu-fundamental-rights-agency\n","cleanText":"getting future right -- artificial intelligence fundamental rights report (c) european union agency fundamental rights reproduction authorised provided source acknowledged use reproduction photos material european union agency fundamental rights copyright permission must sought directly copyright holders neither european union agency fundamental rights person acting behalf agency responsible use might made following information luxembourg publications office european union print isbn doi tk en c pdf isbn doi tk en n (c) photo credits cover hquality adobe stock page copyright (c) coded bias rights reserved page mimi potter adobe stock page siberian art adobe stock page monsitj adobe stock page good studio adobe stock page monsitj adobe stock page sikov adobe stock page mykola mazuryk adobe stock page robsonphoto adobe stock page metamorworks adobe stock page thodonal adobe stock page gorodenkoff adobe stock page blackboard adobe stock page dimco adobe stock page blackboard adobe stock page videoflow adobe stock page monopoly919 adobe stock page zapp2photo adobe stock page gorodenkoff adobe stock page bestforbest adobe stock page freedomz adobe stock page zapp2photo adobe stock page copyright (c) coded bias rights reserved page european communities page copyright (c) coded bias rights reserved page blacksalmon adobe stock foreword know artificial intelligence already plays role deciding unemployment benefits someone gets burglary likely take place whether someone risk cancer sees catchy advertisement low mortgage rates speak artificial intelligence ai machines kind things people used able today ai present lives realise - use keeps growing possibilities seem endless fully uphold fundamental rights standards using ai report presents concrete examples companies public administrations eu using trying use ai discusses potential implications fundamental rights shows whether using ai taking rights account fra interviewed hundred public administration officials private company staff well diverse experts - including supervisory oversight authorities non governmental organisations lawyers - variously work ai field based interviews report analyses fundamental rights taken consideration using developing ai applications focuses four core areas - social benefits predictive policing health services targeted advertising ai uses differ terms complex much automation involved potential impact people widely applied findings underscore lot work lies ahead - everyone one way foster rights protection ensure people seek remedies something goes awry need know ai used also means organisations using ai need able explain ai systems deliver decisions based yet systems issue truly complex using ai systems responsible regulating use acknowledge always fully understand hiring staff technical expertise key awareness potential rights implications also lacking know data protection concern refer non discrimination less aware rights - human dignity access justice consumer protection among others - also risk surprisingly developers review potential impact ai systems tend focus technical aspects tackle challenges let' encourage working human rights protection working ai cooperate share much needed knowledge - tech rights develop use ai also need right tools assess comprehensively fundamental rights implications many may immediately obvious accessible fundamental rights impact assessments encourage reflection help ensure ai uses comply legal standards interviews suggest ai use eu growing still infancy technology moves quicker law need seize chance ensure future eu regulatory framework ai firmly grounded respect human fundamental rights hope empirical evidence analysis presented report spurs policymakers embrace challenge michael 'flaherty director contents foreword key findings fra opinions ai fundamental rights - relevant policymaking report mean artificial intelligence ai fundamental rights eu policy framework moving towards regulation endnotes putting fundamental rights context - selected use cases ai eu examples ai use public administration examples ai use private sector endnotes fundamental rights framework applicable ai fundamental rights framework governing use ai 'use case' examples requirements justified interferences fundamental rights endnotes impact current use ai selected fundamental rights perceived risks general awareness fundamental rights legal frameworks ai context human dignity right privacy data protection - selected challenges equality non discrimination access justice right social security social assistance consumer protection right good administration endnotes fundamental rights impact assessment - practical tool protecting fundamental rights calling fundamental rights impact assessment - available guidance tools impact assessments testing practice fundamental rights impact assessment practice endnotes moving forward challenges opportunities figures figure companies using ai member state figure examples different automation complexity levels use cases covered figure words interviewees often used describe ai 'use cases' figure awareness gdpr right opt direct marketing eu united kingdom country region figure awareness right say decisions automated age gender difficulty paying bills figure awareness risks discrimination using ai country figure correlations words respondents often mention discussing future plans use ai key findings fra opinions new technologies profoundly changed organise live lives particular new data driven technologies spurred development artificial intelligence ai including increased automation tasks usually carried humans covid health crisis boosted ai adoption data sharing - creating new opportunities also challenges threats human fundamental rights developments ai received wide attention media civil society academia human rights bodies policymakers much attention focuses potential support economic growth different technologies affect fundamental rights received less attention date yet large body empirical evidence wide range rights ai implicates safeguards needed ensure use ai complies fundamental rights practice february european commission published white paper artificial intelligence - european approach excellence trust outlines main principles future eu regulatory framework ai europe white paper notes vital framework grounded eu' fundamental values including respect human rights - article treaty european union teu report supports goal analysing fundamental rights implications using artificial intelligence based concrete 'use cases' ai selected areas focuses situation ground terms fundamental rights challenges opportunities using ai overarching fundamental rights framework applies use ai legal eu consists charter fundamental rights eu charter well framework european convention human rights multiple council europe international human rights instruments relevant include universal declaration human rights major un human rights conventions addition sector specific secondary eu law notably eu data protection acquis eu non discrimination legislation helps safeguard fundamental rights context ai finally national laws eu member states also apply see fra bringing rights life fundamental rights landscape european union luxembourg publications office european union major conventions include international covenant civil political rights international covenant economic social cultural rights international convention elimination forms racial discrimination convention elimination forms discrimination women convention torture convention rights child convention rights persons disabilities international convention protection persons enforced disappearance universal international human rights law framework including enforcement mechanisms see e g de schutter international human rights law cases materials commentary cambridge cambridge university press 2nd edition report based interviews officials public administration staff private companies selected eu member states asked use ai awareness fundamental rights issues involved practices terms assessing mitigating risks linked use ai moreover interviews conducted experts deal various ways potential fundamental rights challenges ai group included public bodies supervisory oversight authorities non governmental organisations lawyers safeguarding fundamental rights - scope impact assessments accountability considering full scope fundamental rights respect ai fra opinion using ai systems engages wide range fundamental introducing new policies rights regardless field application adopting new legislation ai include - also go beyond - privacy data protection eu legislator member states non discrimination access justice acting within scope eu law must ensure respect full eu charter fundamental rights charter spectrum fundamental rights became legally binding december enshrined charter eu legal value eu treaties brings together treaties taken account specific civil political economic social rights single text fundamental rights safeguards need pursuant article charter institutions accompany relevant policies laws bodies offices agencies union respect eu member rights embodied charter eu member states rely robust evidence states implementing union concerning ai' impact fundamental law applies equally ai field rights ensure restrictions fieldwork research shows large certain fundamental rights respect variety systems used heading ai principles necessity technologies analysed entail different levels proportionality automation complexity also vary terms relevant safeguards need scale potential impact people provided law effectively fra' findings show using ai systems implicate protect arbitrary interference wide spectrum fundamental rights regardless fundamental rights give field application include also go beyond legal certainty ai developers privacy data protection non discrimination users voluntary schemes access justice yet addressing impact ai observing safeguarding respect fundamental rights interviews show fundamental rights development scope often delimited specific rights use ai help mitigate rights violations line wider range rights need considered minimum requirements legal clarity using ai depending technology area use - basic principle rule addition rights concerning privacy data protection law prerequisite securing equality non discrimination access justice fundamental rights - legislator rights could considered include take due care defining example human dignity right social security scope ai law social assistance right good administration mostly relevant public sector consumer protection given variety technology particularly important businesses depending subsumed term ai context ai use right protected lack knowledge full charter needs consideration scope potential fundamental rights impact legal definition ai related terms might need assessed regular basis using effective impact assessments prevent negative effects fra opinion prior impact assessments mainly focus technical eu legislator consider making issues rarely address potential effects mandatory impact assessments fundamental rights knowledge cover full spectrum fundamental rights cover private ai affects rights lacking public sectors applied deploying ai systems engages wide spectrum ai system used fundamental rights regardless field application impact assessments take pursuant article charter eu member states account varying nature scope must respect rights embodied charter ai technologies including level implementing union law line existing automation complexity well international standards - notably united national potential harm include guiding principles business human rights ungps basic screening requirements - businesses place \" human rights due also serve raise awareness potential diligence process identify prevent mitigate account fundamental rights implications address impacts human rights\" impact assessments draw principles irrespective size established good practice sector encompasses businesses working ai fields regularly repeated pursuing commitments ungps eu deployment appropriate adopted several legislative acts addressing sector assessments conducted specific instruments particular context due transparent manner outcomes diligence related obligations human rights discussions recommendations currently underway proposing new eu secondary public domain extent possible law law would require businesses carry due aid impact assessment process diligence potential human rights environmental companies public administration impacts operations supply chains law required collect would likely cross sectoral provide sanctions information needed thoroughly non compliance - encompass use assessing potential fundamental ai see fra' recent report business human rights rights impact - access remedy calls improved horizontal eu member states human rights diligence rules eu based companies consider targeted actions support impact assessments important tool businesses developing using planning public administration alike mitigate potential use ai systems ensure effective negative impact activities fundamental rights compliance fundamental eu law specific sectors requires forms impact rights impact assessment obligations assessments data protection impact assessments actions could include funding general data protection regulation gdpr guidelines training awareness many interviewees reported data protection impact raising particularly - assessment required law conducted however exclusively - target private took different forms moreover prior assessments sector conducted focus mainly technical aspects eu member states rarely address potential impacts fundamental rights consider using existing tools according interviewees fundamental rights impact checklists self evaluation tools assessments carried ai system developed european international appears affect fundamental rights negatively level include developed research shows interviewees' knowledge eu high level group artificial fundamental rights - data protection intelligence extent non discrimination - limited majority acknowledge however use ai impact fundamental rights interviewees indicate systems affect fundamental rights extent linked tasks ai systems used respondents aware data protection issues respondents also realise discrimination could - generally - problem ai used however exact meaning applicability rights related data protection non discrimination remains unclear many respondents research findings show differences private public sector interviewees private sector often less aware wider range fundamental rights could affected data protection issues known private sector however rights non discrimination access justice related rights less well known among business representatives work ai fully aware potential problems others said responsibility checking fundamental rights issues lies clients ensuring effective oversight overall accountability fra opinion businesses public administrations developing using ai contact various eu member states ensure bodies responsible overseeing ai related effective accountability systems place systems within respective mandates sectors monitor needed effectively address negative impact ai systems bodies include data protection authorities fundamental rights consider using ai always sure bodies addition fundamental rights impact responsible overseeing ai systems assessments see fra opinion introducing specific safeguards ensure line well established international human rights accountability regime effective could standards - example article european include legal requirement make available convention human rights echr article enough information allow assessment charter - states obliged secure people' rights fundamental rights impact ai systems freedoms effectively comply states - among would enable external monitoring others - put place effective monitoring enforcement human rights oversight competent bodies mechanisms applies equally respect ai eu member states also level monitoring findings point make better use existing oversight expert important role specialised bodies established specific structures protect fundamental rights sectors also responsible ai oversight within using ai include data protection mandates include example oversight authorities equality bodies national human area banking data protection authorities rights institutions ombuds institutions variety bodies potentially relevant consumer protection bodies oversight ai fundamental rights perspective additional resources earmarked however responsibilities bodies concerning establish effective accountability systems oversight ai remains unclear many 'upskilling' diversifying staff working interviewed private public sector oversight bodies would allow public administrations' use ai sometimes audited deal complex issues linked developing part regular audits private companies using ai specific sectors also specialised oversight bodies similarly appropriate bodies example area health financial services equipped sufficient resources powers also check use ai related technologies - importantly - expertise prevent example part certification schemes private assess fundamental rights violations sector interviewees expressed wish bodies could effectively support whose fundamental provide expert advice possibilities legality rights affected ai potential ai uses facilitating cooperation appropriate eu well developed set independent bodies national european level help bodies mandate protect promote fundamental share expertise experience engaging rights include data protection authorities equality actors relevant expertise - bodies national human rights institutions ombuds specialist civil society organisations - institutions research shows using also help implementing actions planning use ai often contacted different bodies national level member states consider use ai consumer protection bodies using available eu funding mechanisms often users ai contacted data protection authorities seek guidance input approval personal data processing involved interviewed experts highlight relevance data protection authorities overseeing ai systems respect use personal data however also note data protection authorities resourced task lack specific expertise ai issues experts including working oversight bodies equality bodies data protection authorities agree expertise existing oversight bodies needs strengthened allow provide effective oversight ai related issues according experts challenging given bodies' resources already stretched also highlighted important role relevant civil society organisations specialised fields technology digital rights algorithms enhance accountability use ai systems non discrimination data protection access justice three horizontal themes research shows use ai affects various fundamental rights apart context related specific aspects affect different rights varying extent fundamental rights topics emerged research repeatedly apply ai cases include need ensure non discriminatory use ai right discriminated requirement process data legally right personal data protection possibility complain ai based decisions seek redress right effective remedy fair trial two main fundamental rights highlighted interviews data protection non discrimination addition effective ways complain use ai came repeatedly linked right fair trial effective remedy following three fra opinions reflect findings read alongside opinions call comprehensive recognition response full range fundamental rights affected ai specific safeguards ensure non discrimination using ai fra opinion interviewees rarely mentioned carrying detailed assessments potential discrimination using ai eu member states consider encouraging companies public suggests lack depth assessments administration assess potentially discrimination automated decision making discriminatory outcomes using ai systems obligation respect principle non european commission member discrimination enshrined article teu states consider providing funding article tfeu requiring union combat targeted research potentially discrimination number grounds articles discriminatory impacts use ai charter equality law non algorithms research would discrimination range grounds specific benefit adaptation established detailed provisions several eu directives also enshrine research methodologies social principle varying scopes application sciences employed identify automation use ai greatly increase potential discrimination different areas efficiency services scale tasks - ranging recruitment customer humans would able undertake however profiling necessary ensure services decisions based building results research ai discriminatory recognising european guidance tools support commission recently highlighted need additional using ai detect possible discriminatory outcomes developed legislation safeguard non discrimination using ai eu anti racism action plan interviewees principle aware discrimination might happen yet rarely raised issue believe systems could actually discriminate interviewees also rarely mentioned detailed assessments potential discrimination meaning lack depth assessment potential discrimination common perception omitting information protected attributes gender age ethnic origin guarantee ai system discriminate necessarily true however information potentially indicating protected characteristics proxies often found datasets could lead discrimination certain cases ai systems also used test detect discriminatory behaviour encoded datasets however interviewees mentioned possibility collecting information disadvantaged groups detect potential discrimination absence depth analysis potential discrimination actual use ai systems also almost discussion analysis potential positive effect using algorithms make decisions fairer moreover none interviewees working ai mentioned using ai detect possible discrimination positive outcome sense discrimination better detected data analysed potential bias since detecting potential discrimination use ai algorithms remains challenging interviewees briefly addressed issue different measures needed address include requirement consider issues linked discrimination assessing use ai investment studies potential discrimination use diverse range methodologies could involve example discrimination testing could build similar established methodologies testing bias everyday life respect job applications applicant' name changed indirectly identify ethnicity relation ai applications tests could involve possible creation fake profiles online tools differ respect protected attributes way outcomes checked respect potential discrimination research could also benefit advanced statistical analysis detect differences datasets concerning protected groups therefore used basis exploring potential discrimination finally research interviews underscored results complex machine learning algorithms often difficult understand explain thus research better understand explain results called 'explainable ai' also help better detect discrimination using ai guidance data protection clarity needed scope meaning fra opinion legal provisions regarding automated decision making european data protection board data protection critical development use edpb european data ai article charter article tfeu protection supervisor edps provide everyone right protection consider providing guidance personal data gdpr law enforcement support effectively implement directive directive eu elaborate gdpr provisions directly apply right include many provisions applicable use ai safeguarding use ai fundamental rights particular regards meaning personal data interviewees indicated ai systems use ai including ai training employ use personal data meaning data protection datasets affected many different ways however applications - according interviewees - high level uncertainty use personal data use anonymised data concerning meaning automated hence data protection law would apply personal decision making right data used data protection related principles human review linked use ai provisions apply automated decision making thus edpb edps also report highlights important issue linked data consider clarifying concepts protection also relevant fundamental 'automated decision making' rights respect automated decision making 'human review' according eurobarometer survey mentioned eu law europeans know say decisions automated knowledge right considerably addition national data protection higher among working ai - majority bodies provide practical interviewees raised issue however many guidance data protection interviewees including experts argued clarity provisions apply use needed scope meaning legal provisions ai guidance could include automated decision making recommendations checklists based concrete use cases ai area social benefits interviewees mentioned support compliance data one example fully automated rule based decisions protection provisions applications mentioned reviewed humans interviewees public administration stressed importance human review decisions however rarely described human review actually involves information used reviewing output ai systems interviewees disagree whether existing legislation sufficient many called concrete interpretation existing data protection rules respect automated decision making enshrined article gdpr effective access justice cases involving ai based decisions fra opinion effectively contest decisions based use ai people need know ai used eu legislator member states complain organisations using ai need able ensure effective access justice individuals cases involving explain ai system decisions based ai ai based decisions access justice process goal crucial ensure available remedies individuals seeking benefit procedural accessible practice eu legislator substantive rights encompasses number core member states could consider human rights include right fair trial introducing legal duty public effective remedy article echr administration private companies article eu charter fundamental rights using ai systems provide accordingly notion access justice obliges states seeking redress information guarantee individual' right go court - operation ai systems circumstances alternative dispute resolution includes information body - obtain remedy found individual' ai systems arrive automated rights violated decisions obligation would help achieve equality arms cases accordance standards victim human individuals seeking justice would rights violation arising development use also support effectiveness ai system public private entity provided external monitoring human access remedy national authority line rights oversight ai systems see relevant case law article charter fra opinion article echr remedy must \"effective practice well law\" view difficulty explaining complex ai systems eu jointly research findings identify following preconditions member states remedy effective practice cases consider developing guidelines involving ai systems impact fundamental support transparency efforts rights everyone needs aware ai used area draw informed complain organisations expertise national human rights using ai must ensure public informed bodies civil society organisations ai system decisions based active field findings show explaining ai systems make decisions layman terms challenging intellectual property rights hamper provision detailed information algorithm works addition certain ai systems complex makes difficult provide meaningful information way system works related decisions tackle problem companies interviewed avoid using complex methods certain decision making altogether would able explain decisions alternatively use simpler data analysis methods problem obtain understanding main factors influencing certain outcomes private sector interviewees pointed efforts made gradually improve understanding ai technology ai fundamental rights - relevant policymaking artificial intelligence ai increasingly used private public sectors affecting daily life see ai end human control machines others view technology help humanity address pressing challenges neither portrayal may accurate concerns ai' fundamental rights impact clearly mounting meriting scrutiny use human rights actors examples potential problems using ai related technologies relation fundamental rights increasingly emerged include ---- algorithm used recruit human resources found generally prefer men women ---- online chatbot2 became 'racist' within couple hours ----machine translations showed gender bias ----facial recognition systems detect gender well white men black women ---- public administration' use algorithms categorise unemployed people comply law ---- court stopped algorithmic system supporting social benefit decisions breaching data protection laws examples raise profound questions whether modern ai systems fit purpose fundamental rights standards upheld using considering using ai systems report addresses questions providing snapshot current use ai related technologies eu - based selected use cases - implications fundamental rights report main publication stemming fra' project artificial intelligence fra' work big data fundamental rights project aims assess positive negative ai big fundamental rights implications new technologies including ai big data data fundamental current report builds findings number earlier papers rights * facial recognition technology fundamental rights considerations context law enforcement paper outlines analyses fundamental rights challenges triggered public authorities deploy live frt law enforcement purposes also briefly presents steps take help avoid rights violations * data quality artificial intelligence - mitigating bias error protect fundamental rights paper highlights importance awareness avoidance poor data quality * bigdata discrimination data supported decision making focus paper discusses discrimination occur suggests possible solutions part project fra also exploring feasibility studying concrete examples fundamental rights challenges using algorithms decision making either online experiments simulation studies several fra publications address relevant issues * guide preventing unlawful profiling today future illustrates profiling legal frameworks regulate conducting profiling lawfully necessary comply fundamental rights crucial effective policing border management * handbook european data protection law edition designed familiarise legal practitioners specialised data protection area law * data fra' fundamental rights survey surveyed random sample people across eu including findings people' opinions experiences linked data protection technology security * fra' report business human rights - access remedy analyses obstacles promising practices relation access remedies victims business related human rights abuses analysing complaints mechanisms eu member states research maps hinders facilitates access remedies report growing attention ai potential drive economic growth matched body evidence different technologies affect fundamental rights - positively negatively concrete examples allow thorough examination whether extent applying technology interferes various fundamental rights - whether interference justified line principles necessity proportionality report provides fundamental rights based analysis concrete 'use cases' - case studies 'use case' term software engineering report loosely defines specific application technology certain goal used specified actor report illustrates ways companies public sector eu looking use ai support work whether - - taking fundamental rights considerations account way contributes empirical evidence analysed fundamental rights perspective inform eu national policymaking efforts regulate use ai tools research cover fra conducted fieldwork research five eu member states estonia finland france netherlands spain collected information involved designing using ai systems key private public sectors address relevant fundamental rights issues research - based personal interviews - gathered information ---- purpose practical application ai technologies ---- assessments conducted using ai applicable legal framework oversight mechanisms ---- awareness fundamental rights issues potential safeguards place ----future plans addition experts involved monitoring observing potential fundamental rights violations concerning use ai including civil society lawyers oversight bodies interviewed presenting main findings report presents main findings fieldwork particular report includes ---- overview use ai eu across range sectors focus social benefits predictive policing healthcare targeted advertising ---- analysis awareness fundamental rights implications selected rights focus four use cases ---- discussion measures assess mitigate impact ai related technologies people' fundamental rights two annexes available fra' website supplement report ----annex gives detailed description research methodology questions asked interviews ----annex provides examples potential errors using ai selected areas addition country specific information five member states covered complements fieldwork research delivered contractor also available fra' website maps policy developments ai legal framework governing use different sectors supporting rights compliant policymaking report provides evidence extent fundamental rights considerations brought discussions activities develop test employ monitor ai systems eu also highlights different technologies affect rights set charter reflects protect rights ai becomes widespread sophisticated analysis selected fundamental rights challenges help eu member states well stakeholders assess fundamental rights compatibility ai systems different contexts findings report current views practices among using ai supports policymakers identifying actions needed report aim provide comprehensive mapping use different ai systems five eu member states covered research provide depth technical information different systems mentioned interviewees work conducting interviews report based semi structured interviews representatives public administration private companies involved use ai services businesses fra intentionally provided general definition ai interviewed part research based existing definitions organisations interviewed active public administration general working law enforcement private companies include working health retail pricing marketing financial services insurance employment transport energy importantly except two interviewees research include companies sell ai companies instead entities use ai support operations addition ten interviews conducted experts dealing potential challenges ai public administration e g supervisory authorities non governmental organisations lawyers working field interviews carried five eu member states estonia finland france netherlands spain countries selected based different levels uptake ai technology policy development area ai well incorporate experience across different parts eu fra outsourced fieldwork ecorys fra staff supervised work developed research questions methodology interviewers received dedicated training conducting fieldwork interviews carried anonymously consequence information identifying organisation concerned provided report addition certain details applications described - notably country - omitted protect respondents' anonymity communicated interviewees increasing level trust allowing speak freely work also proved useful recruiting respondents mean artificial intelligence universally accepted definition ai rather referring concrete applications reflects recent technological developments encompass variety technologies although ai usually defined widely survey conducted behalf european commission among companies eu showed eight ten people working companies eu say know ai slightly two respondents companies eu know sure ai fra' research apply strict definition ai use high level cases presents interviews ai defined broadly reference definition provided high level expert group expert group artificial intelligence ai hleg artificial interviewees also expressed variety ways think ai identifying use cases explore research intelligence project focused applications support decision making based data machine learning applications \"artificial intelligence ai refers systems systems contribute automating tasks usually display intelligent behaviour analysing environment taking actions - degree undertaken humans cannot undertaken autonomy - achieve specific goals ai based humans due large scale use cases systems purely software based acting report provide insight different technologies virtual world e g voice assistants image used discussed selected areas broad heading analysis software search engines speech ai may contention concerning whether face recognition systems ai embedded certain use cases constitute ai current level use hardware devices e g advanced robots report often refers 'ai related technologies' autonomous cars drones internet things applications \" past years seen enormous increase computing initial definition ai hleg subject power increased availability data development discussion groups see ai hleg new technologies analysing data increased amount definition ai main capabilities disciplines variety data sometimes available almost real time internet often referred big data machine learning technologies related algorithms including deep learning benefit enormously increased computing power data availability development use flourishing use terms however limited use even prove counterproductive triggers ideas linked science fiction rather real application ai variety myths exist ai often spread via social media example claim ai act form entity distracts fact ai systems made humans computers follow instructions made given humans human centric approach ai important note ai never anything - human beings use technology achieve certain goals however human work decision making behind ai systems often visible centre attention \"currently lawyer entire studies many discussions explored possible ai definitions tell definition ai european commission' joint research centre analysed ai definitions ' asked around pretty highlights often refer issues linked perception thoroughly one tell \" environment e way system receives input data environment public administration netherlands e g sensors information processing decision making achievement specific goals definitions frequently refer machines behaving like humans taking tasks associated human intelligence given difficulty defining intelligence many definitions remain vague makes use ai hard measure practice10 equally challenging define law report discusses use ai based concrete applications differ terms complexity level automation potential impact individuals scale application discussion around actual use ai involves deploying machine learning technologies seen sub domain ai also confusion around term \"learning\" implies machines learn like humans reality much current machine learning based statistical learning methodologies machine learning uses statistical methods find rules form correlations help predict certain outcomes different traditional statistical analysis involve detailed checks predictions produced often referred 'black boxes' traditional statistical analysis based specific theoretical assumptions data generation processes correlations used machine learning geared towards producing accurate outcomes used automating workflows decisions acceptable level accuracy obtained usual example email spam filter uses statistical methods predict email spam important know certain email blocked spam predicted high accuracy really need understand algorithm works e based rules emails get blocked however depending complexity task prediction always possible high accuracy moreover report highlights understanding certain outcomes predicted acceptable certain tasks area machine learning incorporates several approaches often machine learning refers finding rules link data certain outcome based dataset includes outcomes supervised learning example dataset emails labelled spam 'ham' used find correlations rules associated spam emails dataset rules used 'predict' degree likelihood future email spam sometimes machine learning used find hidden groups datasets without defining certain outcome unsupervised learning - example segmenting people groups based similarities demographics finally rules correlations found trial error reinforcement learning systems try optimise certain goal experimentation update rules automatically best possible output systems need enormous amounts data hardly used humans involves experimentation mainly responsible success winning board games humans often sensationalised media ai fundamental rights eu policy framework moving towards regulation policymakers time highlighted potential ai related technologies improve efficiency drive economic growth yet public authorities international organisations recently reflected fundamental rights challenges associated technologies coupled growing use accuracy ai systems turned attention whether regulate use european parliament resolution marked milestone eu' recognition fundamental rights implications ai resolution stressed \"prospects opportunities big data fully tapped citizens public private sectors academia scientific community public trust technologies ensured strong enforcement fundamental rights\" called european commission member states data protection authorities \" develop strong common ethical framework transparent processing personal data automated decision making may guide data usage ongoing enforcement union law\" later year european council called \"sense urgency address emerging trends\" including \"issues artificial intelligence ... time ensuring high level data protection digital rights ethical standards\" european council invited european commission put forward european approach ai responding calls european commission published communication ai europe18 set high level expert group ai initiatives include strong reference fundamental rights commission facilitated high level expert group made independent experts academia civil society industry including representative fra published 'ethics guidelines trustworthy ai' 'policy investment recommendations trustworthy ai' developed work triggered discussion importance framing ai human rights terms alongside ethical considerations led development ethics guidelines refer charter place fundamental rights consideration respect ai ethics guidelines include assessment list trustworthy ai translated checklist guide develop deploy ai indicating political support highest level european council calls strategic guidelines \"ensure europe digitally sovereign\" policy \"shaped way embodies societal values\" similarly commission president von der leyen committed \"put forward legislation coordinated european approach human ethical implications ai \" prompted significant moves towards setting eu legal framework govern development use ai related technologies including respect impact fundamental rights february european commission published white paper artificial intelligence sets policy options meeting twin objectives \"promoting uptake ai addressing risks associated certain uses new technology\" paper promotes common european approach ai deems necessary \" reach sufficient scale avoid fragmentation single market\" notes \" introduction national initiatives risks endanger legal certainty weaken citizens' trust prevent emergence dynamic european industry\" legal uncertainty also concern companies planning use ai commission white paper ai highlights risks fundamental rights one main concerns associated ai acknowledges \" use ai affect values eu founded lead breaches fundamental rights result flaws overall design ai systems use data without correcting possible bias\" also lists wide range rights affected white paper ai indicates commission' preference possible new regulatory framework follow risk based approach mandatory requirements would principle apply high risk applications would determined basis two cumulative criteria employed sector healthcare transport parts public sector significant risks expected occur used manner significant risks likely arise latter risk could assessed based impact affected parties adding harm based element white paper also highlights instances ai use certain purposes considered high risk irrespective sector include use ai applications recruitment processes remote biometric identification including facial recognition technologies following public consultation ran february june commission expected propose legislation ai first quarter ahead proposal eu' co legislators considered various aspects potential legal framework october european parliament adopted resolutions recommendations european commission framework ethical aspects ai robotics related technologies civil liability regime ai also adopted resolution intellectual property rights development artificial intelligence technologies continues work resolutions ai criminal law use police judicial authorities criminal matters ai education culture audio visual sector also established special committee artificial intelligence digital age following meeting october heads state government eu member states declared \"eu needs global leader development secure trustworthy ethical artificial intelligence\" invited commission \"provide clear objective definition high risk artificial intelligence systems addition council eu adopted conclusions shaping europe' digital future35 seizing opportunities digitalisation access justice included dedicated section deploying ai systems justice sector german presidency council eu published conclusions charter fundamental rights context artificial intelligence digital change text supported objected member states growing reference fundamental rights discussions indicates fundamental rights framework alongside legal frameworks38 necessary effective human rights compliant evaluation many opportunities challenges brought new technologies many existing ai initiatives guided ethical frameworks typically voluntary fundamental rights centred approach ai underpinned legal regulation responsibility respecting protecting fulfilling rights rests state guarantee high level legal protection possible misuse new technologies also provides clear legal basis develop ai reference fundamental rights - application practice - fully embedded addition steps towards legal regulation eu taking significant policy financial actions support development ai related technologies alongside white paper commission published european data strategy aims set single market data including nine common european data spaces covering areas health data financial data proposal multiannual financial framework would create digital europe programme worth EUR billion invest eu' \"strategic digital capacities\" including ai addition funding horizon europe connecting europe facility international actors also considering steps regulate ai notably council europe active player field ai related technologies september committee ministers council europe set ad hoc committee artificial intelligence cahai aims examine \" feasibility potential elements legal framework development design application ai based council europe' standards human rights democracy rule law\" april committee ministers council europe adopted recommendations human rights impact algorithmic systems addition organisation economic cooperation development oecd adopted ai principles created ai policy observatory global level unesco starting develop global standard setting instrument ai selected examples wide range legal policy initiatives aiming contribute standard setting area ai includes amongst others actual draft legislation soft law guidelines recommendations use ai reports recommendations law policy fra put together non exhaustive list initiatives linked ai policymaking also include legislative initiatives eu member states many organisations businesses launched initiatives tackle ethical concerns ai however useful tackle potential problems ai ethical approaches often rely voluntary action sufficiently address obligation respect fundamental rights fra pointed fundamental rights report \" rights based approach guarantees high level protection possible misuse new technologies wrongdoings using \" european commission' initiative regulating ai helps avoid disjointed responses ai across member states undermine businesses across eu entities outside eu endnotes reuters 'amazon scraps secret ai recruiting tool showed bias women' october chatbot chatterbot common ai feature embedded messaging applications simulate human conversation voice text independent 'ai robots learning racism sexism prejudices humans study finds' april prates avelar p lamb l 'assessing gender bias machine translation - case study google translate' march gender shades project evaluating accuracy ai powered gender classification products see example der standard datenschutzbehorde kippt umstrittenen ams algorithmus algorithmwatch poland government scrap controversial unemployment scoring system privacy first dutch risk profiling system syri banned following court decision european commission european enterprise survey use technologies based artificial intelligence luxembourg july see example website \"ai myths\" samoili lopez cobo gomez e de prato g martinez plumed f delipetrev b ai watch defining artificial intelligence towards operational definition taxonomy artificial intelligence luxembourg schuett j legal definition ai arxiv hastie tibshirani r friedman j elements statistical learning data mining inference prediction springer see example pasquale f black box society secret algorithms control money information harvard university press cambrigde london rai 'explainable ai black box glass box' journal academy marketing science vol pp seminal paper describing difference breiman l 'statistical modeling two cultures' statistical science vol pp european parliament resolution march fundamental rights implications big data privacy data protection non discrimination security law enforcement ini para ibid para european council european council meeting october - conclusions euco brussels october p european commission communication commission european parliament european council council european economic social committee committee regions artificial intelligence europe com final april information available webpage high level expert group high level expert group artificial intelligence ethics guidelines trustworthy artificial intelligence policy investment recommendations trustworthy ai high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment european council new strategic agenda p vonder leyen ursula union strives agenda europe p european commission white paper artificial intelligence - european approach excellence trust com final brussels february p ibid p european commission white paper artificial intelligence public consultation towards european approach excellence trust july european commission adjusted commission work programme annex new initiatives may european parliament legislative observatory framework ethical aspects artificial intelligence robotics related technologies inl european parliament resolution october recommendations commission civil liability regime artificial intelligence inl european parliament resolution october intellectual property rights development artificial intelligence technologies ini european parliament artificial intelligence criminal law use police judicial authorities criminal matters ini european parliament legislative observatory artificial intelligence education culture audiovisual sector ini european parliament decision june setting special committee artificial intelligence digital age defining responsibilities numerical strength term office rso european council special meeting european council october - conclusions euco october council european union shaping europe' digital future - council conclusions june council european union council conclusions \"access justice - seizing opportunities digitalisation\" october council european union presidency conclusions - charter fundamental rights context artificial intelligence digital change october see e g pagallo u casanovas p madelin r ' middle approach assessing models legal governance data protection artificial intelligence web data' theory practice legislation pp see fra fundamental rights report luxembourg publications office chapter communication commission european parliament council european economic social committee committee regions european strategy data com final european council conclusions special meeting european council july euco july council europe ad hoc committee artificial intelligence cahai factsheet governance digital transformation council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems adopted committee ministers april 1373rd meeting ministers' deputies see dedicated oecd website see dedicated unesco website see overview fra ai policy initiatives council europe website fra fundamental rights report luxembourg publications office p putting fundamental rights context - selected use cases ai eu eu use ai related technologies relatively wide spread recent survey shows companies use ai related technologies - plan chapter presents selected cases ai use note - typically referred 'use cases' ai field fra collected information cases interviewees use cases presented five eu member states estonia france finland netherlands spain involve chapter based information different areas application across public obtained interviews public private sector administration private companies special representatives focus put use ai areas social benefits predictive policing health services interviewed representatives targeted advertising public administration work areas health services chapter provides information current infrastructure energy use ai well basic information eu judiciary law enforcement competence select areas use cases migration border management social benefits tax well provide good sense kind ai transportation traffic control related technologies currently used interviewees private companies examples also offer context mainly work retail marketing fundamental rights analysis looking pricing health sector broad variety use cases provides important financial services energy insurance insights actual use ai affect employment transport well people' fundamental rights chapter includes cross cutting areas focus discussion fundamental rights implications ai development different sectors makes reference cases described chapter according european enterprise survey beginning companies use ai eu said use technologies depend ai percentage ranges companies estonia cyprus czechia see figure another companies eu planning use ai future survey indicates ai used mostly sector technologies used comprise variety applications aiming process equipment optimisation anomaly detection process automation forecasting price optimisation decision making figure companies using ai member state eu ro uk se si nl bg lu el cz lt mt lv dk hr de hu fr pt pl fi es ie ee cy sk currently using ai planning use ai notes survey asked use plans use ten different ai related technologies speech recognition visual diagnostics fraud detection analysis emotions forecasting based machine learning includes percentage companies using least one ai technologies n source fra based data extracted european commission european enterprise survey use technologies based artificial intelligence luxembourg july noted report focuses four broad ai 'use cases' ----social benefits ----predictive policing ----health services ----targeted advertising areas particularly sensitive regards fundamental rights two \"ai machine learning cover mainly public administration' use ai social benefits allocation different concepts ai umbrella predictive policing two concern private companies health term \" services targeted advertising use cases provide basis private company estonia report' fundamental rights analysis offering necessary context appropriate report also highlights findings interviews cover areas four areas detailed studies taxonomy ai available providing \" see categorisations technology noted introduction interviewees everyone something different views ai also stated machine learning labelling clear definition ai 'ai' \" public administration netherlands report discusses specific use cases without classifying technology applied yet use ai cases examined differed use technology described interviewees involved varying levels complexity varying levels automation figure provides overview different examples use interviewees discussed heading ai applications relatively straightforward understand rule based decision making algorithms defined based ' rules' example person income certain threshold eligible certain benefits algorithms used area social benefits different levels automation examples full partial human review involved applications used traditional statistical methods inform decisions include example regression analysis classical statistical method analyses correlation several pieces information 'variables' outcome credit score example others used complex machine learning methodologies feed production forecasts statistics government reports also algorithms much higher levels complexity deep learning diagnosis support area health tools still include high level human review hence include high level automation contrast targeted advertising example potentially using highly complex algorithms without human review output decision also using highly complex algorithms including deep learning reinforcement learning see chapter descriptions terms human review would also possible area due scale algorithms operate figure examples different automation complexity levels use cases covered high rule based regression analysis deep learning automated predictions fully reinforcement areas examples decision making automated credit learning social benefits scoring advertising social welfare rule based regression analysis facial recognition marketing automation decision predictions technology positive human reviewed identification law enforcement outcomes credit scoring human social benefits review health services financial services rule based machine learning medical images human decisions supported analysis social production diagnosis benefits forecasts low low complexity high source fra ai systems also vary according potential harm could result notes examples financial erroneous decision based use ai depending area services use facial recognition application wrong decisions - based erroneous outputs technology covered system - different impacts using ai decision making detailed use case descriptions consequences different decision affirmative wrong false mentioned interviews positive negative wrong false negative examples illustrate different levels complexity automation used practice issues particularly important machine learning used based statistical calculations always come degree error rule based algorithms also make mistakes especially grow complex risks lower deterministic nature rules developed example using ai make decisions social benefits false positive means person may erroneously receive benefits necessarily negative impact person concerned unless error found later money needs paid back however negatively impacts public administration money paid line good administration practices contrast false negative would negative impact individual would receive benefits entitled annex available fra' website provides hypothetical examples effects wrong decisions based use cases discussed importantly automating tasks impact could also scale potentially exacerbating negative effect society whole severity scale potential harm one aspect needs taken consideration analysing potential limitations fundamental rights respect use ai example small error rates using facial recognition technology used law enforcement might still lead flagging many innocent people technology used places many people analysed might apply airports train stations thousands people could scanned daily basis potential bias error rates could lead disproportionally targeting certain groups society interviewees mostly mention 'machine learning' including use neural networks technologies extensions see chapter description machine learning respondents used across either directly mentioned mentioned subfields machine learning image cases recognition facial recognition technology frt identified research often interviewees mentioned use 'supervised machine learning' mainly used optimise specifically defined outcome yet sometimes 'unsupervised machine learning' also used categorise cluster data one case referred use 'reinforcement learning' without going much detail several respondents used 'natural language processing nlp ' technology analyse text speech sometimes combined machine learning algorithms mention examples involve rule based algorithms meaning rules algorithm follow directly encoded e based ' rules' cases interviewees disclose could provide detailed information technology used generally interviewees referred one use case asked focus one application interviews ----importantly fieldwork shows companies public administrations often still beginning looking use ai two thirds use cases actually use deployed practice many use cases described interviewees pilot stage development still research phase ----two ai driven applications halted tests figure words interviewees often used describe ai 'use cases' notes fra visualisation words frequently used descriptions use cases bigger size word often interviewees mentioned terms source fra figure shows frequently used words describe use cases covered report highlights importance data using ai systems well relevance supporting decision making fra previously highlighted thorough description data used ai applications essential identifying mitigating potential fundamental rights challenges variety data used ai systems covered report however difficult obtain detailed information data used respondents remained rather vague data sources rather generically many respondents mentioned using 'open data' 'historical data' 'metadata' concretely respondents mentioned using customer data e g purchases browsing behaviour administrative records \" mostly used save time ... data social benefits taxes interviewees also mentioned go lot medical records police records court records well social media material \" traffic data data included text data e g e mails audio recordings video public administration netherlands geolocation data data come internal databases companies public administration also external sources \" important deal cases efficiently ' single important reason using ai increased efficiency making use workforce vast majority respondents across public private sector mentioned people handle cases using ai greater speed fewer errors cost reduction fewer human effectively possible \" resources needed interviewees law enforcement also said public administration netherlands use ai safety security well crime prevention humans previously performed many use cases respondents said use ai entails fewer mistakes humans carry certain tasks respondents also use ai tasks humans previously carry quantity information could processed humans - example area genome analysis traffic predictions importantly half respondents interviewed use ai relevant decision making however ai mainly used support decision making final decisions remain largely hands humans interviewees pointed enthusiastic public administration companies still cautious deploying ai many use cases still testing phase described stopped phase nevertheless almost interviewees aware plans reduce level technology used fact expressed intentions invest innovation new ways employ currently available ai systems examples ai use public administration use case automating social welfare systems - using algorithms area social benefits background eu legal framework united nations special rapporteur extreme poverty human rights philip alston warned october report introducing 'digital welfare' state including use ai lead \"digital welfare dystopia\" digitalisation welfare systems often accompanied reductions overall welfare budgets narrowing beneficiary pool measures reduce availability welfare digitalisation also increases power states offering opportunities control people particularly worrying countries significant rule law deficits use algorithms public administration welfare raises major concerns respect potentially negative impact poverty inequality applied erroneously area social benefits includes areas child welfare services6 unemployment benefits yet public authorities keen use new technologies make decision making social security benefits efficient potentially fairer globally new technologies used many ways administer welfare systems include identity verification eligibility assessments benefit calculations fraud prevention detection risk scoring need classification well communication authorities beneficiaries oecd defines social benefits transfers made households need certain events particular circumstances arisen including sickness unemployment retirement housing education family circumstances however commonly agreed definition social benefits social benefits particular social insurance systems different private insurance schemes involve compulsory contributions made employees employers sometimes form taxation social policy including social security social protection area shared competence eu member states article b tfeu pursuant article tfeu eu pursues objectives among things promote \"improved living working conditions\" \"proper social protection\" end eu supports complements activities member states number fields including social security social protection workers combating social exclusion article tfeu eu actions encourage cooperation member states adopt directives minimum requirements moreover decisions social security social protection adopted special legislative procedure unanimous vote council backdrop eu member states mostly free shape social security social protection policies since virtually harmonisation social security systems differ significantly across eu terms benefits provided conditions eligibility benefits calculated contributions need paid etc public administrations eu member states working implementing ai related technologies area public welfare however information applications limited fra collected information use cases linked ----using algorithms comes compensating job seekers ----processing social benefits applications ----machine learning supported data analysis use pensions several private insurance companies interviewed research use ai related private technologies includes handling requests customers complementary health insurance insurance insurance compensation decision support evaluating credit risk companies' individuals insurance pricing insurance claims management decision making support use ai related management functions credit decisions private insurance companies generally embrace ai related technologies help make business profitable oecd report highlights importance technology sector also argues risk classification could lead exclusion belonging certain vulnerable groups ways undesirable societal political perspective oecd impact big data artificial intelligence ai insurance sector use practice use cases outlined exemplify challenges using planning use ai area social benefits linked algorithmic decision making experimenting new technologies support jobseekers course three year project public organisation experimented several ai related technologies concerning work related processing benefits job seekers assisting return work representative interviewed states tested technologies improve foster relationship job seekers improve advice given job seekers companies testing completed organisation decide apply technologies day day work tests include machine learning based detection attractiveness job offers system detecting whether job seekers still actively looking job tests also looking profiling job seekers provide advice would include calculating probability someone offered available job within given time identifying parameters make job offers relevant may also reflected advice companies best practices formulating job offers profiling would allow organisation determine appropriate services according profile background job seeker rather analysis advice drawn employees practically would done requiring job seekers complete monthly diary job search however still consideration whether programme limited providing descriptive analyses whether go provide recommendations organisation hesitant latter aspect additionally natural language processing system tested analysing content job seekers' e mails e mails categorised relevant data extracted urgency relevance e mails identified using chatbot using automatic replies emails considered data used systems come several sources within organisation data job seekers background including personal tax data well data salaries social security allowances used strict conditions derived highly regulated data sources e g salary statements cannot accessed data job offers companies also used generate knowledge job market organisation currently use external data professional social media networks legal provisions place using data processing housing benefits - failure success public body responsible processing social benefits piloted ai tool process applications subsequently support staff making decisions housing benefits system selected cases new benefit applications relatively straightforward calculate include new applications housing benefits submitted individual living alone children individual income government benefits overall cases deemed simple result always individual receives benefits technological solution based decision tree model following rules housing benefits calculating general housing benefits requires income estimates advance data used testing stemmed internal database contains data benefit application processes data pseudonymised need use personal information simple statistical model linear regression used input income cost limits output amount benefit however even simplified cases found difficult use ai practice frequent changes legislation test terminated according interviewee lack legal basis using machine learning allow using administrative decisions plans use ai support decision making social benefits organisation pursuing particular project due aforementioned legal challenges interviewee noted potential applications solutions area future noted ai related technologies support operations without legal impact particularly good organisation syri case netherlands called time organisation using image processing social benefits applications 'system risk indication' syri generally benefit applicants complete developed government tool several forms attachments often alert dutch public administration submitted paper format efficient fraud risk citizens time saving handling documents processing linking large amounts agency' staff hard copies received personal data public scanned classified automated authorities system broad coalition civil society organisations dealing privacy first step turn images right way issues initiated lawsuit prompting round algorithms align documents district court hague aligned properly scrutinise algorithm based syri scanned remove spots clean edit colouring document identify court ruled syri impinges disproportionately private columns paragraphs tables elements life citizens court found distinctive blocks recognise script etc everyone whose data analysed application checks received syri exposed risk application form attachment marked addition due opacity correctly e g document marked algorithm used citizens could invoice system determines whether \"neither anticipate intrusion correct private life guard \" turning classification images good description syri done image recognition optical found ilja braun high risk character recognition ocr technologies citizens algorithm watch recognise text stemming images including photographs scans documents ruling february handwritten notes ocr technology dutch available online converts recognised text text data privacy first dutch machine readable pattern recognition risk profiling system syri banned process input scanned images following court decision first isolated compared 'glyphs' e variations letters stored system pixel pixel basis agency continue processing images develop example potentially making possible scan bar codes attachments would help speed confirmation correctness documents attachments also solutions related natural language processing automating unemployment benefits one countries selected decisions unemployment benefits fully automated national institution responsible unemployment insurance benefits updated system fully automate processing benefit applications decisions done relevant legislation adapted allow automated decisions person registers unemployed lodges application benefits system draws information applicant various databases includes example population register tax authorities' databases containing information salaries work experience etc conditions receiving unemployment benefits fulfilled system calculates period payments based length person contributed insurance system amount benefits based average daily salary procedure fully automated however employee institution must intervene necessary information cannot extracted databases contradictory information databases decision case involves level discretion e decision cannot definitively determined based data available human leeway deciding case main reason using system improved efficiency addition system believed achieve consistency processes every application subject discretion handled way use case predictive policing - trying anticipate crime advance fra activity background eu legal framework ai technologies used law enforcement particularly predictive policing preventing existing research tools affect fundamental rights unlawful profiling highlighted particular issues concerning discrimination among rights one recurrent concern potential predictive policing reproduce today entrench existing discriminatory practices particularly reliance future guide historical crime data may biased incomplete developing using algorithmic many crimes - domestic violence hate crime - remain largely profiling bias may introduced unreported therefore counted official police statistics step process avoid subsequent potential violations focus certain crimes violence drug related crime public fundamental rights experts places - rather business fraud non payment taxes example - officers interpreting data also make law enforcement responses less equitable clear understanding former often associated certain demographics neighbourhoods fundamental rights ultimately undermine police relations particular communities fra guide explains profiling criminological research crime 'hotspots' around several legal frameworks regulate decades - notably uk usa uses police data map certain conducting profiling crimes undertakes statistical tests explore crime probabilities various lawfully necessary comply police forces used developed address different types fundamental rights crucial crime concentrations clusters 'hotspots' effective policing border management recently adaptations area applied research used ai information see fra tool enhance effectiveness suggesting using algorithmic preventing unlawful profiling today tools could reduce police' reliance subjective human judgments future guide may reflect biases stereotypes studies also indicated predictive policing could potentially reduce unnecessary surveillance questioning physical checks searches reducing humiliation harassment individuals may occur activities predictive policing aims forecast probability crime anticipate emerging trends patterns inform crime prevention intervention strategies may also part investigation crime already taken place authoritative definition predictive policing typically characterised analysing data identify common patterns trends crime using algorithms create models based analysis used forecast criminal activity may occur future ai technologies area generally either aim 'predict' crimes 'predict' individuals either commit victims crimes tools aiming predict crimes generally fed historical data - largely official sources - time place type crimes committed complemented environmental variables population density presence certain public places services major events holidays generally use personal data applied fra activity contrast ai systems focused predicting potential perpetrators victims facial recognition crime employ historical real time personal data could include criminal records data addresses phone numbers location data data extracted technology social media information known associates health income rise data combined criminal environmental data fundamental rights eu member states shared competence area freedom considerations security justice article j tfeu includes judicial cooperation criminal matters police cooperation articles law enforcement tfeu already treaty lisbon adopted annexed declaration protection personal data judicial cooperation eu law recognises 'sensitive data' criminal matters police cooperation observed \"specific rules people' facial images protection personal data free movement data form biometric data processed fields ... police cooperation based article tfeu ... prove facial recognition software necessary specific nature fields \" images also quite easy capture public places although accuracy within framework predictive policing collection storage processing matches improving risk analysis exchange information particularly relevant processing errors remains real - particularly personal data context law enforcement operations regulated certain minority groups people whose eu level law enforcement directive directive eu sets images captured processed comprehensive standards safeguards processing including might know happening safeguarding prevention threats public security - cannot challenge possible misuses use practice fra paper outlines analyses use cases collected fra signal variety ways law fundamental rights enforcement authorities already use plan use ai related technologies challenges triggered support work public authorities deploy live frt law enforcement purposes also examples mentioned interviewees range data mining systems briefly presents steps take help designed map crime patterns detecting online hate speech making avoid rights violations risk assessments gender based violence automating certain prison information see fra guard duties use cases include detecting illicit objects satellite facial recognition technology images generally recognising objects images addition tool fundamental rights considerations mentioned research used private sector fraud prevention context law enforcement crime detection money transfers interviewees emphasised ai related technology systems used automate speed tasks previously done humans thus freeing better distributing resources mapping crime support efficient allocation investigation capacity national intelligence agency public prosecutor' office employ data driven system help employees make choices use available investigation capacity aim improve allocation human resources ensuring officers present right time place interviewees suggest system could make precise assessments compared humans often rely gut feeling decisions still system always used combination human appraisal non ai systems make operational decisions based system generated outcomes analysts create 'heat map' outlines prevalence certain crimes certain areas replicates long standing manual version crime anticipation system whereby police officers put pins map indicate specific risk areas using ai increase speed process also makes reliable users believe analyse data system based data mining machine learning processes primarily built unique police data contained crime reports witness statements suspect declarations gaps extent possible addressed using data sources criminology research social demographic information obtained national office statistics system also uses data open sources specific parameters calculation depend type crime predictive factors vary relevance across crime areas example case burglaries data burglaries collected combined data place residence known criminals distance burgled fra activity houses relevant criteria preselected allow system produce heat map detecting hate location based predictions made next six months indicate speech online time location burglary may occur result map small public agency combatting hate squares risk crime occurring indicated different shades crime uses ai based tool detect interviewees indicated visualisation helps officers analyse online hate speech analysing neighbourhoods observe correlations different locations patterns speech online basis processing system assessing risk gender based domestic violence determines social groups targeted helps law enforcement national police force uses internal system track cases gender adopt measures protect based domestic violence system helps police officers take decisions threats realised distribute resources across domestic violence cases system categorises cases basis assessed risk relapse repetition order although tool aims identify focus 'riskiest' cases potential victims rather perpetrators law enforcement specialist team could complete risk analysis without using ai however use information generated system able compute large amount data short amount system ask social media providers time assist untrained non specialist police officers risk analysis information users pursue criminal investigations case alleged gender based domestic violence reported police one particular challenge officer starts initial investigation includes collecting evidence taking understanding context witness statements - potentially - making arrest using information statements made example gathered process officer fills two detailed questionnaires journalists academics may use assess complaints evaluate probability reoffending examine words associated hate speech evolution case assess behaviour perpetrator report analyse occurrence victim police officers also indicate level gravity nature threats faced attitudes concerning victim fra plans initiate research online hate present social system produces risk 'score' three point scale police media allow fra provide officer raise level risk manually cannot lower risk level input policy developments indicated system level confirmed specific area online content moderation measures applied line established police protocols system uses ai also informs judge potentially 'severe cases' automated system examples ai use private sector use case ai health - analysing medical records save lives background eu legal framework healthcare particularly prominent discussions use ai medical data online applications potential support improved health outcomes - result - wider socio economic benefits covid pandemic increased focus interest area particularly terms potential online data applications enhance ability governments health services track spread disease health also prominent general population' views uses ai eurobarometer survey found every second european thinks ai best used improve medical diagnostics develop personalised medicine improve surgery use case covers applications ai related technologies public private sector stakeholders area medical records disease prediction feeding data electronic medical records emr electronic health records ehr ai systems related technologies support development preventative medicine recognises early risks disease designs appropriate interventions researchers predict clinical events mortality hospitalisation readmissions length stay hospital beyond disease prediction medical record data analysed predict patients' adherence treatment keeping medical appointments technologies potential support improved health outcomes well increase efficiency healthcare system article tfeu eu supporting competence protecting improving human health member states retain full responsibility defining health policies organising managing health systems delivering health services article tfeu within eu competence union action complement national policies directed towards improving public health preventing physical mental illness diseases obviating sources danger physical mental health action cover health information education well monitoring early warning combating serious cross border threats health article tfeu latter areas eu adopt incentive measures excluding harmonisation laws regulations member states rules policies adopted eu level aim ensure free movement citizens equal treatment non discrimination abroad well availability safety medical products services single market considering development technologies application health care exchange medical records patients' rights cross border situations disease prediction matter public health particularly relevant gdpr health genetic data considered special category data article called 'sensitive data' require specific protection processing could create significant risks data subjects' health genetic data shared specific circumstances article gdpr gdpr provides exemption purpose limitation principle data used research purposes line article researchers required ensure technical organisational safeguards - pseudonymisation anonymity - place using patient data eu also taken action regarding exchange medical records european commission recommendation c european electronic health record exchange format24 \"seeks facilitate cross border interoperability ehrs eu supporting members states efforts ensure citizens securely access exchange health data wherever eu \" recommendation lays technical specifications exchange data eu member states european data strategy february also strong focus health data 'common european health data space' one nine common european data spaces whose establishment european commission support early warning response system ewrs owned european commission operated european centre disease prevention control aims \"notifying eu level serious cross border threats health\" enabling \" european commission eu countries permanent communication purposes alerting assessing public health risks determining measures may required protect public health \" emr computerised medical record created patients healthcare organisation ehr contains patient' medical history beyond one organisation involve sharing data across healthcare system include large amount personal data encompass among others name contact details individual next kin demographic information diagnoses test results medication treatment may also include patient generated data wearable devices uniform emr ehr system operating across eu member states germany national emr ehr system others - including belgium denmark - different emr ehr systems regional level systems differ considerably depending data recorded access data european commission stakeholders highlighted diversity country level emr ehr systems lack interoperability major barrier digital single market health studies highlight potential ai related technologies enable earlier diagnosis widen possibilities disease prevention improve patient safety strengthening right access preventive healthcare benefit medical treatment emr ehr may also help make healthcare personalised possibility rapid sharing data facilitate coordinated timely treatment however use emr ehr presents significant data protection risks healthcare sector leads terms personal data breaches amount personal data stored highest among industries combined large data sharing network number access points makes healthcare sector attractive target hackers quality data emr ehr also raises concern studies patients shown medical files asked accuracy found information incomplete erroneous lot important data emr ehr unstructured form free text reduces data quality low levels accuracy completeness overall data quality increases risk medical error use practice applications described interviews include simple advanced models employed public private sectors largest number use cases refer image based diagnosis tools however interviewees also discussed tools automate various working procedures mapping text data filing medical records analyses measurements body tissues nerve fibres smaller number examples touched advanced projects systems monitor remotely certain health indicators heart rate case systems complement expertise health professionals next sections present examples diagnostic remote monitoring tools image based tools help detect diagnose disease tools used support detection diagnosis diseases described interviewees work similar ways example privately owned hospital uses ai system interpret images ct scans stroke patients stroke imaging used detect damage brain occurred may blockages blood supply brain also generate measures compared particular values medical specialist interviewee feels application helps determine characteristics images quickly potentially - depending uses tool - improving quality diagnosis however highlight necessarily efficient rely ai application since medical professional must present could examine image rather tool offer support - example specialist finds difficult interpret certain image find abnormalities system built trained validated using dataset partially based large scientific study hospital contributed supplemented purchasing foreign datasets algorithm trained adapted future based new data new versions released developers feel allowing system continue learn would make difficult using ai public authority responsible validate operation target health inspecting food safety standards restaurants uses machine learning private company developed algorithm supports detection breast cancer inspections process customer review data major online platforms helps mammography exams tool gives decide conduct inspections previously process probability degree certainty based complaints help radiologists speed analysis authority received previous results decide whether additional reports since introduction tests warranted algorithm detects tool rate non compliant characterises anomalies mammography restaurants identified doubled cancerous around interviewee indicates first step involves text mining algorithm identifies reviews system low rate false containing key words may negatives false positives note indicate health safety issues many cases deliver clear outcome 'sick 'nausea' 'rodents' system trained radiography second step authority mammography data europe eu compared results coming written reports past biopsies acting customer reviews previous control data inspection reports improve algorithm' accuracy reliability monitoring patients' vital statistics remotely hospital piloting system support early detection potential illness monitoring patients' health indicators - example blood pressure heart rate - typically takes place manually captures situation specific moment time constantly monitoring indicators potential identify trends doctors may otherwise recognise detect health issues early prevent illness system uses biosensor - kind plaster - gathers hemodynamic data patients continuously constantly monitoring heart pulsation respiration data used system come hospital patient data anonymised shared third party provider information besides gathered monitoring plaster used build train system data environmental factors incorporated pilot interviewee pointed could contain biases future system combine information gathered biosensor separate information patients' emr draw conclusions trends observed monitoring use case targeted advertising - profiling consumers boost profit background eu legal framework internet transformed way live many people make use internet services often offered free daily basis companies offering services free mainly generate revenue advertising adverts automatically targeted individual consumers based information availability data online individual behaviour combined machine learning technologies considerably improved ability commercial enterprises target individuals could even go far manipulating consumers predicting reactions based irrational aspects psychology reasoned choice cambridge analytica scandal underscored particularly negative impact uses political purposes case company illegally obtained personal data millions social media users target political adverts different social groups based certain psychological profiles recent declaration committee ministers council europe highlights lack knowledge manipulative power algorithms \" effects targeted use constantly expanding volumes aggregated data exercise human rights broader sense significantly beyond current notions personal data protection privacy remain understudied require serious consideration \" concerns also raised online advertising powered ai technologies affect data protection privacy consumer protection right non discrimination even way democracies work word 'advertising' associated messages designed influence consumer behaviour advertising one form another always targeted specific groups based characteristics behaviour growth social media however taken targeted advertising another level using direct access consumer data micro targeting directed towards specific groups - data gathered online activities targeted activities social media providers platforms like google amazon gather comprehensive user data monitoring various activities users advertisers access detailed specific information area targeted advertising systems recommend content e g news movies one real life examples also involves called reinforcement learning technology based optimising certain goal experimenting updating rules automatically best possible output means systems tries different ad placements trial error finds best way optimise revenue - including element self learning little knowledge actual use reinforcement learning available european countries major companies working area researching issue issues related targeted advertising fall consumer protection falls shared eu competence member states article f tfeu eu consumer protection measures seek protect health safety economic interests consumers promote right information education organise safeguard interests article tfeu eu adopt minimum harmonisation measures achieve high level consumer protection article tfeu yet allowing eu member states introduce even stringent measures nationally secondary eu legislation rules advertising covered directive ec concerning misleading comparative advertising directive provides minimum level protection misleading advertising also harmonises rules comparative advertising across eu provisions directive ec apply consumer business business business relations however practically applied latter53 directive ec unfair business consumer commercial practices internal market practices54 took effect directive ec services internal market55 covers services include advertising additionally directive ec certain legal aspects information society services particular electronic commerce internal market e commerce directive also applies directive forms part legal framework digital services eu meet significant developments area new online services practices e commerce directive currently revised part digital services act package package aims \"strengthen single market digital services foster innovation competitiveness european online environment\" fra collected information actual use cases six european companies engaged placing online ads content recommendation personalised marketing use practice examples covered include ----placing ads online based click predictions e learning likelihood online users click certain links adverts automated bidding auctions online advertisement space ----personalised targeted marketing communication via email tasks fully automated examples concern analyses user preferences activity calculations probabilities clicks purchases including measurement effectiveness previously made recommendations also includes methods targeted communication basis identified target groups build long term trust clients service providers targeted online ads based click predictions business models working click predictions targeted advertisements often follow 'click buy' policy companies purchase advertising space media platforms optimise display adverts analysing interests preferences website users showing advertisements interest purpose increase relevance advertisements shown better matching interests see present example company gets paid people click advertisement buy something additionally company uses ai detect inappropriate content advertising advertisements alcohol firearms political content company uses range machine learning techniques field computational advertising estimate probability user clicking advertisement displayed specific context optimising called click rate customers' interests relevance products measured via mapping individuals' browsing histories transaction patterns information derived individuals' navigation merchant websites worldwide advertising company works done via anonymised third party cookies trackers placed merchant websites outline individuals' navigation across also list products seen purchased profiles individuals linked devices used although ip addresses anonymised product purchased recommender system algorithm tries determine products customer could also buy case 'fresh' data valued higher older data browsing histories stored maximum one year interests change purchases older year longer necessarily considered relevant advertisements shown respective person immediately adapted accordingly vary across websites also match content latter advertisement posted continuously analysed combination elements taken account individual' interest confirmed purchase made data shared across platforms includes informing others purchase made stop advertisements particular item purchase made formula reviewed algorithm adapted individuals' continuous online behaviour future company covered example expects work optimising timing terms places advertisements within given budget certain time frame also expects focus displayed ads impact consumers another example based european online market place links buyers sellers range specialised products ai used optimise advertising campaigns categorise products based advertisements shown website market place improve search engine experience predicting complementary substitutable products detect fraud attempts company uses machine learning predict value clicks customers buy advertisement space offered real time auctions examples company indicates ai enables make decisions otherwise would possible without ai would significantly scaled targeted communication customers clients case retail company focusing specialised supplies sold across physical stores online direct marketing personalised advertising used increase appeal customers time measure efficiency particular instance marketing advertising according company issue example marketing emails opened average particularly customers recognise relevant favourite products offered marketing emails sent around registered individuals system used establish may considered relevant individuals done analysing purchases made respective individuals previous six months offers displayed directly based previous purchases meanwhile new suggestions e alternative products category previous purchases similar approach used bank sends emails clients messages offering specific services products sent certain clients data analysts calculate probability clients interested service product probability certain threshold client receive message system used yet include machine learning models fully automated points taken develop system third example grocery retailer uses loyalty cards increase customers' interaction personalise offers loyalty card systems predict many customers likely engage product offering system covered example also suggests new products customers tracks results suggestions groups buyers similar behavioural patterns segments make personalised suggestions every week company' loyalty card owners receive personalised offers email website mobile application access offers store terminals ai system selects offerings based individual purchase history recommends new items might catch buyer' interest prompt purchase endnotes see instance samoili et al ai watch defining artificial intelligence towards operational definition taxonomy artificial intelligence luxembourg karanasiou pinotsis ' study layers automated decision making emergent normative legal aspects deep learning' international review law computers technology pp see fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office p fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office june un human rights council report special rapporteur extreme poverty human rights philip alston eubanks v automating inequality hightech tools profile police punish poor st martin' press redden joanna dencik lina warne harry datafied child welfare services unpacking politics economics power policy studies panoptykon foundation profiling unemployed poland social political implications algorithmic decision making see also algorithm watch poland government scrap controversial unemployment scoring system oecd glossary statistical terms social benefits definition accessed august j henry richardson chapter iv social insurance economic financial aspects social security university toronto press pieters social security overview eu competence domain regulation ec see paju j european union social security law oxford hart publishing ch erik bakke 'predictive policing argument public transparency' new york university annual survey american law vol pp andrew g ferguson 'policing predictive policing' washington university law review vol pp example one five women experienced violence brought serious incident attention police see fra violence women eu wide survey main results report luxembourg publications office p elizabeth e joh new surveillance discretion automated suspicion big data policing uc davis legal studies research paper p braga et al hot spots policing small geographic areas effects crime campbell systematic reviews vol elizabeth e joh new surveillance discretion automated suspicion big data policing uc davis legal studies research paper pp available ssrn erik bakke 'predictive policing argument public transparency' new york university annual survey american law vol pp wim hardyns anneleen rummens 'predictive policing new tool law enforcement recent developments challenges' eur j crim policy res p doi s10610 albert meijer martijn wessels 'predictive policing review benefits drawbacks' international journal public administration p doi law society commission use algorithm justice system algorithms criminal justice system p newbold j n 'predictive policing' 'preventative policing' 'intelligence led policing' future declaration annexed final act intergovernmental conference adopted treaty lisbon signed december directive eu european parliament council april protection natural persons regard processing personal data competent authorities purposes prevention investigation detection prosecution criminal offences execution criminal penalties free movement data repealing council framework decision jha oj l pp european commission standard eurobarometer report europeans artificial intelligence p european patients forum n new eu regulation protection personal data mean patients guide patients patients' organisations commission recommendation eu february european electronic health record exchange format oj l pp digital health society exchange electronic health records across eu february communication commission european parliament council european economic social committee committee regions european strategy data com final brussels february decision eu european parliament council october serious cross border threats health repealing decision ec oj l pp see commission webpage communicable diseases oecd european union healthcare glance europe p vera ehrenstein hadi kharrazi harold lehmann casey overby taylor 'obtaining data electronic health records' gliklich leavy mb dreyer na eds tools technologies registry interoperability registries evaluating patient outcomes user' guide 3rd ed addendum use data insurance industry currently potential see example spender c bullen l altmann richer j cripps r duffy c falkous farrell horn j wigzell w yeap 'wearables internet things considerations life health insurance industry' british actuarial journal pp see visualisation see short overview different ehr systems europe nurses' perspective healtheurope world cloud based services storing health data cloud college europe transformation health care digital single market synopsis report public consultation european commission study big data public health telemedine healthcare roberta pastorino corrado de vito giuseppe migliara katrin glocker ilona binenbaum walter ricciardi stefania boccia benefits challenges big data healthcare overview european initiatives european journal public health vol issue supplement pp - ministry health welfare sport netherlands digitalization health care benefits patient safety literature web reports according multiple reports different cybersecurity companies time see example sc magazine healthcare leads cost data breaches shannon williams new report reveals 'wall shame' health care data breaches tammy lovell statistics reveal healthcare sector affected personal data breaches sc magazine healthcare leads cost data breaches annet sollie reuse sharing electronic health record data focus primary care disease coding doctoral dissertation vrije univesiteit amsterdam pp vera ehrenstein hadi kharrazi harold lehmann casey overby taylor 'obtaining data electronic health records' gliklich leavy mb dreyer na eds tools technologies registry interoperability registries evaluating patient outcomes user' guide 3rd ed addendum mowafa househ bakheet aldosari abdullah alanazi show andre kushniruk elizabeth borycki 'big data big problems healthcare perspective' studies health technology informatics p sartor giovanni new aspects challenges consumer protection study committee internal market consumer protection policy department economic scientific quality life policies european parliament luxembourg neudert lisa marchal nahema polarisation use technology political campaigns communication study request panel future science technology stoa managed scientific foresight unit within directorate general parliamentary research services eprs secretariat european parliament information commissioner' office ico investigation use data analytics political campaigns council europe declaration committee ministers manipulative capabilities algorithmic processes decl example costello roisin aine impacts adtech privacy rights rule law technology regulation - edps opinion edps opinion online manipulation personal data sartor giovanni new aspects challenges consumer protection jablonowska agnieszka et al 'consumer law artificial intelligence challenges eu consumer law policy stemming business' use artificial intelligence' eui working papers law wachter sandra 'affinity profiling discrimination association online behavioural advertising' berkeley technology law journal vol forthcoming available ssrn zuboff shoshana age surveillance capitalism london edps opinion edps opinion online manipulation personal data martin gillian importance marketing segmentation american journal business education vol kaili lambe becca ricks basics microtargeting political ads facebook see example information recsys2020 workshop reveal bandit reinforcement learning user interactions accessed august directive ec european parliament council december concerning misleading comparative advertising oj l pp european commission misleading comparative advertising directive objective directive directive ec european parliament council may concerning unfair business consumer commercial practices internal market amending council directive eec directives ec ec ec european parliament council regulation ec european parliament council 'unfair commercial practices directive' oj l pp directive ec european parliament council december concerning misleading comparative advertising oj l see european commission' webpage digital services act package fundamental rights framework applicable ai use ai - presented four use cases discussed chapter - affect specific fundamental rights outlined chapter full compliance fundamental rights prerequisite using ai driven technologies irrespective area concerned chapter introduces general fundamental rights framework eu governs use ai including selected secondary eu legislation national law section fundamental rights framework provides normative basis benchmarks design development deployment ai tools helps determine whether specific use ai fundamental rights compliant requirements justified interferences fundamental rights outlined section fundamental rights framework governing use ai cornerstone instrument eu fundamental rights framework applicable use ai charter together unwritten general principles eu law main source fundamental rights eu charter enshrines wide array fundamental rights legal value eu treaties eu institutions bodies bound charter member states act within scope eu law article charter many charter rights set european convention human rights echr meaning scope must corresponding echr rights article charter however cannot prevent union law providing extensive protection fundamental rights also found provisions treaties see e g article teu titles v x tfeu eu secondary law rights safeguarded different pieces secondary eu law central piece eu secondary law context ai general data protection regulation gdpr - regulation eu governs automated processing personal data european economic area processing personal data means form part filing system - within scope eu law result gdpr apply national security related data processing gdpr coupled law enforcement directive applies police judicial cooperation criminal matters eu instruments include numerous provisions protection personal data determining key principles data processing lawfulness fairness transparency whether eu data protection legislation applies depends whether personal data processed ai driven applications use personal data example traffic data others use anonymised data cases data protection laws apply applicability entirely clear line personal non personal data blurred risk anonymised data ' identified' - ie anonymisation undone however identification usually illegal addition persons identifying data usually put major efforts potentially need access additional information individuals might included anonymised dataset identification section discusses topic detail linked results interviews carried report addition eu data protection acquis european non discrimination law key safeguarding fundamental rights context use ai related technologies article teu provides non discrimination one fundamental values eu article tfeu requires union combat discrimination number grounds moreover articles charter provide equality law non discrimination beyond several eu non discrimination directives enshrine specific detailed provisions varying scopes application include employment equality directive ec racial equality directive ec gender goods services directive ec recast gender equality directive ec eu member states also party international human rights conventions see list conventions key findings fra opinions section contain legally binding standards safeguards comply act areas fall within scope eu competence main instrument echr ratified eu member states accompanied additional protocols great majority eu member states parties echr wide reach also applies areas covered eu law addition council europe convention protection individuals regard automatic processing personal data13 another source pan european data protection obligations binding eu member states recently modernised sector specific eu national legislation also enshrines safeguards protection fundamental rights overview technical legislation beyond scope report however chapter provides examples relevant use cases discussed report complemented couple examples national laws five eu member states covered none five eu member states covered currently horizontal ai specific laws although countries looking potential need regulation eu countries finland issued recommendations self regulation development responsibility standards private sector estonia assessment concluded separate ai specific law required foreseeable future since current legal framework sufficient according relevant estonian long term strategy however legal environment must adapted avoid unnecessary hindrances implementing ai situation concerning sectoral legislation relevant use ai different sectors varies across eu member states however active policymaking ai recently emerged national level national action plans ai appeared remain core policy development member states countries working growing entrepreneurship others focused enacting market oriented policies compatible un agenda sustainable development educational activities promote ai increasing public use ai often identified ai related strategy goals investment research development also frequently outlined relevant goal domestic ai discussions potential legislative reforms remain attentive european initiatives national sector specific fundamental rights safeguards also enacted instance finland began considering overhaul domestic human rights safeguards public sector proposing broader across board legislative update opposed individual ai laws specific reference processing personal data immigration law finnish constitutional law committee put forward proposal strengthen safeguards finnish constitution overriding constitutional law shortcomings relation among others protection law accountability well ambiguity algorithms automated decision making whenever public authorities automate decision making processes processes must adhere constitutional principle rule law may endanger observance rules good administration due process proposal articulated vision requirements finnish constitution sets ai use automated decision making within public administration research identified initiatives policies linked ai fundamental rights five member states examined example estonian e state charter includes summary citizens' rights better communicating agencies electronically also targets ai relation right know data collected public authorities similarly ministry interior netherlands presented policy brief parliament ai public values fundamental rights brief stresses human centric approach ai applications strong influence human beings society whole also lists important risks ai fundamental rights discrimination result biased data reduced interpersonal relations ai takes certain forms interaction 'use case' examples social welfare use case regulating social welfare eu member states enacted rules aiming protect fundamental rights specifically area addition existing horizontal eu regulations see section mostly define rules processing protection personal data purpose social benefits insurance estonia example insurance activities act applicable types forms insurance regulates processing transmission personal data context states public authorities health care providers insurance undertakings third parties may transmit personal data request insurance undertaking personal health court data necessary insurance undertaking perform insurance contract right obligation disclose data derives law scope act also includes data transfers purpose data processing within ai systems social welfare act contains specific provisions data protection persons need social assistance notified processing data provide consent processing person established target group right opt data processing social welfare act also allows local authorities process including using algorithms personal data youth years age stored state registries identify youth employment education training finland act secondary use health social data applies using ai social care healthcare act based norms securing protecting sensitive personal data outlined gdpr aims establish conditions effective secure \"processing access personal health social data certain secondary purposes research statistics innovation development knowledge management teaching authority planning \" act regulates manner registered health data cannot processed several laws apply various types social benefits france code relations public administration applies purpose processing accessing personal data related social benefits minor amendments entry force gdpr code states \" algorithms used public administrations must published\" \" person subject automated decision making right informed\" predictive policing use case context predictive policing eu' law enforcement directive contains key fundamental rights safeguards stipulate law enforcement authorities apply main data protection principles set gdpr include requirement data controllers e competent law enforcement authorities provide data subjects information controller' data processing activities identity contact details data controller purposes processing information right lodge complaint article specific cases data controllers shall provide information - example legal basis processing - enable data subjects exercise rights right access article requires data controller confirm upon request data subject whether processing operations related case data subject shall able access data also request additional information including purposes legal basis processing categories personal data processed right information right access restricted number cases including avoid obstructing prejudicing prevention detection investigation prosecution criminal offences protect public security national security addition article law enforcement directive explicitly prohibits automated decision making prohibition limited authorised eu national law safeguards data subject' rights including \" least right obtain human intervention part controller\" see section cases scope implementing national legislation broader directive example finnish act processing personal data criminal matters connection maintaining national security strengthens right information distinguishing information provided general special circumstances healthcare use case regards eu level fundamental rights safeguards using ai healthcare gdpr empowers patients rights informed part granting control personal health data data qualifies 'sensitive data' found example medical records rights include rights access one' personal health data object processing personal data rectification erasure data well rights case breach gdpr administrative fines breaches processing data including health data allowed however estonia instance domestic law allows maximum penalty eur application misdemeanour procedure cases data protection inspectorate also impose similar fines misdemeanour procedure france data protection act public health code impose stricter requirements set gdpr regarding health data processing french data protection act amended law modernisation health system allow processing personal health data various purposes provided fall within scope one exceptions general principle prohibition sensitive data processing article gdpr targeted advertising use case considering fundamental rights safeguards relation targeted advertising underlying mechanisms regarding profiling particular eu legal framework privacy data protection provides relevant fundamental rights provisions protection privacy personal data holds status takes precedence economic benefits hence rules processing special categories personal data relevant companies operating area applying targeted advertising place companies certain obligations main legal provisions setting rules protecting personal data eu gdpr directive privacy electronic communications e privacy directive lex specialis gdpr gdpr directly applicable eu member states whenever company based eu processes personal data company based outside eu processes data relating individuals eu e privacy directive strong focus fundamental rights concerns processing personal data protection privacy electronic communications sector e g individuals use computer smartphone tablet european commission proposed e privacy regulation would replace current e privacy directive legislative proposal would broaden scope directive include specific provisions concerning unsolicited marketing cookies confidentiality requirements justified interferences fundamental rights chapter highlights selected fundamental rights - covered charter - particularly affected ai taking account four use cases discussed chapter rights absolute rights subject limitations line article charter accordingly analysing extent different fundamental rights impacted use ai section presents general steps need followed determine whether charter right limited fundamental rights affected ai absolute subject limitations interferences fundamental rights justified respect requirements charter echr case charter rights corresponding rights guaranteed echr article charter pursuant article charter limitation fundamental rights must ---- provided law ----genuinely meet objectives general interest recognised union need protect rights freedoms others ----respect essence right ---- necessary ---- proportionate court justice eu cjeu also emphasised limitation exercise rights freedoms recognised charter must respect \" essence\" rights freedoms means fundamental rights limited certain extent completely disregarded established inalienable essential core right violated measure next step conduct necessity proportionality test outlined charter respect non core aspects right interference charter right needs examined whether given legitimate aim could obtained means interfere less right guaranteed similar requirements also imposed echr interpreted european court human rights ecthr include 'essence right' concept derived object purpose echr whole respect use new technologies ecthr observed marper v uk states \"strike right balance\" protecting fundamental rights developing new technologies given wide range applications ai systems everyday life presented four selected use cases wide range fundamental rights may assessed taking account variety elements depending context particular area use notably specific purpose ai used functionality complexity scale deployed relevant assessing fundamental rights implications endnotes see also van veen c 'artificial intelligence ' human rights got ' data society points - blog data society research institute may barfield w pagallo u advanced introduction law artificial intelligence cheltenham northhampton edward elgar pp see also cjeu aklagaren v hans akerberg fransson gc february paras european convention protection human rights fundamental freedoms amended protocols nos november ets overview application charter see fra 2018a applying charter fundamental rights european union law policy making national level luxembourg publications office regulation eu european parliament council april protection natural persons regard processing personal data free movement data repealing directive ec general data protection regulation oj l pp see fra handbook european data protection law edition luxembourg publications office see example hacker p legal framework ai training data law innovation technology forthcoming available ssrn overview european non discrimination law see fra handbook european non discrimination law edition luxembourg publications office council directive ec november establishing general framework equal treatment employment occupation oj l pp council directive ec june implementing principle equal treatment persons irrespective racial ethnic origin oj l pp council directive ec december implementing principle equal treatment men women access supply goods services oj l pp directive ec european parliament council july implementation principle equal opportunities equal treatment men women matters employment occupation recast oj l pp convention protection individuals regard automatic processing personal data strasbourg january ets protocol amending convention protection individuals regard automatic processing personal data strasbourg october cets ai finland project' ethics working group ethics challenge added emphasis companies self regulation ai finland 'etiikkahaaste ethics challenge ' tekoaly uusi sahko finnish republic estonia report estonia' ai taskforce p estonian government launched preparation long term strategy example see netherlands ministry economic affairs climate policy strategic action ai strategic action plan ai strategisch actieplan ai - sapai example effort adapt goals development sustainable market see spain ministry science innovation universities national ai strategy spanish comprehensive overview see european commission national strategies artificial intelligence oecd ai policy observatory finnish constitutional law committee 'committee opinion pevl vp - vp draft proposal parliament law processing personal data immigration administration related laws' estonia national audit office chancellor justice everyone' rights e state e state charter netherlands ministry interior kingdom relations ai public values fundamental rights dutch elina saxlin hautamaki johanna lilja secondary use health data - new finnish act de donno french code \"des relations entre le public et l'administration\" new european era administrative procedure italian journal public law pp see fra preventing unlawful profiling today future guide luxembourg publications office tables sajfert j quintel data protection directive eu police criminal justice authorities available ssrn note article law enforcement directive seems apply automated decisions taken solely automated processing means safeguard apply human agency involved orla lynskey criminal justice profiling eu data protection law precarious protection predictive policing p english translation available via finlex website gdpr recital art european patients forum n new eu regulation protection personal data mean patients guide patients patients' organisations gdpr arts white case gdpr guide national implementation estonia merav griguer processing health data france look gdpr european commission proposal regulation european parliament council concerning respect private life protection personal data electronic communications repealing directive ec regulation privacy electronic communications com final brussels charter art \" far charter contains rights correspond rights guaranteed convention protection human rights fundamental freedoms meaning scope rights shall laid said convention \" also reiterated explained cjeu see example c satakunnan markkinaporssi satamedia december para joined cases c c volker und markus schecke eifert gbr hartmut eifert november para joined cases c c digital rights ireland ltd v minister communications marine natural resources others karntner landesregierung others april para c maximillian schrems v data protection commissioner october para c webmindlicenses kft v nemzeti ado es vamhivatal kiemelt ado es vam foigazgatosag december paras see cjeu c maximillian schrems v data protection commissioner october paras refer article charter see also scheinin martin sorell tom surveille deliverable d4 - synthesis report wp4 merging ethics law analysis discussing outcomes april p see e g brkan ' essence fundamental rights privacy data protection finding way maze cjeu' constitutional reasoning' german law journal p lenaerts k 'limits limitations essence fundamental rights eu' german law journal pp cjeu joined cases c c digital rights ireland ltd v minister communications marine natural resources others karntner landesregierung others april see instance khelili v switzerland october ecthr marper v united kingdom gc nos december ecthr k v finland july ecthr z v finland february ecthr huvig v france april ecthr leander v sweden march scheinin martin sorell tom surveille deliverable d4 - synthesis report wp4 merging ethics law analysis discussing outcomes april p ecthr marper v united kingdom gc nos december para see also council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems appendix para impact current use ai selected fundamental rights deploying ai systems engages wide range fundamental rights seen chapter use cases presented report involve range technologies varying levels complexity automation different phases development applied different contexts different purposes different scale rights affected depend factors number horizontal sector specific fundamental issues emerge chapter begins general overview risks perceived interviewees general awareness fundamental rights implications using ai chapter highlights selected fundamental rights affected ai related technologies reference four use cases analysed analysis takes account presents views practices awareness issues expressed interviews conducted report interviewees first asked general risks see using ai asked general fundamental rights awareness using ai concrete fundamental rights implications mostly linked data protection non discrimination availability complaints mechanisms perceived risks important recognise many issues cut across different rights example potentially biased decision made algorithm could involve right non discrimination protection personal data right effective remedy similarly particular issue seen perspective different rights instance good explanation decision made algorithm required right protection personal data right good administration right effective remedy fair trial asked general risks using ai interviewees always mention fundamental rights main risks although highlighted related topics private sector representatives often mentioned inaccuracy risk using ai followed potential bias proper legal basis processing personal data one respondent international retail company stated one business risk linked european customers extremely knowledgeable rights namely people hesitate ask data storage automated decision making customers properly informed might complain company may lose client addition interviewee continued breaching law possible fines linked breach another major business risk respect public administration bias often highlighted risk associated using ai addition public authorities often discussed inaccuracy data identification risks using ai example interviewees working social benefits algorithms stated incorrect results general risk occur potentially due rare cases well identified algorithm due errors input data also highlighted difficulties associated moving testing deploying system including technical challenges resources required potential different results deployed respondents working targeted advertising also highlighted business risks - example offering irrelevant inappropriate content one respondent mentioned potentially losing control automated systems addition interviewees indicate challenges linked difficulty interpreting results outputs ai systems one interviewee consultancy sector fears risk related lack absence sufficient ai knowledge understanding cause ongoing projects halted due company' inability explain clearly algorithms perform purpose another interviewee law enforcement sector looking possible use ai support decisions licence applications explains inherent risks system proposes certain response example potentially using ai support decisions license applications firearms respondent asserts would critical understand reasoning behind negative decisions also positive decisions several interviews showed major concern assign properly trained staff sufficient expertise trace explain interact ai system finding also corroborated results european commission survey among companies eu survey indicate obstacle adopting ai technologies difficulty hire new staff right skills mention complexity algorithms obstacle respect ability explain decisions based algorithms interviewee working public administration mentioned alternatives completely transparent making decisions room doubt similar vein respondent working area health private sector mentions 'self learning' algorithms forbidden area work fixed algorithms traced \" use ai bring many benefits also risks like risks reported without providing much additional information include nuclear energy \" cyber security data quality excessive monitoring people due use interviewee working private sector data algorithms job loss due automation profiling spain general awareness fundamental rights legal frameworks ai context everyone eu aware fundamental rights fra' fundamental rights survey shows slightly every second person eu aged older heard charter slightly people two three heard echr universal \" use ai impact declaration human rights might echr older human rights way terms established people' common knowledge decision process matter whether decision made majority people interviewed project acknowledge using machine human \" ai generally affect fundamental rights mention interviewee working public use ai potential impact fundamental rights administration estonia aware implications responses influenced different ways use ai also understanding fundamental rights example one respondent working production pension forecasts based machine learning says producing statistics impact fundamental rights apart data protection issues need addressed another respondent working social benefits algorithms argues impact depends \" widely human rights defined\" - example right receive correct pension \" rights related data none interviewees working targeted advertising believe protection ensured see use ai affects fundamental rights negatively one respondent working human rights relevance targeted communication customers stated one reason \" response relates lack knowledge exactly fundamental private company spain rights practically interviewees showed awareness rights privacy \" touch topic data protection well non discrimination rights assume human dignity right fair trial effective remedy also human rights issues involved mentioned albeit briefly activities within legal framework activities closer look interviewees' responses indicates diverging views across compliant data protection respondent groups respondents working private companies discuss good practices therefore data protection non discrimination rarely mention rights assume human challenges company working targeted advertising mentions rights issues related attentive issues linked freedom speech right information systems \" sense company promotes rights public administration spain posting adverts helps news websites obtain funding continue work one interviewee notes range rights awareness much broader among public sector representatives working ai referred rights human dignity presumption innocence working ai systems different fields application also highlight use systems also covered sector specific laws example system making decisions unemployment benefits regulated national legislation unemployment insurance administrative procedures data protection however respondents aware legal standards apply use ai unsure absence ai specific regulation several respondents mention ethics guidelines certification schemes work existing guidelines standards necessarily specifically aimed ai case example security system 'iske' estonia3 area financial services payment card industry data security standard respondents also refer standards developed international organization standardization iso institute electrical electronics engineers ieee european committee standardization cen respondent working targeted advertising argues certification needed field posting ads issues linked health sector work banks several interviewees noted \" think organisations developing internal guidelines regulate specific technology like ai sufficient general respondents mention guidelines developed eu principles technology neutral international level guidelines european commission' rules \" private sector estonia high level expert group ai oecd' guidelines unesco standards \"yes codes yes aware ongoing developments eu council europe level procedures codes procedures date refer need update sector specific regulations able using something innovate ai - example area health yet one interviewee created analog world states existing standards sufficient ai need digital world \" regulated separately private sector spain human dignity using ai driven technologies broadly implicates duty respect human dignity foundation fundamental rights guaranteed charter article charter states human dignity inviolable must respected protected times cjeu confirmed case law fundamental right dignity part eu law ai driven processing personal data must carried manner respects human dignity puts human centre discussions actions related ai rather technology 'human ' creating affected new technology needs focus taking human dignity starting point help ensure use ai benefits everyone - example supporting ageing access healthcare dignified manner use ai also risks infringing closely connected charter rights right life article right integrity person article context important consider avoid harmful use ai prevent violations rights example comes use ai people engaging criminal activities ai used weapons apart extreme cases preserving dignity includes avoiding subjecting people ai without knowledge informed consent strongly linked privacy data protection example people' applications social benefits decided upon use ai people need made aware consent use automated decisions taken give another example certain proportion population feel comfortable subjected biometric identification systems hence using without allowing opt could potentially violate dignity respondents public administration referred right dignity discussing fundamental rights one respondent considering use ai prisons mentions particular context first needs assessed whether risk violating fundamental rights would high right human dignity interviewees made general references right without discussing relation concrete use ai right privacy data protection - selected challenges right respect private life protection personal data articles charter core fundamental rights discussions around use ai closely related rights respect private life protection personal data distinct self standing rights described \"classic\" right protection privacy \"modern\" right right data protection strive protect similar values e autonomy human dignity individuals granting personal sphere freely develop personalities think shape opinions thus form essential prerequisite exercise fundamental rights freedom thought conscience religion article charter freedom expression information article charter freedom assembly association article charter given two rights absolute rights subject limitations however interference needs adequately justified11 cannot compromise essential inalienable core right explained section concept \"private life\" \"privacy\" complex broad susceptible exhaustive definition covers physical psychological integrity person therefore embrace multiple aspects person' physical social identity also zone interaction person others even public context may fall within scope \"privacy\" contexts ecthr used concept \"reasonable expectation privacy\" - referring extent people expect privacy public spaces without subjected surveillance - one factors albeit necessarily conclusive one decide violation right respect private life relevance scope application however appears limited similarly according un human rights committee mere fact participants assemblies public mean privacy cannot infringed applies monitoring social media glean information participation peaceful assemblies widespread use ai technologies may technologies continue develop raise unchartered issues novel concerns right respect private life ai driven technologies may change way think privacy algorithmic tools predict reveal information people' behaviour unprecedented ways - without people even realizing giving away information personal data obtained internet may instance used targeted advertising raising many fundamental rights concerns issues linked personal data sharing via smart phone apps particularly raises significant concerns including variety potential harmful effects manipulation exploitation vulnerabilities discrimination security issues fraud e g identity theft reduced trust digital economy using ai driven technologies often implies computerised processing large amounts personal data constitutes interference right protection personal data set article charter embodying pre existing eu data protection law well right private life article charter article echr awareness data protection issues use personal data eu people heard gdpr contrast virtually \" little anxious interviewees aware gdpr discussed data protection issues gdpr implemented data protection rules deriving gdpr national law clearly end meant managing datasets well known applied rights area ai fundamental access rights ... good rights less known reminder everything done \" discussing legal framework governing use ai public administration finland respondents mentioned data protection rules well sectoral laws clearly say legal framework apart data protection laws interviewee working spanish public \"actually ' concerned administration notes \" rely data protection regulation norms gdpr might hinder ai research ' available moment\" afraid large databases used previously cannot one interviewee reflecting image based diagnostic tool expressed used research anymore \" view gdpr could hinder research hospital using tool private company netherlands support diagnosis strokes clear rules data protection interviewee indicated although know whether data protection certification requested \" gdpr give specific rules gives others referred general data protection guidelines indicated principles comes aware documents ethical issues interpretation \" private company estonia respondents working target advertising aware privacy data protection issues although responsible data protection issues companies aware efforts protect data privacy one interviewee mentioned contrary earlier years personal data stored much securely handled care attention given properly handling consent data processing consequence high level awareness data protection privacy issues linked ai use however data protection law applies personal data processed example using anonymised data develop ai tools e training data likely permissible many instances would trigger gdpr research shows data often de anonymised however efforts often require expert knowledge potentially additional information illegal illegality de anonymisation necessarily preclude applicability gdpr important consider identification anonymised data reasonably likely anonymising data one aspect protecting privacy data subjects assessing risks identification aspects also important consider disseminating anonymised data include use data purpose outputs produced interviews respondents always entirely clear use personal data often superficially described data used mentioned chapter several instances interviewees indicated use non personal data anonymised data arguing data protection relevant cases example semi public organisation working environmental management uses aggregated data water consumption machine learning based predictions water consumption data available individual level interviewees said use personal data although data originally stem individuals tool supporting restaurant inspection collecting data online sources use personal data - interviewee indicated however indicated need careful mining data online even publicly available might include personal data usernames \" would great retrieve another example insurance company using chatbot make client data another service contact effective data used train system chat protocols client ' repeat conversation logs linked personal data however line go example linking data personal data might possible reusing data \" future according respondent public administration finland companies working targeted online advertising indicate using pseudo anonymised data done example excluding names social security keys encrypting data identity consumers relevant company interviewee mentioned indicate use non personal anonymised data others possible data used make predictions decisions specific individuals example interviewee company working credit rating mentioned need know identity consumers assessments case even important right forgotten according interviewee exhaustive discussion data protection issues possible report however two aspects clearly emerged interviews automated decision making linked right human review right obtain meaningful information decisions automated automated decision making article gdpr article law enforcement directive generally forbid automated decision making meaning \"decision based solely automated processing including profiling produces legal effects concerning similarly significantly affects \" article gdpr explicit consent needed decisions solely automated legal similarly significant effect people automated decision making authorised law authorisation union national law sole precondition law enforcement directive article processing decision considered fully automated instruments require human review controller however concept 'automated' decision making elusive requires discussion research example cases human intervention might limited 'signing ' outcomes ai system rendering virtually automated importantly human review must mean human signing recommendations outputs algorithm must done someone \"authority competence change decision\" considering relevant data hand humans review potentially override outcomes system must also evaluated research indicates humans overrule outcomes algorithms mainly result algorithm line stereotypes behaviour threatens possible added value automated processing potentially accurate even fairer humans may also put minority groups disadvantage therefore also relevant non discrimination issues discussed overall disagreement exact scope provisions eu data protection acquis whether impose general ban certain types automated decisions provide data subjects rights context certain types ai driven decision making using algorithms area social benefits health predictive policing clearly potential legal significant consequences interviews suggest working areas well aware concept human review decisions taken support ai many interviewees indicate automated decisions taken one exception automation unemployment benefits based national law fully automated decisions involve discretion another example another country positive decisions based pre defined rules automated student benefits case negative decisions made humans cases refer rule based decisions involving use statistics machine learning another respondent testing use ai systems including machine learning area social benefits mentions equality could negatively impacted automation makes human behaviour visible including existing biased practices makes precautions necessary consequence organisation would allow decisions made humans interviewees working health highlighted risks linked automation decisions interviewee discussing tool support stroke diagnosis feels important rely system avoid risk automation confirmation bias caution early positive experiences application could prompt users rely easily devote less attention assessment images interviewees raised similar concerns one interviewee discussing tool analyses images provide probability presence certain type lesion notes technology supports diagnosis simple cases expertise doctors particularly important - trusted - complex cases targeted advertising often considered significant effect people however may case example individual' vulnerabilities used successful advertising considering vulnerabilities particularly important people disadvantaged groups may aware opt direct marketing see box right say decisions automated absence case law area information research needed identify impact automated decisions e advertisement delivered answering questions challenging targeted advertising based highly complex technology scale eurobarometer survey asked people eu aware right awareness opt direct marketing overall eu citizens heard right right opt exercised people exercise right aware direct - becomes even important direct marketing made much marketing efficient machine learning among general awareness levels strongly vary across eu percentage people know population right opt direct marketing ranges bulgaria netherlands figure shows percentages also highlights - based fra' analysis eurobarometer data - strong variation within countries broken regions regions fewer one four heard right areas higher shares people risk poverty indicates general problem people disadvantaged society tend less aware right data show people working often struggle pay bills living rural areas older less aware right figure awareness gdpr right opt direct marketing eu united kingdom country region fi- se- ee- insufficient data available lv- dk- uk- lt- ie- nl- pl- de- - lu- cz- sk- - hu- fr- si- ro- hr- - bg- pt- es- el- mt- cy- note map show non eu countries uk light shading aware right dark shading less aware right results regions within countries represented light grey spaces excluded fewer respondents meaning numbers observations low reliable results n question \" general data protection regulation gdpr guarantees number rights heard following rights ... right object receiving direct marketing \" source fra calculations presentation based european commission eurobarometer experiences use cases general interviewed experts highlighted data protection law difficult interpret lacks clarity comes meaning automated decision making one expert france felt automated decision making difficult explain automated decision making banned meaning exceptions gdpr allow automated decision making removed pointed ai used decision support tool another expert independent lawyer netherlands views current laws standards sufficient says need concretised per sector particularly expert mentions scope existing rules permissible automated decision making clear remains unclear comprehensive assessment 'human loop' means also raised relation syri case remained unclear extent decisions reviewed another expert working supervisory authority generally sees need adapting data protection laws \" legislation quite comprehensive organisation supervision thereof also political behind \" concerns reflect findings research also raise serious \" risk much issues concerning right human review example responsible trust machine \" officers questioned results algorithmic system built profile public administration france unemployed people poland less one percent cases essentially makes supporting tool automated decision making tool \" huge tension surrounding linked question reviewing decisions outputs ai systems gdpr want well challenge clear lack knowledge ai works interviewees might fact worse often could explain detail system use works interpretation data data uses due lack knowledge lack transparency turns impossible \" meaningful information logic involved explaining outcomes public administration netherlands algorithms essential several fundamental rights crucial processing personal data also ensuring algorithms fair discriminate also necessary enable \" explain model people properly challenge decisions ai systems ' able model statistical explainable \" one interviewee working public administration explains complexity public administration france differs depending tasks licence administration systems relatively straightforward crime prevention analysis uses data sources makes harder understand another interviewee working law \"internally explain enforcement says current ai used police organisations yet decisions machine learning complex would make explanations difficult might models several means case future \" private sector estonia respondent working financial data transactions indicates traditional models straightforward understand however new methodologies difficult explain company invest resources \" systems black making models explainable still level explainability boxes information processes required gdpr clear respondent already take step forward defence human rights \" public administration spain \" strongly attached idea ai explainable \" public administration france people aware right say decisions awareness automated evidence suggests eurobarometer survey showed europeans right know data protection rights say decisions fra' analysis eurobarometer survey shows figure drops considerably automated among people lower socio economic status eu citizens report struggling pay bills time know right lack rights awareness among socially disadvantaged could contribute social exclusion already disadvantaged less aware challenge automated decisions see figure gender differences small yet women even less aware right women men older people considerably less aware among aged older figure awareness right say decisions automated age gender difficulty paying bills years older years age years years women gender men refusal difficulty paying bills almost never never time time time total notes n question \" general data protection regulation gdpr guarantees number rights heard following rights ... right say decisions automated e g algorithm decides granted loan \" source fra calculations presentation based european commission eurobarometer equality non discrimination equality law non discrimination enshrined articles charter discrimination \" one person treated less favourably another would treated comparable situation\" based perceived real personal characteristic28 called 'protected grounds characteristics' article charter prohibits discrimination based ground sex race colour ethnic social origin genetic features language religion belief political opinion membership national minority property birth disability age sexual orientation charter prohibition reflects corresponding rights echr article protocol echr article even broader establishes non exhaustive open list extending protection wide range new grounds unlike article echr charter right non discrimination freestanding right applies situations need covered charter provision main challenges discrimination crucial topic comes use ai purpose machine learning algorithms categorise classify separate one interviewed expert points making differences per se bad thing according expert deciding grant loan credit history used differentiate individuals basis protected attributes gender religion however many personal attributes life experiences often strongly correlated protected attributes credit history might systematically different men women due differences earnings job histories interviewees often mention efficiency main purpose using ai related technologies yet important note cannot justify unfair differential treatment often protected attributes might highly correlated risks example differences life situations among men women might often linked different insurance risks however acceptable test achats ruling shows case cjeu put end gender discrimination insurance pricing certain circumstances areas using algorithms could positively contribute reducing bias stereotyping algorithmic data analysis may produce results could dispel prejudicial attitudes example predictive policing might contexts lead equitable non discriminatory policing reducing reliance subjective human judgments predictive techniques may used identify called 'white collar crimes' financial crimes historically policed nevertheless direct indirect discrimination34 use algorithms involve big data considered one pressing challenges use ai driven technologies bias discrimination including gender based discrimination data supported algorithmic decision making occur several reasons many levels ai systems difficult detect mitigate often quality data biases within source potential discrimination unfair treatment discriminatory effects generated certain groups practice difficult individuals challenge far limited number court cases dealt discrimination relating ai systems first instance decision divisional court cardiff dismissed claim uk court concerning lawfulness south wales police' use \"afr locate\" face appeal police recognition system court appeal overturned decision use facial recognition found facial recognition programme used police unlawful court violates appeal ruled \" much discretion currently left individual police officers\" added \" clear placed watch list clear human rights criteria determining technology deployed\" court also held police sufficiently investigate software use exhibited race gender bias judgment first merit specifically matter europe considerably narrows scope permissible law enforcement agencies need fully comply human rights law uk court appeal r bridges v cc south wales ewca civ august ars technica 'police use facial recognition violates human rights uk court rules' august studies highlighted potential discrimination prompted use ai systems across areas covered report area predictive policing example particular risk relates potential automated decision making tools reproduce entrench existing discriminatory practices undermine equality law article charter historical crime data underpins predictive policing may biased reflecting inherent data gaps e g chronic underreporting certain types crime alongside issues data recorded e g human error also bias individual officers crime victimisation surveys consistently show large proportion crime never reported police public - particularly crimes involving physical sexual violence hate crimes example fra' survey violence women - respondents - showed one five women experienced violence partner anyone else brought serious incident attention police fra' eu midis ii survey respondents across eu showed three ten reported incidents racially motivated hate crime police organisation compared violent crime hate crime property crime - burglary - higher rate reporting police particularly developed countries may requirement claiming insurance policy sum - relying official crime statistics based reported crime looking develop ai models field predictive policing particularly problematic comes specific crimes specific groups variables used ai modelling proxies race ethnicity gender protected categories complexity algorithms makes harder identify remove biases instead providing objective analysis predictive policing software may turn 'echo chamber' cementing existing systemic flaws injustices 'stamp' appears scientific legitimacy use predictive policing may also make law enforcement responses less equitable focusing certain crimes areas predictive policing currently focused property crimes theft burglaries often associated certain demographics neighbourhoods result certain demographics neighbourhoods - individuals living - stigmatised meanwhile white collar crime - typically committed different demographics - less prioritised patterns policing - whereby certain neighbourhoods communities disproportionately policed - predates use ai however 'promise' ai 'objective' turn used counteract discriminatory policing needs verified practice oxford university researcher sandra wachter highlights discrimination may occur due information linked protected attributes targeted advertising newly created profiles purpose advertising might amount indirect discrimination potentially even require new characteristics added non discrimination legislation extend scope expanded areas experiences use cases \" want machine many interviewees noted use ai general discriminate discriminate basis sex systems working many indicated belief put variable sex excluding information protected attributes sufficient protection easy make examples discrimination however discrimination occur due information symmetrical notice sex contained datasets may indicate protected attributes traces certain relevance \" protected groups often hidden information public administration spain example public authority uses ai tax customs shows challenges linked identifying possible bias potential discrimination using algorithms scrutinising algorithms public administration body found higher degree errors tax declarations among recently issued national identification numbers almost always attributed immigrants prompted research correlation turned outputs people recent identification numbers often contained errors never filed taxes know also case non migrants also example proxy information parts number could indicate immigrant status another interviewee working potential use ai detecting benefits fraud mentioned respect \" want prevent discrimination based ethnicity instance suffice remove 'ethnicity label' neighbourhood composition often also determined ethnicity ethnicity plays role preventing discrimination often goes beyond 'direct' characteristics\" \" f access even respondents aware general potential sensitive personal data discrimination using ai often ruled system impossible check discriminates people based protected characteristics profiling basis \" respondents also believe tools positive impact terms public administration netherlands non discrimination one respondent testing ai social benefits decisions regrets able use ai data protection reasons even though respondent' view automation could process big datasets effectively without discrimination noting protection personal data needs observed respondent feels hinders prompt decision making non discrimination - \" automated automated\" respondents clear sure whether use ai could discriminate respondents repeatedly stated system cannot discriminate include data protected characteristics example several interviewees working predictive policing law enforcement felt potential discrimination ai systems use data return outcomes related protected grounds system aim identify people others working predictive policing felt discrimination could occur particular issues training data relation predictive policing 'heat map' case example one interviewee noted - dataset never fully neutral representative complete - strong risk bias possible discrimination towards particular groups identified sharing datasets increase amount data available one way mitigate risk felt impeded data protection regulations also indicated multi level teams task travel different police authorities check quality systems used set area targeted advertising interviewees mentioned discrimination potential problem mainly asked directly overall respondents think systems discriminate three respondents mention information gender age used consequently discrimination respect occur another interviewee sure information included \" discrimination ' complicated respondent working breast cancer detection tool highlighted diseases age gender ethnicity relevant factors population groups present certain ethnic groups likely develop certain types cancer respondents working predictions take account health highlighted potential discrimination also linked sexual ethnic genetic character uses system suggesting could become greater challenge discriminatory violation system used non medical staff human rights \" private sector france different related example comes respondent working credit rating private company selling credit scores individuals created algorithm company uses information gender age citizenship credit risk models information impact outcome credit scores example younger people non citizens higher credit risk score influence demographics much smaller compared credit history data according interviewee system \"certainly impact right non discrimination make decisions sell data data analytics creditors monitor discriminate\" another interviewee working data strategy financial institution private sector using ai analyse financial transactions clearly mentions challenges understanding non discrimination constitutes work interviewee mentions example clear extent illegal exclude older people receiving credit life expectancy expected lower mortgage repayment period asked findings point uncertainty ambiguity financial sector respect article charter - non discrimination - translates real life situations vulnerable groups much discussion research discrimination using ai linked biased results respect ethnic origin gender extent age although important analyse potential discrimination groups charter covers several grounds discrimination less often part discussions research grounds include example political opinion sexual orientation disability charter provides particular rights special groups beyond articles including rights child article rights elderly article rights persons disabilities article question age - respect older age groups younger adults - came interviews notably comes insurance credit see however none interviewees experts directly mentioned rights child might linked extent nature use cases investigated clearly reflects fact topic high agenda many working ai article charter emphasises best interests child must primary consideration activities public authorities private actors concern children applies course - equally - field ai two respondents public administration mentioned possible use ai area child custody distribution children schools address consideration rights child respondents wish go detail concerning use cases - potentially reflecting sensitivity topic finally issues linked integration people disabilities raised interviews eurobarometer survey included questions ai asked respondents areas awareness mostly concerned comes use ai including discrimination among general decision making unclear responsibility nobody complain population potential around eu citizens indicated concerned using ai could ai lead lead discrimination terms age gender race nationality - example taking decisions recruitment credit worthiness etc discrimination results vary across countries higher proportions people concerned discrimination netherlands luxembourg sweden lower proportions expressed concern estonia hungary lithuania see figure however question clear people know discrimination happen aware happen think problem figure awareness risks discrimination using ai country eu nl uk lu se fr el si de es cy ie hr dk cz fi bg pt sk mt lv ro pl ee hu lt notes includes people indicated concerned ai could lead discrimination among three possible issues three issues source fra calculations based european commission eurobarometer tackling gender charter stipulates equality inequality women men must ensured areas including design employment work pay article gender discrimination use ai major concern comes design use ai related technologies development side european economic social committee notes development ai taking place within homogenous environment principally consisting young white men results cultural gender disparities embedded ai technologies example training data prone manipulation may biased reflect cultural gender prejudices preferences contain errors see also european also reflected research commission white despite efforts achieve paper artificial intelligence - gender balance majority european approach interviewees men excellence trust com final disparities design brussels february deployment stage linked p systematic disadvantages affecting european economic women labour market social committee potential lack awareness artificial intelligence - gender biases recent study consequences showed increased use artificial intelligence industrial robots could widen digital single gender gap despite market production genders benefitting increased consumption automation analysis indicated employment society men medium high initiative opinion skill occupations would benefit may jo c p disproportionally aksoy c ozcan b looking ahead using data philipp j algorithms could help better robots gender mainstream gender equality pay gap europe iza discussion paper policies processes paying attention gendered datasets drawing discussions around see webpage data feminism gender inequalities use datasociety' website data 'data feminism' could help raise awareness criado perez c invisible male point view women exposing data taken default view bias world designed also finds way men london datasets access justice right effective remedy tribunal fair trial article charter one often used charter right legal proceedings highlights importance upholding fundamental rights rule law right horizontal character empowers individuals challenge measure affecting right conferred eu law respect guaranteed charter cjeu underlined article charter constitutes reaffirmation principle effective judicial protection characteristics remedy must determined manner consistent principle right effective remedy also covers decisions taken support ai technologies eu data protection law reconfirms right effective judicial remedy must provided relation decisions controller processor53 well supervisory authority data processed ai driven technologies exception crucial note possibility lodge administrative complaint supervisory authority provided gdpr law enforcement directive55 considered effective judicial remedy article charter court involved review judicial review always remain available accessible internal alternative dispute settlement mechanisms prove insufficient person concerned opts judicial review using ai challenge right effective remedy different ways one prominent concern lack transparency use operation new technologies algorithmic decision making notoriously opaque data collection algorithm training selection data modelling profiling situation around individual consent effectiveness error rates algorithm aspects often transparently reported without access information individuals may able defend assign responsibility decisions affecting appeal decision negatively affecting fair trial includes principle equality arms adversarial proceedings established ecthr requirements also form part corresponding charter right article view article charter main challenges issues reflected specific challenges right effective remedy fair trial interviewed experts outlined generally experts indicate difference accessing remedies private companies public administration public authorities often forced transparent use ai meanwhile companies appear secretive assessment several experts suggests however expert netherlands said people might readily complain companies reluctant complain public authorities public services often concern vulnerable people need social benefits would less inclined complain decisions opportunities successfully complain use ai challenge decisions based ai essential providing access justice interviews emphasised following important respect ----making people aware ai used ----making people aware complain ----making sure ai system decisions based ai explained first everyone needs know dealing ai system taken decision affects people e g social benefits people concerned might complain general - able complain use ai know ai involved expert explained general willingness complain biggest problem people often know ai used organisations transparent even though required gdpr several interviewees indicate informing people decision made based partly automated tools first step providing access complaints second everyone needs know complain may difficult people know body deals type complaints one expert pointed consumers often know complain - example bank might use algorithms deciding financial matters public administration issues automated decisions decided add names employees decisions provide contact persons potentially challenging automated decision interviewees indicated ways procedures complaints place procedures complaints linked use ai companies organisations use ai anonymised aggregated data indicate complaint mechanisms place finally complaining need enough information challenge underlying decision thorough information ai systems provides equality arms meaningfully challenge decisions however straightforward comes use ai particularly ----potential intellectual property rights issues ---- complex systems difficult explain intellectual property rights form one hurdle providing enough information decision made system works algorithms part implemented software technical invention may subject intellectual property rights - right protected article charter actors often seek copyright patent trade secret protection safeguard knowledge ai one interviewee insurance sector claims due highly competitive market \"one may share much workings used technology\" instance particular price given customer essentially competitors could benefit knowledge underlying software subject scrutiny another respondent using ai handle visa applications notes using systems developed external providers whose algorithms covered intellectual property rights hinder necessary transparency later stage another challenge successfully complaining automated decisions use ai general challenge explain decisions based complex systems interviewees working public administration suggest usually clear guidance complain administrative decision area interviewees highlight importance detailed explanations example systems automatically provide unemployment benefits cases involve discretion clients ask reasoning behind automated administrative acts interviewee indicates clients wish see calculations behind financial decisions may self service system organisation' website publications contain detailed descriptions calculations used interviewees recognise open transparent logic essential providing explanations regarding ai supported decisions often challenging impossible achieve one interviewee working bank mentions complex machine learning solutions cannot used certain decision making reasoning system cannot explained easily systems used purposes however interviewee working another bank indicates systems used use simpler methods addition complex ones get idea probable reasons decisions one expert raised problem companies internally might enough information way algorithms work lack expertise knowledge appears major hindrance practice seeking access effective remedy experiences use cases \" topic transparency respondents discussing predictive policing tools highlighted transparency important nowadays important many procedures publish information many automatic gender based violence use case felt sending police means help upload file outcome ai system judge informing victim information portals level risk attributed case police measures lot work done apply result enhances transparency terms transparency \" public administration spain interviewees discussing heat map example referred numerous requests police explain system' purpose works highlighted transparency way reduce public anxiety number interviewees pointed possibility individuals affected system make complaints police courts ombudsinstitution reference domestic violence case however interviewee indicated procedure place question system police protocol terms measures protect fundamental rights health services use cases several interviewees referred ethics committees well general legal safeguards data protection rules checks controls primarily mentioned take place external actors specific complaints procedures place organisations interviewees responded question interviewees highlighted doctors ultimately take responsibility decisions patients often know use ai tool first place example breast cancer detection example interviewee indicated possibility legal recourse developer tool radiologist makes decision diagnosis liable errors safeguards place targeted advertising cases mainly follow data protection requirements ensuring consent obtained respected one company makes sure clients engaged illicit practices rejects clients certain sectors political advertising complaints received organisations interviewed received complaints challenging use ai cases interviewees claim received complaints complainants aware ai used noticed incorrect outputs decision making example individuals lodged complaints regarding traffic fines whereby police officer stopped car driver upon hearing car driver' explanation fine wrongfully administered proceeded manually correct information system without able update system' historical data cases fines remain visible throughout system particular person would continue profiled high risk occasion even though organisations rarely received formal complaints respect \" number complaints use ai interviewees often state due early stages data use miniscule rather ai implementation nonetheless interviewees reported repeated people may asked delete requests access rectification personal data people information \" requested information removed well explanations private company estonia certain recommendation made majority interviewees claim procedures decision processed undertaken human hand interviewees showed interest opening new channels analyse explain redress decisions involving ai solutions rights linked access justice set charter also impacted notably use ai law enforcement include example presumption innocence article charter identifying people suspected committed crime police may target activities specifically one person put suspicion based flawed fragmented data algorithmic profiling uncritical reliance automated tools without proper human review takes account information might contribute discrimination decision making right social security social assistance right social security assistance enshrined article charter classic social right inspired various international european legal standards provision combining elements right principle great significance eu view free movement people within union instead tying issues social protection labour market charter right takes new communitarian approach broadly referring \"providing social protection cases maternity illness industrial accidents dependency old age case loss employment\" article however primarily programmatic statement prescribe minimum standard protection principle eu member states determine conditions entitlement access social benefits clarification needed cjeu yet article charter provides protection measures restricting abolishing existing social security rights addition access social rights guaranteed individuals legally residing within eu exercise right free movement regardless nationality subject eu national laws article thus creates justiciable rights national courts cjeu becoming increasingly apparent impact ai technologies social protection systems lives many individuals rely upon far reaching - potentially - problematic introducing ai driven technologies social welfare systems risks creating barriers access right example using ai social security needs account potential negative - discriminatory - effects non nationals eu citizens third country nationals exercising right freedom movement eu could negatively affected example system relies data job histories available moving eu member states one respondent addressed 'right receive correct pension' aspect wider definition human rights meanwhile none interviewed referred fundamental right social security social assistance could partly reflect nature use cases however lack references social rights among public sector interviewees notable consumer protection charter stipulates eu policies must ensure high level consumer protection based article tfeu eu institutions bodies needs observe principle member state authorities implementing eu law charter principle provides guarantee particular goal \" high level consumer protection\" article tfeu concrete also determines means achieve stated aim - example protecting health safety economic interest consumers well promoting right information education among use cases use ai targeted advertising use medical records companies particular importance comes targeted advertising consumers need aware opt targeted aware might subjected advertising want particularly problematic combination highly sophisticated ai systems advertising amount sort manipulation consumer preferences consumer protection also major relevance use health data ehrs european consumer organisation beuc noted ai area health brings challenges consumers recommends ai technologies must fully respect data protection rules transparent consumer avoid discrimination beuc also called updated regulation legislative measures market surveillance law enforcement efficient redress concerning digital health products services fully protect eu consumers beuc carried survey among consumers views ai selected eu member states shows one two respondents agree companies using ai manipulate consumer decisions addition almost half respondents believe personalised content adverts e commerce platforms added value slightly half survey respondents expressed low trust governments effectively control ai interviews conducted study consumer protection mentioned margins discussing risks using ai fundamental rights however respondents businesses refer consumer protection legislation relevant framework also applying use ai moreover respondents deem consumer protection authorities potentially relevant oversight bodies ai used general terms many interviewees business sector stress importance consumer satisfaction example company using video surveillance security customers premises mention consumer protection regulations relevant technical solutions use systems aim improve situation consumers also preserving rights several ai tools built understand profile consumers enable businesses improve services marketing data protection important aspect business also linked fact breaching data protection rules considered business risk mentioned one major concern companies obtaining managing consent consumers customers process data using ai tools marketing purposes interviewees report gdpr impact improving systems handle consent right good administration right good administration well established general principle eu law elaborated cjeu binding eu member states also fundamental right enshrined article charter although actions eu institutions bodies agencies general principle eu law requires eu member states apply requirements right good administration public action right includes limited right individual access file obligation public authority give sufficient reasons decisions access file facilitates understanding evidentiary basis decision made reasons underlying places individual better position put forward counter arguments exercising right heard right effective remedy obligation give reasons makes perspective individuals affected decision making process transparent person concerned know understand measure action taken transparency also enabling principle provides foundations rights including exercise right effective remedy according cjeu context individual decisions made important determining extent duty give reasons france instance code relations public administration requires written explanations factual legal considerations decision based right good administration also applies ai systems process personal data support decision making public authorities although right good administration may subjected certain limitations question arises ensure potentially huge number individuals access files personal data used ai systems another question make sure public authorities always give sufficient reasons operation ai driven technologies cannot fully explained due inherent opacity complexity use system categorise unemployed people set poland highlighted problems linked public administration use algorithms based questions answered unemployed people categorisation developed statistical algorithm system received lot criticism civil society respect lack opportunities complain potential discrimination end complaint ombudsinstitution - based administrative grounds - led constitutional court ruling put end system' use intent increase efficiency drives use ai public sector - aim directly speaks improving administration benefiting citizens respondents public administration far often indicate efficiency reason considering use ai presently using ai one respondent advises ministries digital strategies use ai said main reasons adopting ai improve service citizens reduce costs services public administration interviewees also indicate public administration particular requirements meaning ai cannot used purposes needs particular attention comes decision making however efficiency system also considered important added value sense respondent working digitalisation migration management indicates building complex ai systems risk afterwards would require lot work understand system retrospect interviewee indicates team needs careful allow ai make final decisions taken human - society clients ready according interviewee although systems appealing work effectively could result extra work negative results however interviewee also indicates dimension efficiency \" often side lined discussing data protection\" requirements good administration also directly link issues raised respect data protection non discrimination right effective remedy fair trial public administration process data legal basis decisions need fair transparent pathways challenge decisions need available accessible result requirements good administration directly linked discussion analysis respect legal processing data data protection fair decisions linked discussion non discrimination alongside transparency ways challenge explain decisions respect access justice endnotes see european commission european enterprise survey use technologies based artificial intelligence luxembourg july fra fundamental rights mean people eu luxembourg publications office p see webpage three level baseline security system iske ther website estonia' information system authority see website pci security standards council barak 'human dignity framework right motherright ' barak human dignity constitutional value constitutional right cambridge cambridge university press ch pp cjeu c netherlands v european parliament council october paras discussion malicious use ai see example brundage et al malicious use artificial intelligence forecasting prevention mitigation fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office november cjeu joined cases c c volker und markus schecke eifert gbr hartmut eifert opinion advocate general sharpston june para fra council europe edps handbook european data protection law edition luxembourg publications office june p see also ibid pp ecthr guide article european convention human rights - right respect private family life home correspondence strasbourg council europe updated august paras ecthr lopez ribalda others v spain nos october para comprehensive legal analysis meaning content 'privacy' see also koops b j et al ' typology privacy' university pennsylvania journal international law vol issue pp vermeulen surveille deliverable d4 - scope right private life public places july p un human rights committee general comment right peaceful assembly article ccpr c gc september para costello roisin aine impacts adtech privacy rights rule law technology regulation norwegian consumer council control consumers exploited online advertising industry fra rights matter data protection privacy - fundamental rights survey luxembourg publications office rocher l hendrickx j de montjoye estimating success identifications incomplete datasets using generative models nature communications hacker p legal framework ai training data law innovation technology forthcoming available ssrn article data protection working party opinion anonymisation techniques see also finck michele pallas frank must identified distinguishing personal non personal data gdpr october forthcoming international data privacy law max planck institute innovation competition research paper available ssrn sartor g lagioia f impact general data protection regulation gdpr artificial intelligence study prepared panel future science technology stoa european parliament see example uk data service' blog \"access sensitive data research ' safes'\" see also discussion ohm p \"broken promises privacy responding surprising failure anonymization\" ucla law review p gdpr art law enforcement directive art veale edwards l 'clarity surprises questions article working party draft guidance automated decision making profiling' computer law security review vol april pp article data protection working party guidelines automated individual decision making profiling purposes regulation adopted october last revised adopted february green b chen 'disparate interactions algorithm loop analysis fairness risk assessments' fat ' conference fairness accountability transparency fat ' january gonzalez fuster g artificial intelligence law enforcement - impact fundamental rights european parliament policy department citizens' rights constitutional affairs directorate general internal policies pe july p brkan ' algorithms rule world algorithmic decision making data protection framework gdpr beyond' international journal law information technology vol p article working party guidelines automated individual decision making profiling purposes regulation adopted october last revised adopted february wp251rev p misuraca g van noordt c overview use impact ai public services eu european commission joint research centre luxembourg council directive ec june implementing principle equal treatment persons irrespective racial ethnic origin oj l pp art council directive ec november establishing general framework equal treatment employment occupation oj l pp art fra coe handbook european non discrimination law edition luxembourg publications office june p cjeu c association belge des consommateurs test achats asbl others v conseil des ministres january european commission eu rules gender neutral pricing insurance industry enter force press release ip december elizabeth e joh ' new surveillance discretion automated suspicion big data policing' uc davis legal studies research paper pp ales zavrsnik 'algorithmic justice algorithms big data criminal justice settings' european journal criminology p doi see also european commission white paper artificial intelligence - european approach excellence trust com final brussels february p fra bigdata discrimination data supported decision making luxembourg publications office june p ibid fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office korff browne ' use internet related services private life data protection trends technologies threats implications' council europe pd see national non discrimination equality tribunal finland decision march see also syri case discussed uk court appeal r bridges v cc south wales ewca civ august see also equinet regulating equal ai new role equality bodies brussels report prepared allen r masters tolan miron gomez e castillo c ' machine learning may lead unfairness evidence risk assessment juvenile justice catalonia' best paper award international conference ai law richardson r schultz j crawford k dirty data bad predictions civil rights violations impact police data predictive policing systems justice n u l rev online available ssrn fra violence women eu wide survey main results report luxembourg publications office p fra second european union minorities discrimination survey main results luxembourg publications office p erik bakke \"predictive policing argument public transparency\" new york university annual survey american law vol pp andrew g ferguson 'policing predictive policing' washington university law review vol pp andcouncil europe committee experts internet intermediaries msi net algorithms human rights council europe dgi p elizabeth e joh ' new surveillance discretion automated suspicion big data policing' uc davis legal studies research paper p gstrein j bunnik zwitter 'ethical legal social challenges predictive policing' catolica law review pp albert meijer martijn wessels 'predictive policing review benefits drawbacks' international journal public administration p doi ales zavrsnik 'algorithmic justice algorithms big data criminal justice settings' european journal criminology pp doi wachter sandra 'affinity profiling discrimination association online behavioural advertising' berkeley technology law journal vol forthcoming available ssrn use ai financial industries leading unequal access financial services see legal literature e g boyd levy k marwick ' networked nature algorithmic discrimination' gangadharan p eubanks v barocas eds data discrimination collected essays open technology institute pp overview child rights issues see unicef innovation human rights center uc berkeley artificial intelligence children' rights eu network independent experts fundamental rights commentary charter fundamental rights european union june p see also fra coe handbook european law relating access justice luxembourg publications office june p cjeu c unibet london ltd unibet international ltd v justitiekanslern march para cjeu c et agrokonsulting velko stoyanov v izpalnitelen direktor na darzhaven fond 'zemedelie' - razplashtatelna agentsia june para cjeu c centre public 'action sociale 'ottignies louvain la neuve v moussa abdida december para law enforcement directive art gdpr art law enforcement directive art gdpr art law enforcement directive art gdpr art council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems adopted committee ministers april 1373rd meeting ministers' deputies appendix para b andrew g ferguson 'policing predictive policing' washington university law review pp gstrein j bunnik zwitter ethical legal social challenges predictive policing' catolica law review pp yeung k study implications advanced digital technologies including ai systems concept responsibility within human rights framework prepared council europe expert committee human rights dimensions automated data processing different forms artificial intelligence msi aut council europe algorithms human rights pp international technology law association 'responsible ai global policy framework' pp lack expertise ai also reflected survey among companies eu lack skills among existing staff difficulties hiring new staff prominent obstacle ai adoption european commission european enterprise survey use technologies based artificial intelligence luxembourg july p see detailed assessment impact predictive policing presumption innocence mendola marco one step 'surveillance society' case predictive policing see e g egorov wujczyk eds right social security constitutions world broadening moral legal space social justice geneva ilo global study vol europe pp xv xvii include arts tfeu arts european social charter well points community charter fundamental social rights workers see explanations relating charter fundamental rights oj c pp explanations relating charter fundamental rights oj c pp explanation article -- scope interpretation rights principles lukasz bojarski dieter schindlauer katrin wladasch european charter fundamental rights living instrument - manual rome warsaw vienna pp de becker e ' possible role right social security eu economic monitoring process' german law journal vol pp paju j european union social security law oxford hart publishing sub section ibid pp peers prechal 'scope interpretation rights principles' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp exception poland united kingdom see protocol application charter fundamental rights european union poland united kingdom oj c pp art christiaan van veen ben zevenbergen 'conference social protection artificial intelligence decoding human rights digital age' freedom tinker - research expert commentary digital technologies public life may art charter see also explanations relating charter fundamental rights oj c pp explanation article -- scope interpretation rights principles lukasz bojarski dieter schindlauer katrin wladasch european charter fundamental rights living instrument - manual rome warsaw vienna p sartor giovanni new aspects challenges consumer protection study committee internal market consumer protection policy department economic scientific quality life policies european parliament luxembourg european consumer organisation beuc digital health principles recommendations beuc artificial intelligence consumers say findings policy recommendations multi country survey ai recent case law see cjeu c h n v minister justice equality law reform ireland attorney general may para also confirmed cjeu joined cases c c ys v minister voor immigratie integratie en asiel minister voor immigratie integratie en asiel v july paras components initially developed cjeu case law codified article charter right leading academic literature see craig p 'article - right good administration' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp ibid p finck 'automated decision making administrative law' max planck institute innovation competition research paper p craig p 'article - right good administration' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp france code des relations entre le public et l'administration article l2111 panoptykon foundation profiling unemployed poland social political implications algorithmic decision making see also algorithm watch poland scrap controversial unemployment scoring system see decision k available constitutional tribunal' website fundamental rights impact assessment - practical tool protecting fundamental rights chapter illustrated extent using ai affects different fundamental rights chapter analyses fundamental rights impact assessments fria could reduce negative impacts using ai fundamental rights section provides brief overview current discussion need fundamental rights impact assessments field section analyses current practices addressing fundamental rights implications based interviews conducted report interviewees asked sort testing done system used controls tasks affected use technology chapter ends suggestions assess fundamental rights impact using ai related technologies calling fundamental rights impact assessment - available guidance tools international organisations academics2 civil society3 called fundamental rights impact assessments conducted using ai related technologies example committee ministers council europe' guidelines addressing human rights impacts algorithmic systems recommend states conduct \"impact assessments prior public procurement development regular milestones throughout context specific deployment order identify risks rights adverse outcomes\" need flexible impact assessments adapt different situations given fundamental rights violations always contextual scholars exemplify based eu anti discrimination law equality always contextual depends case hand fundamental rights compliance cannot automated hard coded computer software rather use case needs separate examination determine whether fundamental rights issue arises nevertheless assessments follow systematic approach provide similar information existing standards provide guidance fundamental rights impact assessment ai related technology include hard law soft law instruments recommendations declarations practical tools e g guidelines checklists beyond requirements flowing data protection legislation see box examples laws requiring mandatory assessments effects ai general view increasing uptake ai canadian government issued guidelines including mandatory requirements assessing ai use public administration applies system tool statistical model used recommend make administrative decision client european data protection law requires data protection impact assessment dpia learning coe modernised convention provides general obligation examine likely data impact data processing individuals' rights fundamental freedoms use protection following assessment controllers design processing manner impact prevent minimise identified risks b assessments eu law imposes similar detailed obligation gdpr foresees data protection impact assessment dpia data processing likely \" result high risk rights freedoms natural persons \"c therefore required law dpia ai technology could potentially also address broader fundamental rights implications besides impact right privacy used tool investigate algorithms impacts e however gdpr article dpia limited 'high risk' cases processing personal data therefore may miss high risk cases primarily obviously related protection personal data time gdpr delimited specific field application accompanying expertise field means potential extension scope dpia fundamental rights might limited gdpr also gives indications modalities undertake dpia first dpia conducted high risk processing f second dpia provide systematic description envisaged operations purpose legitimate interests pursued must also assess necessity proportionality processing possible risks rights individuals addition must contain planned security measures address risks identified g pointing different methodologies apply article working party wp proposes - check list form - minimum criteria controller use assess dpia comprehensively complies gdpr h finally gdpr foresees prior mandatory consultation relevant supervisory authority impact assessment indicates processing presents risks cannot mitigated gives crucial role dpas independent bodies established law j european data protection supervisor edps provides guidance carrying dpias k data protection authorities also discussed provide guidance assess ai technologies l information data protection impact assessment see fra council europe edps handbook european data protection law edition p b council europe modernised convention art c gdpr art gdpr recitals article working party guidelines data protection impact assessment dpia wp248rev october e edwards veale fra bigdata discrimination data supported decision making luxembourg publications office june f gdpr art wp29 specifies 'carrying dpia continual process one time exercise ' g gdpr art well recitals h article working party guidelines data protection impact assessment dpia wp248rev october annex gdpr art j gdpr art k edps accountability ground part ii data protection impact assessments prior consultation v july l see example declaration ethics data protection ai adopted 40th international conference data protection privacy commissioners icdppc many examples non binding guidelines global level united nations guiding principles business human rights recommend enterprises integrate findings human rights impact assessments across relevant internal functions processes take appropriate action although refer specifically ai guidelines relevant supporting development ai technology rights compliant manner eu level ethics guidelines trustworthy ai prepared european commission' high level group artificial intelligence9 also recommend performing fria system' development \" risks fundamental rights negatively affected technology\" also emphasise need put place mechanisms receive external feedback ai systems potentially infringe fundamental rights addition private companies associations private companies12 public private interests well ngos14 organisations15 developed different types guidance support ai impact assessments documents usually contain clear guidelines impact assessment instead highlight different aspects criteria taken account developing carrying impact assessment broad categories include purpose system description technology assessment impact targeted population individual evaluating fairness diversity description audits planned performed well accountability explicitly refer applicable international human rights law standards various codes ethics conducts standards well certification schemes also place several practical tools available assess impact ai technologies mitigate risks developed wide range actors include checklists lists questions online self evaluation tools risk management frameworks focus specifically assessing fundamental rights risks others focus ethical societal economic implications useful references performing thorough fundamental rights impact assessment ai technologies july example high level group artificial intelligence issued \"assessment list trustworthy ai\" altai six month pilot involving stakeholders altai helps organisations self evaluate - voluntary basis - reliability trustworthiness ai reduce potential risks users supports businesses public administrations ask right questions around seven requirements responsible ai identified ethics guidelines trustworthy ai altai specifically refers need perform fundamental rights impact assessment includes examples questions assess impact non discrimination equality right privacy rights child freedom expression well freedom information association several online assessment tools target use ai public authorities canadian government developed algorithmic impact assessment tool aia pursuant canadian directive automated decision making aia represents automated assessment consisting questions unfold requirements directive questions relate fundamental rights concerns - ai system' impact freedom movement likelihood incarceration individual legal status access funding benefits indigenous people score attributed reply final impact scoring provided made publically available government' website another example ethics toolkit31 freely accessible tool designed local governments based risk management approach supports fair automated decisions minimising unintentional harm individuals field criminal justice higher education social media areas among national human rights bodies danish institute human rights proposed human rights compliance \"quick check involves interactive online computer programme allows companies select modify information database suit type business area operations check rights compliance quick check based human rights compliance assessment tool runs database questions corresponding human rights indicators uses international human rights law standards benchmarks applying fields operations provide guidance developing impact assessment ai technology academic work also suggested operational frameworks assessing risks using ai technology focus specifically identifying addressing fundamental rights implications private sector focus developing ethical values oriented models analysing societal impact data used creation ad hoc expert review committee others developed guidance frameworks specific case studies example field criminal justice algo care framework36 introduced step step assessment evaluate key legal practical concerns considered relation police using algorithmic risk assessment tools argued participatory ways involve consider views affected rights holders stakeholders communities developing impact assessment publically engage start others joined cross discipline expertise science law design practical frameworks impact assessments testing practice virtually systems discussed interviews subject sort testing included elements impact assessment however mainly technical data protection impact assessments rarely address potential impacts fundamental rights interviewees argue conducting fundamental rights impact assessment view system negatively affect fundamental rights unsure example respondent working traffic management using cameras monitoring traffic indicated tested accuracy system fundamental rights apart respecting data protection rules respondents simply know fundamental rights assessed part general impact assessment carried testing stages development much testing done new ai system used respondents \" testing system highlighted moving ai system production challenging task really look legal aspects mentioned public administration well private companies usually looked whether system careful using ai many projects interviewees refer still profitable \" development pilot phase started concrete testing private company estonia testing done several stages include development stage called proof concept pilot stages deployment tests deployment possible live experimentation carried initial stages often involves staged deployment example organisation interviewed tests different applications support job seekers conducts continuous step step testing selected members organisation test tool real situations using check lists interviewee mentioned challenging move deployment stage planned supervise tool real time another example involving automated rule based granting social benefits different assessments carried implementation group lawyers data protection specialists compensation specialists accountants performed general impact assessment department responsible using system conducted tests decide whether system could used following system monitored implementation using step step approach first step half decisions taken system next step decisions taken automatically expanded negative decisions another area decisions added including decisions ending compensation payments time interviews conducted decisions automated interviewee indicated carrying tests feel sure system secure outstanding risks company working fraud detection system replaced rule based system machine learning tool changing system old new system run parallel see machine learning system performs better rule based one interviewee mentioned \" rigorous analysis behind direct feedback saw would impact losses versus many good customers impacting negatively\" interviewee added \" comfortable machine learning system better static rule system aspects deployed entirety\" use cases previous automated system existed tests reviewed humans example automated transcription service tested court hearings allowed judge included regular feedback correctness transcription services judges one interviewee law enforcement working tool detect domestic violence identifies issues precision accuracy using system police officer sufficient training knowledge system indicators required system cannot gather required information could lead miscalculation highlight robustness system tested annually assure quality two questionnaires used completeness data training police officers using ai system process also considers personal data protection laws protocols applied tests discussed focus strongly technical aspects general operations fundamental rights data protection impact assessments apart data protection respondents mentioned fundamental rights typically considered respondents reflected potential impacts fundamental rights mentioned aspects considered prompted interviewer many respondents generally aware discrimination issues - often discussed explicitly asked discrimination yet gave information formal depth tests discrimination generally respondents ruled possibility system discriminates based protected attributes example one interviewee states test system data protection laws specific applicable legal acts fundamental rights however interviewee consider potential discrimination ruled needs kept mind future technologies interviewee stated however cases non discrimination generally considered testing phase ai systems one respondent municipal authority mentioned cannot assess fairness model cannot access data needed due data protection reasons according interviewee \" huge tension surrounding gdpr want well might fact worse interpretation data turns impossible\" \"yes assess legality personal data protection respondents reported data protection impact assessment conformity specific legal required law conducted although took different forms bank acts \" tested tool analysing speech customer calls find public administration estonia reoccurring problems carried data protection impact assessment dpia specifically testing tool outcome system tested data used testing phase deleted certain period test access data employees restricted testing phase supervised deployment tool another dpia required case sometimes lack clarity extent use ai related technologies notably use algorithms belongs dpia area predictive policing instance dpias done underlying architecture system rather specific ai tool another interviewee using algorithms financial services also mentioned assessing machine learning tool within framework dpia belief apply machine learning system underlying data one interviewee felt data protection impact assessment crime heat map example sufficiently depth safeguard quality model system equipped deal cross sectoral use data different rules might apply indicated standards required respondent working migration management indicated data protection officers involved analysis legal service specialised quality control ai tool study data protection aspects system however respondent also mentioned guidance needed companies working targeted advertising looked data protection issues although respondents sure impact assessment conducted companies assessed example whether people consented approached targeted communication targeted ads assessed whether information possible identification deleted including whether cookies trackers anonymised respect dpias generally respondents know area responsibility others knew positive dpia aware details appears legal assessment sometimes detached technical side technical people knowing legal assessments one interviewee private company working credit risk scoring mentioned \" make suggestions system could developed compliance manager tells conformity laws\" audits working external oversight bodies public administrations private companies involved fra' research carry tests deploying ai often linked existing internal external oversight processes use ai frequently subjected internal review processes within companies public administration although necessarily formalised review processes interviewees mentioned working formalising existing internal review processes overseeing ai systems interviewees public sector say particularly cautious using ai support decisions representative working migration management public administration indicates \" n private sector wrong results might cause business related losses police impacts people' lives fundamental rights\" yet always clear public administration businesses responsible checking overseeing use ai public administrations appear stronger scrutiny comes oversight ai systems oversight often done regular audits example connected budgetary review interviewees public private organisations report ai systems currently checked framework existing review e g regular database checks absence review processes specifically look use ai addition interviewees report sector specific certification schemes also look use ai - example area health financial services several interviewees mentioned contact data protection authorities companies public administrations sought permission data protection authorities using ai system least generally contact example one company working targeted advertising mentioned discussing use personal data national data protection authority experts interviewed report highlighted relevance data protection authorities overseeing ai systems respect use personal data however experts strongly highlighted data protection authorities resourced task two reasons data protection authorities often relevant ai related expertise additionally budgets overstretched workload heavy experts' views differ respect need additional oversight bodies potential creation ai specific institution however agree existing bodies work topics linked ai within mandates equality bodies well human rights institutions mentioned interviewed experts providing oversight concerning possible discrimination using ai highlighted institutions need build expertise area better contribute oversight ai however similar data protection authorities challenging task equality bodies given lack resources \" proactive among several interviewees mentioned consumer protection authorities potentially mitigate risks providing relevant oversight use ai one respondent working also get additional audits also retail company would like advisory agency could consulted see sometimes regulatory possible use ai innovation without investigated right away audits quite sloppy us moment company prefers consult consumer authorities good lots data protection authorities potential future marketing campaigns customer data \" data protection authorities might start investigation private company estonia efforts discussing oversight developing using ai well experts repeatedly mention challenge really understand impact using ai despite need engage existing oversight bodies responsibilities oversee use ai fundamental rights perspective remain unclear fundamental rights impact assessment practice many key actors field fundamental rights called conducting fundamental rights impact assessments using ai driven systems section highlights elements could incorporated assessment fundamental rights impact assessments needed given contextualised assessment required uses ai vary considerably terms complexity level automation potential errors harm scale application well area use complex ai system difficult assess potential impact fundamental rights implicated vary depending area application full spectrum rights needs considered use ai however uses ai likely involve rights often affected ai systems discussion preceding chapter makes clear issues linked data protection non discrimination well access effective remedies fair trial relevant uses ai thus following horizontal points could basic starting point considering impact ai selected rights ---- legal processing data needs confirmed line data protection laws personal data used full data protection framework applies ensures processing legal violate person' rights respect private family life data protection ---- processing lead unfair treatment discrimination protected groups assessing non discrimination needs core assessing ai even apparently miniscule differences scale create risks contravening principle non discrimination disadvantage people depends nature kind harm severity strength harm significance many people put disadvantage compared another group people statistical assessments group differences important tool assess unfair discriminatory uses ai ----people subjected ai related technologies able complain receive effective remedies accessible ways people complain potential decisions made effectively access remedies includes availability information allows explanation decisions addition relevant rights charter apply public administrations using ai need consider good administration principles businesses take consumer protection account rights relevant depending area application examples include ---- right social protection working social benefits ---- right freedom expression information using ai support online content moderation ---- right assembly association considering use facial recognition technology public spaces ---- right education using ai education sector ---- right asylum using ai support migration management ---- right collective bargaining action using ai 'gig economy' ---- right fair working conditions using ai workplace ---- right access preventive health care using ai health services ---- right presumption innocence right defence using ai justice sector law enforcement purposes information needed assess potential impact fundamental rights implementing ai given variety tools purposes area application assessments contextual able meaningfully respond horizontal points raised assess specific rights linked different use cases least following information needs available ---- description purpose context system well legal basis ---- description possible harm using system including questions around false positives false negatives possible harm due automation scale use ---- description technology used includes information data used building system legal basis processing description relevant information include provided fra' paper data quality ai ---- evidence based description accuracy ai system terms outcomes based training data possible tests experiments real life situations appropriate false positives false negatives considered separately include breakdowns many groups possible allow checking potential discrimination e g differences accuracy women men ---- already available provision information compliance existing standards potential certifications obtained ex post assessments safeguards lastly envisaging ex post safeguards contributes fundamental rights compliant use ai could include ----regular repetition assessments deployment appropriate important learn potential feedback loops case rules updated also requires recording information use outcomes system extent data protection respected ----making people subjected ai systems aware subjected technology otherwise challenge decision affecting ----making available easily accessible channels effectively complaining decisions made based ai system engaging external experts stakeholders oversight bodies information could basis consultation different stakeholders experts particular ai system used depending nature application legal basis consultation relevant stakeholders would ensure potential harm omitted different perspectives brought assessment stakeholders could include civil society different public private organisations well experts different fields fundamental rights including data protection ten experts interviewed report highlighted existing oversight bodies also responsible ai oversight within mandates sector specific bodies certification schemes extent interviews suggest - example health care financial oversight monitor comprehend effectively respond potential impacts ai wide spectrum fundamental rights data protection authorities equality bodies ombuds institutions national human rights institutions could play important role providing input oversight various points expertise however interviews indicated extensive upskilling resource allocation needed underpin endnotes council europe commissioner human rights unboxing artificial intelligence steps protect human rights - recommendation council europe strasbourg may heleen l janssen ' approach fundamental rights impact assessment automated decision making international data privacy law' international data privacy law vol issue february pp alessandro mantelero 'ai big data blueprint human rights social ethical impact assessment' computer law security review vol issue august pp edwards lilian veale michael slave algorithm 'right explanation' probably remedy looking may duke law technology review accessnow access ' submission consultation \"white paper artificial intelligence european approach excellence trust\" council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems april para human rights impact assessment detailed discussion respect non discrimination see wachter mittelstatt b russel c fairness cannot automated bridging gap eu non discrimination law ai government canada directive automated decision making united nations un guiding principles business human rights endorsed human rights council resolution hrc res july principles heleen l janssen approach fundamental rights impact assessment automated decision making international data privacy law vol issue february pp high level expert group artificial intelligence ethics guidelines trustworthy ai april chapter iii ibid p see example ibm everyday ethics artificial intelligence sony sony group ai ethics guidelines vodaphone vodaphone' ai framework arborus international orange international charter inclusive ai april signed private companies including camfil danone edf l'oreal metro sodexo etc information technology industry council iti iti ai policy principles ecp platform information society artificial intelligence impact assessment netherlands november amnesty international access human rights watch wikimedia foundation toronto declaration protecting rights equality non discrimination machine learning systems may rightscon toronto university montreal montreal declaration responsible ai electrical electronics engineers ieee global initiative ethics autonomous intelligent systems ethically aligned design prioritizing human wellbeing autonomous intelligent systems future life institute asilomar ai principles conference outcome future life institute' second conference future artificial intelligence see example ecp platform information society artificial intelligence impact assessment netherlands november ieee initiative association computer machinery acm acm code ethics professional conduct june future humanity institute university oxford standards ai governance international standards enable global coordination ai research development april iso standards iso iec jtc sc artificial intelligence sstandard project direct responsibility iso iec jtc sc secretariat iso iso iec tr standard information technology -- artificial intelligence -- overview trustworthiness artificial intelligence may establishes among others \"approaches assess achieve availability resiliency reliability accuracy safety security privacy ai systems \" iso standards development september iso iec cd information technology -- artificial intelligence -- risk management iso iec awi tr information technology -- artificial intelligence ai -- bias ai systems ai aided decision making iso iec awi tr information technology -- artificial intelligence -- overview ethical societal concerns information available iso' website electrical electronics engineers ieee ieee p7003(tm) algorithmic bias considerations german ai federal association ki bundesverband german ai federal association seal quality ki bundesverband guetesiegel march article working party guidelines data protection impact assessment dpia wp248rev october annex - criteria acceptable dpia high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july government canada algorithmic impact assessment tool danish institute human rights human rights compliance assessment quick check june center government excellence johns hopkins university ethics algorithm toolkit government canada algorithmic impact assessment tool article working party guidelines data protection impact assessment dpia wp248rev october annex data protection focus danish institute human rights human rights impact assessment guidance toolbox ai pulse creating tool reproducibly estimate ethical impact artificial intelligence september fairness accountability transparency machine learning fat ml principles accountable algorithms social impact statement algorithms fat ml social impact statement algorithms high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july see e g human agency oversight technical robustness safety privacy data governance transparency diversity non discrimination fairness societal environmental well accountability high level expert group artificial intelligence ethics guidelines trustworthy ai april high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july p government canada algorithmic impact assessment tool government canada directive automated decision making article appendix c center government excellence johns hopkins university ethics algorithm toolkit danish institute human rights human rights compliance assessment quick check june danish institute human rights human rights impact assessment guidance toolbox heleen l janssen approach fundamental rights impact assessment automated decision making international data privacy law international data privacy law vol issue february alessandro mantelero ai big data blueprint human rights social ethical impact assessment computer law security review vol issue august pp ai pulse program understanding law science evidence pulse ucla school law creating tool reproducibly estimate ethical impact artificial intelligence september model includes series questions assessing human rights impact ai enabled projects marion oswald jamie grace sheena urwin geoffrey c barnes algorithmic risk assessment policing models lessons durham hart model 'experimental' proportionality journal vol - issue ainow algorithmic impact assessments practical framework public agency accountability april institute ethical ai machine learning ethical ml network beta machine learning maturity model brave europe' governments failing gdpr see wachter mittelstatt b russel c fairness cannot automated bridging gap eu non discrimination law ai fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office june moving forward challenges opportunities report published amidst ongoing european legislative policy developments artificial intelligence global fight coronavirus covid pandemic potentially quickened acceptance innovative technologies yet also shown ai panacea problems comes various challenges report clearly shows using ai systems engages wide range fundamental rights also shows many businesses public administrations already using planning use ai related technologies however technologies involve different levels complexity examples use relatively simple algorithms level automation also varies - - decision making subject human review applications currently used also often development stage eu national legislators policymakers keep reality mind - especially presented optimistic expectations ai' potential vis vis challenges related using new technologies need regulate \" try look future vast majority public administrations businesses interviewed plan automate \" keep working using ai two interviewees indicated private company estonia use develop ai another two interviewees cautious plan wait see others including lack resources work using ai \" next steps related transparency open data however said develop continue test tools say publish information data infrastructure respect use ai includes starting pdf also information new continuing ongoing pilots evaluating existing efforts sharing data reusable formatting could results others increasing data quality trying obtain reused internally data sources private sector \" public administration spain interviewees mentioned engaged ongoing debates expressed desire contribute development legislation still see current situation - absence harmonised law \"ai great thing must area - obstacle use ai addition respondents learn use \" said working issues linked interpretability ai private company spain means working methods enhance understanding explanation decisions based complex ai indicated desire look closely ethical legal matters figure shows correlations words interviewees often use talking future use ai figure indicates topics often raised example interviewees often used term 'data' discussing future developments figure correlations words respondents often mention discussing future plans use ai technologies systems plans data development company ai tool police management solutions companies level system project technology administration tax analysis organisation service processing lot translation process related customer easy national phase payment information services application time quality improve learning business human energy algorithms decisions support machine ml continue potential people develop future notes based text interview summaries respondents spoke future use ai including words mentioned least ten times lines connecting words indicate strength word correlations within text passages size dots indicate frequency words used source fra effectively adequately protecting fundamental rights eu key objective current efforts better regulate use ai context upcoming eu legislation ai european commission' white paper addresses current gaps helping mitigate uncertainty around use ai respect fundamental rights making use ai transparent accountable terms fundamental rights includes requirements ai use directly link information needed assess impact ai fundamental rights discussed requirements linked description training data data record keeping information provided subjected ai robustness accuracy well human oversight highly relevant assessing protecting fundamental rights respect body evidence presented report offers general insights different technologies affect fundamental rights safeguards needed ensure fully fundamental rights compliant use ai practice time research fundamental rights implications use ai specific areas support policy legislative efforts eu level aiming shape europe' digital future widely fra continue look fundamental implications ai carrying focussed analysis specific use cases increase knowledge potentially go wrong consequently help mitigate prevent fundamental rights violations fra look potential simulation studies showcase biased algorithms negatively affect fundamental rights use ai often involves automating tasks previously carried humans need acknowledge human behaviour sometimes line fundamental rights using ai using ai example police might engage unlawful profiling decisions public administration companies might sometimes driven negative stereotypes current developments use ai need acknowledge potential discrimination respect data ai system built respect underlying assumptions humans turn may feed development deployment system automating certain tasks without fully understanding automated could lead unlawful processing data use technology treats people unfairly might make impossible challenge certain outcomes - name challenges however increased availability data technological tools also used better understand unequal treatment occurs current technological developments increased availability data also provide unique opportunity better understand structures society used support fundamental rights compliance opportunities created ai also contribute better understanding consequently mitigation fundamental rights violations getting touch eu person european union hundreds europe direct information centres find address centre nearest https europa eu european union contact en phone email europe direct service answers questions european union contact service --b freephone certain operators may charge calls -- following standard number -- email via https europa eu european union contact en finding information eu online information european union official languages eu available europa website https europa eu european union index en eu publications download order free priced eu publications https op europa eu en publications multiple copies free publications may obtained contacting europe direct local information centre see https europa eu european union contact en eu law related documents access legal information eu including eu law since official language versions go eur lex http eur lex europa eu open data eu eu open data portal http data europa eu euodp en provides access datasets eu data downloaded reused free commercial non commercial purposes promoting protecting fundamental rights across eu -- artificial intelligence ai already plays role deciding unemployment benefits someone gets burglary likely take place whether someone risk cancer sees catchy advertisement low mortgage rates use keeps growing presenting seemingly endless possibilities need make sure fully uphold fundamental rights standards using ai report presents concrete examples companies public administrations eu using trying use ai focuses four core areas - social benefits predictive policing health services targeted advertising report discusses potential implications fundamental rights analyses rights taken account using developing ai applications aims help ensure future eu regulatory framework ai firmly grounded respect human fundamental rights eu charter fundamental rights access justice non discrimination information society fra - european union agency fundamental rights schwarzenbergplatz - vienna - austria tel - fax fra europa eu facebook com fundamentalrights twitter com eurightsagency linkedin com company eu fundamental rights agency","NLP":"getting future right -- artificial intelligence fundamental rights report (c) european union agency fundamental rights reproduction authorised provided source acknowledged use reproduction photos material european union agency fundamental rights copyright permission must sought directly copyright holders neither european union agency fundamental rights person acting behalf agency responsible use might made following information luxembourg publications office european union print isbn doi tk en c pdf isbn doi tk en n (c) photo credits cover hquality adobe stock page copyright (c) coded bias rights reserved page mimi potter adobe stock page siberian art adobe stock page monsitj adobe stock page good studio adobe stock page monsitj adobe stock page sikov adobe stock page mykola mazuryk adobe stock page robsonphoto adobe stock page metamorworks adobe stock page thodonal adobe stock page gorodenkoff adobe stock page blackboard adobe stock page dimco adobe stock page blackboard adobe stock page videoflow adobe stock page monopoly919 adobe stock page zapp2photo adobe stock page gorodenkoff adobe stock page bestforbest adobe stock page freedomz adobe stock page zapp2photo adobe stock page copyright (c) coded bias rights reserved page european communities page copyright (c) coded bias rights reserved page blacksalmon adobe stock foreword know artificial intelligence already plays role deciding unemployment benefits someone gets burglary likely take place whether someone risk cancer sees catchy advertisement low mortgage rates speak artificial intelligence ai machines kind things people used able today ai present lives realise - use keeps growing possibilities seem endless fully uphold fundamental rights standards using ai report presents concrete examples companies public administrations eu using trying use ai discusses potential implications fundamental rights shows whether using ai taking rights account fra interviewed hundred public administration officials private company staff well diverse experts - including supervisory oversight authorities non governmental organisations lawyers - variously work ai field based interviews report analyses fundamental rights taken consideration using developing ai applications focuses four core areas - social benefits predictive policing health services targeted advertising ai uses differ terms complex much automation involved potential impact people widely applied findings underscore lot work lies ahead - everyone one way foster rights protection ensure people seek remedies something goes awry need know ai used also means organisations using ai need able explain ai systems deliver decisions based yet systems issue truly complex using ai systems responsible regulating use acknowledge always fully understand hiring staff technical expertise key awareness potential rights implications also lacking know data protection concern refer non discrimination less aware rights - human dignity access justice consumer protection among others - also risk surprisingly developers review potential impact ai systems tend focus technical aspects tackle challenges let' encourage working human rights protection working ai cooperate share much needed knowledge - tech rights develop use ai also need right tools assess comprehensively fundamental rights implications many may immediately obvious accessible fundamental rights impact assessments encourage reflection help ensure ai uses comply legal standards interviews suggest ai use eu growing still infancy technology moves quicker law need seize chance ensure future eu regulatory framework ai firmly grounded respect human fundamental rights hope empirical evidence analysis presented report spurs policymakers embrace challenge michael 'flaherty director contents foreword key findings fra opinions ai fundamental rights - relevant policymaking report mean artificial intelligence ai fundamental rights eu policy framework moving towards regulation endnotes putting fundamental rights context - selected use cases ai eu examples ai use public administration examples ai use private sector endnotes fundamental rights framework applicable ai fundamental rights framework governing use ai 'use case' examples requirements justified interferences fundamental rights endnotes impact current use ai selected fundamental rights perceived risks general awareness fundamental rights legal frameworks ai context human dignity right privacy data protection - selected challenges equality non discrimination access justice right social security social assistance consumer protection right good administration endnotes fundamental rights impact assessment - practical tool protecting fundamental rights calling fundamental rights impact assessment - available guidance tools impact assessments testing practice fundamental rights impact assessment practice endnotes moving forward challenges opportunities figures figure companies using ai member state figure examples different automation complexity levels use cases covered figure words interviewees often used describe ai 'use cases' figure awareness gdpr right opt direct marketing eu united kingdom country region figure awareness right say decisions automated age gender difficulty paying bills figure awareness risks discrimination using ai country figure correlations words respondents often mention discussing future plans use ai key findings fra opinions new technologies profoundly changed organise live lives particular new data driven technologies spurred development artificial intelligence ai including increased automation tasks usually carried humans covid health crisis boosted ai adoption data sharing - creating new opportunities also challenges threats human fundamental rights developments ai received wide attention media civil society academia human rights bodies policymakers much attention focuses potential support economic growth different technologies affect fundamental rights received less attention date yet large body empirical evidence wide range rights ai implicates safeguards needed ensure use ai complies fundamental rights practice february european commission published white paper artificial intelligence - european approach excellence trust outlines main principles future eu regulatory framework ai europe white paper notes vital framework grounded eu' fundamental values including respect human rights - article treaty european union teu report supports goal analysing fundamental rights implications using artificial intelligence based concrete 'use cases' ai selected areas focuses situation ground terms fundamental rights challenges opportunities using ai overarching fundamental rights framework applies use ai legal eu consists charter fundamental rights eu charter well framework european convention human rights multiple council europe international human rights instruments relevant include universal declaration human rights major un human rights conventions addition sector specific secondary eu law notably eu data protection acquis eu non discrimination legislation helps safeguard fundamental rights context ai finally national laws eu member states also apply see fra bringing rights life fundamental rights landscape european union luxembourg publications office european union major conventions include international covenant civil political rights international covenant economic social cultural rights international convention elimination forms racial discrimination convention elimination forms discrimination women convention torture convention rights child convention rights persons disabilities international convention protection persons enforced disappearance universal international human rights law framework including enforcement mechanisms see e g de schutter international human rights law cases materials commentary cambridge cambridge university press 2nd edition report based interviews officials public administration staff private companies selected eu member states asked use ai awareness fundamental rights issues involved practices terms assessing mitigating risks linked use ai moreover interviews conducted experts deal various ways potential fundamental rights challenges ai group included public bodies supervisory oversight authorities non governmental organisations lawyers safeguarding fundamental rights - scope impact assessments accountability considering full scope fundamental rights respect ai fra opinion using ai systems engages wide range fundamental introducing new policies rights regardless field application adopting new legislation ai include - also go beyond - privacy data protection eu legislator member states non discrimination access justice acting within scope eu law must ensure respect full eu charter fundamental rights charter spectrum fundamental rights became legally binding december enshrined charter eu legal value eu treaties brings together treaties taken account specific civil political economic social rights single text fundamental rights safeguards need pursuant article charter institutions accompany relevant policies laws bodies offices agencies union respect eu member rights embodied charter eu member states rely robust evidence states implementing union concerning ai' impact fundamental law applies equally ai field rights ensure restrictions fieldwork research shows large certain fundamental rights respect variety systems used heading ai principles necessity technologies analysed entail different levels proportionality automation complexity also vary terms relevant safeguards need scale potential impact people provided law effectively fra' findings show using ai systems implicate protect arbitrary interference wide spectrum fundamental rights regardless fundamental rights give field application include also go beyond legal certainty ai developers privacy data protection non discrimination users voluntary schemes access justice yet addressing impact ai observing safeguarding respect fundamental rights interviews show fundamental rights development scope often delimited specific rights use ai help mitigate rights violations line wider range rights need considered minimum requirements legal clarity using ai depending technology area use - basic principle rule addition rights concerning privacy data protection law prerequisite securing equality non discrimination access justice fundamental rights - legislator rights could considered include take due care defining example human dignity right social security scope ai law social assistance right good administration mostly relevant public sector consumer protection given variety technology particularly important businesses depending subsumed term ai context ai use right protected lack knowledge full charter needs consideration scope potential fundamental rights impact legal definition ai related terms might need assessed regular basis using effective impact assessments prevent negative effects fra opinion prior impact assessments mainly focus technical eu legislator consider making issues rarely address potential effects mandatory impact assessments fundamental rights knowledge cover full spectrum fundamental rights cover private ai affects rights lacking public sectors applied deploying ai systems engages wide spectrum ai system used fundamental rights regardless field application impact assessments take pursuant article charter eu member states account varying nature scope must respect rights embodied charter ai technologies including level implementing union law line existing automation complexity well international standards - notably united national potential harm include guiding principles business human rights ungps basic screening requirements - businesses place \" human rights due also serve raise awareness potential diligence process identify prevent mitigate account fundamental rights implications address impacts human rights\" impact assessments draw principles irrespective size established good practice sector encompasses businesses working ai fields regularly repeated pursuing commitments ungps eu deployment appropriate adopted several legislative acts addressing sector assessments conducted specific instruments particular context due transparent manner outcomes diligence related obligations human rights discussions recommendations currently underway proposing new eu secondary public domain extent possible law law would require businesses carry due aid impact assessment process diligence potential human rights environmental companies public administration impacts operations supply chains law required collect would likely cross sectoral provide sanctions information needed thoroughly non compliance - encompass use assessing potential fundamental ai see fra' recent report business human rights rights impact - access remedy calls improved horizontal eu member states human rights diligence rules eu based companies consider targeted actions support impact assessments important tool businesses developing using planning public administration alike mitigate potential use ai systems ensure effective negative impact activities fundamental rights compliance fundamental eu law specific sectors requires forms impact rights impact assessment obligations assessments data protection impact assessments actions could include funding general data protection regulation gdpr guidelines training awareness many interviewees reported data protection impact raising particularly - assessment required law conducted however exclusively - target private took different forms moreover prior assessments sector conducted focus mainly technical aspects eu member states rarely address potential impacts fundamental rights consider using existing tools according interviewees fundamental rights impact checklists self evaluation tools assessments carried ai system developed european international appears affect fundamental rights negatively level include developed research shows interviewees' knowledge eu high level group artificial fundamental rights - data protection intelligence extent non discrimination - limited majority acknowledge however use ai impact fundamental rights interviewees indicate systems affect fundamental rights extent linked tasks ai systems used respondents aware data protection issues respondents also realise discrimination could - generally - problem ai used however exact meaning applicability rights related data protection non discrimination remains unclear many respondents research findings show differences private public sector interviewees private sector often less aware wider range fundamental rights could affected data protection issues known private sector however rights non discrimination access justice related rights less well known among business representatives work ai fully aware potential problems others said responsibility checking fundamental rights issues lies clients ensuring effective oversight overall accountability fra opinion businesses public administrations developing using ai contact various eu member states ensure bodies responsible overseeing ai related effective accountability systems place systems within respective mandates sectors monitor needed effectively address negative impact ai systems bodies include data protection authorities fundamental rights consider using ai always sure bodies addition fundamental rights impact responsible overseeing ai systems assessments see fra opinion introducing specific safeguards ensure line well established international human rights accountability regime effective could standards - example article european include legal requirement make available convention human rights echr article enough information allow assessment charter - states obliged secure people' rights fundamental rights impact ai systems freedoms effectively comply states - among would enable external monitoring others - put place effective monitoring enforcement human rights oversight competent bodies mechanisms applies equally respect ai eu member states also level monitoring findings point make better use existing oversight expert important role specialised bodies established specific structures protect fundamental rights sectors also responsible ai oversight within using ai include data protection mandates include example oversight authorities equality bodies national human area banking data protection authorities rights institutions ombuds institutions variety bodies potentially relevant consumer protection bodies oversight ai fundamental rights perspective additional resources earmarked however responsibilities bodies concerning establish effective accountability systems oversight ai remains unclear many 'upskilling' diversifying staff working interviewed private public sector oversight bodies would allow public administrations' use ai sometimes audited deal complex issues linked developing part regular audits private companies using ai specific sectors also specialised oversight bodies similarly appropriate bodies example area health financial services equipped sufficient resources powers also check use ai related technologies - importantly - expertise prevent example part certification schemes private assess fundamental rights violations sector interviewees expressed wish bodies could effectively support whose fundamental provide expert advice possibilities legality rights affected ai potential ai uses facilitating cooperation appropriate eu well developed set independent bodies national european level help bodies mandate protect promote fundamental share expertise experience engaging rights include data protection authorities equality actors relevant expertise - bodies national human rights institutions ombuds specialist civil society organisations - institutions research shows using also help implementing actions planning use ai often contacted different bodies national level member states consider use ai consumer protection bodies using available eu funding mechanisms often users ai contacted data protection authorities seek guidance input approval personal data processing involved interviewed experts highlight relevance data protection authorities overseeing ai systems respect use personal data however also note data protection authorities resourced task lack specific expertise ai issues experts including working oversight bodies equality bodies data protection authorities agree expertise existing oversight bodies needs strengthened allow provide effective oversight ai related issues according experts challenging given bodies' resources already stretched also highlighted important role relevant civil society organisations specialised fields technology digital rights algorithms enhance accountability use ai systems non discrimination data protection access justice three horizontal themes research shows use ai affects various fundamental rights apart context related specific aspects affect different rights varying extent fundamental rights topics emerged research repeatedly apply ai cases include need ensure non discriminatory use ai right discriminated requirement process data legally right personal data protection possibility complain ai based decisions seek redress right effective remedy fair trial two main fundamental rights highlighted interviews data protection non discrimination addition effective ways complain use ai came repeatedly linked right fair trial effective remedy following three fra opinions reflect findings read alongside opinions call comprehensive recognition response full range fundamental rights affected ai specific safeguards ensure non discrimination using ai fra opinion interviewees rarely mentioned carrying detailed assessments potential discrimination using ai eu member states consider encouraging companies public suggests lack depth assessments administration assess potentially discrimination automated decision making discriminatory outcomes using ai systems obligation respect principle non european commission member discrimination enshrined article teu states consider providing funding article tfeu requiring union combat targeted research potentially discrimination number grounds articles discriminatory impacts use ai charter equality law non algorithms research would discrimination range grounds specific benefit adaptation established detailed provisions several eu directives also enshrine research methodologies social principle varying scopes application sciences employed identify automation use ai greatly increase potential discrimination different areas efficiency services scale tasks - ranging recruitment customer humans would able undertake however profiling necessary ensure services decisions based building results research ai discriminatory recognising european guidance tools support commission recently highlighted need additional using ai detect possible discriminatory outcomes developed legislation safeguard non discrimination using ai eu anti racism action plan interviewees principle aware discrimination might happen yet rarely raised issue believe systems could actually discriminate interviewees also rarely mentioned detailed assessments potential discrimination meaning lack depth assessment potential discrimination common perception omitting information protected attributes gender age ethnic origin guarantee ai system discriminate necessarily true however information potentially indicating protected characteristics proxies often found datasets could lead discrimination certain cases ai systems also used test detect discriminatory behaviour encoded datasets however interviewees mentioned possibility collecting information disadvantaged groups detect potential discrimination absence depth analysis potential discrimination actual use ai systems also almost discussion analysis potential positive effect using algorithms make decisions fairer moreover none interviewees working ai mentioned using ai detect possible discrimination positive outcome sense discrimination better detected data analysed potential bias since detecting potential discrimination use ai algorithms remains challenging interviewees briefly addressed issue different measures needed address include requirement consider issues linked discrimination assessing use ai investment studies potential discrimination use diverse range methodologies could involve example discrimination testing could build similar established methodologies testing bias everyday life respect job applications applicant' name changed indirectly identify ethnicity relation ai applications tests could involve possible creation fake profiles online tools differ respect protected attributes way outcomes checked respect potential discrimination research could also benefit advanced statistical analysis detect differences datasets concerning protected groups therefore used basis exploring potential discrimination finally research interviews underscored results complex machine learning algorithms often difficult understand explain thus research better understand explain results called 'explainable ai' also help better detect discrimination using ai guidance data protection clarity needed scope meaning fra opinion legal provisions regarding automated decision making european data protection board data protection critical development use edpb european data ai article charter article tfeu protection supervisor edps provide everyone right protection consider providing guidance personal data gdpr law enforcement support effectively implement directive directive eu elaborate gdpr provisions directly apply right include many provisions applicable use ai safeguarding use ai fundamental rights particular regards meaning personal data interviewees indicated ai systems use ai including ai training employ use personal data meaning data protection datasets affected many different ways however applications - according interviewees - high level uncertainty use personal data use anonymised data concerning meaning automated hence data protection law would apply personal decision making right data used data protection related principles human review linked use ai provisions apply automated decision making thus edpb edps also report highlights important issue linked data consider clarifying concepts protection also relevant fundamental 'automated decision making' rights respect automated decision making 'human review' according eurobarometer survey mentioned eu law europeans know say decisions automated knowledge right considerably addition national data protection higher among working ai - majority bodies provide practical interviewees raised issue however many guidance data protection interviewees including experts argued clarity provisions apply use needed scope meaning legal provisions ai guidance could include automated decision making recommendations checklists based concrete use cases ai area social benefits interviewees mentioned support compliance data one example fully automated rule based decisions protection provisions applications mentioned reviewed humans interviewees public administration stressed importance human review decisions however rarely described human review actually involves information used reviewing output ai systems interviewees disagree whether existing legislation sufficient many called concrete interpretation existing data protection rules respect automated decision making enshrined article gdpr effective access justice cases involving ai based decisions fra opinion effectively contest decisions based use ai people need know ai used eu legislator member states complain organisations using ai need able ensure effective access justice individuals cases involving explain ai system decisions based ai ai based decisions access justice process goal crucial ensure available remedies individuals seeking benefit procedural accessible practice eu legislator substantive rights encompasses number core member states could consider human rights include right fair trial introducing legal duty public effective remedy article echr administration private companies article eu charter fundamental rights using ai systems provide accordingly notion access justice obliges states seeking redress information guarantee individual' right go court - operation ai systems circumstances alternative dispute resolution includes information body - obtain remedy found individual' ai systems arrive automated rights violated decisions obligation would help achieve equality arms cases accordance standards victim human individuals seeking justice would rights violation arising development use also support effectiveness ai system public private entity provided external monitoring human access remedy national authority line rights oversight ai systems see relevant case law article charter fra opinion article echr remedy must \"effective practice well law\" view difficulty explaining complex ai systems eu jointly research findings identify following preconditions member states remedy effective practice cases consider developing guidelines involving ai systems impact fundamental support transparency efforts rights everyone needs aware ai used area draw informed complain organisations expertise national human rights using ai must ensure public informed bodies civil society organisations ai system decisions based active field findings show explaining ai systems make decisions layman terms challenging intellectual property rights hamper provision detailed information algorithm works addition certain ai systems complex makes difficult provide meaningful information way system works related decisions tackle problem companies interviewed avoid using complex methods certain decision making altogether would able explain decisions alternatively use simpler data analysis methods problem obtain understanding main factors influencing certain outcomes private sector interviewees pointed efforts made gradually improve understanding ai technology ai fundamental rights - relevant policymaking artificial intelligence ai increasingly used private public sectors affecting daily life see ai end human control machines others view technology help humanity address pressing challenges neither portrayal may accurate concerns ai' fundamental rights impact clearly mounting meriting scrutiny use human rights actors examples potential problems using ai related technologies relation fundamental rights increasingly emerged include ---- algorithm used recruit human resources found generally prefer men women ---- online chatbot2 became 'racist' within couple hours ----machine translations showed gender bias ----facial recognition systems detect gender well white men black women ---- public administration' use algorithms categorise unemployed people comply law ---- court stopped algorithmic system supporting social benefit decisions breaching data protection laws examples raise profound questions whether modern ai systems fit purpose fundamental rights standards upheld using considering using ai systems report addresses questions providing snapshot current use ai related technologies eu - based selected use cases - implications fundamental rights report main publication stemming fra' project artificial intelligence fra' work big data fundamental rights project aims assess positive negative ai big fundamental rights implications new technologies including ai big data data fundamental current report builds findings number earlier papers rights * facial recognition technology fundamental rights considerations context law enforcement paper outlines analyses fundamental rights challenges triggered public authorities deploy live frt law enforcement purposes also briefly presents steps take help avoid rights violations * data quality artificial intelligence - mitigating bias error protect fundamental rights paper highlights importance awareness avoidance poor data quality * bigdata discrimination data supported decision making focus paper discusses discrimination occur suggests possible solutions part project fra also exploring feasibility studying concrete examples fundamental rights challenges using algorithms decision making either online experiments simulation studies several fra publications address relevant issues * guide preventing unlawful profiling today future illustrates profiling legal frameworks regulate conducting profiling lawfully necessary comply fundamental rights crucial effective policing border management * handbook european data protection law edition designed familiarise legal practitioners specialised data protection area law * data fra' fundamental rights survey surveyed random sample people across eu including findings people' opinions experiences linked data protection technology security * fra' report business human rights - access remedy analyses obstacles promising practices relation access remedies victims business related human rights abuses analysing complaints mechanisms eu member states research maps hinders facilitates access remedies report growing attention ai potential drive economic growth matched body evidence different technologies affect fundamental rights - positively negatively concrete examples allow thorough examination whether extent applying technology interferes various fundamental rights - whether interference justified line principles necessity proportionality report provides fundamental rights based analysis concrete 'use cases' - case studies 'use case' term software engineering report loosely defines specific application technology certain goal used specified actor report illustrates ways companies public sector eu looking use ai support work whether - - taking fundamental rights considerations account way contributes empirical evidence analysed fundamental rights perspective inform eu national policymaking efforts regulate use ai tools research cover fra conducted fieldwork research five eu member states estonia finland france netherlands spain collected information involved designing using ai systems key private public sectors address relevant fundamental rights issues research - based personal interviews - gathered information ---- purpose practical application ai technologies ---- assessments conducted using ai applicable legal framework oversight mechanisms ---- awareness fundamental rights issues potential safeguards place ----future plans addition experts involved monitoring observing potential fundamental rights violations concerning use ai including civil society lawyers oversight bodies interviewed presenting main findings report presents main findings fieldwork particular report includes ---- overview use ai eu across range sectors focus social benefits predictive policing healthcare targeted advertising ---- analysis awareness fundamental rights implications selected rights focus four use cases ---- discussion measures assess mitigate impact ai related technologies people' fundamental rights two annexes available fra' website supplement report ----annex gives detailed description research methodology questions asked interviews ----annex provides examples potential errors using ai selected areas addition country specific information five member states covered complements fieldwork research delivered contractor also available fra' website maps policy developments ai legal framework governing use different sectors supporting rights compliant policymaking report provides evidence extent fundamental rights considerations brought discussions activities develop test employ monitor ai systems eu also highlights different technologies affect rights set charter reflects protect rights ai becomes widespread sophisticated analysis selected fundamental rights challenges help eu member states well stakeholders assess fundamental rights compatibility ai systems different contexts findings report current views practices among using ai supports policymakers identifying actions needed report aim provide comprehensive mapping use different ai systems five eu member states covered research provide depth technical information different systems mentioned interviewees work conducting interviews report based semi structured interviews representatives public administration private companies involved use ai services businesses fra intentionally provided general definition ai interviewed part research based existing definitions organisations interviewed active public administration general working law enforcement private companies include working health retail pricing marketing financial services insurance employment transport energy importantly except two interviewees research include companies sell ai companies instead entities use ai support operations addition ten interviews conducted experts dealing potential challenges ai public administration e g supervisory authorities non governmental organisations lawyers working field interviews carried five eu member states estonia finland france netherlands spain countries selected based different levels uptake ai technology policy development area ai well incorporate experience across different parts eu fra outsourced fieldwork ecorys fra staff supervised work developed research questions methodology interviewers received dedicated training conducting fieldwork interviews carried anonymously consequence information identifying organisation concerned provided report addition certain details applications described - notably country - omitted protect respondents' anonymity communicated interviewees increasing level trust allowing speak freely work also proved useful recruiting respondents mean artificial intelligence universally accepted definition ai rather referring concrete applications reflects recent technological developments encompass variety technologies although ai usually defined widely survey conducted behalf european commission among companies eu showed eight ten people working companies eu say know ai slightly two respondents companies eu know sure ai fra' research apply strict definition ai use high level cases presents interviews ai defined broadly reference definition provided high level expert group expert group artificial intelligence ai hleg artificial interviewees also expressed variety ways think ai identifying use cases explore research intelligence project focused applications support decision making based data machine learning applications \"artificial intelligence ai refers systems systems contribute automating tasks usually display intelligent behaviour analysing environment taking actions - degree undertaken humans cannot undertaken autonomy - achieve specific goals ai based humans due large scale use cases systems purely software based acting report provide insight different technologies virtual world e g voice assistants image used discussed selected areas broad heading analysis software search engines speech ai may contention concerning whether face recognition systems ai embedded certain use cases constitute ai current level use hardware devices e g advanced robots report often refers 'ai related technologies' autonomous cars drones internet things applications \" past years seen enormous increase computing initial definition ai hleg subject power increased availability data development discussion groups see ai hleg new technologies analysing data increased amount definition ai main capabilities disciplines variety data sometimes available almost real time internet often referred big data machine learning technologies related algorithms including deep learning benefit enormously increased computing power data availability development use flourishing use terms however limited use even prove counterproductive triggers ideas linked science fiction rather real application ai variety myths exist ai often spread via social media example claim ai act form entity distracts fact ai systems made humans computers follow instructions made given humans human centric approach ai important note ai never anything - human beings use technology achieve certain goals however human work decision making behind ai systems often visible centre attention \"currently lawyer entire studies many discussions explored possible ai definitions tell definition ai european commission' joint research centre analysed ai definitions ' asked around pretty highlights often refer issues linked perception thoroughly one tell \" environment e way system receives input data environment public administration netherlands e g sensors information processing decision making achievement specific goals definitions frequently refer machines behaving like humans taking tasks associated human intelligence given difficulty defining intelligence many definitions remain vague makes use ai hard measure practice10 equally challenging define law report discusses use ai based concrete applications differ terms complexity level automation potential impact individuals scale application discussion around actual use ai involves deploying machine learning technologies seen sub domain ai also confusion around term \"learning\" implies machines learn like humans reality much current machine learning based statistical learning methodologies machine learning uses statistical methods find rules form correlations help predict certain outcomes different traditional statistical analysis involve detailed checks predictions produced often referred 'black boxes' traditional statistical analysis based specific theoretical assumptions data generation processes correlations used machine learning geared towards producing accurate outcomes used automating workflows decisions acceptable level accuracy obtained usual example email spam filter uses statistical methods predict email spam important know certain email blocked spam predicted high accuracy really need understand algorithm works e based rules emails get blocked however depending complexity task prediction always possible high accuracy moreover report highlights understanding certain outcomes predicted acceptable certain tasks area machine learning incorporates several approaches often machine learning refers finding rules link data certain outcome based dataset includes outcomes supervised learning example dataset emails labelled spam 'ham' used find correlations rules associated spam emails dataset rules used 'predict' degree likelihood future email spam sometimes machine learning used find hidden groups datasets without defining certain outcome unsupervised learning - example segmenting people groups based similarities demographics finally rules correlations found trial error reinforcement learning systems try optimise certain goal experimentation update rules automatically best possible output systems need enormous amounts data hardly used humans involves experimentation mainly responsible success winning board games humans often sensationalised media ai fundamental rights eu policy framework moving towards regulation policymakers time highlighted potential ai related technologies improve efficiency drive economic growth yet public authorities international organisations recently reflected fundamental rights challenges associated technologies coupled growing use accuracy ai systems turned attention whether regulate use european parliament resolution marked milestone eu' recognition fundamental rights implications ai resolution stressed \"prospects opportunities big data fully tapped citizens public private sectors academia scientific community public trust technologies ensured strong enforcement fundamental rights\" called european commission member states data protection authorities \" develop strong common ethical framework transparent processing personal data automated decision making may guide data usage ongoing enforcement union law\" later year european council called \"sense urgency address emerging trends\" including \"issues artificial intelligence ... time ensuring high level data protection digital rights ethical standards\" european council invited european commission put forward european approach ai responding calls european commission published communication ai europe18 set high level expert group ai initiatives include strong reference fundamental rights commission facilitated high level expert group made independent experts academia civil society industry including representative fra published 'ethics guidelines trustworthy ai' 'policy investment recommendations trustworthy ai' developed work triggered discussion importance framing ai human rights terms alongside ethical considerations led development ethics guidelines refer charter place fundamental rights consideration respect ai ethics guidelines include assessment list trustworthy ai translated checklist guide develop deploy ai indicating political support highest level european council calls strategic guidelines \"ensure europe digitally sovereign\" policy \"shaped way embodies societal values\" similarly commission president von der leyen committed \"put forward legislation coordinated european approach human ethical implications ai \" prompted significant moves towards setting eu legal framework govern development use ai related technologies including respect impact fundamental rights february european commission published white paper artificial intelligence sets policy options meeting twin objectives \"promoting uptake ai addressing risks associated certain uses new technology\" paper promotes common european approach ai deems necessary \" reach sufficient scale avoid fragmentation single market\" notes \" introduction national initiatives risks endanger legal certainty weaken citizens' trust prevent emergence dynamic european industry\" legal uncertainty also concern companies planning use ai commission white paper ai highlights risks fundamental rights one main concerns associated ai acknowledges \" use ai affect values eu founded lead breaches fundamental rights result flaws overall design ai systems use data without correcting possible bias\" also lists wide range rights affected white paper ai indicates commission' preference possible new regulatory framework follow risk based approach mandatory requirements would principle apply high risk applications would determined basis two cumulative criteria employed sector healthcare transport parts public sector significant risks expected occur used manner significant risks likely arise latter risk could assessed based impact affected parties adding harm based element white paper also highlights instances ai use certain purposes considered high risk irrespective sector include use ai applications recruitment processes remote biometric identification including facial recognition technologies following public consultation ran february june commission expected propose legislation ai first quarter ahead proposal eu' co legislators considered various aspects potential legal framework october european parliament adopted resolutions recommendations european commission framework ethical aspects ai robotics related technologies civil liability regime ai also adopted resolution intellectual property rights development artificial intelligence technologies continues work resolutions ai criminal law use police judicial authorities criminal matters ai education culture audio visual sector also established special committee artificial intelligence digital age following meeting october heads state government eu member states declared \"eu needs global leader development secure trustworthy ethical artificial intelligence\" invited commission \"provide clear objective definition high risk artificial intelligence systems addition council eu adopted conclusions shaping europe' digital future35 seizing opportunities digitalisation access justice included dedicated section deploying ai systems justice sector german presidency council eu published conclusions charter fundamental rights context artificial intelligence digital change text supported objected member states growing reference fundamental rights discussions indicates fundamental rights framework alongside legal frameworks38 necessary effective human rights compliant evaluation many opportunities challenges brought new technologies many existing ai initiatives guided ethical frameworks typically voluntary fundamental rights centred approach ai underpinned legal regulation responsibility respecting protecting fulfilling rights rests state guarantee high level legal protection possible misuse new technologies also provides clear legal basis develop ai reference fundamental rights - application practice - fully embedded addition steps towards legal regulation eu taking significant policy financial actions support development ai related technologies alongside white paper commission published european data strategy aims set single market data including nine common european data spaces covering areas health data financial data proposal multiannual financial framework would create digital europe programme worth EUR billion invest eu' \"strategic digital capacities\" including ai addition funding horizon europe connecting europe facility international actors also considering steps regulate ai notably council europe active player field ai related technologies september committee ministers council europe set ad hoc committee artificial intelligence cahai aims examine \" feasibility potential elements legal framework development design application ai based council europe' standards human rights democracy rule law\" april committee ministers council europe adopted recommendations human rights impact algorithmic systems addition organisation economic cooperation development oecd adopted ai principles created ai policy observatory global level unesco starting develop global standard setting instrument ai selected examples wide range legal policy initiatives aiming contribute standard setting area ai includes amongst others actual draft legislation soft law guidelines recommendations use ai reports recommendations law policy fra put together non exhaustive list initiatives linked ai policymaking also include legislative initiatives eu member states many organisations businesses launched initiatives tackle ethical concerns ai however useful tackle potential problems ai ethical approaches often rely voluntary action sufficiently address obligation respect fundamental rights fra pointed fundamental rights report \" rights based approach guarantees high level protection possible misuse new technologies wrongdoings using \" european commission' initiative regulating ai helps avoid disjointed responses ai across member states undermine businesses across eu entities outside eu endnotes reuters 'amazon scraps secret ai recruiting tool showed bias women' october chatbot chatterbot common ai feature embedded messaging applications simulate human conversation voice text independent 'ai robots learning racism sexism prejudices humans study finds' april prates avelar p lamb l 'assessing gender bias machine translation - case study google translate' march gender shades project evaluating accuracy ai powered gender classification products see example der standard datenschutzbehorde kippt umstrittenen ams algorithmus algorithmwatch poland government scrap controversial unemployment scoring system privacy first dutch risk profiling system syri banned following court decision european commission european enterprise survey use technologies based artificial intelligence luxembourg july see example website \"ai myths\" samoili lopez cobo gomez e de prato g martinez plumed f delipetrev b ai watch defining artificial intelligence towards operational definition taxonomy artificial intelligence luxembourg schuett j legal definition ai arxiv hastie tibshirani r friedman j elements statistical learning data mining inference prediction springer see example pasquale f black box society secret algorithms control money information harvard university press cambrigde london rai 'explainable ai black box glass box' journal academy marketing science vol pp seminal paper describing difference breiman l 'statistical modeling two cultures' statistical science vol pp european parliament resolution march fundamental rights implications big data privacy data protection non discrimination security law enforcement ini para ibid para european council european council meeting october - conclusions euco brussels october p european commission communication commission european parliament european council council european economic social committee committee regions artificial intelligence europe com final april information available webpage high level expert group high level expert group artificial intelligence ethics guidelines trustworthy artificial intelligence policy investment recommendations trustworthy ai high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment european council new strategic agenda p vonder leyen ursula union strives agenda europe p european commission white paper artificial intelligence - european approach excellence trust com final brussels february p ibid p european commission white paper artificial intelligence public consultation towards european approach excellence trust july european commission adjusted commission work programme annex new initiatives may european parliament legislative observatory framework ethical aspects artificial intelligence robotics related technologies inl european parliament resolution october recommendations commission civil liability regime artificial intelligence inl european parliament resolution october intellectual property rights development artificial intelligence technologies ini european parliament artificial intelligence criminal law use police judicial authorities criminal matters ini european parliament legislative observatory artificial intelligence education culture audiovisual sector ini european parliament decision june setting special committee artificial intelligence digital age defining responsibilities numerical strength term office rso european council special meeting european council october - conclusions euco october council european union shaping europe' digital future - council conclusions june council european union council conclusions \"access justice - seizing opportunities digitalisation\" october council european union presidency conclusions - charter fundamental rights context artificial intelligence digital change october see e g pagallo u casanovas p madelin r ' middle approach assessing models legal governance data protection artificial intelligence web data' theory practice legislation pp see fra fundamental rights report luxembourg publications office chapter communication commission european parliament council european economic social committee committee regions european strategy data com final european council conclusions special meeting european council july euco july council europe ad hoc committee artificial intelligence cahai factsheet governance digital transformation council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems adopted committee ministers april 1373rd meeting ministers' deputies see dedicated oecd website see dedicated unesco website see overview fra ai policy initiatives council europe website fra fundamental rights report luxembourg publications office p putting fundamental rights context - selected use cases ai eu eu use ai related technologies relatively wide spread recent survey shows companies use ai related technologies - plan chapter presents selected cases ai use note - typically referred 'use cases' ai field fra collected information cases interviewees use cases presented five eu member states estonia france finland netherlands spain involve chapter based information different areas application across public obtained interviews public private sector administration private companies special representatives focus put use ai areas social benefits predictive policing health services interviewed representatives targeted advertising public administration work areas health services chapter provides information current infrastructure energy use ai well basic information eu judiciary law enforcement competence select areas use cases migration border management social benefits tax well provide good sense kind ai transportation traffic control related technologies currently used interviewees private companies examples also offer context mainly work retail marketing fundamental rights analysis looking pricing health sector broad variety use cases provides important financial services energy insurance insights actual use ai affect employment transport well people' fundamental rights chapter includes cross cutting areas focus discussion fundamental rights implications ai development different sectors makes reference cases described chapter according european enterprise survey beginning companies use ai eu said use technologies depend ai percentage ranges companies estonia cyprus czechia see figure another companies eu planning use ai future survey indicates ai used mostly sector technologies used comprise variety applications aiming process equipment optimisation anomaly detection process automation forecasting price optimisation decision making figure companies using ai member state eu ro uk se si nl bg lu el cz lt mt lv dk hr de hu fr pt pl fi es ie ee cy sk currently using ai planning use ai notes survey asked use plans use ten different ai related technologies speech recognition visual diagnostics fraud detection analysis emotions forecasting based machine learning includes percentage companies using least one ai technologies n source fra based data extracted european commission european enterprise survey use technologies based artificial intelligence luxembourg july noted report focuses four broad ai 'use cases' ----social benefits ----predictive policing ----health services ----targeted advertising areas particularly sensitive regards fundamental rights two \"ai machine learning cover mainly public administration' use ai social benefits allocation different concepts ai umbrella predictive policing two concern private companies health term \" services targeted advertising use cases provide basis private company estonia report' fundamental rights analysis offering necessary context appropriate report also highlights findings interviews cover areas four areas detailed studies taxonomy ai available providing \" see categorisations technology noted introduction interviewees everyone something different views ai also stated machine learning labelling clear definition ai 'ai' \" public administration netherlands report discusses specific use cases without classifying technology applied yet use ai cases examined differed use technology described interviewees involved varying levels complexity varying levels automation figure provides overview different examples use interviewees discussed heading ai applications relatively straightforward understand rule based decision making algorithms defined based ' rules' example person income certain threshold eligible certain benefits algorithms used area social benefits different levels automation examples full partial human review involved applications used traditional statistical methods inform decisions include example regression analysis classical statistical method analyses correlation several pieces information 'variables' outcome credit score example others used complex machine learning methodologies feed production forecasts statistics government reports also algorithms much higher levels complexity deep learning diagnosis support area health tools still include high level human review hence include high level automation contrast targeted advertising example potentially using highly complex algorithms without human review output decision also using highly complex algorithms including deep learning reinforcement learning see chapter descriptions terms human review would also possible area due scale algorithms operate figure examples different automation complexity levels use cases covered high rule based regression analysis deep learning automated predictions fully reinforcement areas examples decision making automated credit learning social benefits scoring advertising social welfare rule based regression analysis facial recognition marketing automation decision predictions technology positive human reviewed identification law enforcement outcomes credit scoring human social benefits review health services financial services rule based machine learning medical images human decisions supported analysis social production diagnosis benefits forecasts low low complexity high source fra ai systems also vary according potential harm could result notes examples financial erroneous decision based use ai depending area services use facial recognition application wrong decisions - based erroneous outputs technology covered system - different impacts using ai decision making detailed use case descriptions consequences different decision affirmative wrong false mentioned interviews positive negative wrong false negative examples illustrate different levels complexity automation used practice issues particularly important machine learning used based statistical calculations always come degree error rule based algorithms also make mistakes especially grow complex risks lower deterministic nature rules developed example using ai make decisions social benefits false positive means person may erroneously receive benefits necessarily negative impact person concerned unless error found later money needs paid back however negatively impacts public administration money paid line good administration practices contrast false negative would negative impact individual would receive benefits entitled annex available fra' website provides hypothetical examples effects wrong decisions based use cases discussed importantly automating tasks impact could also scale potentially exacerbating negative effect society whole severity scale potential harm one aspect needs taken consideration analysing potential limitations fundamental rights respect use ai example small error rates using facial recognition technology used law enforcement might still lead flagging many innocent people technology used places many people analysed might apply airports train stations thousands people could scanned daily basis potential bias error rates could lead disproportionally targeting certain groups society interviewees mostly mention 'machine learning' including use neural networks technologies extensions see chapter description machine learning respondents used across either directly mentioned mentioned subfields machine learning image cases recognition facial recognition technology frt identified research often interviewees mentioned use 'supervised machine learning' mainly used optimise specifically defined outcome yet sometimes 'unsupervised machine learning' also used categorise cluster data one case referred use 'reinforcement learning' without going much detail several respondents used 'natural language processing nlp ' technology analyse text speech sometimes combined machine learning algorithms mention examples involve rule based algorithms meaning rules algorithm follow directly encoded e based ' rules' cases interviewees disclose could provide detailed information technology used generally interviewees referred one use case asked focus one application interviews ----importantly fieldwork shows companies public administrations often still beginning looking use ai two thirds use cases actually use deployed practice many use cases described interviewees pilot stage development still research phase ----two ai driven applications halted tests figure words interviewees often used describe ai 'use cases' notes fra visualisation words frequently used descriptions use cases bigger size word often interviewees mentioned terms source fra figure shows frequently used words describe use cases covered report highlights importance data using ai systems well relevance supporting decision making fra previously highlighted thorough description data used ai applications essential identifying mitigating potential fundamental rights challenges variety data used ai systems covered report however difficult obtain detailed information data used respondents remained rather vague data sources rather generically many respondents mentioned using 'open data' 'historical data' 'metadata' concretely respondents mentioned using customer data e g purchases browsing behaviour administrative records \" mostly used save time ... data social benefits taxes interviewees also mentioned go lot medical records police records court records well social media material \" traffic data data included text data e g e mails audio recordings video public administration netherlands geolocation data data come internal databases companies public administration also external sources \" important deal cases efficiently ' single important reason using ai increased efficiency making use workforce vast majority respondents across public private sector mentioned people handle cases using ai greater speed fewer errors cost reduction fewer human effectively possible \" resources needed interviewees law enforcement also said public administration netherlands use ai safety security well crime prevention humans previously performed many use cases respondents said use ai entails fewer mistakes humans carry certain tasks respondents also use ai tasks humans previously carry quantity information could processed humans - example area genome analysis traffic predictions importantly half respondents interviewed use ai relevant decision making however ai mainly used support decision making final decisions remain largely hands humans interviewees pointed enthusiastic public administration companies still cautious deploying ai many use cases still testing phase described stopped phase nevertheless almost interviewees aware plans reduce level technology used fact expressed intentions invest innovation new ways employ currently available ai systems examples ai use public administration use case automating social welfare systems - using algorithms area social benefits background eu legal framework united nations special rapporteur extreme poverty human rights philip alston warned october report introducing 'digital welfare' state including use ai lead \"digital welfare dystopia\" digitalisation welfare systems often accompanied reductions overall welfare budgets narrowing beneficiary pool measures reduce availability welfare digitalisation also increases power states offering opportunities control people particularly worrying countries significant rule law deficits use algorithms public administration welfare raises major concerns respect potentially negative impact poverty inequality applied erroneously area social benefits includes areas child welfare services6 unemployment benefits yet public authorities keen use new technologies make decision making social security benefits efficient potentially fairer globally new technologies used many ways administer welfare systems include identity verification eligibility assessments benefit calculations fraud prevention detection risk scoring need classification well communication authorities beneficiaries oecd defines social benefits transfers made households need certain events particular circumstances arisen including sickness unemployment retirement housing education family circumstances however commonly agreed definition social benefits social benefits particular social insurance systems different private insurance schemes involve compulsory contributions made employees employers sometimes form taxation social policy including social security social protection area shared competence eu member states article b tfeu pursuant article tfeu eu pursues objectives among things promote \"improved living working conditions\" \"proper social protection\" end eu supports complements activities member states number fields including social security social protection workers combating social exclusion article tfeu eu actions encourage cooperation member states adopt directives minimum requirements moreover decisions social security social protection adopted special legislative procedure unanimous vote council backdrop eu member states mostly free shape social security social protection policies since virtually harmonisation social security systems differ significantly across eu terms benefits provided conditions eligibility benefits calculated contributions need paid etc public administrations eu member states working implementing ai related technologies area public welfare however information applications limited fra collected information use cases linked ----using algorithms comes compensating job seekers ----processing social benefits applications ----machine learning supported data analysis use pensions several private insurance companies interviewed research use ai related private technologies includes handling requests customers complementary health insurance insurance insurance compensation decision support evaluating credit risk companies' individuals insurance pricing insurance claims management decision making support use ai related management functions credit decisions private insurance companies generally embrace ai related technologies help make business profitable oecd report highlights importance technology sector also argues risk classification could lead exclusion belonging certain vulnerable groups ways undesirable societal political perspective oecd impact big data artificial intelligence ai insurance sector use practice use cases outlined exemplify challenges using planning use ai area social benefits linked algorithmic decision making experimenting new technologies support jobseekers course three year project public organisation experimented several ai related technologies concerning work related processing benefits job seekers assisting return work representative interviewed states tested technologies improve foster relationship job seekers improve advice given job seekers companies testing completed organisation decide apply technologies day day work tests include machine learning based detection attractiveness job offers system detecting whether job seekers still actively looking job tests also looking profiling job seekers provide advice would include calculating probability someone offered available job within given time identifying parameters make job offers relevant may also reflected advice companies best practices formulating job offers profiling would allow organisation determine appropriate services according profile background job seeker rather analysis advice drawn employees practically would done requiring job seekers complete monthly diary job search however still consideration whether programme limited providing descriptive analyses whether go provide recommendations organisation hesitant latter aspect additionally natural language processing system tested analysing content job seekers' e mails e mails categorised relevant data extracted urgency relevance e mails identified using chatbot using automatic replies emails considered data used systems come several sources within organisation data job seekers background including personal tax data well data salaries social security allowances used strict conditions derived highly regulated data sources e g salary statements cannot accessed data job offers companies also used generate knowledge job market organisation currently use external data professional social media networks legal provisions place using data processing housing benefits - failure success public body responsible processing social benefits piloted ai tool process applications subsequently support staff making decisions housing benefits system selected cases new benefit applications relatively straightforward calculate include new applications housing benefits submitted individual living alone children individual income government benefits overall cases deemed simple result always individual receives benefits technological solution based decision tree model following rules housing benefits calculating general housing benefits requires income estimates advance data used testing stemmed internal database contains data benefit application processes data pseudonymised need use personal information simple statistical model linear regression used input income cost limits output amount benefit however even simplified cases found difficult use ai practice frequent changes legislation test terminated according interviewee lack legal basis using machine learning allow using administrative decisions plans use ai support decision making social benefits organisation pursuing particular project due aforementioned legal challenges interviewee noted potential applications solutions area future noted ai related technologies support operations without legal impact particularly good organisation syri case netherlands called time organisation using image processing social benefits applications 'system risk indication' syri generally benefit applicants complete developed government tool several forms attachments often alert dutch public administration submitted paper format efficient fraud risk citizens time saving handling documents processing linking large amounts agency' staff hard copies received personal data public scanned classified automated authorities system broad coalition civil society organisations dealing privacy first step turn images right way issues initiated lawsuit prompting round algorithms align documents district court hague aligned properly scrutinise algorithm based syri scanned remove spots clean edit colouring document identify court ruled syri impinges disproportionately private columns paragraphs tables elements life citizens court found distinctive blocks recognise script etc everyone whose data analysed application checks received syri exposed risk application form attachment marked addition due opacity correctly e g document marked algorithm used citizens could invoice system determines whether \"neither anticipate intrusion correct private life guard \" turning classification images good description syri done image recognition optical found ilja braun high risk character recognition ocr technologies citizens algorithm watch recognise text stemming images including photographs scans documents ruling february handwritten notes ocr technology dutch available online converts recognised text text data privacy first dutch machine readable pattern recognition risk profiling system syri banned process input scanned images following court decision first isolated compared 'glyphs' e variations letters stored system pixel pixel basis agency continue processing images develop example potentially making possible scan bar codes attachments would help speed confirmation correctness documents attachments also solutions related natural language processing automating unemployment benefits one countries selected decisions unemployment benefits fully automated national institution responsible unemployment insurance benefits updated system fully automate processing benefit applications decisions done relevant legislation adapted allow automated decisions person registers unemployed lodges application benefits system draws information applicant various databases includes example population register tax authorities' databases containing information salaries work experience etc conditions receiving unemployment benefits fulfilled system calculates period payments based length person contributed insurance system amount benefits based average daily salary procedure fully automated however employee institution must intervene necessary information cannot extracted databases contradictory information databases decision case involves level discretion e decision cannot definitively determined based data available human leeway deciding case main reason using system improved efficiency addition system believed achieve consistency processes every application subject discretion handled way use case predictive policing - trying anticipate crime advance fra activity background eu legal framework ai technologies used law enforcement particularly predictive policing preventing existing research tools affect fundamental rights unlawful profiling highlighted particular issues concerning discrimination among rights one recurrent concern potential predictive policing reproduce today entrench existing discriminatory practices particularly reliance future guide historical crime data may biased incomplete developing using algorithmic many crimes - domestic violence hate crime - remain largely profiling bias may introduced unreported therefore counted official police statistics step process avoid subsequent potential violations focus certain crimes violence drug related crime public fundamental rights experts places - rather business fraud non payment taxes example - officers interpreting data also make law enforcement responses less equitable clear understanding former often associated certain demographics neighbourhoods fundamental rights ultimately undermine police relations particular communities fra guide explains profiling criminological research crime 'hotspots' around several legal frameworks regulate decades - notably uk usa uses police data map certain conducting profiling crimes undertakes statistical tests explore crime probabilities various lawfully necessary comply police forces used developed address different types fundamental rights crucial crime concentrations clusters 'hotspots' effective policing border management recently adaptations area applied research used ai information see fra tool enhance effectiveness suggesting using algorithmic preventing unlawful profiling today tools could reduce police' reliance subjective human judgments future guide may reflect biases stereotypes studies also indicated predictive policing could potentially reduce unnecessary surveillance questioning physical checks searches reducing humiliation harassment individuals may occur activities predictive policing aims forecast probability crime anticipate emerging trends patterns inform crime prevention intervention strategies may also part investigation crime already taken place authoritative definition predictive policing typically characterised analysing data identify common patterns trends crime using algorithms create models based analysis used forecast criminal activity may occur future ai technologies area generally either aim 'predict' crimes 'predict' individuals either commit victims crimes tools aiming predict crimes generally fed historical data - largely official sources - time place type crimes committed complemented environmental variables population density presence certain public places services major events holidays generally use personal data applied fra activity contrast ai systems focused predicting potential perpetrators victims facial recognition crime employ historical real time personal data could include criminal records data addresses phone numbers location data data extracted technology social media information known associates health income rise data combined criminal environmental data fundamental rights eu member states shared competence area freedom considerations security justice article j tfeu includes judicial cooperation criminal matters police cooperation articles law enforcement tfeu already treaty lisbon adopted annexed declaration protection personal data judicial cooperation eu law recognises 'sensitive data' criminal matters police cooperation observed \"specific rules people' facial images protection personal data free movement data form biometric data processed fields ... police cooperation based article tfeu ... prove facial recognition software necessary specific nature fields \" images also quite easy capture public places although accuracy within framework predictive policing collection storage processing matches improving risk analysis exchange information particularly relevant processing errors remains real - particularly personal data context law enforcement operations regulated certain minority groups people whose eu level law enforcement directive directive eu sets images captured processed comprehensive standards safeguards processing including might know happening safeguarding prevention threats public security - cannot challenge possible misuses use practice fra paper outlines analyses use cases collected fra signal variety ways law fundamental rights enforcement authorities already use plan use ai related technologies challenges triggered support work public authorities deploy live frt law enforcement purposes also examples mentioned interviewees range data mining systems briefly presents steps take help designed map crime patterns detecting online hate speech making avoid rights violations risk assessments gender based violence automating certain prison information see fra guard duties use cases include detecting illicit objects satellite facial recognition technology images generally recognising objects images addition tool fundamental rights considerations mentioned research used private sector fraud prevention context law enforcement crime detection money transfers interviewees emphasised ai related technology systems used automate speed tasks previously done humans thus freeing better distributing resources mapping crime support efficient allocation investigation capacity national intelligence agency public prosecutor' office employ data driven system help employees make choices use available investigation capacity aim improve allocation human resources ensuring officers present right time place interviewees suggest system could make precise assessments compared humans often rely gut feeling decisions still system always used combination human appraisal non ai systems make operational decisions based system generated outcomes analysts create 'heat map' outlines prevalence certain crimes certain areas replicates long standing manual version crime anticipation system whereby police officers put pins map indicate specific risk areas using ai increase speed process also makes reliable users believe analyse data system based data mining machine learning processes primarily built unique police data contained crime reports witness statements suspect declarations gaps extent possible addressed using data sources criminology research social demographic information obtained national office statistics system also uses data open sources specific parameters calculation depend type crime predictive factors vary relevance across crime areas example case burglaries data burglaries collected combined data place residence known criminals distance burgled fra activity houses relevant criteria preselected allow system produce heat map detecting hate location based predictions made next six months indicate speech online time location burglary may occur result map small public agency combatting hate squares risk crime occurring indicated different shades crime uses ai based tool detect interviewees indicated visualisation helps officers analyse online hate speech analysing neighbourhoods observe correlations different locations patterns speech online basis processing system assessing risk gender based domestic violence determines social groups targeted helps law enforcement national police force uses internal system track cases gender adopt measures protect based domestic violence system helps police officers take decisions threats realised distribute resources across domestic violence cases system categorises cases basis assessed risk relapse repetition order although tool aims identify focus 'riskiest' cases potential victims rather perpetrators law enforcement specialist team could complete risk analysis without using ai however use information generated system able compute large amount data short amount system ask social media providers time assist untrained non specialist police officers risk analysis information users pursue criminal investigations case alleged gender based domestic violence reported police one particular challenge officer starts initial investigation includes collecting evidence taking understanding context witness statements - potentially - making arrest using information statements made example gathered process officer fills two detailed questionnaires journalists academics may use assess complaints evaluate probability reoffending examine words associated hate speech evolution case assess behaviour perpetrator report analyse occurrence victim police officers also indicate level gravity nature threats faced attitudes concerning victim fra plans initiate research online hate present social system produces risk 'score' three point scale police media allow fra provide officer raise level risk manually cannot lower risk level input policy developments indicated system level confirmed specific area online content moderation measures applied line established police protocols system uses ai also informs judge potentially 'severe cases' automated system examples ai use private sector use case ai health - analysing medical records save lives background eu legal framework healthcare particularly prominent discussions use ai medical data online applications potential support improved health outcomes - result - wider socio economic benefits covid pandemic increased focus interest area particularly terms potential online data applications enhance ability governments health services track spread disease health also prominent general population' views uses ai eurobarometer survey found every second european thinks ai best used improve medical diagnostics develop personalised medicine improve surgery use case covers applications ai related technologies public private sector stakeholders area medical records disease prediction feeding data electronic medical records emr electronic health records ehr ai systems related technologies support development preventative medicine recognises early risks disease designs appropriate interventions researchers predict clinical events mortality hospitalisation readmissions length stay hospital beyond disease prediction medical record data analysed predict patients' adherence treatment keeping medical appointments technologies potential support improved health outcomes well increase efficiency healthcare system article tfeu eu supporting competence protecting improving human health member states retain full responsibility defining health policies organising managing health systems delivering health services article tfeu within eu competence union action complement national policies directed towards improving public health preventing physical mental illness diseases obviating sources danger physical mental health action cover health information education well monitoring early warning combating serious cross border threats health article tfeu latter areas eu adopt incentive measures excluding harmonisation laws regulations member states rules policies adopted eu level aim ensure free movement citizens equal treatment non discrimination abroad well availability safety medical products services single market considering development technologies application health care exchange medical records patients' rights cross border situations disease prediction matter public health particularly relevant gdpr health genetic data considered special category data article called 'sensitive data' require specific protection processing could create significant risks data subjects' health genetic data shared specific circumstances article gdpr gdpr provides exemption purpose limitation principle data used research purposes line article researchers required ensure technical organisational safeguards - pseudonymisation anonymity - place using patient data eu also taken action regarding exchange medical records european commission recommendation c european electronic health record exchange format24 \"seeks facilitate cross border interoperability ehrs eu supporting members states efforts ensure citizens securely access exchange health data wherever eu \" recommendation lays technical specifications exchange data eu member states european data strategy february also strong focus health data 'common european health data space' one nine common european data spaces whose establishment european commission support early warning response system ewrs owned european commission operated european centre disease prevention control aims \"notifying eu level serious cross border threats health\" enabling \" european commission eu countries permanent communication purposes alerting assessing public health risks determining measures may required protect public health \" emr computerised medical record created patients healthcare organisation ehr contains patient' medical history beyond one organisation involve sharing data across healthcare system include large amount personal data encompass among others name contact details individual next kin demographic information diagnoses test results medication treatment may also include patient generated data wearable devices uniform emr ehr system operating across eu member states germany national emr ehr system others - including belgium denmark - different emr ehr systems regional level systems differ considerably depending data recorded access data european commission stakeholders highlighted diversity country level emr ehr systems lack interoperability major barrier digital single market health studies highlight potential ai related technologies enable earlier diagnosis widen possibilities disease prevention improve patient safety strengthening right access preventive healthcare benefit medical treatment emr ehr may also help make healthcare personalised possibility rapid sharing data facilitate coordinated timely treatment however use emr ehr presents significant data protection risks healthcare sector leads terms personal data breaches amount personal data stored highest among industries combined large data sharing network number access points makes healthcare sector attractive target hackers quality data emr ehr also raises concern studies patients shown medical files asked accuracy found information incomplete erroneous lot important data emr ehr unstructured form free text reduces data quality low levels accuracy completeness overall data quality increases risk medical error use practice applications described interviews include simple advanced models employed public private sectors largest number use cases refer image based diagnosis tools however interviewees also discussed tools automate various working procedures mapping text data filing medical records analyses measurements body tissues nerve fibres smaller number examples touched advanced projects systems monitor remotely certain health indicators heart rate case systems complement expertise health professionals next sections present examples diagnostic remote monitoring tools image based tools help detect diagnose disease tools used support detection diagnosis diseases described interviewees work similar ways example privately owned hospital uses ai system interpret images ct scans stroke patients stroke imaging used detect damage brain occurred may blockages blood supply brain also generate measures compared particular values medical specialist interviewee feels application helps determine characteristics images quickly potentially - depending uses tool - improving quality diagnosis however highlight necessarily efficient rely ai application since medical professional must present could examine image rather tool offer support - example specialist finds difficult interpret certain image find abnormalities system built trained validated using dataset partially based large scientific study hospital contributed supplemented purchasing foreign datasets algorithm trained adapted future based new data new versions released developers feel allowing system continue learn would make difficult using ai public authority responsible validate operation target health inspecting food safety standards restaurants uses machine learning private company developed algorithm supports detection breast cancer inspections process customer review data major online platforms helps mammography exams tool gives decide conduct inspections previously process probability degree certainty based complaints help radiologists speed analysis authority received previous results decide whether additional reports since introduction tests warranted algorithm detects tool rate non compliant characterises anomalies mammography restaurants identified doubled cancerous around interviewee indicates first step involves text mining algorithm identifies reviews system low rate false containing key words may negatives false positives note indicate health safety issues many cases deliver clear outcome 'sick 'nausea' 'rodents' system trained radiography second step authority mammography data europe eu compared results coming written reports past biopsies acting customer reviews previous control data inspection reports improve algorithm' accuracy reliability monitoring patients' vital statistics remotely hospital piloting system support early detection potential illness monitoring patients' health indicators - example blood pressure heart rate - typically takes place manually captures situation specific moment time constantly monitoring indicators potential identify trends doctors may otherwise recognise detect health issues early prevent illness system uses biosensor - kind plaster - gathers hemodynamic data patients continuously constantly monitoring heart pulsation respiration data used system come hospital patient data anonymised shared third party provider information besides gathered monitoring plaster used build train system data environmental factors incorporated pilot interviewee pointed could contain biases future system combine information gathered biosensor separate information patients' emr draw conclusions trends observed monitoring use case targeted advertising - profiling consumers boost profit background eu legal framework internet transformed way live many people make use internet services often offered free daily basis companies offering services free mainly generate revenue advertising adverts automatically targeted individual consumers based information availability data online individual behaviour combined machine learning technologies considerably improved ability commercial enterprises target individuals could even go far manipulating consumers predicting reactions based irrational aspects psychology reasoned choice cambridge analytica scandal underscored particularly negative impact uses political purposes case company illegally obtained personal data millions social media users target political adverts different social groups based certain psychological profiles recent declaration committee ministers council europe highlights lack knowledge manipulative power algorithms \" effects targeted use constantly expanding volumes aggregated data exercise human rights broader sense significantly beyond current notions personal data protection privacy remain understudied require serious consideration \" concerns also raised online advertising powered ai technologies affect data protection privacy consumer protection right non discrimination even way democracies work word 'advertising' associated messages designed influence consumer behaviour advertising one form another always targeted specific groups based characteristics behaviour growth social media however taken targeted advertising another level using direct access consumer data micro targeting directed towards specific groups - data gathered online activities targeted activities social media providers platforms like google amazon gather comprehensive user data monitoring various activities users advertisers access detailed specific information area targeted advertising systems recommend content e g news movies one real life examples also involves called reinforcement learning technology based optimising certain goal experimenting updating rules automatically best possible output means systems tries different ad placements trial error finds best way optimise revenue - including element self learning little knowledge actual use reinforcement learning available european countries major companies working area researching issue issues related targeted advertising fall consumer protection falls shared eu competence member states article f tfeu eu consumer protection measures seek protect health safety economic interests consumers promote right information education organise safeguard interests article tfeu eu adopt minimum harmonisation measures achieve high level consumer protection article tfeu yet allowing eu member states introduce even stringent measures nationally secondary eu legislation rules advertising covered directive ec concerning misleading comparative advertising directive provides minimum level protection misleading advertising also harmonises rules comparative advertising across eu provisions directive ec apply consumer business business business relations however practically applied latter53 directive ec unfair business consumer commercial practices internal market practices54 took effect directive ec services internal market55 covers services include advertising additionally directive ec certain legal aspects information society services particular electronic commerce internal market e commerce directive also applies directive forms part legal framework digital services eu meet significant developments area new online services practices e commerce directive currently revised part digital services act package package aims \"strengthen single market digital services foster innovation competitiveness european online environment\" fra collected information actual use cases six european companies engaged placing online ads content recommendation personalised marketing use practice examples covered include ----placing ads online based click predictions e learning likelihood online users click certain links adverts automated bidding auctions online advertisement space ----personalised targeted marketing communication via email tasks fully automated examples concern analyses user preferences activity calculations probabilities clicks purchases including measurement effectiveness previously made recommendations also includes methods targeted communication basis identified target groups build long term trust clients service providers targeted online ads based click predictions business models working click predictions targeted advertisements often follow 'click buy' policy companies purchase advertising space media platforms optimise display adverts analysing interests preferences website users showing advertisements interest purpose increase relevance advertisements shown better matching interests see present example company gets paid people click advertisement buy something additionally company uses ai detect inappropriate content advertising advertisements alcohol firearms political content company uses range machine learning techniques field computational advertising estimate probability user clicking advertisement displayed specific context optimising called click rate customers' interests relevance products measured via mapping individuals' browsing histories transaction patterns information derived individuals' navigation merchant websites worldwide advertising company works done via anonymised third party cookies trackers placed merchant websites outline individuals' navigation across also list products seen purchased profiles individuals linked devices used although ip addresses anonymised product purchased recommender system algorithm tries determine products customer could also buy case 'fresh' data valued higher older data browsing histories stored maximum one year interests change purchases older year longer necessarily considered relevant advertisements shown respective person immediately adapted accordingly vary across websites also match content latter advertisement posted continuously analysed combination elements taken account individual' interest confirmed purchase made data shared across platforms includes informing others purchase made stop advertisements particular item purchase made formula reviewed algorithm adapted individuals' continuous online behaviour future company covered example expects work optimising timing terms places advertisements within given budget certain time frame also expects focus displayed ads impact consumers another example based european online market place links buyers sellers range specialised products ai used optimise advertising campaigns categorise products based advertisements shown website market place improve search engine experience predicting complementary substitutable products detect fraud attempts company uses machine learning predict value clicks customers buy advertisement space offered real time auctions examples company indicates ai enables make decisions otherwise would possible without ai would significantly scaled targeted communication customers clients case retail company focusing specialised supplies sold across physical stores online direct marketing personalised advertising used increase appeal customers time measure efficiency particular instance marketing advertising according company issue example marketing emails opened average particularly customers recognise relevant favourite products offered marketing emails sent around registered individuals system used establish may considered relevant individuals done analysing purchases made respective individuals previous six months offers displayed directly based previous purchases meanwhile new suggestions e alternative products category previous purchases similar approach used bank sends emails clients messages offering specific services products sent certain clients data analysts calculate probability clients interested service product probability certain threshold client receive message system used yet include machine learning models fully automated points taken develop system third example grocery retailer uses loyalty cards increase customers' interaction personalise offers loyalty card systems predict many customers likely engage product offering system covered example also suggests new products customers tracks results suggestions groups buyers similar behavioural patterns segments make personalised suggestions every week company' loyalty card owners receive personalised offers email website mobile application access offers store terminals ai system selects offerings based individual purchase history recommends new items might catch buyer' interest prompt purchase endnotes see instance samoili et al ai watch defining artificial intelligence towards operational definition taxonomy artificial intelligence luxembourg karanasiou pinotsis ' study layers automated decision making emergent normative legal aspects deep learning' international review law computers technology pp see fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office p fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office june un human rights council report special rapporteur extreme poverty human rights philip alston eubanks v automating inequality hightech tools profile police punish poor st martin' press redden joanna dencik lina warne harry datafied child welfare services unpacking politics economics power policy studies panoptykon foundation profiling unemployed poland social political implications algorithmic decision making see also algorithm watch poland government scrap controversial unemployment scoring system oecd glossary statistical terms social benefits definition accessed august j henry richardson chapter iv social insurance economic financial aspects social security university toronto press pieters social security overview eu competence domain regulation ec see paju j european union social security law oxford hart publishing ch erik bakke 'predictive policing argument public transparency' new york university annual survey american law vol pp andrew g ferguson 'policing predictive policing' washington university law review vol pp example one five women experienced violence brought serious incident attention police see fra violence women eu wide survey main results report luxembourg publications office p elizabeth e joh new surveillance discretion automated suspicion big data policing uc davis legal studies research paper p braga et al hot spots policing small geographic areas effects crime campbell systematic reviews vol elizabeth e joh new surveillance discretion automated suspicion big data policing uc davis legal studies research paper pp available ssrn erik bakke 'predictive policing argument public transparency' new york university annual survey american law vol pp wim hardyns anneleen rummens 'predictive policing new tool law enforcement recent developments challenges' eur j crim policy res p doi s10610 albert meijer martijn wessels 'predictive policing review benefits drawbacks' international journal public administration p doi law society commission use algorithm justice system algorithms criminal justice system p newbold j n 'predictive policing' 'preventative policing' 'intelligence led policing' future declaration annexed final act intergovernmental conference adopted treaty lisbon signed december directive eu european parliament council april protection natural persons regard processing personal data competent authorities purposes prevention investigation detection prosecution criminal offences execution criminal penalties free movement data repealing council framework decision jha oj l pp european commission standard eurobarometer report europeans artificial intelligence p european patients forum n new eu regulation protection personal data mean patients guide patients patients' organisations commission recommendation eu february european electronic health record exchange format oj l pp digital health society exchange electronic health records across eu february communication commission european parliament council european economic social committee committee regions european strategy data com final brussels february decision eu european parliament council october serious cross border threats health repealing decision ec oj l pp see commission webpage communicable diseases oecd european union healthcare glance europe p vera ehrenstein hadi kharrazi harold lehmann casey overby taylor 'obtaining data electronic health records' gliklich leavy mb dreyer na eds tools technologies registry interoperability registries evaluating patient outcomes user' guide 3rd ed addendum use data insurance industry currently potential see example spender c bullen l altmann richer j cripps r duffy c falkous farrell horn j wigzell w yeap 'wearables internet things considerations life health insurance industry' british actuarial journal pp see visualisation see short overview different ehr systems europe nurses' perspective healtheurope world cloud based services storing health data cloud college europe transformation health care digital single market synopsis report public consultation european commission study big data public health telemedine healthcare roberta pastorino corrado de vito giuseppe migliara katrin glocker ilona binenbaum walter ricciardi stefania boccia benefits challenges big data healthcare overview european initiatives european journal public health vol issue supplement pp - ministry health welfare sport netherlands digitalization health care benefits patient safety literature web reports according multiple reports different cybersecurity companies time see example sc magazine healthcare leads cost data breaches shannon williams new report reveals 'wall shame' health care data breaches tammy lovell statistics reveal healthcare sector affected personal data breaches sc magazine healthcare leads cost data breaches annet sollie reuse sharing electronic health record data focus primary care disease coding doctoral dissertation vrije univesiteit amsterdam pp vera ehrenstein hadi kharrazi harold lehmann casey overby taylor 'obtaining data electronic health records' gliklich leavy mb dreyer na eds tools technologies registry interoperability registries evaluating patient outcomes user' guide 3rd ed addendum mowafa househ bakheet aldosari abdullah alanazi show andre kushniruk elizabeth borycki 'big data big problems healthcare perspective' studies health technology informatics p sartor giovanni new aspects challenges consumer protection study committee internal market consumer protection policy department economic scientific quality life policies european parliament luxembourg neudert lisa marchal nahema polarisation use technology political campaigns communication study request panel future science technology stoa managed scientific foresight unit within directorate general parliamentary research services eprs secretariat european parliament information commissioner' office ico investigation use data analytics political campaigns council europe declaration committee ministers manipulative capabilities algorithmic processes decl example costello roisin aine impacts adtech privacy rights rule law technology regulation - edps opinion edps opinion online manipulation personal data sartor giovanni new aspects challenges consumer protection jablonowska agnieszka et al 'consumer law artificial intelligence challenges eu consumer law policy stemming business' use artificial intelligence' eui working papers law wachter sandra 'affinity profiling discrimination association online behavioural advertising' berkeley technology law journal vol forthcoming available ssrn zuboff shoshana age surveillance capitalism london edps opinion edps opinion online manipulation personal data martin gillian importance marketing segmentation american journal business education vol kaili lambe becca ricks basics microtargeting political ads facebook see example information recsys2020 workshop reveal bandit reinforcement learning user interactions accessed august directive ec european parliament council december concerning misleading comparative advertising oj l pp european commission misleading comparative advertising directive objective directive directive ec european parliament council may concerning unfair business consumer commercial practices internal market amending council directive eec directives ec ec ec european parliament council regulation ec european parliament council 'unfair commercial practices directive' oj l pp directive ec european parliament council december concerning misleading comparative advertising oj l see european commission' webpage digital services act package fundamental rights framework applicable ai use ai - presented four use cases discussed chapter - affect specific fundamental rights outlined chapter full compliance fundamental rights prerequisite using ai driven technologies irrespective area concerned chapter introduces general fundamental rights framework eu governs use ai including selected secondary eu legislation national law section fundamental rights framework provides normative basis benchmarks design development deployment ai tools helps determine whether specific use ai fundamental rights compliant requirements justified interferences fundamental rights outlined section fundamental rights framework governing use ai cornerstone instrument eu fundamental rights framework applicable use ai charter together unwritten general principles eu law main source fundamental rights eu charter enshrines wide array fundamental rights legal value eu treaties eu institutions bodies bound charter member states act within scope eu law article charter many charter rights set european convention human rights echr meaning scope must corresponding echr rights article charter however cannot prevent union law providing extensive protection fundamental rights also found provisions treaties see e g article teu titles v x tfeu eu secondary law rights safeguarded different pieces secondary eu law central piece eu secondary law context ai general data protection regulation gdpr - regulation eu governs automated processing personal data european economic area processing personal data means form part filing system - within scope eu law result gdpr apply national security related data processing gdpr coupled law enforcement directive applies police judicial cooperation criminal matters eu instruments include numerous provisions protection personal data determining key principles data processing lawfulness fairness transparency whether eu data protection legislation applies depends whether personal data processed ai driven applications use personal data example traffic data others use anonymised data cases data protection laws apply applicability entirely clear line personal non personal data blurred risk anonymised data ' identified' - ie anonymisation undone however identification usually illegal addition persons identifying data usually put major efforts potentially need access additional information individuals might included anonymised dataset identification section discusses topic detail linked results interviews carried report addition eu data protection acquis european non discrimination law key safeguarding fundamental rights context use ai related technologies article teu provides non discrimination one fundamental values eu article tfeu requires union combat discrimination number grounds moreover articles charter provide equality law non discrimination beyond several eu non discrimination directives enshrine specific detailed provisions varying scopes application include employment equality directive ec racial equality directive ec gender goods services directive ec recast gender equality directive ec eu member states also party international human rights conventions see list conventions key findings fra opinions section contain legally binding standards safeguards comply act areas fall within scope eu competence main instrument echr ratified eu member states accompanied additional protocols great majority eu member states parties echr wide reach also applies areas covered eu law addition council europe convention protection individuals regard automatic processing personal data13 another source pan european data protection obligations binding eu member states recently modernised sector specific eu national legislation also enshrines safeguards protection fundamental rights overview technical legislation beyond scope report however chapter provides examples relevant use cases discussed report complemented couple examples national laws five eu member states covered none five eu member states covered currently horizontal ai specific laws although countries looking potential need regulation eu countries finland issued recommendations self regulation development responsibility standards private sector estonia assessment concluded separate ai specific law required foreseeable future since current legal framework sufficient according relevant estonian long term strategy however legal environment must adapted avoid unnecessary hindrances implementing ai situation concerning sectoral legislation relevant use ai different sectors varies across eu member states however active policymaking ai recently emerged national level national action plans ai appeared remain core policy development member states countries working growing entrepreneurship others focused enacting market oriented policies compatible un agenda sustainable development educational activities promote ai increasing public use ai often identified ai related strategy goals investment research development also frequently outlined relevant goal domestic ai discussions potential legislative reforms remain attentive european initiatives national sector specific fundamental rights safeguards also enacted instance finland began considering overhaul domestic human rights safeguards public sector proposing broader across board legislative update opposed individual ai laws specific reference processing personal data immigration law finnish constitutional law committee put forward proposal strengthen safeguards finnish constitution overriding constitutional law shortcomings relation among others protection law accountability well ambiguity algorithms automated decision making whenever public authorities automate decision making processes processes must adhere constitutional principle rule law may endanger observance rules good administration due process proposal articulated vision requirements finnish constitution sets ai use automated decision making within public administration research identified initiatives policies linked ai fundamental rights five member states examined example estonian e state charter includes summary citizens' rights better communicating agencies electronically also targets ai relation right know data collected public authorities similarly ministry interior netherlands presented policy brief parliament ai public values fundamental rights brief stresses human centric approach ai applications strong influence human beings society whole also lists important risks ai fundamental rights discrimination result biased data reduced interpersonal relations ai takes certain forms interaction 'use case' examples social welfare use case regulating social welfare eu member states enacted rules aiming protect fundamental rights specifically area addition existing horizontal eu regulations see section mostly define rules processing protection personal data purpose social benefits insurance estonia example insurance activities act applicable types forms insurance regulates processing transmission personal data context states public authorities health care providers insurance undertakings third parties may transmit personal data request insurance undertaking personal health court data necessary insurance undertaking perform insurance contract right obligation disclose data derives law scope act also includes data transfers purpose data processing within ai systems social welfare act contains specific provisions data protection persons need social assistance notified processing data provide consent processing person established target group right opt data processing social welfare act also allows local authorities process including using algorithms personal data youth years age stored state registries identify youth employment education training finland act secondary use health social data applies using ai social care healthcare act based norms securing protecting sensitive personal data outlined gdpr aims establish conditions effective secure \"processing access personal health social data certain secondary purposes research statistics innovation development knowledge management teaching authority planning \" act regulates manner registered health data cannot processed several laws apply various types social benefits france code relations public administration applies purpose processing accessing personal data related social benefits minor amendments entry force gdpr code states \" algorithms used public administrations must published\" \" person subject automated decision making right informed\" predictive policing use case context predictive policing eu' law enforcement directive contains key fundamental rights safeguards stipulate law enforcement authorities apply main data protection principles set gdpr include requirement data controllers e competent law enforcement authorities provide data subjects information controller' data processing activities identity contact details data controller purposes processing information right lodge complaint article specific cases data controllers shall provide information - example legal basis processing - enable data subjects exercise rights right access article requires data controller confirm upon request data subject whether processing operations related case data subject shall able access data also request additional information including purposes legal basis processing categories personal data processed right information right access restricted number cases including avoid obstructing prejudicing prevention detection investigation prosecution criminal offences protect public security national security addition article law enforcement directive explicitly prohibits automated decision making prohibition limited authorised eu national law safeguards data subject' rights including \" least right obtain human intervention part controller\" see section cases scope implementing national legislation broader directive example finnish act processing personal data criminal matters connection maintaining national security strengthens right information distinguishing information provided general special circumstances healthcare use case regards eu level fundamental rights safeguards using ai healthcare gdpr empowers patients rights informed part granting control personal health data data qualifies 'sensitive data' found example medical records rights include rights access one' personal health data object processing personal data rectification erasure data well rights case breach gdpr administrative fines breaches processing data including health data allowed however estonia instance domestic law allows maximum penalty eur application misdemeanour procedure cases data protection inspectorate also impose similar fines misdemeanour procedure france data protection act public health code impose stricter requirements set gdpr regarding health data processing french data protection act amended law modernisation health system allow processing personal health data various purposes provided fall within scope one exceptions general principle prohibition sensitive data processing article gdpr targeted advertising use case considering fundamental rights safeguards relation targeted advertising underlying mechanisms regarding profiling particular eu legal framework privacy data protection provides relevant fundamental rights provisions protection privacy personal data holds status takes precedence economic benefits hence rules processing special categories personal data relevant companies operating area applying targeted advertising place companies certain obligations main legal provisions setting rules protecting personal data eu gdpr directive privacy electronic communications e privacy directive lex specialis gdpr gdpr directly applicable eu member states whenever company based eu processes personal data company based outside eu processes data relating individuals eu e privacy directive strong focus fundamental rights concerns processing personal data protection privacy electronic communications sector e g individuals use computer smartphone tablet european commission proposed e privacy regulation would replace current e privacy directive legislative proposal would broaden scope directive include specific provisions concerning unsolicited marketing cookies confidentiality requirements justified interferences fundamental rights chapter highlights selected fundamental rights - covered charter - particularly affected ai taking account four use cases discussed chapter rights absolute rights subject limitations line article charter accordingly analysing extent different fundamental rights impacted use ai section presents general steps need followed determine whether charter right limited fundamental rights affected ai absolute subject limitations interferences fundamental rights justified respect requirements charter echr case charter rights corresponding rights guaranteed echr article charter pursuant article charter limitation fundamental rights must ---- provided law ----genuinely meet objectives general interest recognised union need protect rights freedoms others ----respect essence right ---- necessary ---- proportionate court justice eu cjeu also emphasised limitation exercise rights freedoms recognised charter must respect \" essence\" rights freedoms means fundamental rights limited certain extent completely disregarded established inalienable essential core right violated measure next step conduct necessity proportionality test outlined charter respect non core aspects right interference charter right needs examined whether given legitimate aim could obtained means interfere less right guaranteed similar requirements also imposed echr interpreted european court human rights ecthr include 'essence right' concept derived object purpose echr whole respect use new technologies ecthr observed marper v uk states \"strike right balance\" protecting fundamental rights developing new technologies given wide range applications ai systems everyday life presented four selected use cases wide range fundamental rights may assessed taking account variety elements depending context particular area use notably specific purpose ai used functionality complexity scale deployed relevant assessing fundamental rights implications endnotes see also van veen c 'artificial intelligence ' human rights got ' data society points - blog data society research institute may barfield w pagallo u advanced introduction law artificial intelligence cheltenham northhampton edward elgar pp see also cjeu aklagaren v hans akerberg fransson gc february paras european convention protection human rights fundamental freedoms amended protocols nos november ets overview application charter see fra 2018a applying charter fundamental rights european union law policy making national level luxembourg publications office regulation eu european parliament council april protection natural persons regard processing personal data free movement data repealing directive ec general data protection regulation oj l pp see fra handbook european data protection law edition luxembourg publications office see example hacker p legal framework ai training data law innovation technology forthcoming available ssrn overview european non discrimination law see fra handbook european non discrimination law edition luxembourg publications office council directive ec november establishing general framework equal treatment employment occupation oj l pp council directive ec june implementing principle equal treatment persons irrespective racial ethnic origin oj l pp council directive ec december implementing principle equal treatment men women access supply goods services oj l pp directive ec european parliament council july implementation principle equal opportunities equal treatment men women matters employment occupation recast oj l pp convention protection individuals regard automatic processing personal data strasbourg january ets protocol amending convention protection individuals regard automatic processing personal data strasbourg october cets ai finland project' ethics working group ethics challenge added emphasis companies self regulation ai finland 'etiikkahaaste ethics challenge ' tekoaly uusi sahko finnish republic estonia report estonia' ai taskforce p estonian government launched preparation long term strategy example see netherlands ministry economic affairs climate policy strategic action ai strategic action plan ai strategisch actieplan ai - sapai example effort adapt goals development sustainable market see spain ministry science innovation universities national ai strategy spanish comprehensive overview see european commission national strategies artificial intelligence oecd ai policy observatory finnish constitutional law committee 'committee opinion pevl vp - vp draft proposal parliament law processing personal data immigration administration related laws' estonia national audit office chancellor justice everyone' rights e state e state charter netherlands ministry interior kingdom relations ai public values fundamental rights dutch elina saxlin hautamaki johanna lilja secondary use health data - new finnish act de donno french code \"des relations entre le public et l'administration\" new european era administrative procedure italian journal public law pp see fra preventing unlawful profiling today future guide luxembourg publications office tables sajfert j quintel data protection directive eu police criminal justice authorities available ssrn note article law enforcement directive seems apply automated decisions taken solely automated processing means safeguard apply human agency involved orla lynskey criminal justice profiling eu data protection law precarious protection predictive policing p english translation available via finlex website gdpr recital art european patients forum n new eu regulation protection personal data mean patients guide patients patients' organisations gdpr arts white case gdpr guide national implementation estonia merav griguer processing health data france look gdpr european commission proposal regulation european parliament council concerning respect private life protection personal data electronic communications repealing directive ec regulation privacy electronic communications com final brussels charter art \" far charter contains rights correspond rights guaranteed convention protection human rights fundamental freedoms meaning scope rights shall laid said convention \" also reiterated explained cjeu see example c satakunnan markkinaporssi satamedia december para joined cases c c volker und markus schecke eifert gbr hartmut eifert november para joined cases c c digital rights ireland ltd v minister communications marine natural resources others karntner landesregierung others april para c maximillian schrems v data protection commissioner october para c webmindlicenses kft v nemzeti ado es vamhivatal kiemelt ado es vam foigazgatosag december paras see cjeu c maximillian schrems v data protection commissioner october paras refer article charter see also scheinin martin sorell tom surveille deliverable d4 - synthesis report wp4 merging ethics law analysis discussing outcomes april p see e g brkan ' essence fundamental rights privacy data protection finding way maze cjeu' constitutional reasoning' german law journal p lenaerts k 'limits limitations essence fundamental rights eu' german law journal pp cjeu joined cases c c digital rights ireland ltd v minister communications marine natural resources others karntner landesregierung others april see instance khelili v switzerland october ecthr marper v united kingdom gc nos december ecthr k v finland july ecthr z v finland february ecthr huvig v france april ecthr leander v sweden march scheinin martin sorell tom surveille deliverable d4 - synthesis report wp4 merging ethics law analysis discussing outcomes april p ecthr marper v united kingdom gc nos december para see also council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems appendix para impact current use ai selected fundamental rights deploying ai systems engages wide range fundamental rights seen chapter use cases presented report involve range technologies varying levels complexity automation different phases development applied different contexts different purposes different scale rights affected depend factors number horizontal sector specific fundamental issues emerge chapter begins general overview risks perceived interviewees general awareness fundamental rights implications using ai chapter highlights selected fundamental rights affected ai related technologies reference four use cases analysed analysis takes account presents views practices awareness issues expressed interviews conducted report interviewees first asked general risks see using ai asked general fundamental rights awareness using ai concrete fundamental rights implications mostly linked data protection non discrimination availability complaints mechanisms perceived risks important recognise many issues cut across different rights example potentially biased decision made algorithm could involve right non discrimination protection personal data right effective remedy similarly particular issue seen perspective different rights instance good explanation decision made algorithm required right protection personal data right good administration right effective remedy fair trial asked general risks using ai interviewees always mention fundamental rights main risks although highlighted related topics private sector representatives often mentioned inaccuracy risk using ai followed potential bias proper legal basis processing personal data one respondent international retail company stated one business risk linked european customers extremely knowledgeable rights namely people hesitate ask data storage automated decision making customers properly informed might complain company may lose client addition interviewee continued breaching law possible fines linked breach another major business risk respect public administration bias often highlighted risk associated using ai addition public authorities often discussed inaccuracy data identification risks using ai example interviewees working social benefits algorithms stated incorrect results general risk occur potentially due rare cases well identified algorithm due errors input data also highlighted difficulties associated moving testing deploying system including technical challenges resources required potential different results deployed respondents working targeted advertising also highlighted business risks - example offering irrelevant inappropriate content one respondent mentioned potentially losing control automated systems addition interviewees indicate challenges linked difficulty interpreting results outputs ai systems one interviewee consultancy sector fears risk related lack absence sufficient ai knowledge understanding cause ongoing projects halted due company' inability explain clearly algorithms perform purpose another interviewee law enforcement sector looking possible use ai support decisions licence applications explains inherent risks system proposes certain response example potentially using ai support decisions license applications firearms respondent asserts would critical understand reasoning behind negative decisions also positive decisions several interviews showed major concern assign properly trained staff sufficient expertise trace explain interact ai system finding also corroborated results european commission survey among companies eu survey indicate obstacle adopting ai technologies difficulty hire new staff right skills mention complexity algorithms obstacle respect ability explain decisions based algorithms interviewee working public administration mentioned alternatives completely transparent making decisions room doubt similar vein respondent working area health private sector mentions 'self learning' algorithms forbidden area work fixed algorithms traced \" use ai bring many benefits also risks like risks reported without providing much additional information include nuclear energy \" cyber security data quality excessive monitoring people due use interviewee working private sector data algorithms job loss due automation profiling spain general awareness fundamental rights legal frameworks ai context everyone eu aware fundamental rights fra' fundamental rights survey shows slightly every second person eu aged older heard charter slightly people two three heard echr universal \" use ai impact declaration human rights might echr older human rights way terms established people' common knowledge decision process matter whether decision made majority people interviewed project acknowledge using machine human \" ai generally affect fundamental rights mention interviewee working public use ai potential impact fundamental rights administration estonia aware implications responses influenced different ways use ai also understanding fundamental rights example one respondent working production pension forecasts based machine learning says producing statistics impact fundamental rights apart data protection issues need addressed another respondent working social benefits algorithms argues impact depends \" widely human rights defined\" - example right receive correct pension \" rights related data none interviewees working targeted advertising believe protection ensured see use ai affects fundamental rights negatively one respondent working human rights relevance targeted communication customers stated one reason \" response relates lack knowledge exactly fundamental private company spain rights practically interviewees showed awareness rights privacy \" touch topic data protection well non discrimination rights assume human dignity right fair trial effective remedy also human rights issues involved mentioned albeit briefly activities within legal framework activities closer look interviewees' responses indicates diverging views across compliant data protection respondent groups respondents working private companies discuss good practices therefore data protection non discrimination rarely mention rights assume human challenges company working targeted advertising mentions rights issues related attentive issues linked freedom speech right information systems \" sense company promotes rights public administration spain posting adverts helps news websites obtain funding continue work one interviewee notes range rights awareness much broader among public sector representatives working ai referred rights human dignity presumption innocence working ai systems different fields application also highlight use systems also covered sector specific laws example system making decisions unemployment benefits regulated national legislation unemployment insurance administrative procedures data protection however respondents aware legal standards apply use ai unsure absence ai specific regulation several respondents mention ethics guidelines certification schemes work existing guidelines standards necessarily specifically aimed ai case example security system 'iske' estonia3 area financial services payment card industry data security standard respondents also refer standards developed international organization standardization iso institute electrical electronics engineers ieee european committee standardization cen respondent working targeted advertising argues certification needed field posting ads issues linked health sector work banks several interviewees noted \" think organisations developing internal guidelines regulate specific technology like ai sufficient general respondents mention guidelines developed eu principles technology neutral international level guidelines european commission' rules \" private sector estonia high level expert group ai oecd' guidelines unesco standards \"yes codes yes aware ongoing developments eu council europe level procedures codes procedures date refer need update sector specific regulations able using something innovate ai - example area health yet one interviewee created analog world states existing standards sufficient ai need digital world \" regulated separately private sector spain human dignity using ai driven technologies broadly implicates duty respect human dignity foundation fundamental rights guaranteed charter article charter states human dignity inviolable must respected protected times cjeu confirmed case law fundamental right dignity part eu law ai driven processing personal data must carried manner respects human dignity puts human centre discussions actions related ai rather technology 'human ' creating affected new technology needs focus taking human dignity starting point help ensure use ai benefits everyone - example supporting ageing access healthcare dignified manner use ai also risks infringing closely connected charter rights right life article right integrity person article context important consider avoid harmful use ai prevent violations rights example comes use ai people engaging criminal activities ai used weapons apart extreme cases preserving dignity includes avoiding subjecting people ai without knowledge informed consent strongly linked privacy data protection example people' applications social benefits decided upon use ai people need made aware consent use automated decisions taken give another example certain proportion population feel comfortable subjected biometric identification systems hence using without allowing opt could potentially violate dignity respondents public administration referred right dignity discussing fundamental rights one respondent considering use ai prisons mentions particular context first needs assessed whether risk violating fundamental rights would high right human dignity interviewees made general references right without discussing relation concrete use ai right privacy data protection - selected challenges right respect private life protection personal data articles charter core fundamental rights discussions around use ai closely related rights respect private life protection personal data distinct self standing rights described \"classic\" right protection privacy \"modern\" right right data protection strive protect similar values e autonomy human dignity individuals granting personal sphere freely develop personalities think shape opinions thus form essential prerequisite exercise fundamental rights freedom thought conscience religion article charter freedom expression information article charter freedom assembly association article charter given two rights absolute rights subject limitations however interference needs adequately justified11 cannot compromise essential inalienable core right explained section concept \"private life\" \"privacy\" complex broad susceptible exhaustive definition covers physical psychological integrity person therefore embrace multiple aspects person' physical social identity also zone interaction person others even public context may fall within scope \"privacy\" contexts ecthr used concept \"reasonable expectation privacy\" - referring extent people expect privacy public spaces without subjected surveillance - one factors albeit necessarily conclusive one decide violation right respect private life relevance scope application however appears limited similarly according un human rights committee mere fact participants assemblies public mean privacy cannot infringed applies monitoring social media glean information participation peaceful assemblies widespread use ai technologies may technologies continue develop raise unchartered issues novel concerns right respect private life ai driven technologies may change way think privacy algorithmic tools predict reveal information people' behaviour unprecedented ways - without people even realizing giving away information personal data obtained internet may instance used targeted advertising raising many fundamental rights concerns issues linked personal data sharing via smart phone apps particularly raises significant concerns including variety potential harmful effects manipulation exploitation vulnerabilities discrimination security issues fraud e g identity theft reduced trust digital economy using ai driven technologies often implies computerised processing large amounts personal data constitutes interference right protection personal data set article charter embodying pre existing eu data protection law well right private life article charter article echr awareness data protection issues use personal data eu people heard gdpr contrast virtually \" little anxious interviewees aware gdpr discussed data protection issues gdpr implemented data protection rules deriving gdpr national law clearly end meant managing datasets well known applied rights area ai fundamental access rights ... good rights less known reminder everything done \" discussing legal framework governing use ai public administration finland respondents mentioned data protection rules well sectoral laws clearly say legal framework apart data protection laws interviewee working spanish public \"actually ' concerned administration notes \" rely data protection regulation norms gdpr might hinder ai research ' available moment\" afraid large databases used previously cannot one interviewee reflecting image based diagnostic tool expressed used research anymore \" view gdpr could hinder research hospital using tool private company netherlands support diagnosis strokes clear rules data protection interviewee indicated although know whether data protection certification requested \" gdpr give specific rules gives others referred general data protection guidelines indicated principles comes aware documents ethical issues interpretation \" private company estonia respondents working target advertising aware privacy data protection issues although responsible data protection issues companies aware efforts protect data privacy one interviewee mentioned contrary earlier years personal data stored much securely handled care attention given properly handling consent data processing consequence high level awareness data protection privacy issues linked ai use however data protection law applies personal data processed example using anonymised data develop ai tools e training data likely permissible many instances would trigger gdpr research shows data often de anonymised however efforts often require expert knowledge potentially additional information illegal illegality de anonymisation necessarily preclude applicability gdpr important consider identification anonymised data reasonably likely anonymising data one aspect protecting privacy data subjects assessing risks identification aspects also important consider disseminating anonymised data include use data purpose outputs produced interviews respondents always entirely clear use personal data often superficially described data used mentioned chapter several instances interviewees indicated use non personal data anonymised data arguing data protection relevant cases example semi public organisation working environmental management uses aggregated data water consumption machine learning based predictions water consumption data available individual level interviewees said use personal data although data originally stem individuals tool supporting restaurant inspection collecting data online sources use personal data - interviewee indicated however indicated need careful mining data online even publicly available might include personal data usernames \" would great retrieve another example insurance company using chatbot make client data another service contact effective data used train system chat protocols client ' repeat conversation logs linked personal data however line go example linking data personal data might possible reusing data \" future according respondent public administration finland companies working targeted online advertising indicate using pseudo anonymised data done example excluding names social security keys encrypting data identity consumers relevant company interviewee mentioned indicate use non personal anonymised data others possible data used make predictions decisions specific individuals example interviewee company working credit rating mentioned need know identity consumers assessments case even important right forgotten according interviewee exhaustive discussion data protection issues possible report however two aspects clearly emerged interviews automated decision making linked right human review right obtain meaningful information decisions automated automated decision making article gdpr article law enforcement directive generally forbid automated decision making meaning \"decision based solely automated processing including profiling produces legal effects concerning similarly significantly affects \" article gdpr explicit consent needed decisions solely automated legal similarly significant effect people automated decision making authorised law authorisation union national law sole precondition law enforcement directive article processing decision considered fully automated instruments require human review controller however concept 'automated' decision making elusive requires discussion research example cases human intervention might limited 'signing ' outcomes ai system rendering virtually automated importantly human review must mean human signing recommendations outputs algorithm must done someone \"authority competence change decision\" considering relevant data hand humans review potentially override outcomes system must also evaluated research indicates humans overrule outcomes algorithms mainly result algorithm line stereotypes behaviour threatens possible added value automated processing potentially accurate even fairer humans may also put minority groups disadvantage therefore also relevant non discrimination issues discussed overall disagreement exact scope provisions eu data protection acquis whether impose general ban certain types automated decisions provide data subjects rights context certain types ai driven decision making using algorithms area social benefits health predictive policing clearly potential legal significant consequences interviews suggest working areas well aware concept human review decisions taken support ai many interviewees indicate automated decisions taken one exception automation unemployment benefits based national law fully automated decisions involve discretion another example another country positive decisions based pre defined rules automated student benefits case negative decisions made humans cases refer rule based decisions involving use statistics machine learning another respondent testing use ai systems including machine learning area social benefits mentions equality could negatively impacted automation makes human behaviour visible including existing biased practices makes precautions necessary consequence organisation would allow decisions made humans interviewees working health highlighted risks linked automation decisions interviewee discussing tool support stroke diagnosis feels important rely system avoid risk automation confirmation bias caution early positive experiences application could prompt users rely easily devote less attention assessment images interviewees raised similar concerns one interviewee discussing tool analyses images provide probability presence certain type lesion notes technology supports diagnosis simple cases expertise doctors particularly important - trusted - complex cases targeted advertising often considered significant effect people however may case example individual' vulnerabilities used successful advertising considering vulnerabilities particularly important people disadvantaged groups may aware opt direct marketing see box right say decisions automated absence case law area information research needed identify impact automated decisions e advertisement delivered answering questions challenging targeted advertising based highly complex technology scale eurobarometer survey asked people eu aware right awareness opt direct marketing overall eu citizens heard right right opt exercised people exercise right aware direct - becomes even important direct marketing made much marketing efficient machine learning among general awareness levels strongly vary across eu percentage people know population right opt direct marketing ranges bulgaria netherlands figure shows percentages also highlights - based fra' analysis eurobarometer data - strong variation within countries broken regions regions fewer one four heard right areas higher shares people risk poverty indicates general problem people disadvantaged society tend less aware right data show people working often struggle pay bills living rural areas older less aware right figure awareness gdpr right opt direct marketing eu united kingdom country region fi- se- ee- insufficient data available lv- dk- uk- lt- ie- nl- pl- de- - lu- cz- sk- - hu- fr- si- ro- hr- - bg- pt- es- el- mt- cy- note map show non eu countries uk light shading aware right dark shading less aware right results regions within countries represented light grey spaces excluded fewer respondents meaning numbers observations low reliable results n question \" general data protection regulation gdpr guarantees number rights heard following rights ... right object receiving direct marketing \" source fra calculations presentation based european commission eurobarometer experiences use cases general interviewed experts highlighted data protection law difficult interpret lacks clarity comes meaning automated decision making one expert france felt automated decision making difficult explain automated decision making banned meaning exceptions gdpr allow automated decision making removed pointed ai used decision support tool another expert independent lawyer netherlands views current laws standards sufficient says need concretised per sector particularly expert mentions scope existing rules permissible automated decision making clear remains unclear comprehensive assessment 'human loop' means also raised relation syri case remained unclear extent decisions reviewed another expert working supervisory authority generally sees need adapting data protection laws \" legislation quite comprehensive organisation supervision thereof also political behind \" concerns reflect findings research also raise serious \" risk much issues concerning right human review example responsible trust machine \" officers questioned results algorithmic system built profile public administration france unemployed people poland less one percent cases essentially makes supporting tool automated decision making tool \" huge tension surrounding linked question reviewing decisions outputs ai systems gdpr want well challenge clear lack knowledge ai works interviewees might fact worse often could explain detail system use works interpretation data data uses due lack knowledge lack transparency turns impossible \" meaningful information logic involved explaining outcomes public administration netherlands algorithms essential several fundamental rights crucial processing personal data also ensuring algorithms fair discriminate also necessary enable \" explain model people properly challenge decisions ai systems ' able model statistical explainable \" one interviewee working public administration explains complexity public administration france differs depending tasks licence administration systems relatively straightforward crime prevention analysis uses data sources makes harder understand another interviewee working law \"internally explain enforcement says current ai used police organisations yet decisions machine learning complex would make explanations difficult might models several means case future \" private sector estonia respondent working financial data transactions indicates traditional models straightforward understand however new methodologies difficult explain company invest resources \" systems black making models explainable still level explainability boxes information processes required gdpr clear respondent already take step forward defence human rights \" public administration spain \" strongly attached idea ai explainable \" public administration france people aware right say decisions awareness automated evidence suggests eurobarometer survey showed europeans right know data protection rights say decisions fra' analysis eurobarometer survey shows figure drops considerably automated among people lower socio economic status eu citizens report struggling pay bills time know right lack rights awareness among socially disadvantaged could contribute social exclusion already disadvantaged less aware challenge automated decisions see figure gender differences small yet women even less aware right women men older people considerably less aware among aged older figure awareness right say decisions automated age gender difficulty paying bills years older years age years years women gender men refusal difficulty paying bills almost never never time time time total notes n question \" general data protection regulation gdpr guarantees number rights heard following rights ... right say decisions automated e g algorithm decides granted loan \" source fra calculations presentation based european commission eurobarometer equality non discrimination equality law non discrimination enshrined articles charter discrimination \" one person treated less favourably another would treated comparable situation\" based perceived real personal characteristic28 called 'protected grounds characteristics' article charter prohibits discrimination based ground sex race colour ethnic social origin genetic features language religion belief political opinion membership national minority property birth disability age sexual orientation charter prohibition reflects corresponding rights echr article protocol echr article even broader establishes non exhaustive open list extending protection wide range new grounds unlike article echr charter right non discrimination freestanding right applies situations need covered charter provision main challenges discrimination crucial topic comes use ai purpose machine learning algorithms categorise classify separate one interviewed expert points making differences per se bad thing according expert deciding grant loan credit history used differentiate individuals basis protected attributes gender religion however many personal attributes life experiences often strongly correlated protected attributes credit history might systematically different men women due differences earnings job histories interviewees often mention efficiency main purpose using ai related technologies yet important note cannot justify unfair differential treatment often protected attributes might highly correlated risks example differences life situations among men women might often linked different insurance risks however acceptable test achats ruling shows case cjeu put end gender discrimination insurance pricing certain circumstances areas using algorithms could positively contribute reducing bias stereotyping algorithmic data analysis may produce results could dispel prejudicial attitudes example predictive policing might contexts lead equitable non discriminatory policing reducing reliance subjective human judgments predictive techniques may used identify called 'white collar crimes' financial crimes historically policed nevertheless direct indirect discrimination34 use algorithms involve big data considered one pressing challenges use ai driven technologies bias discrimination including gender based discrimination data supported algorithmic decision making occur several reasons many levels ai systems difficult detect mitigate often quality data biases within source potential discrimination unfair treatment discriminatory effects generated certain groups practice difficult individuals challenge far limited number court cases dealt discrimination relating ai systems first instance decision divisional court cardiff dismissed claim uk court concerning lawfulness south wales police' use \"afr locate\" face appeal police recognition system court appeal overturned decision use facial recognition found facial recognition programme used police unlawful court violates appeal ruled \" much discretion currently left individual police officers\" added \" clear placed watch list clear human rights criteria determining technology deployed\" court also held police sufficiently investigate software use exhibited race gender bias judgment first merit specifically matter europe considerably narrows scope permissible law enforcement agencies need fully comply human rights law uk court appeal r bridges v cc south wales ewca civ august ars technica 'police use facial recognition violates human rights uk court rules' august studies highlighted potential discrimination prompted use ai systems across areas covered report area predictive policing example particular risk relates potential automated decision making tools reproduce entrench existing discriminatory practices undermine equality law article charter historical crime data underpins predictive policing may biased reflecting inherent data gaps e g chronic underreporting certain types crime alongside issues data recorded e g human error also bias individual officers crime victimisation surveys consistently show large proportion crime never reported police public - particularly crimes involving physical sexual violence hate crimes example fra' survey violence women - respondents - showed one five women experienced violence partner anyone else brought serious incident attention police fra' eu midis ii survey respondents across eu showed three ten reported incidents racially motivated hate crime police organisation compared violent crime hate crime property crime - burglary - higher rate reporting police particularly developed countries may requirement claiming insurance policy sum - relying official crime statistics based reported crime looking develop ai models field predictive policing particularly problematic comes specific crimes specific groups variables used ai modelling proxies race ethnicity gender protected categories complexity algorithms makes harder identify remove biases instead providing objective analysis predictive policing software may turn 'echo chamber' cementing existing systemic flaws injustices 'stamp' appears scientific legitimacy use predictive policing may also make law enforcement responses less equitable focusing certain crimes areas predictive policing currently focused property crimes theft burglaries often associated certain demographics neighbourhoods result certain demographics neighbourhoods - individuals living - stigmatised meanwhile white collar crime - typically committed different demographics - less prioritised patterns policing - whereby certain neighbourhoods communities disproportionately policed - predates use ai however 'promise' ai 'objective' turn used counteract discriminatory policing needs verified practice oxford university researcher sandra wachter highlights discrimination may occur due information linked protected attributes targeted advertising newly created profiles purpose advertising might amount indirect discrimination potentially even require new characteristics added non discrimination legislation extend scope expanded areas experiences use cases \" want machine many interviewees noted use ai general discriminate discriminate basis sex systems working many indicated belief put variable sex excluding information protected attributes sufficient protection easy make examples discrimination however discrimination occur due information symmetrical notice sex contained datasets may indicate protected attributes traces certain relevance \" protected groups often hidden information public administration spain example public authority uses ai tax customs shows challenges linked identifying possible bias potential discrimination using algorithms scrutinising algorithms public administration body found higher degree errors tax declarations among recently issued national identification numbers almost always attributed immigrants prompted research correlation turned outputs people recent identification numbers often contained errors never filed taxes know also case non migrants also example proxy information parts number could indicate immigrant status another interviewee working potential use ai detecting benefits fraud mentioned respect \" want prevent discrimination based ethnicity instance suffice remove 'ethnicity label' neighbourhood composition often also determined ethnicity ethnicity plays role preventing discrimination often goes beyond 'direct' characteristics\" \" f access even respondents aware general potential sensitive personal data discrimination using ai often ruled system impossible check discriminates people based protected characteristics profiling basis \" respondents also believe tools positive impact terms public administration netherlands non discrimination one respondent testing ai social benefits decisions regrets able use ai data protection reasons even though respondent' view automation could process big datasets effectively without discrimination noting protection personal data needs observed respondent feels hinders prompt decision making non discrimination - \" automated automated\" respondents clear sure whether use ai could discriminate respondents repeatedly stated system cannot discriminate include data protected characteristics example several interviewees working predictive policing law enforcement felt potential discrimination ai systems use data return outcomes related protected grounds system aim identify people others working predictive policing felt discrimination could occur particular issues training data relation predictive policing 'heat map' case example one interviewee noted - dataset never fully neutral representative complete - strong risk bias possible discrimination towards particular groups identified sharing datasets increase amount data available one way mitigate risk felt impeded data protection regulations also indicated multi level teams task travel different police authorities check quality systems used set area targeted advertising interviewees mentioned discrimination potential problem mainly asked directly overall respondents think systems discriminate three respondents mention information gender age used consequently discrimination respect occur another interviewee sure information included \" discrimination ' complicated respondent working breast cancer detection tool highlighted diseases age gender ethnicity relevant factors population groups present certain ethnic groups likely develop certain types cancer respondents working predictions take account health highlighted potential discrimination also linked sexual ethnic genetic character uses system suggesting could become greater challenge discriminatory violation system used non medical staff human rights \" private sector france different related example comes respondent working credit rating private company selling credit scores individuals created algorithm company uses information gender age citizenship credit risk models information impact outcome credit scores example younger people non citizens higher credit risk score influence demographics much smaller compared credit history data according interviewee system \"certainly impact right non discrimination make decisions sell data data analytics creditors monitor discriminate\" another interviewee working data strategy financial institution private sector using ai analyse financial transactions clearly mentions challenges understanding non discrimination constitutes work interviewee mentions example clear extent illegal exclude older people receiving credit life expectancy expected lower mortgage repayment period asked findings point uncertainty ambiguity financial sector respect article charter - non discrimination - translates real life situations vulnerable groups much discussion research discrimination using ai linked biased results respect ethnic origin gender extent age although important analyse potential discrimination groups charter covers several grounds discrimination less often part discussions research grounds include example political opinion sexual orientation disability charter provides particular rights special groups beyond articles including rights child article rights elderly article rights persons disabilities article question age - respect older age groups younger adults - came interviews notably comes insurance credit see however none interviewees experts directly mentioned rights child might linked extent nature use cases investigated clearly reflects fact topic high agenda many working ai article charter emphasises best interests child must primary consideration activities public authorities private actors concern children applies course - equally - field ai two respondents public administration mentioned possible use ai area child custody distribution children schools address consideration rights child respondents wish go detail concerning use cases - potentially reflecting sensitivity topic finally issues linked integration people disabilities raised interviews eurobarometer survey included questions ai asked respondents areas awareness mostly concerned comes use ai including discrimination among general decision making unclear responsibility nobody complain population potential around eu citizens indicated concerned using ai could ai lead lead discrimination terms age gender race nationality - example taking decisions recruitment credit worthiness etc discrimination results vary across countries higher proportions people concerned discrimination netherlands luxembourg sweden lower proportions expressed concern estonia hungary lithuania see figure however question clear people know discrimination happen aware happen think problem figure awareness risks discrimination using ai country eu nl uk lu se fr el si de es cy ie hr dk cz fi bg pt sk mt lv ro pl ee hu lt notes includes people indicated concerned ai could lead discrimination among three possible issues three issues source fra calculations based european commission eurobarometer tackling gender charter stipulates equality inequality women men must ensured areas including design employment work pay article gender discrimination use ai major concern comes design use ai related technologies development side european economic social committee notes development ai taking place within homogenous environment principally consisting young white men results cultural gender disparities embedded ai technologies example training data prone manipulation may biased reflect cultural gender prejudices preferences contain errors see also european also reflected research commission white despite efforts achieve paper artificial intelligence - gender balance majority european approach interviewees men excellence trust com final disparities design brussels february deployment stage linked p systematic disadvantages affecting european economic women labour market social committee potential lack awareness artificial intelligence - gender biases recent study consequences showed increased use artificial intelligence industrial robots could widen digital single gender gap despite market production genders benefitting increased consumption automation analysis indicated employment society men medium high initiative opinion skill occupations would benefit may jo c p disproportionally aksoy c ozcan b looking ahead using data philipp j algorithms could help better robots gender mainstream gender equality pay gap europe iza discussion paper policies processes paying attention gendered datasets drawing discussions around see webpage data feminism gender inequalities use datasociety' website data 'data feminism' could help raise awareness criado perez c invisible male point view women exposing data taken default view bias world designed also finds way men london datasets access justice right effective remedy tribunal fair trial article charter one often used charter right legal proceedings highlights importance upholding fundamental rights rule law right horizontal character empowers individuals challenge measure affecting right conferred eu law respect guaranteed charter cjeu underlined article charter constitutes reaffirmation principle effective judicial protection characteristics remedy must determined manner consistent principle right effective remedy also covers decisions taken support ai technologies eu data protection law reconfirms right effective judicial remedy must provided relation decisions controller processor53 well supervisory authority data processed ai driven technologies exception crucial note possibility lodge administrative complaint supervisory authority provided gdpr law enforcement directive55 considered effective judicial remedy article charter court involved review judicial review always remain available accessible internal alternative dispute settlement mechanisms prove insufficient person concerned opts judicial review using ai challenge right effective remedy different ways one prominent concern lack transparency use operation new technologies algorithmic decision making notoriously opaque data collection algorithm training selection data modelling profiling situation around individual consent effectiveness error rates algorithm aspects often transparently reported without access information individuals may able defend assign responsibility decisions affecting appeal decision negatively affecting fair trial includes principle equality arms adversarial proceedings established ecthr requirements also form part corresponding charter right article view article charter main challenges issues reflected specific challenges right effective remedy fair trial interviewed experts outlined generally experts indicate difference accessing remedies private companies public administration public authorities often forced transparent use ai meanwhile companies appear secretive assessment several experts suggests however expert netherlands said people might readily complain companies reluctant complain public authorities public services often concern vulnerable people need social benefits would less inclined complain decisions opportunities successfully complain use ai challenge decisions based ai essential providing access justice interviews emphasised following important respect ----making people aware ai used ----making people aware complain ----making sure ai system decisions based ai explained first everyone needs know dealing ai system taken decision affects people e g social benefits people concerned might complain general - able complain use ai know ai involved expert explained general willingness complain biggest problem people often know ai used organisations transparent even though required gdpr several interviewees indicate informing people decision made based partly automated tools first step providing access complaints second everyone needs know complain may difficult people know body deals type complaints one expert pointed consumers often know complain - example bank might use algorithms deciding financial matters public administration issues automated decisions decided add names employees decisions provide contact persons potentially challenging automated decision interviewees indicated ways procedures complaints place procedures complaints linked use ai companies organisations use ai anonymised aggregated data indicate complaint mechanisms place finally complaining need enough information challenge underlying decision thorough information ai systems provides equality arms meaningfully challenge decisions however straightforward comes use ai particularly ----potential intellectual property rights issues ---- complex systems difficult explain intellectual property rights form one hurdle providing enough information decision made system works algorithms part implemented software technical invention may subject intellectual property rights - right protected article charter actors often seek copyright patent trade secret protection safeguard knowledge ai one interviewee insurance sector claims due highly competitive market \"one may share much workings used technology\" instance particular price given customer essentially competitors could benefit knowledge underlying software subject scrutiny another respondent using ai handle visa applications notes using systems developed external providers whose algorithms covered intellectual property rights hinder necessary transparency later stage another challenge successfully complaining automated decisions use ai general challenge explain decisions based complex systems interviewees working public administration suggest usually clear guidance complain administrative decision area interviewees highlight importance detailed explanations example systems automatically provide unemployment benefits cases involve discretion clients ask reasoning behind automated administrative acts interviewee indicates clients wish see calculations behind financial decisions may self service system organisation' website publications contain detailed descriptions calculations used interviewees recognise open transparent logic essential providing explanations regarding ai supported decisions often challenging impossible achieve one interviewee working bank mentions complex machine learning solutions cannot used certain decision making reasoning system cannot explained easily systems used purposes however interviewee working another bank indicates systems used use simpler methods addition complex ones get idea probable reasons decisions one expert raised problem companies internally might enough information way algorithms work lack expertise knowledge appears major hindrance practice seeking access effective remedy experiences use cases \" topic transparency respondents discussing predictive policing tools highlighted transparency important nowadays important many procedures publish information many automatic gender based violence use case felt sending police means help upload file outcome ai system judge informing victim information portals level risk attributed case police measures lot work done apply result enhances transparency terms transparency \" public administration spain interviewees discussing heat map example referred numerous requests police explain system' purpose works highlighted transparency way reduce public anxiety number interviewees pointed possibility individuals affected system make complaints police courts ombudsinstitution reference domestic violence case however interviewee indicated procedure place question system police protocol terms measures protect fundamental rights health services use cases several interviewees referred ethics committees well general legal safeguards data protection rules checks controls primarily mentioned take place external actors specific complaints procedures place organisations interviewees responded question interviewees highlighted doctors ultimately take responsibility decisions patients often know use ai tool first place example breast cancer detection example interviewee indicated possibility legal recourse developer tool radiologist makes decision diagnosis liable errors safeguards place targeted advertising cases mainly follow data protection requirements ensuring consent obtained respected one company makes sure clients engaged illicit practices rejects clients certain sectors political advertising complaints received organisations interviewed received complaints challenging use ai cases interviewees claim received complaints complainants aware ai used noticed incorrect outputs decision making example individuals lodged complaints regarding traffic fines whereby police officer stopped car driver upon hearing car driver' explanation fine wrongfully administered proceeded manually correct information system without able update system' historical data cases fines remain visible throughout system particular person would continue profiled high risk occasion even though organisations rarely received formal complaints respect \" number complaints use ai interviewees often state due early stages data use miniscule rather ai implementation nonetheless interviewees reported repeated people may asked delete requests access rectification personal data people information \" requested information removed well explanations private company estonia certain recommendation made majority interviewees claim procedures decision processed undertaken human hand interviewees showed interest opening new channels analyse explain redress decisions involving ai solutions rights linked access justice set charter also impacted notably use ai law enforcement include example presumption innocence article charter identifying people suspected committed crime police may target activities specifically one person put suspicion based flawed fragmented data algorithmic profiling uncritical reliance automated tools without proper human review takes account information might contribute discrimination decision making right social security social assistance right social security assistance enshrined article charter classic social right inspired various international european legal standards provision combining elements right principle great significance eu view free movement people within union instead tying issues social protection labour market charter right takes new communitarian approach broadly referring \"providing social protection cases maternity illness industrial accidents dependency old age case loss employment\" article however primarily programmatic statement prescribe minimum standard protection principle eu member states determine conditions entitlement access social benefits clarification needed cjeu yet article charter provides protection measures restricting abolishing existing social security rights addition access social rights guaranteed individuals legally residing within eu exercise right free movement regardless nationality subject eu national laws article thus creates justiciable rights national courts cjeu becoming increasingly apparent impact ai technologies social protection systems lives many individuals rely upon far reaching - potentially - problematic introducing ai driven technologies social welfare systems risks creating barriers access right example using ai social security needs account potential negative - discriminatory - effects non nationals eu citizens third country nationals exercising right freedom movement eu could negatively affected example system relies data job histories available moving eu member states one respondent addressed 'right receive correct pension' aspect wider definition human rights meanwhile none interviewed referred fundamental right social security social assistance could partly reflect nature use cases however lack references social rights among public sector interviewees notable consumer protection charter stipulates eu policies must ensure high level consumer protection based article tfeu eu institutions bodies needs observe principle member state authorities implementing eu law charter principle provides guarantee particular goal \" high level consumer protection\" article tfeu concrete also determines means achieve stated aim - example protecting health safety economic interest consumers well promoting right information education among use cases use ai targeted advertising use medical records companies particular importance comes targeted advertising consumers need aware opt targeted aware might subjected advertising want particularly problematic combination highly sophisticated ai systems advertising amount sort manipulation consumer preferences consumer protection also major relevance use health data ehrs european consumer organisation beuc noted ai area health brings challenges consumers recommends ai technologies must fully respect data protection rules transparent consumer avoid discrimination beuc also called updated regulation legislative measures market surveillance law enforcement efficient redress concerning digital health products services fully protect eu consumers beuc carried survey among consumers views ai selected eu member states shows one two respondents agree companies using ai manipulate consumer decisions addition almost half respondents believe personalised content adverts e commerce platforms added value slightly half survey respondents expressed low trust governments effectively control ai interviews conducted study consumer protection mentioned margins discussing risks using ai fundamental rights however respondents businesses refer consumer protection legislation relevant framework also applying use ai moreover respondents deem consumer protection authorities potentially relevant oversight bodies ai used general terms many interviewees business sector stress importance consumer satisfaction example company using video surveillance security customers premises mention consumer protection regulations relevant technical solutions use systems aim improve situation consumers also preserving rights several ai tools built understand profile consumers enable businesses improve services marketing data protection important aspect business also linked fact breaching data protection rules considered business risk mentioned one major concern companies obtaining managing consent consumers customers process data using ai tools marketing purposes interviewees report gdpr impact improving systems handle consent right good administration right good administration well established general principle eu law elaborated cjeu binding eu member states also fundamental right enshrined article charter although actions eu institutions bodies agencies general principle eu law requires eu member states apply requirements right good administration public action right includes limited right individual access file obligation public authority give sufficient reasons decisions access file facilitates understanding evidentiary basis decision made reasons underlying places individual better position put forward counter arguments exercising right heard right effective remedy obligation give reasons makes perspective individuals affected decision making process transparent person concerned know understand measure action taken transparency also enabling principle provides foundations rights including exercise right effective remedy according cjeu context individual decisions made important determining extent duty give reasons france instance code relations public administration requires written explanations factual legal considerations decision based right good administration also applies ai systems process personal data support decision making public authorities although right good administration may subjected certain limitations question arises ensure potentially huge number individuals access files personal data used ai systems another question make sure public authorities always give sufficient reasons operation ai driven technologies cannot fully explained due inherent opacity complexity use system categorise unemployed people set poland highlighted problems linked public administration use algorithms based questions answered unemployed people categorisation developed statistical algorithm system received lot criticism civil society respect lack opportunities complain potential discrimination end complaint ombudsinstitution - based administrative grounds - led constitutional court ruling put end system' use intent increase efficiency drives use ai public sector - aim directly speaks improving administration benefiting citizens respondents public administration far often indicate efficiency reason considering use ai presently using ai one respondent advises ministries digital strategies use ai said main reasons adopting ai improve service citizens reduce costs services public administration interviewees also indicate public administration particular requirements meaning ai cannot used purposes needs particular attention comes decision making however efficiency system also considered important added value sense respondent working digitalisation migration management indicates building complex ai systems risk afterwards would require lot work understand system retrospect interviewee indicates team needs careful allow ai make final decisions taken human - society clients ready according interviewee although systems appealing work effectively could result extra work negative results however interviewee also indicates dimension efficiency \" often side lined discussing data protection\" requirements good administration also directly link issues raised respect data protection non discrimination right effective remedy fair trial public administration process data legal basis decisions need fair transparent pathways challenge decisions need available accessible result requirements good administration directly linked discussion analysis respect legal processing data data protection fair decisions linked discussion non discrimination alongside transparency ways challenge explain decisions respect access justice endnotes see european commission european enterprise survey use technologies based artificial intelligence luxembourg july fra fundamental rights mean people eu luxembourg publications office p see webpage three level baseline security system iske ther website estonia' information system authority see website pci security standards council barak 'human dignity framework right motherright ' barak human dignity constitutional value constitutional right cambridge cambridge university press ch pp cjeu c netherlands v european parliament council october paras discussion malicious use ai see example brundage et al malicious use artificial intelligence forecasting prevention mitigation fra facial recognition technology fundamental rights considerations context law enforcement luxembourg publications office november cjeu joined cases c c volker und markus schecke eifert gbr hartmut eifert opinion advocate general sharpston june para fra council europe edps handbook european data protection law edition luxembourg publications office june p see also ibid pp ecthr guide article european convention human rights - right respect private family life home correspondence strasbourg council europe updated august paras ecthr lopez ribalda others v spain nos october para comprehensive legal analysis meaning content 'privacy' see also koops b j et al ' typology privacy' university pennsylvania journal international law vol issue pp vermeulen surveille deliverable d4 - scope right private life public places july p un human rights committee general comment right peaceful assembly article ccpr c gc september para costello roisin aine impacts adtech privacy rights rule law technology regulation norwegian consumer council control consumers exploited online advertising industry fra rights matter data protection privacy - fundamental rights survey luxembourg publications office rocher l hendrickx j de montjoye estimating success identifications incomplete datasets using generative models nature communications hacker p legal framework ai training data law innovation technology forthcoming available ssrn article data protection working party opinion anonymisation techniques see also finck michele pallas frank must identified distinguishing personal non personal data gdpr october forthcoming international data privacy law max planck institute innovation competition research paper available ssrn sartor g lagioia f impact general data protection regulation gdpr artificial intelligence study prepared panel future science technology stoa european parliament see example uk data service' blog \"access sensitive data research ' safes'\" see also discussion ohm p \"broken promises privacy responding surprising failure anonymization\" ucla law review p gdpr art law enforcement directive art veale edwards l 'clarity surprises questions article working party draft guidance automated decision making profiling' computer law security review vol april pp article data protection working party guidelines automated individual decision making profiling purposes regulation adopted october last revised adopted february green b chen 'disparate interactions algorithm loop analysis fairness risk assessments' fat ' conference fairness accountability transparency fat ' january gonzalez fuster g artificial intelligence law enforcement - impact fundamental rights european parliament policy department citizens' rights constitutional affairs directorate general internal policies pe july p brkan ' algorithms rule world algorithmic decision making data protection framework gdpr beyond' international journal law information technology vol p article working party guidelines automated individual decision making profiling purposes regulation adopted october last revised adopted february wp251rev p misuraca g van noordt c overview use impact ai public services eu european commission joint research centre luxembourg council directive ec june implementing principle equal treatment persons irrespective racial ethnic origin oj l pp art council directive ec november establishing general framework equal treatment employment occupation oj l pp art fra coe handbook european non discrimination law edition luxembourg publications office june p cjeu c association belge des consommateurs test achats asbl others v conseil des ministres january european commission eu rules gender neutral pricing insurance industry enter force press release ip december elizabeth e joh ' new surveillance discretion automated suspicion big data policing' uc davis legal studies research paper pp ales zavrsnik 'algorithmic justice algorithms big data criminal justice settings' european journal criminology p doi see also european commission white paper artificial intelligence - european approach excellence trust com final brussels february p fra bigdata discrimination data supported decision making luxembourg publications office june p ibid fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office korff browne ' use internet related services private life data protection trends technologies threats implications' council europe pd see national non discrimination equality tribunal finland decision march see also syri case discussed uk court appeal r bridges v cc south wales ewca civ august see also equinet regulating equal ai new role equality bodies brussels report prepared allen r masters tolan miron gomez e castillo c ' machine learning may lead unfairness evidence risk assessment juvenile justice catalonia' best paper award international conference ai law richardson r schultz j crawford k dirty data bad predictions civil rights violations impact police data predictive policing systems justice n u l rev online available ssrn fra violence women eu wide survey main results report luxembourg publications office p fra second european union minorities discrimination survey main results luxembourg publications office p erik bakke \"predictive policing argument public transparency\" new york university annual survey american law vol pp andrew g ferguson 'policing predictive policing' washington university law review vol pp andcouncil europe committee experts internet intermediaries msi net algorithms human rights council europe dgi p elizabeth e joh ' new surveillance discretion automated suspicion big data policing' uc davis legal studies research paper p gstrein j bunnik zwitter 'ethical legal social challenges predictive policing' catolica law review pp albert meijer martijn wessels 'predictive policing review benefits drawbacks' international journal public administration p doi ales zavrsnik 'algorithmic justice algorithms big data criminal justice settings' european journal criminology pp doi wachter sandra 'affinity profiling discrimination association online behavioural advertising' berkeley technology law journal vol forthcoming available ssrn use ai financial industries leading unequal access financial services see legal literature e g boyd levy k marwick ' networked nature algorithmic discrimination' gangadharan p eubanks v barocas eds data discrimination collected essays open technology institute pp overview child rights issues see unicef innovation human rights center uc berkeley artificial intelligence children' rights eu network independent experts fundamental rights commentary charter fundamental rights european union june p see also fra coe handbook european law relating access justice luxembourg publications office june p cjeu c unibet london ltd unibet international ltd v justitiekanslern march para cjeu c et agrokonsulting velko stoyanov v izpalnitelen direktor na darzhaven fond 'zemedelie' - razplashtatelna agentsia june para cjeu c centre public 'action sociale 'ottignies louvain la neuve v moussa abdida december para law enforcement directive art gdpr art law enforcement directive art gdpr art law enforcement directive art gdpr art council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems adopted committee ministers april 1373rd meeting ministers' deputies appendix para b andrew g ferguson 'policing predictive policing' washington university law review pp gstrein j bunnik zwitter ethical legal social challenges predictive policing' catolica law review pp yeung k study implications advanced digital technologies including ai systems concept responsibility within human rights framework prepared council europe expert committee human rights dimensions automated data processing different forms artificial intelligence msi aut council europe algorithms human rights pp international technology law association 'responsible ai global policy framework' pp lack expertise ai also reflected survey among companies eu lack skills among existing staff difficulties hiring new staff prominent obstacle ai adoption european commission european enterprise survey use technologies based artificial intelligence luxembourg july p see detailed assessment impact predictive policing presumption innocence mendola marco one step 'surveillance society' case predictive policing see e g egorov wujczyk eds right social security constitutions world broadening moral legal space social justice geneva ilo global study vol europe pp xv xvii include arts tfeu arts european social charter well points community charter fundamental social rights workers see explanations relating charter fundamental rights oj c pp explanations relating charter fundamental rights oj c pp explanation article -- scope interpretation rights principles lukasz bojarski dieter schindlauer katrin wladasch european charter fundamental rights living instrument - manual rome warsaw vienna pp de becker e ' possible role right social security eu economic monitoring process' german law journal vol pp paju j european union social security law oxford hart publishing sub section ibid pp peers prechal 'scope interpretation rights principles' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp exception poland united kingdom see protocol application charter fundamental rights european union poland united kingdom oj c pp art christiaan van veen ben zevenbergen 'conference social protection artificial intelligence decoding human rights digital age' freedom tinker - research expert commentary digital technologies public life may art charter see also explanations relating charter fundamental rights oj c pp explanation article -- scope interpretation rights principles lukasz bojarski dieter schindlauer katrin wladasch european charter fundamental rights living instrument - manual rome warsaw vienna p sartor giovanni new aspects challenges consumer protection study committee internal market consumer protection policy department economic scientific quality life policies european parliament luxembourg european consumer organisation beuc digital health principles recommendations beuc artificial intelligence consumers say findings policy recommendations multi country survey ai recent case law see cjeu c h n v minister justice equality law reform ireland attorney general may para also confirmed cjeu joined cases c c ys v minister voor immigratie integratie en asiel minister voor immigratie integratie en asiel v july paras components initially developed cjeu case law codified article charter right leading academic literature see craig p 'article - right good administration' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp ibid p finck 'automated decision making administrative law' max planck institute innovation competition research paper p craig p 'article - right good administration' hervey kenner j peers ward eds eu charter fundamental rights commentary oxford portland oregon hart publishing pp france code des relations entre le public et l'administration article l2111 panoptykon foundation profiling unemployed poland social political implications algorithmic decision making see also algorithm watch poland scrap controversial unemployment scoring system see decision k available constitutional tribunal' website fundamental rights impact assessment - practical tool protecting fundamental rights chapter illustrated extent using ai affects different fundamental rights chapter analyses fundamental rights impact assessments fria could reduce negative impacts using ai fundamental rights section provides brief overview current discussion need fundamental rights impact assessments field section analyses current practices addressing fundamental rights implications based interviews conducted report interviewees asked sort testing done system used controls tasks affected use technology chapter ends suggestions assess fundamental rights impact using ai related technologies calling fundamental rights impact assessment - available guidance tools international organisations academics2 civil society3 called fundamental rights impact assessments conducted using ai related technologies example committee ministers council europe' guidelines addressing human rights impacts algorithmic systems recommend states conduct \"impact assessments prior public procurement development regular milestones throughout context specific deployment order identify risks rights adverse outcomes\" need flexible impact assessments adapt different situations given fundamental rights violations always contextual scholars exemplify based eu anti discrimination law equality always contextual depends case hand fundamental rights compliance cannot automated hard coded computer software rather use case needs separate examination determine whether fundamental rights issue arises nevertheless assessments follow systematic approach provide similar information existing standards provide guidance fundamental rights impact assessment ai related technology include hard law soft law instruments recommendations declarations practical tools e g guidelines checklists beyond requirements flowing data protection legislation see box examples laws requiring mandatory assessments effects ai general view increasing uptake ai canadian government issued guidelines including mandatory requirements assessing ai use public administration applies system tool statistical model used recommend make administrative decision client european data protection law requires data protection impact assessment dpia learning coe modernised convention provides general obligation examine likely data impact data processing individuals' rights fundamental freedoms use protection following assessment controllers design processing manner impact prevent minimise identified risks b assessments eu law imposes similar detailed obligation gdpr foresees data protection impact assessment dpia data processing likely \" result high risk rights freedoms natural persons \"c therefore required law dpia ai technology could potentially also address broader fundamental rights implications besides impact right privacy used tool investigate algorithms impacts e however gdpr article dpia limited 'high risk' cases processing personal data therefore may miss high risk cases primarily obviously related protection personal data time gdpr delimited specific field application accompanying expertise field means potential extension scope dpia fundamental rights might limited gdpr also gives indications modalities undertake dpia first dpia conducted high risk processing f second dpia provide systematic description envisaged operations purpose legitimate interests pursued must also assess necessity proportionality processing possible risks rights individuals addition must contain planned security measures address risks identified g pointing different methodologies apply article working party wp proposes - check list form - minimum criteria controller use assess dpia comprehensively complies gdpr h finally gdpr foresees prior mandatory consultation relevant supervisory authority impact assessment indicates processing presents risks cannot mitigated gives crucial role dpas independent bodies established law j european data protection supervisor edps provides guidance carrying dpias k data protection authorities also discussed provide guidance assess ai technologies l information data protection impact assessment see fra council europe edps handbook european data protection law edition p b council europe modernised convention art c gdpr art gdpr recitals article working party guidelines data protection impact assessment dpia wp248rev october e edwards veale fra bigdata discrimination data supported decision making luxembourg publications office june f gdpr art wp29 specifies 'carrying dpia continual process one time exercise ' g gdpr art well recitals h article working party guidelines data protection impact assessment dpia wp248rev october annex gdpr art j gdpr art k edps accountability ground part ii data protection impact assessments prior consultation v july l see example declaration ethics data protection ai adopted 40th international conference data protection privacy commissioners icdppc many examples non binding guidelines global level united nations guiding principles business human rights recommend enterprises integrate findings human rights impact assessments across relevant internal functions processes take appropriate action although refer specifically ai guidelines relevant supporting development ai technology rights compliant manner eu level ethics guidelines trustworthy ai prepared european commission' high level group artificial intelligence9 also recommend performing fria system' development \" risks fundamental rights negatively affected technology\" also emphasise need put place mechanisms receive external feedback ai systems potentially infringe fundamental rights addition private companies associations private companies12 public private interests well ngos14 organisations15 developed different types guidance support ai impact assessments documents usually contain clear guidelines impact assessment instead highlight different aspects criteria taken account developing carrying impact assessment broad categories include purpose system description technology assessment impact targeted population individual evaluating fairness diversity description audits planned performed well accountability explicitly refer applicable international human rights law standards various codes ethics conducts standards well certification schemes also place several practical tools available assess impact ai technologies mitigate risks developed wide range actors include checklists lists questions online self evaluation tools risk management frameworks focus specifically assessing fundamental rights risks others focus ethical societal economic implications useful references performing thorough fundamental rights impact assessment ai technologies july example high level group artificial intelligence issued \"assessment list trustworthy ai\" altai six month pilot involving stakeholders altai helps organisations self evaluate - voluntary basis - reliability trustworthiness ai reduce potential risks users supports businesses public administrations ask right questions around seven requirements responsible ai identified ethics guidelines trustworthy ai altai specifically refers need perform fundamental rights impact assessment includes examples questions assess impact non discrimination equality right privacy rights child freedom expression well freedom information association several online assessment tools target use ai public authorities canadian government developed algorithmic impact assessment tool aia pursuant canadian directive automated decision making aia represents automated assessment consisting questions unfold requirements directive questions relate fundamental rights concerns - ai system' impact freedom movement likelihood incarceration individual legal status access funding benefits indigenous people score attributed reply final impact scoring provided made publically available government' website another example ethics toolkit31 freely accessible tool designed local governments based risk management approach supports fair automated decisions minimising unintentional harm individuals field criminal justice higher education social media areas among national human rights bodies danish institute human rights proposed human rights compliance \"quick check involves interactive online computer programme allows companies select modify information database suit type business area operations check rights compliance quick check based human rights compliance assessment tool runs database questions corresponding human rights indicators uses international human rights law standards benchmarks applying fields operations provide guidance developing impact assessment ai technology academic work also suggested operational frameworks assessing risks using ai technology focus specifically identifying addressing fundamental rights implications private sector focus developing ethical values oriented models analysing societal impact data used creation ad hoc expert review committee others developed guidance frameworks specific case studies example field criminal justice algo care framework36 introduced step step assessment evaluate key legal practical concerns considered relation police using algorithmic risk assessment tools argued participatory ways involve consider views affected rights holders stakeholders communities developing impact assessment publically engage start others joined cross discipline expertise science law design practical frameworks impact assessments testing practice virtually systems discussed interviews subject sort testing included elements impact assessment however mainly technical data protection impact assessments rarely address potential impacts fundamental rights interviewees argue conducting fundamental rights impact assessment view system negatively affect fundamental rights unsure example respondent working traffic management using cameras monitoring traffic indicated tested accuracy system fundamental rights apart respecting data protection rules respondents simply know fundamental rights assessed part general impact assessment carried testing stages development much testing done new ai system used respondents \" testing system highlighted moving ai system production challenging task really look legal aspects mentioned public administration well private companies usually looked whether system careful using ai many projects interviewees refer still profitable \" development pilot phase started concrete testing private company estonia testing done several stages include development stage called proof concept pilot stages deployment tests deployment possible live experimentation carried initial stages often involves staged deployment example organisation interviewed tests different applications support job seekers conducts continuous step step testing selected members organisation test tool real situations using check lists interviewee mentioned challenging move deployment stage planned supervise tool real time another example involving automated rule based granting social benefits different assessments carried implementation group lawyers data protection specialists compensation specialists accountants performed general impact assessment department responsible using system conducted tests decide whether system could used following system monitored implementation using step step approach first step half decisions taken system next step decisions taken automatically expanded negative decisions another area decisions added including decisions ending compensation payments time interviews conducted decisions automated interviewee indicated carrying tests feel sure system secure outstanding risks company working fraud detection system replaced rule based system machine learning tool changing system old new system run parallel see machine learning system performs better rule based one interviewee mentioned \" rigorous analysis behind direct feedback saw would impact losses versus many good customers impacting negatively\" interviewee added \" comfortable machine learning system better static rule system aspects deployed entirety\" use cases previous automated system existed tests reviewed humans example automated transcription service tested court hearings allowed judge included regular feedback correctness transcription services judges one interviewee law enforcement working tool detect domestic violence identifies issues precision accuracy using system police officer sufficient training knowledge system indicators required system cannot gather required information could lead miscalculation highlight robustness system tested annually assure quality two questionnaires used completeness data training police officers using ai system process also considers personal data protection laws protocols applied tests discussed focus strongly technical aspects general operations fundamental rights data protection impact assessments apart data protection respondents mentioned fundamental rights typically considered respondents reflected potential impacts fundamental rights mentioned aspects considered prompted interviewer many respondents generally aware discrimination issues - often discussed explicitly asked discrimination yet gave information formal depth tests discrimination generally respondents ruled possibility system discriminates based protected attributes example one interviewee states test system data protection laws specific applicable legal acts fundamental rights however interviewee consider potential discrimination ruled needs kept mind future technologies interviewee stated however cases non discrimination generally considered testing phase ai systems one respondent municipal authority mentioned cannot assess fairness model cannot access data needed due data protection reasons according interviewee \" huge tension surrounding gdpr want well might fact worse interpretation data turns impossible\" \"yes assess legality personal data protection respondents reported data protection impact assessment conformity specific legal required law conducted although took different forms bank acts \" tested tool analysing speech customer calls find public administration estonia reoccurring problems carried data protection impact assessment dpia specifically testing tool outcome system tested data used testing phase deleted certain period test access data employees restricted testing phase supervised deployment tool another dpia required case sometimes lack clarity extent use ai related technologies notably use algorithms belongs dpia area predictive policing instance dpias done underlying architecture system rather specific ai tool another interviewee using algorithms financial services also mentioned assessing machine learning tool within framework dpia belief apply machine learning system underlying data one interviewee felt data protection impact assessment crime heat map example sufficiently depth safeguard quality model system equipped deal cross sectoral use data different rules might apply indicated standards required respondent working migration management indicated data protection officers involved analysis legal service specialised quality control ai tool study data protection aspects system however respondent also mentioned guidance needed companies working targeted advertising looked data protection issues although respondents sure impact assessment conducted companies assessed example whether people consented approached targeted communication targeted ads assessed whether information possible identification deleted including whether cookies trackers anonymised respect dpias generally respondents know area responsibility others knew positive dpia aware details appears legal assessment sometimes detached technical side technical people knowing legal assessments one interviewee private company working credit risk scoring mentioned \" make suggestions system could developed compliance manager tells conformity laws\" audits working external oversight bodies public administrations private companies involved fra' research carry tests deploying ai often linked existing internal external oversight processes use ai frequently subjected internal review processes within companies public administration although necessarily formalised review processes interviewees mentioned working formalising existing internal review processes overseeing ai systems interviewees public sector say particularly cautious using ai support decisions representative working migration management public administration indicates \" n private sector wrong results might cause business related losses police impacts people' lives fundamental rights\" yet always clear public administration businesses responsible checking overseeing use ai public administrations appear stronger scrutiny comes oversight ai systems oversight often done regular audits example connected budgetary review interviewees public private organisations report ai systems currently checked framework existing review e g regular database checks absence review processes specifically look use ai addition interviewees report sector specific certification schemes also look use ai - example area health financial services several interviewees mentioned contact data protection authorities companies public administrations sought permission data protection authorities using ai system least generally contact example one company working targeted advertising mentioned discussing use personal data national data protection authority experts interviewed report highlighted relevance data protection authorities overseeing ai systems respect use personal data however experts strongly highlighted data protection authorities resourced task two reasons data protection authorities often relevant ai related expertise additionally budgets overstretched workload heavy experts' views differ respect need additional oversight bodies potential creation ai specific institution however agree existing bodies work topics linked ai within mandates equality bodies well human rights institutions mentioned interviewed experts providing oversight concerning possible discrimination using ai highlighted institutions need build expertise area better contribute oversight ai however similar data protection authorities challenging task equality bodies given lack resources \" proactive among several interviewees mentioned consumer protection authorities potentially mitigate risks providing relevant oversight use ai one respondent working also get additional audits also retail company would like advisory agency could consulted see sometimes regulatory possible use ai innovation without investigated right away audits quite sloppy us moment company prefers consult consumer authorities good lots data protection authorities potential future marketing campaigns customer data \" data protection authorities might start investigation private company estonia efforts discussing oversight developing using ai well experts repeatedly mention challenge really understand impact using ai despite need engage existing oversight bodies responsibilities oversee use ai fundamental rights perspective remain unclear fundamental rights impact assessment practice many key actors field fundamental rights called conducting fundamental rights impact assessments using ai driven systems section highlights elements could incorporated assessment fundamental rights impact assessments needed given contextualised assessment required uses ai vary considerably terms complexity level automation potential errors harm scale application well area use complex ai system difficult assess potential impact fundamental rights implicated vary depending area application full spectrum rights needs considered use ai however uses ai likely involve rights often affected ai systems discussion preceding chapter makes clear issues linked data protection non discrimination well access effective remedies fair trial relevant uses ai thus following horizontal points could basic starting point considering impact ai selected rights ---- legal processing data needs confirmed line data protection laws personal data used full data protection framework applies ensures processing legal violate person' rights respect private family life data protection ---- processing lead unfair treatment discrimination protected groups assessing non discrimination needs core assessing ai even apparently miniscule differences scale create risks contravening principle non discrimination disadvantage people depends nature kind harm severity strength harm significance many people put disadvantage compared another group people statistical assessments group differences important tool assess unfair discriminatory uses ai ----people subjected ai related technologies able complain receive effective remedies accessible ways people complain potential decisions made effectively access remedies includes availability information allows explanation decisions addition relevant rights charter apply public administrations using ai need consider good administration principles businesses take consumer protection account rights relevant depending area application examples include ---- right social protection working social benefits ---- right freedom expression information using ai support online content moderation ---- right assembly association considering use facial recognition technology public spaces ---- right education using ai education sector ---- right asylum using ai support migration management ---- right collective bargaining action using ai 'gig economy' ---- right fair working conditions using ai workplace ---- right access preventive health care using ai health services ---- right presumption innocence right defence using ai justice sector law enforcement purposes information needed assess potential impact fundamental rights implementing ai given variety tools purposes area application assessments contextual able meaningfully respond horizontal points raised assess specific rights linked different use cases least following information needs available ---- description purpose context system well legal basis ---- description possible harm using system including questions around false positives false negatives possible harm due automation scale use ---- description technology used includes information data used building system legal basis processing description relevant information include provided fra' paper data quality ai ---- evidence based description accuracy ai system terms outcomes based training data possible tests experiments real life situations appropriate false positives false negatives considered separately include breakdowns many groups possible allow checking potential discrimination e g differences accuracy women men ---- already available provision information compliance existing standards potential certifications obtained ex post assessments safeguards lastly envisaging ex post safeguards contributes fundamental rights compliant use ai could include ----regular repetition assessments deployment appropriate important learn potential feedback loops case rules updated also requires recording information use outcomes system extent data protection respected ----making people subjected ai systems aware subjected technology otherwise challenge decision affecting ----making available easily accessible channels effectively complaining decisions made based ai system engaging external experts stakeholders oversight bodies information could basis consultation different stakeholders experts particular ai system used depending nature application legal basis consultation relevant stakeholders would ensure potential harm omitted different perspectives brought assessment stakeholders could include civil society different public private organisations well experts different fields fundamental rights including data protection ten experts interviewed report highlighted existing oversight bodies also responsible ai oversight within mandates sector specific bodies certification schemes extent interviews suggest - example health care financial oversight monitor comprehend effectively respond potential impacts ai wide spectrum fundamental rights data protection authorities equality bodies ombuds institutions national human rights institutions could play important role providing input oversight various points expertise however interviews indicated extensive upskilling resource allocation needed underpin endnotes council europe commissioner human rights unboxing artificial intelligence steps protect human rights - recommendation council europe strasbourg may heleen l janssen ' approach fundamental rights impact assessment automated decision making international data privacy law' international data privacy law vol issue february pp alessandro mantelero 'ai big data blueprint human rights social ethical impact assessment' computer law security review vol issue august pp edwards lilian veale michael slave algorithm 'right explanation' probably remedy looking may duke law technology review accessnow access ' submission consultation \"white paper artificial intelligence european approach excellence trust\" council europe recommendation cm rec committee ministers member states human rights impacts algorithmic systems april para human rights impact assessment detailed discussion respect non discrimination see wachter mittelstatt b russel c fairness cannot automated bridging gap eu non discrimination law ai government canada directive automated decision making united nations un guiding principles business human rights endorsed human rights council resolution hrc res july principles heleen l janssen approach fundamental rights impact assessment automated decision making international data privacy law vol issue february pp high level expert group artificial intelligence ethics guidelines trustworthy ai april chapter iii ibid p see example ibm everyday ethics artificial intelligence sony sony group ai ethics guidelines vodaphone vodaphone' ai framework arborus international orange international charter inclusive ai april signed private companies including camfil danone edf l'oreal metro sodexo etc information technology industry council iti iti ai policy principles ecp platform information society artificial intelligence impact assessment netherlands november amnesty international access human rights watch wikimedia foundation toronto declaration protecting rights equality non discrimination machine learning systems may rightscon toronto university montreal montreal declaration responsible ai electrical electronics engineers ieee global initiative ethics autonomous intelligent systems ethically aligned design prioritizing human wellbeing autonomous intelligent systems future life institute asilomar ai principles conference outcome future life institute' second conference future artificial intelligence see example ecp platform information society artificial intelligence impact assessment netherlands november ieee initiative association computer machinery acm acm code ethics professional conduct june future humanity institute university oxford standards ai governance international standards enable global coordination ai research development april iso standards iso iec jtc sc artificial intelligence sstandard project direct responsibility iso iec jtc sc secretariat iso iso iec tr standard information technology -- artificial intelligence -- overview trustworthiness artificial intelligence may establishes among others \"approaches assess achieve availability resiliency reliability accuracy safety security privacy ai systems \" iso standards development september iso iec cd information technology -- artificial intelligence -- risk management iso iec awi tr information technology -- artificial intelligence ai -- bias ai systems ai aided decision making iso iec awi tr information technology -- artificial intelligence -- overview ethical societal concerns information available iso' website electrical electronics engineers ieee ieee p7003(tm) algorithmic bias considerations german ai federal association ki bundesverband german ai federal association seal quality ki bundesverband guetesiegel march article working party guidelines data protection impact assessment dpia wp248rev october annex - criteria acceptable dpia high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july government canada algorithmic impact assessment tool danish institute human rights human rights compliance assessment quick check june center government excellence johns hopkins university ethics algorithm toolkit government canada algorithmic impact assessment tool article working party guidelines data protection impact assessment dpia wp248rev october annex data protection focus danish institute human rights human rights impact assessment guidance toolbox ai pulse creating tool reproducibly estimate ethical impact artificial intelligence september fairness accountability transparency machine learning fat ml principles accountable algorithms social impact statement algorithms fat ml social impact statement algorithms high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july see e g human agency oversight technical robustness safety privacy data governance transparency diversity non discrimination fairness societal environmental well accountability high level expert group artificial intelligence ethics guidelines trustworthy ai april high level expert group artificial intelligence assessment list trustworthy artificial intelligence altai self assessment july p government canada algorithmic impact assessment tool government canada directive automated decision making article appendix c center government excellence johns hopkins university ethics algorithm toolkit danish institute human rights human rights compliance assessment quick check june danish institute human rights human rights impact assessment guidance toolbox heleen l janssen approach fundamental rights impact assessment automated decision making international data privacy law international data privacy law vol issue february alessandro mantelero ai big data blueprint human rights social ethical impact assessment computer law security review vol issue august pp ai pulse program understanding law science evidence pulse ucla school law creating tool reproducibly estimate ethical impact artificial intelligence september model includes series questions assessing human rights impact ai enabled projects marion oswald jamie grace sheena urwin geoffrey c barnes algorithmic risk assessment policing models lessons durham hart model 'experimental' proportionality journal vol - issue ainow algorithmic impact assessments practical framework public agency accountability april institute ethical ai machine learning ethical ml network beta machine learning maturity model brave europe' governments failing gdpr see wachter mittelstatt b russel c fairness cannot automated bridging gap eu non discrimination law ai fra data quality artificial intelligence - mitigating bias error protect fundamental rights luxembourg publications office june moving forward challenges opportunities report published amidst ongoing european legislative policy developments artificial intelligence global fight coronavirus covid pandemic potentially quickened acceptance innovative technologies yet also shown ai panacea problems comes various challenges report clearly shows using ai systems engages wide range fundamental rights also shows many businesses public administrations already using planning use ai related technologies however technologies involve different levels complexity examples use relatively simple algorithms level automation also varies - - decision making subject human review applications currently used also often development stage eu national legislators policymakers keep reality mind - especially presented optimistic expectations ai' potential vis vis challenges related using new technologies need regulate \" try look future vast majority public administrations businesses interviewed plan automate \" keep working using ai two interviewees indicated private company estonia use develop ai another two interviewees cautious plan wait see others including lack resources work using ai \" next steps related transparency open data however said develop continue test tools say publish information data infrastructure respect use ai includes starting pdf also information new continuing ongoing pilots evaluating existing efforts sharing data reusable formatting could results others increasing data quality trying obtain reused internally data sources private sector \" public administration spain interviewees mentioned engaged ongoing debates expressed desire contribute development legislation still see current situation - absence harmonised law \"ai great thing must area - obstacle use ai addition respondents learn use \" said working issues linked interpretability ai private company spain means working methods enhance understanding explanation decisions based complex ai indicated desire look closely ethical legal matters figure shows correlations words interviewees often use talking future use ai figure indicates topics often raised example interviewees often used term 'data' discussing future developments figure correlations words respondents often mention discussing future plans use ai technologies systems plans data development company ai tool police management solutions companies level system project technology administration tax analysis organisation service processing lot translation process related customer easy national phase payment information services application time quality improve learning business human energy algorithms decisions support machine ml continue potential people develop future notes based text interview summaries respondents spoke future use ai including words mentioned least ten times lines connecting words indicate strength word correlations within text passages size dots indicate frequency words used source fra effectively adequately protecting fundamental rights eu key objective current efforts better regulate use ai context upcoming eu legislation ai european commission' white paper addresses current gaps helping mitigate uncertainty around use ai respect fundamental rights making use ai transparent accountable terms fundamental rights includes requirements ai use directly link information needed assess impact ai fundamental rights discussed requirements linked description training data data record keeping information provided subjected ai robustness accuracy well human oversight highly relevant assessing protecting fundamental rights respect body evidence presented report offers general insights different technologies affect fundamental rights safeguards needed ensure fully fundamental rights compliant use ai practice time research fundamental rights implications use ai specific areas support policy legislative efforts eu level aiming shape europe' digital future widely fra continue look fundamental implications ai carrying focussed analysis specific use cases increase knowledge potentially go wrong consequently help mitigate prevent fundamental rights violations fra look potential simulation studies showcase biased algorithms negatively affect fundamental rights use ai often involves automating tasks previously carried humans need acknowledge human behaviour sometimes line fundamental rights using ai using ai example police might engage unlawful profiling decisions public administration companies might sometimes driven negative stereotypes current developments use ai need acknowledge potential discrimination respect data ai system built respect underlying assumptions humans turn may feed development deployment system automating certain tasks without fully understanding automated could lead unlawful processing data use technology treats people unfairly might make impossible challenge certain outcomes - name challenges however increased availability data technological tools also used better understand unequal treatment occurs current technological developments increased availability data also provide unique opportunity better understand structures society used support fundamental rights compliance opportunities created ai also contribute better understanding consequently mitigation fundamental rights violations getting touch eu person european union hundreds europe direct information centres find address centre nearest https europa eu european union contact en phone email europe direct service answers questions european union contact service --b freephone certain operators may charge calls -- following standard number -- email via https europa eu european union contact en finding information eu online information european union official languages eu available europa website https europa eu european union index en eu publications download order free priced eu publications https op europa eu en publications multiple copies free publications may obtained contacting europe direct local information centre see https europa eu european union contact en eu law related documents access legal information eu including eu law since official language versions go eur lex http eur lex europa eu open data eu eu open data portal http data europa eu euodp en provides access datasets eu data downloaded reused free commercial non commercial purposes promoting protecting fundamental rights across eu -- artificial intelligence ai already plays role deciding unemployment benefits someone gets burglary likely take place whether someone risk cancer sees catchy advertisement low mortgage rates use keeps growing presenting seemingly endless possibilities need make sure fully uphold fundamental rights standards using ai report presents concrete examples companies public administrations eu using trying use ai focuses four core areas - social benefits predictive policing health services targeted advertising report discusses potential implications fundamental rights analyses rights taken account using developing ai applications aims help ensure future eu regulatory framework ai firmly grounded respect human fundamental rights eu charter fundamental rights access justice non discrimination information society fra - european union agency fundamental rights schwarzenbergplatz - vienna - austria tel - fax fra europa eu facebook com fundamentalrights twitter com eurightsagency linkedin com company eu fundamental rights agency","text_word_count":49776,"text_unique_words":8464,"_deepnote_index_column":1},{"Name":"./sampledocs/EPAM_Streamlining_the_Auto_Claims_Process_via_Integrated_IA.pdf","Type":"pdf","Text":"                    WHITE PAPER\nStreamlining the Automotive Claims Process\n       via Integrated Intelligent Automation\nMANUBHAV JAIN        STAS CHULSKY        ASTGHIK MKHITARYAN\n  PRINCIPAL,        SENIOR MANAGER,        SENIOR MANAGER,\n  INSURANCE           INTELLIGENT            INTELLIGENT\n C O N S U LT I N G   AUTOMATION             AUTOMATION\n                      C O N S U LT I N G     C O N S U LT I N G\n                    NOVEMBER 2019\nContents\nINTRODUCTION........................................................................................................................................ 3\nFIRST NOTICE OF LOSS (FNOL) & CLAIM REGISTRATIONS. . .............................................................. 4\nCLAIMS TRIAGE, ADJUDICATION & RESERVE ESTIMATION . . ............................................................ 5\nREPAIRS & QUALITY CONTROL ............................................................................................................. 6\nSUBROGATION BETWEEN INSURANCE COMPANIES & PAYMENTS TO VENDORS ..........................7\nTOTAL LOSS SETTLEMENT & SALVAGES/RECOVERY . . ........................................................................ 8\nSPECIAL INVESTIGATION UNIT & LEGAL ............................................................................................. 9\nCONCLUSION ......................................................................................................................................... 10\nIntroduction\nAs intelligent automation (IA) emerges as a best practice across numerous industries, insurers and technology\nservice providers alike are racing to build out use cases for IA that streamline the various stages of an\nautomotive insurance claim. While automation cannot be viewed as an isolated lever or standalone solution\nto bring efficiency and cost savings, when IA integrates with other channels, technologies and platforms, it\nunleashes its full potential. This white paper provides insight on how an integrated IA approach can improve\nthe auto claims process and deliver a more favorable customer experience.\n    FIRST NOTICE OF LOSS (FNOL)             CLAIMS TRIAGE, ADJUDICATION                        REPAIRS & QUALITY\n        & CLAIM REGISTRATIONS                   & RESERVE ESTIMATION                                  CONTROL\n        SUBROGATION BETWEEN\n                                              TOTAL LOSS SETTLEMENT &                       SPECIAL INVESTIGATION\n       INSURANCE COMPANIES &\n                                                  SALVAGES/RECOVERY                                UNIT & LEGAL\n        PAYMENTS TO VENDORS\n                                                                           TY P I C A L VA LU E C H A I N O F A N A U TO C LA I M\nW H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation               2019 • 3\nFirst Notice of Loss (FNOL) & Claim Registrations\nSTAGE OVERVIEW\nThe FNOL & claim registration stage is where call center agents collect data on the claim as well as the insured\nand ultimately register a claim. Call center agents may alleviate any customer doubts and provide support\nrelated to the registration of a claim. FNOL is an intake process which potentially impacts all downstream\nstages of claim processing.\nCOMMON PROBLEM AREAS\nUnstructured data stemming from voice interaction and decentralized information is kept in various systems.\nBoth voice interaction and decentralization of information result in swaths of discrete and unstructured data\ndeposits in the organization that require extensive operational effort to convert into actionable structured\ndata to fulfill a business task. The results of unstructured data at this stage not only affect claim registration\nbut have a substantial impact on all downstream claims processes, which ultimately leads to both increased\noperational expenses for the insurer and a poor customer experience for the insured.\nINTEGRATED IA APPROACH\n•   Apply real-time natural language processing (NLP) and speech-to-text to authenticate caller and assist call\n    agent with data collection\n•   Supplement all interactions between the insured and the carrier via mobile app and/or portal as an\n    additional channel\n•   Enable real-time request-response through this app to trigger any forms (i.e. disclaimers) or information\n    exchanges needed for the claim filing process while a phone conversation with the agent is taking place\n•   Prefill a significant amount of client data for expedited processing and, in the long term, predict relevant\n    data to be captured via machine learning (ML)\n•   Provide the means to request additional services such as notifying emergency contacts associated with\n    policy, requesting ambulances or tow trucks, and providing tracking of those services until fulfilled\n•   Use AI to identify, then notify the most optimal (location, cost, service quality, etc.) service providers and\n    then track the status until the service has been fulfilled\n•   Ensure the ability to launch a video conferencing (VC) call for interaction and guidance with a customer\n    service rep in cases where special handling or in-person assistance is required\nW H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation   2019 • 4\nClaims Triage, Adjudication & Reserve Estimation\nSTAGE OVERVIEW\nThe claims triaging stage is where the optimal claims adjudicator is assigned to a case based on their\nskillset. Once an adjudicator is assigned, they provide the initial estimate of the claim amount and create a\ndamage appraisal report by collecting additional information/evidence.\nCOMMON PROBLEM AREAS\nManual triaging of claims may lead to non-optimal assignment of skilled adjusters while decentralized and\ninconsistent data-gathering frameworks between adjudicators and other departments lead to increases in\nturnaround time and operational expenses. Additionally, prevailing manual reserve booking and management\nprocesses can delay both accounting and financial visibility and accuracy.\nINTEGRATED IA APPROACH\n•    A decision engine should be created to effectively assign an adjudicator to a case based on their personal\n     profile, availability, skillset and type of claim\n•    The adjudicator should be presented with information provided by the claimant, offered relevant forms\n     automatically, and have the ability to request additional information (if needed) via a common channel\n•    The claimant should be able to receive all documents and forms electronically and have the option to sign\n     forms digitally\n•    Business process management (BPM) tools can be used to orchestrate processes and transmit data and\n     forms from various departments and platforms\n•    Claim data can be integrated with actuarial and/or financial systems for automatic reserve management\n•    Artificial intelligence (AI) can be used to determine the initial estimate of loss and allow reserves to be\n     triggered for booking automatically\n•    Initial estimates can be produced using ML models and algorithms which could be used for settling claim\n     variance at later stages of claims processing\nW H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation 2019 • 5\nRepairs & Quality Control\nSTAGE OVERVIEW\nThe repair stage is where an approved body shop repairs damages, procures parts and provides customer\nservice directly to the insured. Once the body shop determines which parts are required, they are procured\nbased on an agreed upon parts and services rate.\nIn conjunction with the repair stage, a quality control audit is performed by the insurer to make sure that the\nbody shop does not overprice repair items and provides quality customer service.\nCOMMON PROBLEM AREAS\nInconsistent and unstructured data flow between the external vendor and insurer may cause important data\nloss, and the extra time taken to decide on a repair shop may impact the quality of the experience for the\ncustomer, who is already dealing with the stress of an accident. A lack of real-time status-of-repair results in\nuninformed and agitated customers while time spent between FNOL and sending the vehicle to a body shop\n(customer service, tow truck and body shop selection) results in high cost to the insurer. All these components\nthreaten to reduce the quality of service, incur indirect expenses for the insurer and exacerbate a tense situation\nfor the customer.\nINTEGRATED IA APPROACH\n•    Depending on the captured damage type, incident location, customer address and customer reviews, IA can\n     suggest a list of appropriate repair shops and create initial estimates based on FNOL data and experience\n     with each body shop for the insured to decide on a suitable option\n•    Using ML models, the system should be able to flag any pricing inconsistency\n•    A carrier portal or app can be extended to interact with body shops and allow them to see the insurer’s\n     original estimate — along with all underlying data and pictures gathered through the FNOL and valuation\n     process — and adjust it if needed. The same portal can enable the body shop to provide a status of repair\n     and feed that information back to the insured as well as the carrier\n•    A random audit can be automatically triggered to validate estimation samples\nW H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation   2019 • 6\nSubrogation Between Insurance Companies &\nPayments to Vendors\nSTAGE OVERVIEW\nSubrogation is the substitution of one person or group by another in respect to a debt or insurance claim,\naccompanied by the transfer of any associated rights and duties. Put simply, the Subrogation Principle in\ninsurance means that when an insurer pays full compensation for any insured loss (of insured property), the\ninsurer holds the legal right (claim) to collect the loss amount from the third party (insurance or person).\nPayment to vendor occurs when the vendor (repair shop or supplier) has rendered the services.\nCOMMON PROBLEM AREAS\nTedious paperwork to settle/initiate subrogation and lengthy external party interaction increases turnaround\ntime and operational cost. Manual accounts payable (AP) or accounts receivable (AR) reconciliation often\ncauses human error and delays the recognition of funds. The paper trail builds a three-way barrier between\nthe customer, insurer and third party by making the process convoluted for everyone involved. It also puts\nconstraints on the insurer’s cash reserves, as months are spent gathering discrete information and tracking\ndown the reimbursement of funds.\nINTEGRATED IA APPROACH\n•   Based on pre-set rules, an IA-enabled platform can flag cases where subrogation is applicable\n•   Integration with virtual payment providers can expedite the payment process to vendors\n•   IA/ML should be utilized to extract information from invoices received from vendors and apply a consistent\n    approach for entering data into the platform\n•   A reconciliation process can be enabled via a mix of business rules and ML\n•   The rules engine can track service level agreements (SLAs) around AP and AR to avoid penalties for AP and\n    minimize risk profile for AR\n•   Digital content management should be leveraged to enable communication between insurance companies,\n    customers and vendors\nW H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation  2019 • 7\nTotal Loss Settlement & Salvages/Recovery\nSTAGE OVERVIEW\nThe total loss settlement process is aimed to make sure that total loss damages are paid to the insured\naccording to the contract.\nThe salvages and recovery process is aimed to make sure that all salvaged cars are sold.\nCOMMON PROBLEM AREAS\nManually tracking total loss cases results in material process inefficiencies and ultimately leads to an increase\nin operational costs. A lack of controls and sufficient market data to evaluate the appropriate market value of\nvehicles increases the risk of overestimating or underestimating that value, which could subsequently lead to\nthe loss of funds. Additionally, manually tracking salvage inventory increases the time required to recycle the\nvehicle and incur the appropriate amount.\nINTEGRATED IA APPROACH\n•   In cases where the contract is defined in a document rather than the policy administration database, IA can\n    be utilized to extract relevant information (deductibles, contract exclusions, state thresholds, etc.) using\n    cognitive document processing (CDP)\n•   A rules engine can flag potential total loss cases when damage estimates are higher than the market value\n    of the vehicle\n•   Automated payments or electronic fund transfers (EFTs) can be scheduled based on confirmation of total loss\n•   Digital content management should be leveraged to enable communication between insurance companies\n    and customers\n•   Integration with third parties allows insurers to uncover the market value of a vehicle to be utilized in\n    determining total loss amount\nW H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation  2019 • 8\nSpecial Investigation Unit & Legal\nSTAGE OVERVIEW\nThe Special Investigation Unit (SIU) reviews and investigates claims that are flagged as potential fraud\nautomatically by the system or manually by an adjuster.\nCOMMON PROBLEM AREA\nA lack of relevant rules to flag potential fraud increases the possibility of fraud cases going undetected, and\nthe inaccuracy of this data misleads the machine learning engine behind the predictive and/or adoptive\nmodel. False positives and negatives in this process lead to the displacement of valuable talent capital when\nemployees spend time researching legitimate claims for fraud while financial loss occurs when material fraud\ncases are not identified.\nINTEGRATED IA APPROACH\n•   A rules engine, combined with IA, increases the accuracy and likelihood of detecting potential fraud cases\n•   Adjusters can mark cases as potential fraud within a portal and provide structured input and\n    granular categorization which is later evaluated through IA to minimize false positives or expediate\n    fraud investigations\n•   A portal can provide visibility for legal departments when collecting data, relevant information and key\n    details for a case\n•   Integration with third-party data providers allows for insight into customer profiles before proceeding with\n    any litigation case\nW H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation 2019 • 9\nConclusion\nFrom FNOL through legal operating procedures, it is paramount to integrate all communication channels and\nsystems to enable seamless automation. However, before automating these processes, insurance organizations\nmust have a centralized process team which focuses on reviewing and optimizing processes (macro- and micro-\nlevel) across departments and identifies opportunities to re-engineer processes in order for any automation\nprogram to garner the expected results. Failure to do so could result in unintended process inefficiencies that\nlead to millions – and even billions – in indirect hidden expenses that manifest through long customer service\ncalls, fees and penalties – all of which can diminish user experience and lead to loss of customers.\nWith customer satisfaction so closely tied to the bottom line, it is vital for insurers to use intelligent automation to\nprioritize process transparency for consumers to enable the real-time status tracking of settlements, increase the\nease of reviewing relevant case details/documents, and improve the visibility of estimated execution timelines so\nthat customers know what to expect next without needing to create an extensive paper trail of correspondences.\nDoing so offers a unique opportunity for insurers to step into a future where convoluted infrastructures designed\nto support numerous discreet and siloed processes become a thing of the past as the industry reaches a whole\nnew level of disruption, competition and innovation.\nW H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation        2019 • 10\n                                       ABOUT EPAM SYSTEMS\nSince 1993, EPAM Systems, Inc. (NYSE: EPAM) has leveraged\n                                                                   GLOBAL\nits software engineering expertise to become a leading global\n                                                                   41 University Drive, Suite 202\nproduct development, digital platform engineering, and top digital\n                                                                   Newtown, PA 18940, USA\nand product design agency. Through its ‘Engineering DNA’ and\ninnovative strategy, consulting, and design capabilities, EPAM     sales@epam.com\nworks in collaboration with its customers to deliver next-gen\n                                                                   P: +1-267-759-9000\nsolutions that turn complex business challenges into real business\n                                                                   F: +1-267-759-8989\noutcomes. EPAM’s global teams serve customers in over 25\ncountries across North America, Europe, Asia and Australia. EPAM\nis a recognized market leader in multiple categories among top\nglobal independent research agencies and was one of only four\ntechnology companies to appear on Forbes 25 Fastest Growing\nPublic Tech Companies list every year of publication since 2013.\nLearn more at http://www.epam.com/ and follow us on Twitter @\nEPAMSYSTEMS and LinkedIn.\n© 1993-2019 EPAM. All Rights Reserved.                                                      EPAM_B352\n","cleanText":"white paper streamlining automotive claims process via integrated intelligent automation manubhav jain stas chulsky astghik mkhitaryan principal senior manager senior manager insurance intelligent intelligent c n u lt n g automation automation c n u lt n g c n u lt n g november contents introduction first notice loss fnol claim registrations claims triage adjudication reserve estimation repairs quality control subrogation insurance companies payments vendors total loss settlement salvages recovery special investigation unit legal conclusion introduction intelligent automation ia emerges best practice across numerous industries insurers technology service providers alike racing build use cases ia streamline various stages automotive insurance claim automation cannot viewed isolated lever standalone solution bring efficiency cost savings ia integrates channels technologies platforms unleashes full potential white paper provides insight integrated ia approach improve auto claims process deliver favorable customer experience first notice loss fnol claims triage adjudication repairs quality claim registrations reserve estimation control subrogation total loss settlement special investigation insurance companies salvages recovery unit legal payments vendors ty p c l va lu e c h n f n u c la w h e p p e r * streamlining automotive claims process via integrated intelligent automation * first notice loss fnol claim registrations stage overview fnol claim registration stage call center agents collect data claim well insured ultimately register claim call center agents may alleviate customer doubts provide support related registration claim fnol intake process potentially impacts downstream stages claim processing common problem areas unstructured data stemming voice interaction decentralized information kept various systems voice interaction decentralization information result swaths discrete unstructured data deposits organization require extensive operational effort convert actionable structured data fulfill business task results unstructured data stage affect claim registration substantial impact downstream claims processes ultimately leads increased operational expenses insurer poor customer experience insured integrated ia approach * apply real time natural language processing nlp speech text authenticate caller assist call agent data collection * supplement interactions insured carrier via mobile app portal additional channel * enable real time request response app trigger forms e disclaimers information exchanges needed claim filing process phone conversation agent taking place * prefill significant amount client data expedited processing long term predict relevant data captured via machine learning ml * provide means request additional services notifying emergency contacts associated policy requesting ambulances tow trucks providing tracking services fulfilled * use ai identify notify optimal location cost service quality etc service providers track status service fulfilled * ensure ability launch video conferencing vc call interaction guidance customer service rep cases special handling person assistance required w h e p p e r * streamlining automotive claims process via integrated intelligent automation * claims triage adjudication reserve estimation stage overview claims triaging stage optimal claims adjudicator assigned case based skillset adjudicator assigned provide initial estimate claim amount create damage appraisal report collecting additional information evidence common problem areas manual triaging claims may lead non optimal assignment skilled adjusters decentralized inconsistent data gathering frameworks adjudicators departments lead increases turnaround time operational expenses additionally prevailing manual reserve booking management processes delay accounting financial visibility accuracy integrated ia approach * decision engine created effectively assign adjudicator case based personal profile availability skillset type claim * adjudicator presented information provided claimant offered relevant forms automatically ability request additional information needed via common channel * claimant able receive documents forms electronically option sign forms digitally * business process management bpm tools used orchestrate processes transmit data forms various departments platforms * claim data integrated actuarial financial systems automatic reserve management * artificial intelligence ai used determine initial estimate loss allow reserves triggered booking automatically * initial estimates produced using ml models algorithms could used settling claim variance later stages claims processing w h e p p e r * streamlining automotive claims process via integrated intelligent automation * repairs quality control stage overview repair stage approved body shop repairs damages procures parts provides customer service directly insured body shop determines parts required procured based agreed upon parts services rate conjunction repair stage quality control audit performed insurer make sure body shop overprice repair items provides quality customer service common problem areas inconsistent unstructured data flow external vendor insurer may cause important data loss extra time taken decide repair shop may impact quality experience customer already dealing stress accident lack real time status repair results uninformed agitated customers time spent fnol sending vehicle body shop customer service tow truck body shop selection results high cost insurer components threaten reduce quality service incur indirect expenses insurer exacerbate tense situation customer integrated ia approach * depending captured damage type incident location customer address customer reviews ia suggest list appropriate repair shops create initial estimates based fnol data experience body shop insured decide suitable option * using ml models system able flag pricing inconsistency * carrier portal app extended interact body shops allow see insurer' original estimate -- along underlying data pictures gathered fnol valuation process -- adjust needed portal enable body shop provide status repair feed information back insured well carrier * random audit automatically triggered validate estimation samples w h e p p e r * streamlining automotive claims process via integrated intelligent automation * subrogation insurance companies payments vendors stage overview subrogation substitution one person group another respect debt insurance claim accompanied transfer associated rights duties put simply subrogation principle insurance means insurer pays full compensation insured loss insured property insurer holds legal right claim collect loss amount third party insurance person payment vendor occurs vendor repair shop supplier rendered services common problem areas tedious paperwork settle initiate subrogation lengthy external party interaction increases turnaround time operational cost manual accounts payable ap accounts receivable ar reconciliation often causes human error delays recognition funds paper trail builds three way barrier customer insurer third party making process convoluted everyone involved also puts constraints insurer' cash reserves months spent gathering discrete information tracking reimbursement funds integrated ia approach * based pre set rules ia enabled platform flag cases subrogation applicable * integration virtual payment providers expedite payment process vendors * ia ml utilized extract information invoices received vendors apply consistent approach entering data platform * reconciliation process enabled via mix business rules ml * rules engine track service level agreements slas around ap ar avoid penalties ap minimize risk profile ar * digital content management leveraged enable communication insurance companies customers vendors w h e p p e r * streamlining automotive claims process via integrated intelligent automation * total loss settlement salvages recovery stage overview total loss settlement process aimed make sure total loss damages paid insured according contract salvages recovery process aimed make sure salvaged cars sold common problem areas manually tracking total loss cases results material process inefficiencies ultimately leads increase operational costs lack controls sufficient market data evaluate appropriate market value vehicles increases risk overestimating underestimating value could subsequently lead loss funds additionally manually tracking salvage inventory increases time required recycle vehicle incur appropriate amount integrated ia approach * cases contract defined document rather policy administration database ia utilized extract relevant information deductibles contract exclusions state thresholds etc using cognitive document processing cdp * rules engine flag potential total loss cases damage estimates higher market value vehicle * automated payments electronic fund transfers efts scheduled based confirmation total loss * digital content management leveraged enable communication insurance companies customers * integration third parties allows insurers uncover market value vehicle utilized determining total loss amount w h e p p e r * streamlining automotive claims process via integrated intelligent automation * special investigation unit legal stage overview special investigation unit siu reviews investigates claims flagged potential fraud automatically system manually adjuster common problem area lack relevant rules flag potential fraud increases possibility fraud cases going undetected inaccuracy data misleads machine learning engine behind predictive adoptive model false positives negatives process lead displacement valuable talent capital employees spend time researching legitimate claims fraud financial loss occurs material fraud cases identified integrated ia approach * rules engine combined ia increases accuracy likelihood detecting potential fraud cases * adjusters mark cases potential fraud within portal provide structured input granular categorization later evaluated ia minimize false positives expediate fraud investigations * portal provide visibility legal departments collecting data relevant information key details case * integration third party data providers allows insight customer profiles proceeding litigation case w h e p p e r * streamlining automotive claims process via integrated intelligent automation * conclusion fnol legal operating procedures paramount integrate communication channels systems enable seamless automation however automating processes insurance organizations must centralized process team focuses reviewing optimizing processes macro micro level across departments identifies opportunities engineer processes order automation program garner expected results failure could result unintended process inefficiencies lead millions - even billions - indirect hidden expenses manifest long customer service calls fees penalties - diminish user experience lead loss customers customer satisfaction closely tied bottom line vital insurers use intelligent automation prioritize process transparency consumers enable real time status tracking settlements increase ease reviewing relevant case details documents improve visibility estimated execution timelines customers know expect next without needing create extensive paper trail correspondences offers unique opportunity insurers step future convoluted infrastructures designed support numerous discreet siloed processes become thing past industry reaches whole new level disruption competition innovation w h e p p e r * streamlining automotive claims process via integrated intelligent automation * epam systems since epam systems inc nyse epam leveraged global software engineering expertise become leading global university drive suite product development digital platform engineering top digital newtown pa usa product design agency 'engineering dna' innovative strategy consulting design capabilities epam sales epam com works collaboration customers deliver next gen p solutions turn complex business challenges real business f outcomes epam' global teams serve customers countries across north america europe asia australia epam recognized market leader multiple categories among top global independent research agencies one four technology companies appear forbes fastest growing public tech companies list every year publication since learn http www epam com follow us twitter epamsystems linkedin (c) epam rights reserved epam b352","NLP":"white paper streamlining automotive claims process via integrated intelligent automation manubhav jain stas chulsky astghik mkhitaryan principal senior manager senior manager insurance intelligent intelligent c n u lt n g automation automation c n u lt n g c n u lt n g november contents introduction first notice loss fnol claim registrations claims triage adjudication reserve estimation repairs quality control subrogation insurance companies payments vendors total loss settlement salvages recovery special investigation unit legal conclusion introduction intelligent automation ia emerges best practice across numerous industries insurers technology service providers alike racing build use cases ia streamline various stages automotive insurance claim automation cannot viewed isolated lever standalone solution bring efficiency cost savings ia integrates channels technologies platforms unleashes full potential white paper provides insight integrated ia approach improve auto claims process deliver favorable customer experience first notice loss fnol claims triage adjudication repairs quality claim registrations reserve estimation control subrogation total loss settlement special investigation insurance companies salvages recovery unit legal payments vendors ty p c l va lu e c h n f n u c la w h e p p e r * streamlining automotive claims process via integrated intelligent automation * first notice loss fnol claim registrations stage overview fnol claim registration stage call center agents collect data claim well insured ultimately register claim call center agents may alleviate customer doubts provide support related registration claim fnol intake process potentially impacts downstream stages claim processing common problem areas unstructured data stemming voice interaction decentralized information kept various systems voice interaction decentralization information result swaths discrete unstructured data deposits organization require extensive operational effort convert actionable structured data fulfill business task results unstructured data stage affect claim registration substantial impact downstream claims processes ultimately leads increased operational expenses insurer poor customer experience insured integrated ia approach * apply real time natural language processing nlp speech text authenticate caller assist call agent data collection * supplement interactions insured carrier via mobile app portal additional channel * enable real time request response app trigger forms e disclaimers information exchanges needed claim filing process phone conversation agent taking place * prefill significant amount client data expedited processing long term predict relevant data captured via machine learning ml * provide means request additional services notifying emergency contacts associated policy requesting ambulances tow trucks providing tracking services fulfilled * use ai identify notify optimal location cost service quality etc service providers track status service fulfilled * ensure ability launch video conferencing vc call interaction guidance customer service rep cases special handling person assistance required w h e p p e r * streamlining automotive claims process via integrated intelligent automation * claims triage adjudication reserve estimation stage overview claims triaging stage optimal claims adjudicator assigned case based skillset adjudicator assigned provide initial estimate claim amount create damage appraisal report collecting additional information evidence common problem areas manual triaging claims may lead non optimal assignment skilled adjusters decentralized inconsistent data gathering frameworks adjudicators departments lead increases turnaround time operational expenses additionally prevailing manual reserve booking management processes delay accounting financial visibility accuracy integrated ia approach * decision engine created effectively assign adjudicator case based personal profile availability skillset type claim * adjudicator presented information provided claimant offered relevant forms automatically ability request additional information needed via common channel * claimant able receive documents forms electronically option sign forms digitally * business process management bpm tools used orchestrate processes transmit data forms various departments platforms * claim data integrated actuarial financial systems automatic reserve management * artificial intelligence ai used determine initial estimate loss allow reserves triggered booking automatically * initial estimates produced using ml models algorithms could used settling claim variance later stages claims processing w h e p p e r * streamlining automotive claims process via integrated intelligent automation * repairs quality control stage overview repair stage approved body shop repairs damages procures parts provides customer service directly insured body shop determines parts required procured based agreed upon parts services rate conjunction repair stage quality control audit performed insurer make sure body shop overprice repair items provides quality customer service common problem areas inconsistent unstructured data flow external vendor insurer may cause important data loss extra time taken decide repair shop may impact quality experience customer already dealing stress accident lack real time status repair results uninformed agitated customers time spent fnol sending vehicle body shop customer service tow truck body shop selection results high cost insurer components threaten reduce quality service incur indirect expenses insurer exacerbate tense situation customer integrated ia approach * depending captured damage type incident location customer address customer reviews ia suggest list appropriate repair shops create initial estimates based fnol data experience body shop insured decide suitable option * using ml models system able flag pricing inconsistency * carrier portal app extended interact body shops allow see insurer' original estimate -- along underlying data pictures gathered fnol valuation process -- adjust needed portal enable body shop provide status repair feed information back insured well carrier * random audit automatically triggered validate estimation samples w h e p p e r * streamlining automotive claims process via integrated intelligent automation * subrogation insurance companies payments vendors stage overview subrogation substitution one person group another respect debt insurance claim accompanied transfer associated rights duties put simply subrogation principle insurance means insurer pays full compensation insured loss insured property insurer holds legal right claim collect loss amount third party insurance person payment vendor occurs vendor repair shop supplier rendered services common problem areas tedious paperwork settle initiate subrogation lengthy external party interaction increases turnaround time operational cost manual accounts payable ap accounts receivable ar reconciliation often causes human error delays recognition funds paper trail builds three way barrier customer insurer third party making process convoluted everyone involved also puts constraints insurer' cash reserves months spent gathering discrete information tracking reimbursement funds integrated ia approach * based pre set rules ia enabled platform flag cases subrogation applicable * integration virtual payment providers expedite payment process vendors * ia ml utilized extract information invoices received vendors apply consistent approach entering data platform * reconciliation process enabled via mix business rules ml * rules engine track service level agreements slas around ap ar avoid penalties ap minimize risk profile ar * digital content management leveraged enable communication insurance companies customers vendors w h e p p e r * streamlining automotive claims process via integrated intelligent automation * total loss settlement salvages recovery stage overview total loss settlement process aimed make sure total loss damages paid insured according contract salvages recovery process aimed make sure salvaged cars sold common problem areas manually tracking total loss cases results material process inefficiencies ultimately leads increase operational costs lack controls sufficient market data evaluate appropriate market value vehicles increases risk overestimating underestimating value could subsequently lead loss funds additionally manually tracking salvage inventory increases time required recycle vehicle incur appropriate amount integrated ia approach * cases contract defined document rather policy administration database ia utilized extract relevant information deductibles contract exclusions state thresholds etc using cognitive document processing cdp * rules engine flag potential total loss cases damage estimates higher market value vehicle * automated payments electronic fund transfers efts scheduled based confirmation total loss * digital content management leveraged enable communication insurance companies customers * integration third parties allows insurers uncover market value vehicle utilized determining total loss amount w h e p p e r * streamlining automotive claims process via integrated intelligent automation * special investigation unit legal stage overview special investigation unit siu reviews investigates claims flagged potential fraud automatically system manually adjuster common problem area lack relevant rules flag potential fraud increases possibility fraud cases going undetected inaccuracy data misleads machine learning engine behind predictive adoptive model false positives negatives process lead displacement valuable talent capital employees spend time researching legitimate claims fraud financial loss occurs material fraud cases identified integrated ia approach * rules engine combined ia increases accuracy likelihood detecting potential fraud cases * adjusters mark cases potential fraud within portal provide structured input granular categorization later evaluated ia minimize false positives expediate fraud investigations * portal provide visibility legal departments collecting data relevant information key details case * integration third party data providers allows insight customer profiles proceeding litigation case w h e p p e r * streamlining automotive claims process via integrated intelligent automation * conclusion fnol legal operating procedures paramount integrate communication channels systems enable seamless automation however automating processes insurance organizations must centralized process team focuses reviewing optimizing processes macro micro level across departments identifies opportunities engineer processes order automation program garner expected results failure could result unintended process inefficiencies lead millions - even billions - indirect hidden expenses manifest long customer service calls fees penalties - diminish user experience lead loss customers customer satisfaction closely tied bottom line vital insurers use intelligent automation prioritize process transparency consumers enable real time status tracking settlements increase ease reviewing relevant case details documents improve visibility estimated execution timelines customers know expect next without needing create extensive paper trail correspondences offers unique opportunity insurers step future convoluted infrastructures designed support numerous discreet siloed processes become thing past industry reaches whole new level disruption competition innovation w h e p p e r * streamlining automotive claims process via integrated intelligent automation * epam systems since epam systems inc nyse epam leveraged global software engineering expertise become leading global university drive suite product development digital platform engineering top digital newtown pa usa product design agency 'engineering dna' innovative strategy consulting design capabilities epam sales epam com works collaboration customers deliver next gen p solutions turn complex business challenges real business f outcomes epam' global teams serve customers countries across north america europe asia australia epam recognized market leader multiple categories among top global independent research agencies one four technology companies appear forbes fastest growing public tech companies list every year publication since learn http www epam com follow us twitter epamsystems linkedin (c) epam rights reserved epam b352","text_word_count":2502,"text_unique_words":1006,"_deepnote_index_column":2},{"Name":"./sampledocs/Issues_Paper_on_Increasing_Digitalisation_in_Insurance_and_its_Potential_Impact_on_Consumer_Outcomes.pdf","Type":"pdf","Text":"      Issues Paper on Increasing Digitalisation in\n              Insurance and its Potential Impact on\n                                   Consumer Outcomes\n                                              November 2018\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 1 of 36\nAbout the IAIS\nThe International Association of Insurance Supervisors (IAIS) is a voluntary membership\norganisation of insurance supervisors and regulators from more than 200 jurisdictions. The\nmission of the IAIS is to promote effective and globally consistent supervision of the insurance\nindustry in order to develop and maintain fair, safe and stable insurance markets for the benefit\nand protection of policyholders and to contribute to global financial stability.\nEstablished in 1994, the IAIS is the international standard setting body responsible for\ndeveloping principles, standards and other supporting material for the supervision of the\ninsurance sector and assisting in their implementation. The IAIS also provides a forum for\nMembers to share their experiences and understanding of insurance supervision and insurance\nmarkets.\nThe IAIS coordinates its work with other international financial policymakers and associations\nof supervisors or regulators, and assists in shaping financial systems globally. In particular, the\nIAIS is a member of the Financial Stability Board (FSB), member of the Standards Advisory\nCouncil of the International Accounting Standards Board (IASB), and partner in the Access to\nInsurance Initiative (A2ii). In recognition of its collective expertise, the IAIS also is routinely\ncalled upon by the G20 leaders and other international standard setting bodies for input on\ninsurance issues as well as on issues related to the regulation and supervision of the global\nfinancial sector.\nIssues Papers provide background on particular topics, describe current practices, actual\nexamples or case studies pertaining to a particular topic and/or identify related regulatory and\nsupervisory issues and challenges. Issues Papers are primarily descriptive and not meant to\ncreate expectations on how supervisors should implement supervisory material. Issues Papers\noften form part of the preparatory work for developing standards and may contain\nrecommendations for future work by the IAIS.\nInternational Association of Insurance Supervisors c/o Bank for International Settlements\nCH-4002 Basel\nSwitzerland\nTel: +41 61 280 8090\nFax: +41 61 280 9151\nwww.iaisweb.org\nThis document was prepared by the Market Conduct Working Group in consultation with IAIS\nMembers.\nThis document is available on the IAIS website (www.iaisweb.org).\n© International Association of Insurance Supervisors (IAIS), 2018.\nAll rights reserved. Brief excerpts may be reproduced or translated provided the source is stated.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 2 of 36\nContents\nExecutive summary ............................................................................................................... 4\nAcronyms .............................................................................................................................. 6\n1    Introduction .................................................................................................................... 7\n2    Product Design .............................................................................................................. 9\n   2.1    Digitalisation Impact on Product Design .................................................................. 9\n   2.2    Examples of digitalisation impact on product design ............................................. 10\n     2.2.1      Background ................................................................................................... 10\n     2.2.2      Shared economy............................................................................................ 10\n     2.2.3      Usage based insurance ................................................................................. 11\n     2.2.4      On-demand insurance ................................................................................... 12\n3    Marketing, Sales & Distribution .................................................................................... 14\n   3.1    Marketing and promotions..................................................................................... 14\n     3.1.1      Benefits and opportunities.............................................................................. 15\n     3.1.2      Potential Risks ............................................................................................... 15\n   3.2    (Robo) Advice ....................................................................................................... 17\n     3.2.1      Types of advice.............................................................................................. 17\n     3.2.2      Benefits and opportunities of robo advice ...................................................... 18\n     3.2.3      Potential risks ................................................................................................ 18\n   3.3    Price Comparison Websites .................................................................................. 19\n     3.3.1      Benefits and opportunities.............................................................................. 20\n     3.3.2      Potential risks ................................................................................................ 20\n   3.4    Disclosure and informed decision-making ............................................................. 21\n     3.4.1      Benefits and opportunities.............................................................................. 21\n     3.4.2      Potential Risks ............................................................................................... 22\n4    Supervisory issues ....................................................................................................... 24\n5    Conclusion and recommendations ............................................................................... 34\nAnnex: Digital technologies and alternative business models affecting insurance business 35\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                                                    Page 3 of 36\nExecutive summary\n1.       Digitalisation is transforming insurance business. Examples such as mobile devices, the\ninternet of things (IoT), Big Data, artificial intelligence (AI), chat-bots, distributed ledger\ntechnology (DLT), and robo advisors have an impact throughout the insurance value chain: from\nthe design, underwriting and pricing of products, their marketing and distribution, through to\nclaims processing and the ongoing management of customers.\n2.       The purpose of this paper is to consider the impact of the trend of increasing digitalisation\nin insurance on consumer outcomes and insurance supervision in light of Insurance Core\nPrinciple 19 on Conduct of Business. The focus is on product design and underwriting, along\nwith marketing, sales and distribution aspects of the insurance value chain. It is recognised that\nthe impact of digitalisation may differ between jurisdictions depending on the legal frameworks\nin place.\n3.       In respect of product design, digitalisation may affect the nature of insurance coverage\nthrough, for example, on-demand insurance, usage-based insurance and insurance based on\nconsumer-generated data from vehicles, homes or wearable devices. This can potentially\nservice a broader clientele (including people that are currently un(der)-served) if insurers are\nable to adapt to evolving demands from the market. The data available to insurers on use of,\nfor example motor vehicles, will inform the pricing of the product, while consumers need to be\naware of such use. Risk pricing can be more tailored to the use and risk profile of the customer,\nwhich can affect both the price and required reserves of insurers.\n4.       In terms of marketing and promotions, digitalisation will have an impact on the\ninformation provided to consumers. Regardless of the use of digital technology, the information\nprovided needs to be timely, clear, accurate and not misleading.\n5.       Greater availability of customer related data, increased analytics and enhanced digital\ndeployment tools enable insurers and intermediaries to identify opportunities across the\ninsurance value chain to reduce customer friction, increase efficiencies and improve the overall\ncustomer experience through digital technology.\n6.       The use of social media may enable insurers and intermediaries to better reach target\nmarkets. This may reduce marketing costs. It can, for example, improve customers’ experience\nby offering easier and quicker ways for the insurer and consumer to communicate. On the other\nhand, social media applications may not be transparent to consumers. This can result in\nconsumers being “nudged” without being aware – such as when consumers are confronted with\nunsolicited offerings based on their use of the internet. There is a risk that customers are\npersuaded into buying products or add-ons that are not in their best interest.\n7.       A specific emerging sales method is the use of robo advice. This may improve\naccessibility of products to the customer. It will, however, require proper design of underlying\nalgorithms and adequate availability and use of customer data. Also, depending on the\nsophistication of the algorithm and available datasources, not all benefits of face-to-face\ninteraction between salesperson and customer may succeed, for example, in identifying (non-\nverbal) hesitation. Furthermore, flaws in the design and operation of the algorithm can create a\nrisk of selling products that are not (entirely) in the interest of the particular customer.\n8.       Another development for promotion and sales is the use of price comparison websites\n(PCWs). These can provide automated suggestions or proposals on products, providers and\nprices based on input by the consumer. Increased accessibility and comparability of information\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 4 of 36\non insurers and products as well as easy use of the on-line systems are benefits to the customer.\nThere may, however, be issues around transparency with respect to the identity and\nindependence of the owner/operator of the comparison website. Consumers may also be at risk\nof selecting products that are less suitable for their needs if they solely focus on price and not\nother elements of the product, such as its coverage.\n9.       More generally, innovations influence the presentation and disclosure of information.\nThey offer the potential to provide relevant information to consumers in a useable manner at the\nrelevant time. Large volumes of information may, however, be difficult to read and understand,\nfor instance when using smart phones. Insurers should be mindful of the risk of misconceptions\nby consumers and, therefore, flaws in consent given by digital means.\n10.      As digitalisation changes the way insurance products are designed and distributed,\nsupervisors should monitor these developments, engage stakeholders both within and outside\nthe insurance industry and consider new supervisory responses to protect consumers’ interests.\nOne of the key challenges to supervisors will be to consider a balanced approach to facilitate\ninnovations while maintaining the level of consumer protection stipulated in laws and\nregulations. Supervisors are likely to be confronted with new insurance market participants, like\nstart-ups and “Big Tech” firms. These entities may have different perspectives on consumer\ninterest and compliance culture than traditional incumbent insurers. Supervisors should be\ncognisant of this and may need to take a proactive approach including by “educating” these new\nmarket participants. Other challenges supervisors face are developing new tools and skills for\nsupervision of increasingly digitalised firms, enhancing cooperation with financial and other\nauthorities, safeguarding the supervisory parameters to prevent regulatory arbitrage and\nenhancing information security.\n11.      Supervisors should consider taking appropriate steps, such as issuing guidelines, to help\npromote responsible use of new technologies by insurers and intermediaries and safeguard the\nfair treatment of customers.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 5 of 36\nAcronyms\n   ACPR             Autorité de contrôle prudentiel et de résolution (France)\n   AI               Artificial Intelligence\n   AFM              Autoriteit Financiele Markten (Netherlands)\n   AMF              Autorité des marchés financiers (Québec)\n   ASIC             Australian Securities and Investments Commission\n   BaFin            Bundesanstalt für Finanzdienstleistungsaufsicht (Germany)\n   BoE              Bank of England\n   DLT              Distributed Ledger Technology\n   FCA              Financial Conduct Authority\n   FINMA            Financial Market Supervisory Authority (Switzerland)\n   FinTech          Financial technology\n   FSB              Financial Stability Board\n   IAIS             International Association of Insurance Supervisors\n   ICP              Insurance Core Principle\n   Insurtech        Insurance technology\n   IoT              Internet of Things\n   IT               Information Technology\n   MAS              Monetary Authority of Singapore\n   ML               Machine Learning\n   NAIC             National Association of Insurance Commissioners (USA)\n   PCW              Price Comparison Website\n   Regtech          Regulatory Technology\n   Suptech          Supervisory Technology\n   UBI              Usage based insurance\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 6 of 36\n1    Introduction\n12.      Described by some observers as the “fourth industrial revolution” 1, digitalisation is\nrapidly transforming societies and their economies. The velocity and scope of change are\nsignificant. Digitalisation has the potential to fundamentally change almost every industry in\nevery country. 2 One of those industries is insurance. There has been and continues to be a\nrevolution in insurance pricing, marketing, product design and claims settlement resulting from\ninsurers' use of new technologies and available data.\n13.      As diagram 1 illustrates, rapid change is evident throughout the insurance value chain:\nfrom the design, underwriting and pricing of products, their marketing and distribution, through\nto claims processing and the ongoing management of customer relationships. The examples of\ndigitalisation technologies – machine learning and artificial intelligence, distributed ledger\ntechnology (eg blockchain) – and applications – telematics, robo-advisers, peer-to-peer and\nplatford business models – as set out in diagram one, are varied. These are described in the\nAnnex.\n         DIAGRAM 1: Digitalisation and the insurance value chain\n  •  Customer-specific “targeted”               •    Automated (including non-                   •    Platform business models\n     marketing                                       human) product service                      •    360 degree view of customer for\n  •  Robo-advice, AI and chat-                       centers using robo-advice,                       consultants\n     bots                                            chat-bots and AI                            •    Continuous real time data\n  •  Internet sales and Price                   •    Big Data enables ability to                      enabling focus on high value\n     Comparison websites                             predict what customers want                      customers\n  •  Social media and SMART                          and need before they ask for                •    Unstructured data (eg voice)\n     phone/device channels for                       it                                               analysis and learning\n     direct distribution                        •    Continuous real-time\n                                                     customer communication and\n                                                     U/W\n        Marketing,\n                                 Pricing and              Product                 Claims                  Customer\n        Sales and\n                                Underwriting            Management               Handling                Interactions\n       Distribution\n              •     Telemetrics – customers and insurers                      •   Fraud detection using Big Data and\n                    understand risk much better (wearables, IoT,                  Blockchain\n                    SMART phones, apps)                                       •   Blockchain facilitating trust-worthy and\n              •     Big Data enabling more granular and                           timely claims information\n                    accurate pricing and faster U/W                           •   AI and drones in assessing processes\n              •     Blockchain technologies to seamlessly                     •   Claims cost efficiencies re online/SMART\n                    manage and instantly verify data sources                      device claims lodgement, AI/automated\n              •     Peer-to-peer insurance models                                 assessing, optimised pay-outs, reduced\n              •     Granular, customer-specific product                           labour costs\n                    offerings including usage based insurance                 •   Supply chain management efficiencies,\n              •     Genetic data – potential impact on pricing                    vertically integrated claims processes\n                    and availability\n1  See for example, Schwab, K., “The Fourth Industrial Revolution: what it means, how to respond”,\nhttps://www.weforum.org/agenda/2016/01/the-fourth-industrial-revolution-what-it-means-and-how-to-\nrespond/, 14 January 2016.\n2 Schwab.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                                             Page 7 of 36\n14.     The availability of data from sources such as telematics and wearable devices enable\ninsurers to design and price products on the basis of more information about the consumer.\nDevelopments in artificial intelligence and machine learning enable the provision of automated\nadvice and facilitate fraud detection. Comparison websites can provide consumers with more\ninformation about the range of products available. New technologies are able to speed up\nprocesses, such as claims handling, and can lead to efficiencies that drive down costs.\n15.     Whilst digitalisation has the potential to benefit consumers, it does give rise to risks that\ncould impact fair consumer outcomes, which should be considered by supervisors in light of the\nrequirements on Conduct of Business in Insurance Core Principle (ICP) 19. These include\npotential impacts from reduced face-to-face contact, insufficient consumer understanding of the\nproduct or service and its provider, risks in the security and potential misuse of increasing\namounts of consumer data, and potential exclusion for some consumers. The collection of data\non policyholders may enable a more granular risk categorisation that could potentially affect risk\npooling principles and may lead to issues around affordability of certain insurance products,\npossibly even leading to exclusion. 3\n16.     Supervisors should, therefore, (possibly in new ways) monitor consumer outcomes\ncarefully to ensure that supervisory regimes continue to facilitate the benefits to consumers from\ntechnology and innovation, whilst safeguarding policyholder protection. This is vital if the intent\nof ICP 19 is to be met.\n17.     The purpose of this paper is to consider the impact of the increasing use of digital\ntechnology in insurance. It will consider consumer outcomes and discuss what digitalisation\nmeans for insurance supervision. It is recognised that the impact of digitalisation may differ\nbetween jurisdictions depending on the legal frameworks in place. A distinction should be made\nbetween applications and the data/information these applications use or generate. While this\npaper cover both innovative applications and – more generally – the use of data, a separate\npaper will discuss in more details the use of personal and other data from a conduct of business\nperspective.\n18.     This paper provides these considerations in the context of other IAIS work on FinTech\nand Insurtech. Accordingly, this paper focuses on the product design and underwriting along\nwith marketing, sales and distribution aspects of the insurance value chain. The impact of the\nincreasing use of digital technology on other aspects of the insurance value chain will be\naddressed in other IAIS material.\n19.     The paper has three sections:\n    •   Section 2 considers the impacts of digitalisation on product design and underwriting.\n        This sections also gives examples of digitalisation impact on product design;\n    •   In section 3, robo advice and price comparison websites are considered to illustrate\n        digitalisation’s impact on the marketing, sale and distribution of those products; and\n    •   Finally, in section 4, the paper describes the challenges faced by supervisors in\n        responding to these developments.\n3 Paragraph 10 of the IAIS report “FinTech Developments in the Insurance Industry”, March 2017.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 8 of 36\n2    Product Design\n2.1    Digitalisation impact on product design\n20.     Digitalisation is changing the risk landscape and creating the need for, and enabling, the\ndevelopment of new types and lines of products. Consumer needs are evolving in the digital\nage with a growing expectation for accessibility of services and solutions at any time, in any\nplace, and in a variety of ways. Consumer trends and habits supported by new technologies,\nsuch as greater connectivity through wearables, smart phones and smart homes, and greater\noptionality through the sharing economy, are impacting the way insurers design products for\ntheir policyholders.\n21.     The changes driven by new technology create new opportunities but also new\nchallenges and risks for insurers and intermediaries. The need for new insurance coverages\nand new products is likely to grow. Digitalisation brings opportunities to better serve customers\nand their changing needs, and may also serve as a means to better reach underserved markets.\n22.     Digitalisation may also lead to a shift from distribution focused product design (supply-\ndriven) to consumer focused product design (demand-driven). While this may provide great\nopportunity, it may be a challenge for insurers to meet consumers’ growing need of covering\nnew risks, or covering them in a different way, in the future.\n23.     Digitalisation is impacting how insurers develop, design and underwrite their products.\nAdvancement in technology may enable the development of more adaptable or tailored products\nand the creation of new insurance products:\n    •   Big Data means more data for risk assessment, which can enable underwriting to be\n        based on more granular data, which may, in turn, increase accuracy and allow for faster\n        and more risk-specific underwriting. This needs to be balanced against the privacy\n        concerns of the individual;\n    •   AI may create new possibilities for risk assessment and underwriting. For example,\n        insurers can use algorithms in combination with AI that uses the customer’s insurance\n        history and lifestyle information to suggest insurance products and for onboarding;\n    •   The IoT may create new products focusing on prevention or situational insurance, for\n        example, a sensor will be able to monitor a household's water consumptions patterns,\n        detecting potential leaks and interrupting the flow before the basement is flooded, thus\n        preventing major damage and costly claims 4. Such tools can improve the interaction with\n        and provide value to the customer, though they can raise concerns if data from devices\n        (eg alerts) are used for premium increases or changes to existing coverage;\n    •   Telematics In the context of IoT, telematics involve telecommunications, sensors and\n        computer science to allow sending, receiving, storing and processing data via\n        telecommunication devices, with or without interfering with or steering of remote objects;\n        and\n    •   DLT may be able to seamlessly manage and instantly verify data sources. Smart\n        contracts (ie programmes that automatically execute the claim payment under pre-\n        defined conditions stored in the blockchain) have the potential to be fully digital and fully\n        automated products, as could be the case for agricultural parametric/index-based\n        insurance. 5 If this technology proves to be a viable tool, it could transform the insurance\n4 Bain&Company, \"Digitalization in Insurance: The Multibillion Dollar Opportunity\", Henrik Naujoks,\nFlorian Mueller and Nikos Kotalakidis, March 20, 2017.\n5 The Geneva Papers 2017: \"The Impact of Digitalization on the Insurance Value Chain and the\nInsurability of Risks\", Martin Eling and Martin Lehman, Institute of Economics, University of St. Gallen.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 9 of 36\n         industry through a shared, transparent record of contract-related information, enabling\n         all parties to have an immutable audit trail underpinning end-to-end underwriting and\n         claims governance without the need for an intermediary. 6\n24.      In the following section, we consider these benefits and risks in the context of examples\nof the impacts of digitalisation on product design.\n2.2      Examples of digitalisation impact on product design\n2.2.1    Background\n25.      There are numerous examples of digitalisation changing the nature of insurance\nproducts. The following section provides examples of three of the most widespread and\nsignificant examples namely:\n     •   Shared economy;\n     •   Usage-based-insurance; and\n     •   On-demand insurance.\n26.      These examples involve a fundamental change in the design of product. But there are\nalso examples of changes where digitalisation has facilitated small specific changes to product\nfeatures.\n   United Kingdom\n   UK-based FinTech start-up Cuvva was set up to address the gap of providing hourly car\n   insurance to infrequent drivers who wanted to borrow other people’s cars. Cuvva allows\n   customers to arrange cover via an app in seconds. Cuvva manages the sale, service and\n   first notification of loss process through a mobile app.\n   Since launching the initial car sharing product, Cuvva has since launched a second\n   proposition designed for those who own a car, but seldom drive it. Customers pay a small\n   amount to insure their car whilst not driving, and pay an additional amount via the app for\n   the hours that they drive it.\n   Netherlands\n   Clixx (a product of Dutch insurer OHRA) offers the opportunity to insure also a borrowed\n   car. The product is bought per day. Clixx’ premium is lower if the borrowed car is already\n   fully insured, as compared to when the borrowed car only has the legally obligated liability\n   insurance.\n2.2.2    Shared economy\n27.      New sharing models are creating a unique challenge as traditional insurance protection\nand coverage may not align with the needs and approaches taken in the shared economy. Many\ninsurance products currently offered are based on exclusive legal or economic ownership of a\ngood. The shared economy is based on the shared use of goods. Additionally, traditional\ninsurance products are generally intended to cover personal or commercial use of a good; they\nare not designed to cover part-time business use, whether compensated or not.\n6 Strategic RISK Europe: \"How digitalisation will transform the risk and insurance industry\": Dieter\nGoebbels, country manager Germany and regional manager Central Europe at XL Catlin.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 10 of 36\n28.     The availability of insurance coverage adapted to the needs of the participants of the\nshared economy is important for the further development of the sharing industry, and for\nacceptance by consumers. To grow the shared economy and adequately mitigate potential risk,\nparticipants – providers and users – need appropriate insurance coverage.\n29.     Currently, participants in the shared economy who try to obtain insurance coverage\nthrough traditional means may be faced with the impossibility of taking out coverage that fully\nmeets their particular needs. For example, drivers working for ride sharing businesses (Uber)\nand homeowners participating as a host in shared hosting services (Airbnb) have not always\nbeen able to find adequate insurance coverage. Traditional protections covering vehicles and\nhomes generally did not extend to new businesses in the sharing industry where personal\nproperty is used in part-time business. The insurance industry has already developed new\nproducts to meet the need for adapted coverage.\n30.     It is important for consumers to understand the differences and limitations of their\ninsurance coverage when acting as either a provider or user in the shared economy. 7 When\noffering products to consumers taking part in the shared economy, it is equally important for\ninsurers to be clear on such limitations. There is the risk of disruption and damage to the\nreputation of the insurance industry if products designed to meet the shared economy do not\ndeliver the same level of consumer protection as traditional insurance products.\n2.2.3   Usage based insurance\n31.     Digitalisation is already used in automobile insurance. In terms of product design, a\ntraditional motor vehicle insurance policy is joined with a data collection and analytics tool to\ncapture data generated by the vehicle. In some cases, UBI product design includes various\nforms of real-time and after-the-fact feedback data transmission between the insurer and the\nconsumers. In terms of pricing, insurers’ model prices are based on vehicle-generated data,\nwhich in turn is based on the use of a vehicle by the insured – including where, how, when and\nby whom the vehicle is driven. UBI also produces data used in claim settlements to discover or\ncorroborate the damage event.\n32.     In order to obtain data on the use of a vehicle, insurers mostly use telematics through\nwhich they may identify granular driving habits (eg distance travelled, hard braking, number of\ntrips, destinations). This data allows insurers to establish a rate more personalised to the\nindividual customer.\n33.     Telematics can be app-based relying on a smartphone’s sensors and GPS signal,\nmaking this functionality dependent on the underlying smartphone’s capabilities. However, this\npersonalisation may have limits as the operation of the vehicle by a person other than the\npolicyholder will affect their data and the calculation of their insurance premiums. Thus it is\nimportant that consumers have the information they need to be properly informed and make\nsound decisions about insurance products that use a UBI programme. In addition, consumers\nshould be aware of whether participation in such programmes is on a voluntary basis or not.\nInformation that may help inform consumers about the features of the UBI programme may\ninclude things such as:\n    •   programme eligibility criteria;\n7 See NAIC White Paper, Insurance Implications of Home-Sharing: Regulator Insights and Consumer\nAwareness (http://www.naic.org/prod_serv/IHS-OP-16.pdf).\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 11 of 36\n     •    type of data collected;\n     •    use of data (eg as part of an investigation for the settlement of a claim; third parties to\n          whom access and use of data has been granted);\n     •    insurer employees who could have access to collected data;\n     •    impact of data on insurance premium; and\n     •    period used for insurance premium reviews.\n  Québec\n  In 2015, the AMF published a notice about its expectations regarding UBI programmes. This\n  initiative was intended to highlight, for insurers, firms and representatives offering non-life\n  insurance, the importance of effectively managing the risks associated with data sent via UBI\n  programmes used for automobile insurance underwriting. It also underlined the need to act\n  fairly in their dealings with consumers who participate in such programmes. 8\n  Netherlands\n  In the Netherlands, the sharing economy marks a trend of offering services that already\n  include an additional insurance. For example, the Dutch initiative Swapfiets (meaning “Swap\n  bike”) offers the opportunity to lease a bicycle for a fixed amount of money per month.\n  Repairments are included, as well as getting a new bike if the original one gets stolen. The\n  customer does not have to arrange an additional insurance product.\n2.2.4     On-demand insurance\n34.       The emergence of the shared economy, underpinned by a changing attitude and\nbehaviour of new consumer groups such as millennials, is causing a shift in the product lines of\ninsurers that are trying to respond to the need for self-directed, tech-savvy and hyper-\npersonalised products and services. Historically, most insurance has been purchased for a fixed\nperiod of coverage – typically six months or a year at a time. Insurers' systems and processes\nhave been developed around this type of product and coverage, with a few exceptions, such as\ntravel insurance. In response to both changing ownership models through the sharing economy\nin which a consumer may use a product or good for a limited period of time but not own the\nproduct, and changing consumer desires for coverage limited to more precise time frames (such\nas insuring a bicycle only when being used), new market entrants and incumbent insurers are\nresponding by developing new products and by adapting existing product lines, pricing and\ncustomer service experience to create on-demand insurance.\n  Trov\n  Trov is a mobile app that allows users to collect and store information about their\n  possessions including the value. It partners with insurers to enable users to insure specific\n  possessions for specified durations. Users can literally turn insurance coverage on and off\n  by sliding the appropriate option on their mobile phone. For example, they could choose to\n  insure their mobile phone only when they are out of their house.\n35.       The key to on-demand insurance is that it is temporal in nature. It provides insurance\ncoverage for specific periods of time that can be turned off and on. Users identify when they\n8                                   https://lautorite.qc.ca/fileadmin/lautorite/reglementation/assurances-inst-\ndepot/notice_automobile_usage-based.pdf\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 12 of 36\nneed insurance and get coverage for a specific period to meet that need. The ability to insure\n“moments” enables consumers to tailor coverage so that they only pay for coverage that they\nneed and to quickly alter insurance coverage to meet changing personal circumstances.\n36.     However, users need to be constantly engaged by actively turning their coverage on or\noff to obtain the benefits of on-demand insurance. Failure to constantly engage may result in\nbeing under or over insured. Insurers should be cognisant of this and build in controls to mitigate\nthe risks they pose, which could include:\n     •  proactive messages to remind consumers that their coverage is still active or, perhaps\n        more importantly, inactive. AI and learning from behavioural economics could be used\n        to optimise this messaging;\n     •  systems that enable customers to turn coverage on and off for set periods on a\n        reoccurring basis. For example they could have insurance for a mobile activated when\n        they are out of their house and use location tracking to verify this; and\n     •  inbuilt terms and conditions that provide back-up coverage in those circumstances when\n        customers inadvertently fail to turn on coverage.\n37.     Issues that have been discussed in this section, notably the issues around the use of\ndata, will be the subject of a seprate paper.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 13 of 36\n3     Marketing, Sales & Distribution\n3.1     Marketing and promotions\n38.       Consistent with ICP 19 (Conduct of Business) insurance products must be marketed and\nsold in a manner that pays due regard to the interests and needs of customers.\n39.       Insurers and intermediaries should be required to provide timely, clear and adequate\npre-contractual and contractual information to customers. 9 Supervisors should apply to digital\ninsurance activities requirements on transparency and disclosure that provide an equivalent\nlevel of protection to customers as those applied to insurance business conducted through non-\ndigital means. 10 Marketing and advertising through digital means offer new opportunities to\ninform and empower consumers but may pose certain additional challenges to the insurance\nindustry and supervisors alike and necessitate further consideration in terms of specific\nregulatory requirements or industry responses.\n  South Africa\n  The Insurance Policyholder Protection Rules were recently amended to ensure that the rules\n  relating to advertising and marketing would apply similarly irrespective of the medium used\n  for such advertising. The definitions of “advertisement” and “direct marketing” were clarified\n  and widened in scope as follows:\n  “advertisement” means any communication published through any medium and in any form,\n  by itself or together with any other communication, which is intended to create public interest\n  in the business, policies or related services of an insurer, or to persuade the public (or a part\n  thereof) to transact in relation to a policy or related service of the insurer in any manner, but\n  which does not purport to provide detailed information to or for a specific policyholder\n  regarding a specific policy or related service\n  “direct marketing” means the marketing of a policy by or on behalf of an insurer by way of\n  telephone, internet, digital application platform, media insert, direct or electronic mail in a\n  manner which entails the completion or submission of an application, proposal, order,\n  instruction or other contractual information required by the insurer in relation to the entering\n  into of a policy or other transaction in relation to a policy or related services, but excludes the\n  publication of an advertisement\n  Australia\n  ASIC's Good Practice Guide on Advertising 11 covers digital advertising, including online\n  advertisements, video streaming, social media and microblogging. Some of the points\n  highlighted are:\n  •    the particular impact of advertising in a 'high trust' environment and the need to distinguish\n       clearly between advertisement and other content (ie on blogs); and\n  •    that, while online advertising can be beneficial if it provides links to additional information\n       for customers, this cannot make up for any misleading impressions created by the initial\n       ad, and the need for balance in the promotion.\n9 Standard 19.7\n10 Guidance 19.7.23\n11 See http://download.asic.gov.au/media/1246974/rg234.pdf\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 14 of 36\n  ASIC has taken action against a number of potentially misleading social media\n  advertisements relating to self-managed super (pension) funds. 12\n3.1.1     Benefits and opportunities\n40.       Insurers and intermediaries are increasingly focusing on ways to improve marketing,\nsales and distribution and to increase their ability to reach customers by the same digitalisation\ntechnologies as seen in product design, including telematics, AI and Big Data. One example is\n\"targeted marketing\"; the ability to develop specific marketing messages for individual\ncustomers or potential customers.\n41.       Digital marketing may reduce the marketing costs of the insurer or the intermediary,\ncreating savings that may be passed on to the customer. The use of Big Data may result in a\nbetter understanding of customers, which can inform personalised marketing and appropriate\nlevels of disclosures.\n42.       Greater availability of customer related data, increased analytics and enhanced digital\ndeployment tools enable insurers and intermediaries to identify opportunities across the\ninsurance value chain to reduce customer friction, increase efficiencies and improve the overall\ncustomer experience through digital technology. Insurers and intermediaries can use enhanced\ncustomer experience as product differentiation in marketing campaigns. For example, a Québec\nstart-up, Covera, 13 based its marketing strategy on its digital solution that promises to break out\nof the standard insurance renewal process, identified as a common painpoint for customers.\n43.       The use of targeted social media campaigns to relay promotional material is a common\nway of targeting particular customers, who are most active on social media platforms. Insurers\nare tapping into this market by using social media to make marketing seem less like “cold\nadvertising” and more like information sharing, entertainment or “infotainment”. Examples in the\nUS include Gecko, Allstate’s Mayhem, and Progressive’s Flo, whose promotional mascots are\ninstantly recognisable to insurance customers and who all have their own social media\npresence. 14\n44.       To overcome fragmented communication with the policyholder, insurers and\nintermediaries can use digital devices and the Internet to connect with consumers throughout\nthe life of the policy, not only at underwriting or claim. For example, some insurers have started\nto provide customers with prevention tools, such as a free water and humidity detector that\nsends an alert by notification, text message or email if it senses a problem. Such initiatives are\npart of the new digital brand marketing strategy. These tools are not designed to provide data\nfor determining premium or coverage, but rather to attract and retain customers.\n3.1.2     Potential risks\n45.       The use of social media platforms and other digital marketing campaigns as well as the\nincreased collection and use of data may increasingly lead to customers being “nudged” or\ndirected, including by advertising, without them being aware. For example,\ninsurers,intermediaries and third party marketers may “target the customer through specific\n12 For example, see: http://asic.gov.au/about-asic/media-centre/find-a-media-release/2016-releases/16-\n041mr-asic-stops-potentially-misleading-smsf-social-media-advertising/\n13 https://covera.ai/\n14 http://www.digitalistmag.com/customer-experience/2017/04/13/social-media-in-insurance-marketing-\ntoday-05030403\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 15 of 36\nsearch engines or click on sponsored links, or may channel customers by highlighting or limiting\nparticular information to produce certain actions from the customer. There is often a lack of\ntransparency in the existence and purpose of these practices.\n46.       Targeted marketing through social media may, in some cases, become confusing for\ncustomers who may struggle with distinguishing neutral opinions on social media from\npromotional material sponsored by insurers.\n47.       Digital marketing and mobile based applications could also be used to respond in real\ntime to individual circumstances, including consumers’ emotions, such as when an individual’s\ninsecurity or want is heightened, or during key life events. In the context of insurance, where an\nintangible product is intended to mitigate personal fears, this type of emotional framing may\npose a concern.\nSingapore\nAn example of such small specific changes in Singapore is PolicyPal, a start-up that helps\nconsumers organise, understand and purchase insurance policies digitally through a mobile\napp. PolicyPal graduated from the MAS sandbox and is now an insurance broker registered\nwith MAS. 15\nAustralia\nAn example is the revelation in Australian media outlets that in May 2017 Facebook disclosed\nto a major Australian bank that it could exploit the moods and insecurities of users for the\npotential benefit of advertisers. 16 This followed media reports in 2012 that Facebook contributed\nto a published study with the Proceedings of the National Academy of Sciences of the USA\nwhere it showed via an experiment on over 689,000 users that it could make people more\npositive or negative through a process of what it described as \"emotional contagion\" – when\npositive expressions were reduced, people produced fewer positive posts and more negative\nposts; when negative expressions were reduced, the opposite pattern occurred. 17\nFrance\nFrench 2016-R-01 Recommendation on the use of social media for business purposes\nTo remind professionals of supervisory expectations and to explain how rules apply to the use\nof social media, the French Autorité de contrôle prudentiel et de résolution (ACPR) issued a\nrecommendation in November 2016, that applies to the banking and insurance sectors from 1\nOctober 2017.\nFirstly, advertising material issued through social media has to fulfil the applicable rules\nregarding the information disclosed and the presentation of this information.\nSecondly, professionals should refrain from having unfair commercial practices when using\nsocial media. For instance, misleading opinions (good or bad) issued by professionals on social\nmedia should be avoided, as well as the practice of buying “likes” or “followers”.\n16 https://www.theguardian.com/technology/2017/may/01/facebook-advertising-data-insecure-teens\n17 See: http://www.pnas.org/content/111/24/8788.full\nhttps://www.mckinsey.com/industries/financial-services/our-insights/insurtech-the-threat-that-inspires\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 16 of 36\nMoreover, according to the ACPR recommendation, professionals should set up procedures on\nthe disclosure of content on social media. 18\n48.      The promotion of “add on” insurance products during the sales of another product has\nbeen on supervisors’ radar for a while. The use of digital means to market and sell insurance\nproducts can nevertheless facilitate these practices. A common example is offering travel\ninsurance during the online sales process for airline tickets. In this example, whilst it could be in\nthe customer's interest to be informed of travel insurance options, the timing of the promotion at\nthe end of the sales process when the customer has already bought the airline ticket(s), and the\nway in which the message is delivered, could result in customers believing that the purchase of\nthe add on insurance product is required before the primary purchase of the airline ticket(s) can\nbe completed. This creates the risk of passive purchases and of the purchase of a cover that is\nnot needed.\n  United Kingdom\n  In 2014, the FCA conducted a market study on the General Insurance Add-Ons. The market\n  study found that the add-on distribution method has a real impact on consumer behaviour\n  and affects consumer decision-making. Consumers often focus on the sale of the primary\n  product, leading many to purchase add-on products that they do not need or understand. The\n  FCA also found that consumers had poor awareness of what products they had bought – with\n  19% being unaware that they owned the add-ons considered in the market study. The findings\n  indicate that consumers’ ability to make choices is often hindered by insufficient information\n  being available about the quality and price of the add-ons, and by this information being\n  presented too late in the buying process. Following these findings, the FCA has implemented\n  two remedies to address these specific issues:\n       • A ban on opt-out selling; and\n       • Improved information provision for add-on buyers.\n3.2     (Robo) advice\n3.2.1    Types of advice\n49.      Robo advice is essentially financial advice that is automated. In practice, a distinction\ncan be made between the following types of advice:\n     •   Full robo advice: the robo adviser completely takes over the work of the traditional\n         financial adviser. The “customer journey” is fully digitalised and the advice is fully\n         automated. The only human role is to develop and maintain the robo advice system and\n         to prevent malfunctions of the algorithm. There is no face-to-face contact;\n     •   Partial robo advice: the advice is fully automated, but the traditional adviser is still\n         available to answer questions;\n     •   Hybrid advice: the robo adviser and human beings interact with each other. For example,\n         the “customer journey” is fully digitalised, but the advice is still provided by a human,\n         possibly face-to-face; and\n     •   Traditional face-to-face advice: technology is only used as an additional tool, for example\n         to show graphs or animations.\n18 For more information, see (in French): https://acpr.banque-\nfrance.fr/fileadmin/user_upload/acp/publications/registre-officiel/20161116-\nAnnexe_Reco_2013_R_01.pdf\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 17 of 36\n3.2.2    Benefits and opportunities of robo advice\n50.      Robo advice has the potential to improve both the accessibility and consistency of\nfinancial advice. Accessibility means that financial advice is easily accessible for the majority of\nconsumers. This includes the continuous availability of advice from one’s home, which may also\nreduce the costs for the consumer. Furthermore, the consistency of advice can be improved\nthrough use of technology. When new financial products become available or when product\nconditions change, the algorithm can instantly take these changes into account. When\nprogrammed correctly and using sufficient and accurate data, robo advice will consistently be\nof the same quality. The robo adviser may be helpful in overcoming cognitive bias or insufficient\ncompetence on the part of the human adviser. Another potential benefit of robo advice is that\ncustomers may find disclosing pre-exiting conditions, such as mental health, easier to do to a\nmachine than a human adviser. This could encourage more disclosures, leading to customers\nreceiving more appropriate coverage.\n51.      Robo advice can be considered as another form of distribution in addition to internet or\ntelephone-based sales, potentially without providing advice to the customer. Robo advice can\nalso be part of enhanced consumer communications throughout the life of the product.\n52.      As with traditional advice, robo advice should have a solid audit trail.By its very nature\nrobo advice will produce an audit trail through the technology used, rather than requiring this to\nbe completed manually. Provided that the technology delivers an audit trail that is reliable and\nof good quality, robo advice could therefore make it easier to deliver traceability and auditability\nof advice . For all advice given, the provider of robo advice should be able to provide insight into\nthe data used, the algorithms used and the information presented to the customer. This would\nmake the robo advice traceable and reproducible allowing the customer, any other subsequent\nadviser and the supervisor to check how the advice was established.\n   Robo advice in the Netherlands\n   In the Netherlands, robo advice has been available for a couple of years for different types\n   of financial products. However, until last year, robo advice was only available for non-\n   complex products, such as car insurance. Since 2017, robo advice is also being developed\n   for complex financial products, such as disability insurance. One of the challenges, according\n   to the developers, is making sure that customers completely understand what is meant by\n   specific questions posed by the robo advisor, since there is no human advisor present to\n   answer questions.\n3.2.3    Potential risks\n53.      According to ICP 19 advice provided to consumers should take into account the\ncustomer’s disclosed circumstances. All advice should be communicated in a clear and accurate\nmanner, comprehensible to the customer. Where advice is provided, this should be\ncommunicated to the customer in written format, on paper or in a durable and accessible\nmedium, and a record kept in a “client file”. 19\n54.      However, there may be specific issues that need to be addressed to safeguard the fair\ntreatment of customers who use robo advice. Robo advice cannot solve every limitation of\ntraditional face-to-face advice. Robo advice does not, for example, overcome problems flowing\n19  Guidance 19.8.6\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 18 of 36\nfrom limited selection of available products. Nor will it overcome all the problems caused by the\ncomplexity of products.\n55.     When the advice is fully automated, the customer might not have the opportunity to ask\nquestions, unless programmed in a robo chat. The risk of misunderstanding is therefore more\npresent in robo advice than in face-to-face advice. The lack of interaction between humans\nmight also lead to a reduced detection of contradicting answers by customers.\n56.     A human adviser can recognise when the customer is in doubt, which a robo adviser\nmay not be able to do. However, robo advice tools could be programmed for that purpose – or\nexample, the algorithm could detect when a customer continuously clicks back and forth\nbetween pages and prompt a pop-up, asking the customer whether he needs additional help or\nexplanations. Detecting doubt is, however, one of the more challenging aspects of robo advice.\n57.     As in conventional advice, in a fully automated advice process the customer is\nresponsible for its own subsequent decisions. However, in the case of face-to-face advice, the\nadviser can discuss the customer’s hesitation to follow the advice with that customer and\npossibly remove any doubts or concerns, while a fully automated concept cannot. Therefore,\nthere may be merit in limiting the possible deviations from the advice in a fully automated\nprocess. This could prevent consumers making suboptimal choices in exchange for a lower\ninsurance premium.\n58.     An incorrectly programmed algorithm can have far-reaching consequences. It is\ntherefore important that an algorithm is carefully developed and tested before it is used in\npractice, and that it is subsequently subject to adequate maintenance. The design of the\nalgorithm of the robo advice needs to be such that the output treats the customer fairly. A faulty\nalgorithm or defective AI tool can lead to inconsistent advice or consistently bad or improper\nadvice. Such tools and the data they rely on can also potentially further reinforce or perpetuate\nexisting biases.\n3.3    Price comparison websites (PCW)\n59.     Price Comparison Websites (also known as Digital Comparison Tools) in insurance are\nwebsites that present multiple insurance products and providers of insurance, thereby enabling,\nin principle, consumers to compare and select products from an array of insurers and/or\nintermediaries. After selecting a product to purchase, the website could direct the customer to\nthe website of the insurer or intermediary to complete the transaction.\n60.     PCWs are currently well established in many jurisdictions for many products and\nservices such as electricity utility and air tickets. They are now also a key distribution channel\nin some insurance markets.\n61.     Whilst various methods of remuneration exist, most PCWs are remunerated by the\ninsurer or the intermediary for any successful transaction usually via a fixed amount per policy.\nThe PCW will usually not own the customer relationship, which is a significant difference from\nother types of intermediation.\n62.     The supervision of PCWs varies across jurisdictions depending on the activities\nperformed and the PCW’s business model and they may be considered as intermediaries. In\nsome jurisdictions PCWs need to comply with insurance intermediary requirements.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 19 of 36\n3.3.1   Benefits and opportunities\n63.     By bringing together product and price information in a single place, a potential benefit\nof PCWs is that they empower a consumer to quickly compare, assess and select among\nproducts available in the insurance market. If this potential is accomplished – reliable\ninformation is objectively presented with related educational tools – PCWs promote competitive\nmarkets by empowering consumers. PCWs further facilitate consumer shopping by being\naccessible at any time, from anywhere, although most insurers offer websites for their products\nwith similar access. Another benefit of PCWs for consumers is the ability to enter their personal\ninformation once to receive personally-relevant products and prices from multiple vendors – a\nclear advantage to entering personal information every time when shopping across different\ninsurers' websites. In this respect PCWs can promote competition as well as reduce marketing\nand underwrting costs which could result in lower insurance premiums.\n3.3.2   Potential risks\n64.     Some of the risks – such as consumers having to self-direct and inform themselves\nwithout further assistance – are common to those of digital sales via the website of an insurer\nor an intermediary. However, there are a number of particular risks. For example, a risk with\nPCWs is non-disclosed conflicts of interest due to compensation arrangements or ownership\nstructures with particular providers shown on the web site. Such conflicts of interest can cause\nthe website to present only some products and/or guide consumers to products that are in the\ninterest of the website and not the interest of the consumer.\n65.     PCWs that are not subject to specific disclosure requirements may lack of transparency.\nThe lack of transparency may relate, for example, to potential conflicts of interest, to\ncompensation arrangements with providers or to the true identity of the PCW owner/operator or\nproviders. This could affect adversely the ability of consumers to make informed decisions.\nConsumers are generally not aware of the number of suppliers consulted during a given product\ncomparision and the criteria used to establish a recommendation (if the PCW is permitted to\nmake recommendations). Consumers may believe the PCW is providing objective and complete\ninformation when, in fact, the PCW is providing limited and biased information that channels the\nconsumer to a specific product.\n66.     Another major risk is that consumers focus only on the price to select a product and, as\na result, are not adequately covered because they bought a product that does not meet their\nneeds. There is also a risk that consumers buy unsuitable products based on the results\nprovided by a PCW because they believed they were receiving some form of advice, and/or that\nall results shown met their cover needs.\n67.     Specific risks have been identified in the use of PCWs that can create harm for\nconsumers including unreliable performance or disorderly failure (for example, caused by a\ntechnology and/or data failure). Due to the volume of transactions generated through a relative\nsmall number of PCWs, an issue with one PCW such a failure in the algorithm to present the\ncorrect results or a display of inaccurate information about a product can have a far-reaching\nimpact and, in markets where PCWs play a big role, can even create a systemic issue across a\nspecific product line.\n68.     It could be expected that if one PCW fails, others will pick up the market segment.\nHowever, in some markets, concentration in certain segments/product lines may create harm\nto consumers if there is a lack of availability of other PCWs.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 20 of 36\n  Netherlands\n  In 2014, the AFM issued a press release on the quality of PCWs.\n  The main findings were:\n  • Overall, the services of PCWs were found to be in the interest of the customer, based on\n      research on the five main PCWs in the Netherlands. Usually, comparisons are ranked on\n      both price and quality, based on the preferences of the customer;\n  • There were no signals that the overall comparison was based on payments of insurers.\n      However, often an insurer would only end up in the top 3, if the consumer was able to close\n      the product via the PCWs – which in turn was only possible if commissions are paid; and\n  • The main points of improvement were the provision of information, the way the top 3 is\n      constructed, the inclusion of one-off discounts in the premium and default preferences.\n  In 2018, the AFM issued another press release on the services of PCWs.\n  • The main finding was that PCWs sometimes provide financial advice, while advertising\n      their services as execution only. Their customer onboarding, however, is not suitable for\n      financial advice, as PCWs are based on execution only and therefore contains a limited\n      set of questions. The consumer, however, might get the impression that financial advice\n      was given;\n  • An example of a PCW giving advice, is presenting “the top 3 best suitable mortgages for\n      you”. This can qualify as financial advice, but it would not be compliant with the advice\n      rules, as there is no adequate customer onboarding; and\n  • In 5 Q&As, the AFM explained when the services of PCWs would qualify as financial\n      advice. Some market players will move from execution only towards robo advice in the\n      next couple of years.\n3.4     Disclosure and informed decision-making\n69.      Standard 19.7 requires insurers and intermediaries to provide timely, clear and adequate\npre-contractual and contractual information to customers. Product disclosure is a key\nrequirement that needs to adapt to new digital channels and habits.\n3.4.1    Benefits and opportunities\n70.      One of the advantages of online services is that providers can use visual information to\ndisclose features of a product. For example, the course of the premium over time can be\npresented in a manner that is easily understandable and easily adjustable when the customer\nenters new information, for example using graphs. The same goes for other product features.\nProviders can experiment with the best way to disclose information to their customers, to\nmaximise the intelligibility thereof.\n71.      Chatbots 20 may also assist when a customer takes too long to scroll past a certain\nsection or moves too quickly across a material term of the policy. This could indicate that the\ncustomer is looking for additional information or further explanation either by the bot or by an\nadviser depending on the complexity of the policy.\n20 A computer programme designed to simulate conversation with human users, especially over the\nInternet.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 21 of 36\n72.       Technologies may utilise customer data to introduce relevant disclosures based on the\ninformation obtained about the customer from different data sources. Examples of such “virtual\nor cognitive customer service representatives” or chatbots include UK based Spixii, which\n“speaks” six languages or Flamingo’s “Rosie” in Australia which “learns from your business” in\norder to respond to customers. 21\n  United States\n  The US based insurer Lemonade used two types of artificial intelligence or “cognitive”\n  systems to interface with customers. One is called “Maya” which signs up customers via\n  mobile devices and the other is called “Jim” which finalises claims without any assistance.\n  Insurify is another example which uses Evia (“Expert Virtual Insurance Agent”) and uses\n  natural language and image recognition to collect auto insurance quotes. Customers can\n  also engage with Evia when it comes to clarification of terms.\n73.       “Comprehension testing” through technology may assist with obtaining certainty that the\ndisclosed information is adequate and that the consequences thereof are properly conveyed to\nthe insured. Technology, particularly machine learning and chatbots, can be used as an enabler\nfor customer comprehension. Online filter and quick test questions may also assist with gauging\nthe customer’s understanding. There may be a need to educate online users to dedicate\nsufficient time to an adequate understanding of the contents of the agreement.\n74.       The means of presentation (for instance through dedicated popup windows) can play an\nimportant role in ensuring proper understanding of the information by customers and obtaining\nexplicit consents when appropriate. In addition, gamified product sales information – whereby\ninformation is disclosed as part of game or challenge – can keep a consumer interested and\nengaged so that critical information that might otherwise be overlooked is seen, understood and\nretained by the consumer.\n3.4.2     Potential risks\n75.       The time efficiencies and instant gratification associated with digitised transacting mean\nthat customers expect a relatively quick transactional experience, particularly in instances where\nsmartphone applications are being used. This poses significant challenges for insurers in\nmaintaining a balance between convenient, seamless contracting versus the risk of inadequate\ndisclosure of material policy terms and conditions.\n  Netherlands\n  In the Netherlands, supervision of the intelligibility of products is part of the product approval\n  process. Also, in online services, customers are often obliged to read information and to\n  confirm the information is understood. It nevertheless remains a risk that customers do not\n  fully understand the details of a product before providing this confirmation. The supervisor\n  therefore encourages parties to write their product conditions in such a way that these are\n  complete, but as easy to read and understand as possible.\n21 https://www.digitalpulse.pwc.com.au/how-insurtech-will-make-you-love-your-insurer/\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 22 of 36\n76.     In contrast to face-to-face interaction, digital interaction can make it difficult to flag\nmisunderstandings and the need for more explanations based on non-verbal communication.\n77.     In a digital context, customers are faced with a plethora of information from different\nsources. It can be difficult to identify reliable product disclosures in a manner that is appropriately\npresented, and differentiate between product disclosure and marketing. In addition, digital tools\ncan be used to highlight information that is relevant but can equally be used to downplay\ninformation in a way that creates a risk of consumers choosing an inappropriate product.\n78.     As with non-digital information and disclosure, consumers face the risks of information\noverload from too many sources with no means to ascertain the accuracy or legitimacy of the\ninformation.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 23 of 36\n4    Supervisory issues\n79.      Digitalisation is not only transforming the insurance industry but society itself. For\nsupervisors it presents a “moving target in a moving environment”. As digitalisation changes the\nway insurance products are designed, marketed and distributed, supervisors should monitor\nthese new developments and engage stakeholders both within and outside the insurance\nindustry to protect consumers’ interests. This includes non-traditional stakeholders such as\ncloud service providers and data vendors. In short, new developments / the shift in risks will\nrequire new supervisory responses that are delivered in an adequate way. Some of the key\nchallenges are described below.\n80.      Consumer outcomes: For supervisors in a digitalised world it is crucial to understand\nhow incumbent insurers and intermediaries as well as newer insurance market participants,\nincluding Insurtech start-ups and Big Techs, are behaving with impact on outcomes for\nconsumers. Digitalisation and use of data have the potential to benefit consumers, but also\ncreate risks of unfair treatment, discrimination, or concerns over access to, or exclusion from,\ninsurance services. Measuring and assessing these outcomes is challenging. Supervisors\nshould consider monitoring behaviour and outcomes by examining information from multiple\nsources.\n  Australia\n  In September 2017, ASIC launched its 2017-18 Data Strategy. With the tag-line:\n  “Connecting the dots to achieve better regulatory outcomes, “its purpose is to describe\n  ASIC’s vision for data, its objectives and an approach to improving how it captures, shares\n  and uses data. 22\n  Germany\n  BaFin launched an internal project in 2015 to learn more about the business model of\n  technological start-ups (FinTech s) and their appearance on the market. Drawing on\n  expertise from the areas of banking, insurance and securities supervision, the objective of\n  the project group was to observe the latest developments in the FinTech market, and to\n  review whether BaFin needed to adjust its processes in view of new developments in the\n  area of digitalisation. As a result of this project, BaFin established an Innovation Hub. This\n  Innovation Hub analyses and evaluates upcoming technological solutions and new business\n  models based on those solutions.\n  Additionally, the Innovation Hub coordinates a network of experts from various areas of\n  responsibility within BaFin, which rates innovative business models with regard to regulatory\n  requirements. Experts from banking, insurance and securities regulators are represented in\n  the network, as well as from the licensing and the pursuit of unauthorised business\n  departments. The combination of experience and expertise from ongoing oversight and\n  review of licensing requirements allows rapid assessment of innovative business models\n  and processes that may not be unique to one department.\n22     http://download.asic.gov.au/media/4479255/asic-data-strategy-2017-20-published-19-september-\n2017.pdf\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 24 of 36\n  Québec\n  AMF has created a Fintech Lab to deepen the AMF’s knowledge of new business models\n  and underlying technologies, explore the current and potential applications of these\n  technologies and explore how the AMF itself can use them.\n  France\n  In 2017, the French central Bank launched The Lab. As an open space for discussions and\n  collaborative work, the Lab links up the Banque de France with various initiators of\n  innovative projects – start-ups and FinTechs, institutional players, universities – in order to\n  experiment with new concepts and technologies in connection with the activities of the\n  institution. The Lab is working on technologies such as blockchain (MADRE project), IoT,\n  IA etc.\n  The ACPR is developing a new tool for supervising business practices:\n  •    A database of the innovations of the insurance sector, which enables monitoring of\n       technological innovations, new services as well as guarantees offered; and\n  •    A web listening platform with an internal analysing tool to capture online messages from\n       consumers concerning bad market practices.\n  In March 2018, the ACPR launched a Task Force to tackle the opportunities and challenges\n  raised by AI in the financial sector. This Task Force (TF) is composed of banks, insurance\n  companies and FinTechs. It also includes other authorities such as the Data Protection\n  Authority. The primary goal of this TF will consist of issuing a Discussion Paper before end\n  of 2018, aiming at summarising the implications of using AI technologies in the financial\n  sector.\n  United Kingdom\n  In 2014, the FCA launched its Innovate department which is committed to encouraging\n  innovation in the interests of consumers. Innovate provides assistance to firms which are\n  using innovation to improve consumer outcomes, and helps firms better understand the\n  FCA’s rules, processes and guidance. Innovate helps the FCA keep ahead of developing\n  trends and potential harms in the market.\n  United States\n  In 2018, the NAIC and US state insurance supervisors launched a three year strategic plan,\n  State Ahead, to drive their efforts, resources and attention to meet ongoing challenges,\n  including the rapidly evolving marketplace fuelled by seismic shifts in consumer behaviour\n  and huge technological advances. As part of the goal to ensure consumer protection keeps\n  pace with changes in the marketplace, one objective is to optimise use of market data and\n  regulatory processes to enhance consumer protections, including:\n  • rebuilding the NAIC's Market Conduct Annual Statement (MCAS) application, as well as\n      those applications utilising MCAS data, as a cloud-based solution;\n  • implementing a business intelligence tool with self-service capabilities;\n  • creating an enterprise market data strategy and analytics data warehouse; and\n  • rebuilding the NAIC's Consumer Information Source (CIS) application.\n  Additionally, US state insurance regulators are trying to gain a good understanding of new,\n  innovative insurance products and services, including the manner in which they impact\n  consumers and other stakeholders in the insurance marketplace. Accordingly, the NAIC\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 25 of 36\n  established the Innovation and Technology Task Force in 2017 to help insurance regulators\n  stay informed on key developments, including new products and services from start-up\n  companies, as well as established insurance industry players. Under this Task Force, the\n  NAIC's Big Data Working Group is charged in part with assessing data needs and required\n  tools for state insurance regulators to appropriately monitor the marketplace and evaluate\n  underwriting, rating, claims and marketing practices. This assessment includes gaining a\n  better understanding of currently available data and tools, as well as recommendations for\n  additional data and tools, as appropriate. Based on this assessment, the Working Group\n  will propose a means to collect, house and analyse needed data. 23\n81.        Need to balance innovation and conduct concerns: Digitalisation and innovation\nhave enormous potential to help insurers and intermediaries build cultures of compliance,\nidentify potential consumer harms and improve outcomes for consumers. However, it could also\npose significant risks that could lead to consumer harms if not properly managed. These could\ninclude technological exclusion, discrimination, and accessibility and affordability issues. 24 One\nof the key challenges to supervisors will be to consider a balanced approach to facilitate\ninnovations by insurers while maintaining the level of consumer protection as stipulated in its\nlaws and regulations.\n  Australia\n  ASIC’s Innovation Hub drives much of the Australian conduct regulator’s support for\n  digitalisation and engagement with FinTech and Insurtech companies.\n  Through the Innovation Hub, ASIC provides informal assistance to Insurtechs on their\n  regulatory obligations, the overarching regulatory framework and, as appropriate, options\n  relating to ASIC’s exemption powers.\n  Germany\n  BaFin’s Innovation Hub serves – besides other responsibilities – as a communication platform\n  for incumbents and start-ups. One of its main aims is to gather and spread knowledge. For\n  example: There is a special contact form on BaFin’s webpage through which company\n  founders and FinTechs can submit preliminary inquiries or concrete questions about eg\n  business models. The term \"contact form\" may seem a bit old fashioned, but contributes to\n  the efficiency of the communication: It serves to quickly determine the responsible section for\n  the respective business model body within BaFin – for a public authority with about 2.700\n  employees, this is a decisive factor.\n  France\n   In 2016, the ACPR launched a FinTech Innovation Unit. It is the point of entry of financial\n   start-ups for their licensing process. The Unit evaluates the opportunities as well as the risks\n   related to innovations in the financial industry and gives recommendations on where\n   adjustments need to be made in the current regulation and in supervision practices. The\n   ACPR is coordinating its actions with the Securities & Markets Authority (SMF). They have\n   both launched the Fintech Forum in 2016 which is leading a dialogue with FinTech\n   professionals regarding regulation and supervision.\n23  https://www.naic.org/state_ahead.htm\n24  Many of these issues are addressed in the Application Paper on the Use of Digital Technology in\nInclusive Insurance (November 2018).\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 26 of 36\n  Québec\n  The AMF has created in 2016 an external advisory committee, the Technological Innovation\n  Advisory Committee that has the mandate to assist the AMF in identifying and analysing\n  trends and issues and help ensure a balance between consumer protection and market\n  efficiency.\n  Switzerland\n  FINMA has been working on the challenges presented by FinTech regarding authorisation,\n  supervision and regulation. Innovative trends and ideas require a solid framework within\n  which to operate, while clients and the financial system as a whole need protection during\n  this shift in direction.\n  FINMA regards innovation as key to the competitiveness of Switzerland’s financial centre, but\n  adopts an essentially neutral approach to certain business models and technologies. It\n  therefore reviewed whether specific provisions in its ordinances and circulars disadvantaged\n  some technologies and concluded that very few such obstacles existed.\n  An increasing number of financial intermediaries interact with their clients via internet and\n  mobile devices. FINMA has therefore been enhancing the regulatory framework to facilitate\n  client onboarding via digital channels. In its new circular, the anti-money laundering due\n  diligence requirements are explained in the context of digitalisation of financial services and\n  the need for technology-neutral regulation, particularly with respect to video identification. The\n  circular came into force on 18 March 2016.\n  Before launching operations, FinTech companies must establish whether they are subject to\n  anti-money laundering and authorisation requirements.\n  In general, authorisation for insurance is required if risks and dangers for clients are insured.\n  If services are rendered voluntarily and without any contractual obligation, authorisation might\n  not be required.\n  United Kingdom\n  The FCA launched its regulatory sandbox in 2016. The regulatory sandbox is a space in which\n  firms can apply to test innovative propositions in a live environment, with oversight and input\n  from the FCA. Firms testing in the sandbox are required to meet all relevant regulatory\n  requirements, and bespoke safeguards and mitigants are built into each testing plan. The\n  sandbox aims to enable firms to get to market to test their propositions at a faster speed and\n  reduced cost, and also gives the FCA an understanding of the opportunities and potential\n  harms that innovation can create in the market. Almost 90 firms have been supported across\n  the first four cohorts of the FCA’s sandbox, including a significant number from the insurance\n  sector.\n82.       Supervisory skills and tools: Supervisors should become “data driven” and “digital-\nintelligence-led”. Supervising market conduct in a digital world requires different skill sets, in\naddition to those of lawyers, economists, actuaries/mathematicians and data scientists.\nInterdisciplinary supervisory teams will be vital in a digitalised world. Supervisors should be\ntechnologically and numerically literate and understand the risks associated with data.\nSupervisors will need new skills to identify, monitor and assess new applications of\ntechnologies, for understanding market structures and the activities of new participants, and for\nunderstanding consumer outcomes. They can arrange to have this knowledge and skill wihin\ntheir own staff or – if this is more efficient – use third party providers. Lawyers, economists,\nactuaries/mathematicians and data scientists will need to work together to supervise insurance\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 27 of 36\nmarkets. In this respect supervisory authorities will need to reconsider what qualifications and\nskill sets they need to become “fit for the future”. They will continue to compete for talent within\nthe industry.\n  Germany\n  IT specialists have been part of supervising teams for a while. But in order to better prepare\n  itself for the challenges posed by inter alia IT and cyber risks, BaFin set up a separate\n  organisational unit for IT supervision in the financial sector as of 1 January 2018.\n  Québec\n  AMF has created a dedicated internal working group of experts on FinTech, involving more\n  than 60 employees working in cross-functional teams.\n  France\n  Alongside the creation of dedicated teams/hubs, the Central Bank has appointed a Chief\n  Digital Officer (CDO) in charge of the digital transformation of the institution, who is also\n  Chairing the innovation Lab.\n  United States\n  As part of the NAIC and US state insurance supervisors' State Ahead strategic plan, the theme\n  of consumer protection and education recognises the need to stay abreast of new\n  developments in the area of innovation and emerging technology and the need to become\n  more engaged in the areas of InsurTech and Regtech. Opportunities being explored to further\n  this objective include providing forums and programmes for state insurance regulator\n  education and discussion regarding changes in the insurance marketplace, including\n  innovation and technology; convening an Autonomous Vehicle Insurance Forum with\n  stakeholders to discuss insurance regulatory issues related to autonomous vehicles; and\n  considering the creation of a cybersecurity insurance institute.\n  Additionally, the NAIC's Innovation and Technology Task Force is charged, in part, with\n  discussing innovation and technology developments in the insurance sector, including the\n  collection and use of data by insurers and state insurance regulators – as well as new\n  products, services and distribution platforms – in order to educate state insurance regulators\n  on how these developments impact consumer protection, privacy, insurer and producer\n  oversight, marketplace dynamics and the state-based insurance regulatory framework.\n83.       In particular, supervisors will need the skills to understand how digitalisation can result\nin consumer harm. For example, supervisors may encounter challenges in supervising the (self-\nlearning) algorithms that underlie the automated decisions made in a digitalised world, which is\nnot only problematic from a consumer protection perspective but also from a risk management\nperspective.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 28 of 36\n  Robo advice\n  Robo advice provides a useful case-study on many of the issues pertinent to supervisors. As\n  a result, many supervisors have recently published guidelines on how they are approaching\n  robo advice including:\n  •   Germany:https://www.bafin.de/EN/Aufsicht/FinTech/Anlageberatung/anlageberatung_no\n      de_en.html;\n  •   Australia: https://asic.gov.au/regulatory-resources/find-a-document/regulatory-guides/rg-\n      255-providing-digital-financial-product-advice-to-retail-clients/; and\n  •   Netherlands:        https://www.afm.nl/~/profmedia/files/onderwerpen/roboadvies-sav/view-\n      robo-advice.pdf.\n  •   United Kingdom: The UK’s FCA launched its Advice Unit in May 2016 to provide\n      regulatory feedback to firms developing automated advice or guidance models across a\n      range of sectors, including insurance. Feedback focuses on helping the firm understand\n      the regulatory implications of their model. The FCA also publishes tools and resources for\n      all firms developing automated advice or guidance propositions, based on its experiences\n      with individual firms.\n  In developing guidelines, supervisors have needed to consider how the quality of the advice\n  provided is measured and verified. Should supervisors directly supervise the algorithm?\n  Should supervisors supervise and monitor the outputs – ie the advice itself? Should\n  supervisors require insurers and intermediaries to self-audit and provide it with annual\n  assurances that the advice its robo advisers are providing is appropriate? Should it require\n  insurers and intermediaries to engage external experts to conduct those audits?\n  Depending on how they address these questions, supervisors may need to establish\n  dedicated teams to address such technical matters involving IT specialists with the required\n  knowledge.\n  As with non-digital advice, supervisors need to mandate that insurers and intermediaries\n  adopt appropriate document management strategies. This includes retaining all versions of\n  the algorithm themselves. The robo adviser needs to save the algorithm, the used data and\n  the information and advice that has been provided to the customer.\n84.       Supervisory authorities should consider how to embrace new technologies to help carry\nout supervision, also referred to as Suptech solutions. 25\n  Germany\n  In the first half of 2018, BaFin has published a report on Big Data and AI together with\n  Partnerschaft Deutschland, the Fraunhofer Institute for Intelligent Analysis and Information\n  Systems and the Boston Consulting Group.\n  https://www.bafin.de/SharedDocs/Downloads/EN/dl_bdai_studie_en.html\n  With advancements in key technologies, increasing data availability and decreasing entry\n  barriers into the usage of Big Data and AI solutions, both incumbent institutions and new\n25 Suptech is the use of technological innovations (or FinTech) by supervisory authorities. Regtech: the\nuse of technological innovations (or FinTech) for compliance purposes and reporting by regulated\nfinancial institutions.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 29 of 36\n  market players are able to structure their work processes more efficiently and develop new\n  business models. Supervisors, however, are facing new challenges as the use of Big Data\n  and AI impacts the financial markets. The aim of the report was to gain a better\n  understanding of these challenges. The report highlights the implications of technology-\n  driven market developments from various regulatory and supervisory perspectives.\n  The six main observations and their implications can be summarised as follows:\n  1. The financial data innovation race is already underway. Systematic dependencies on\n       Big Data and AI companies could develop outside the regulatory framework in the\n       financial system.\n  2. Black-box modelling is spreading. But black-box modelling must never stand in the way\n       of a proper business organisation.\n  3. Big Data and AI accelerate the further automation of the process. But the responsibility\n       will always remain with the senior management.\n  4. The “transparent customer” is more than just a phrase in the age of Big Data and AI. Big\n       Data and AI must be used for the customers, not against them.\n  5. Consumer confidence is a catalyst for Big Data and AI innovations and an anchor of\n       stability for the integrity of the financial system.\n  6. The speed of innovation increasingly tests the limits of the regulatory framework’s\n       adaptability. A level playing field in the age of Big Data and AI means that the speed of\n       innovation has to be met with agile oversight and technology-neutral regulation.\n  United Kingdom\n  In 2017/18 the FCA worked with ING, the Commonwealth Bank of Australia and Pinsent\n  Masons to test the possibilities of using Natural Processing Language and AI technologies\n  to interpret Markets in Financial Instruments Directive II regulations and automatically build\n  and manage a compliance programme.\n85.       Different entities: Supervisors will also need to deal with non-incumbent firms with\ndifferent entity structures and approaches to consumer related risk than incumbents.\nSupervisors will need to engage with new entrants into insurance and financial services who\nmay not have experience or knowledge of financial services regulation. These new entrants\nmay have different entity structures and approaches to consumer related risk than incumbents\nhistorically monitored by supervisors. Unlike incumbents, the general compliance awareness,\nrisk culture and ability to comply with regulatory requirements may differ significantly for these\nnon-traditional firms. This may require a proactive strategy for outreach and engagement with\nthese new entrants to inform and “educate” them on relevant supervisory matters and the proper\ncompliance attitude.\n86.       Well capitalised “BigTech” platform businesses may move into distribution markets.\nSmall, nimble but lowly capitalised Insurtech start-ups may also look to enter insurance markets.\nSupervisors will need to understand the different challenges posed by these new entities that\nmay lack knowledge of regulatory reuqirements, and may not have had experience in engaging\nwith supervisors.\n87.       Supervisory cooperation: Cooperation between financial supervisory authorities is\ncrucial in a digitalised world. As digital innovations and risks do not stop at the border,\njurisdictional supervisors should coordinate with authorities in other jurisdictions. To meet these\nchallenges supervisors should proactively work together across jurisdictional and subject-matter\nboundaries to identify emerging trends and to develop and implement solutions. This includes\ncollaboration between market conduct regulators and prudential, privacy and competition\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 30 of 36\nregulators given the implications of digital technology on consumer outcomes and the significant\nnumber of solutions that are focused on the marketing, sales and distribution end of the value\nchain. Regular and on-going interaction between supervisors will be crucial. The IAIS continues\nto support and facilitate such discussions, including through its FinTech Forum.\n  France\n  In 2018, the ACPR signed an agreement with the authority supervising the security of\n  Information Systems (ANSSI). The authority is responsible for responding to threats targeting\n  public authorities and the private sector, in particular vital information systems, and\n  coordinates government action in the area of defence of those systems.\n88.      Regulatory arbitrage: Supervisors also need to be cognisant of the emergence of\nproduct types that have the effect of insurance but are structured in a way that falls outside the\nlegal definition of a regulated insurance product. This would enable product providers to avoid\nregulatory requirements. To consumers, it means that they would not be able to access\ncompensation or policyholder protection schemes if product manufacturers are unable to meet\ntheir claim costs.\n89.      The arbitrage can take two forms:\n•    Jurisdictional: where the product falls outside the jurisdictional power of a regulator despite\n     being available to customers in that jurisdiction; and\n•    Definitional: where the product does not have the legal characteristics of an insurance\n     product although it has the effects of one.\n90.      In addition, digitalisation and new technologies may increase the potential for regulatory\narbitrage created by products that bundle insurance and non-insurance products or services\nwith choice by the consumer limited to purchasing or not purchasing the package.\n91.      Information security: Storage, protection and third party use of customer data (and the\ninsights gathered from it) is also an issue. As cyber risks and data protection questions become\nof vital importance, with the steady rise of digitalisation, working together with the competent\nauthorities on these issues is of utmost importance.\n  World-wide\n  The \"WannaCry\" and “Petya” ransomware outbreaks in 2016 highlight that cyber risks are\n  on the rise. The data and customer specific behavioural insights that insurers hold would\n  have a high value in this context. This is particularly relevant given that in order to drive\n  efficiencies and reduce costs, many insurers now store data and insights on the \"cloud\" and\n  share data and insights with third parties, many of whom are off-shore. Cloud services are\n  probably more in the focus of hackers but are in most cases much better protected than on-\n  premise installations run by incumbents. On the other hand if a cloud service is successfully\n  attacked, the outcomes can be worse or even systemic.\n92.      Customers need to know that their data and insights specific to them that are derived\nfrom their data are secure, not corrupted or tainted, and clearly who has access to such data.\nThis is a challenge not just to privacy regulators but also to financial services regulators.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 31 of 36\n  EU\n  The EU has reformed its data protection rules to simplify the use of Big Data for businesses 26\n  and to set high standards of data protection. As of May 2018, with the entry into application\n  of the General Data Protection Regulation 27, there is one set of data protection rules for all\n  companies operating in the EU, wherever they are based. 28\n  Also, the EU aims to strengthen its cybersecurity regulation to cope with the growing threat\n  of cyberattacks and take advantage of the opportunities offered by the new digital era. In\n  October 2017, the European Council called for a common approach to cybersecurity in line\n  with the European Commission's September reform package. 29\n  United States\n  The NAIC's Big Data Working Group is charged, in part, with reviewing current regulatory\n  frameworks used to oversee insurers' use of consumer and non-insurance data and, if\n  appropriate, recommend modifications to model laws and/or regulations regarding marketing,\n  rating, underwriting and claims, regulation of data vendors and brokers, regulatory reporting\n  requirements, and consumer disclosure requirements.\n93.       Use of cloud computing is increasing in the insurance industry, and with it the need for\nsupervisors to enhance their regulatory frameworks and supervisory practices to effectively\ncapture the risks. Most supervisors typically apply existing frameworks on outsourcing,\ngovernance, risk management, internal control, and information security to insurers’ cloud\ncomputing activities, while others have issued cloud-specific recommendations and\nexpectations. Meanwhile, most authorities have put in place formal and informal communication\narrangements and are reinforcing their supervisory processes to better monitor and assess the\nrisks of cloud computing. 30\n94.       Cloud services give rise to specific questions. For example:\n     •    In which country will the data be stored and how can this be verified?\n     •    Who has access to the data?\n     •    Which key controls will be provided?\n     •    Is there a danger of risk concentration as there are not so many cloud service providers?\n     •    Is there a (possible) conflict of interest if data of different insurers are stored on the same\n          server?\n26  Information Commissioner's Office: \"Big data, artificial intelligence, machine learning and data\nprotection\", 20170904, Version: 2.2.\n27 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the\nprotection of natural persons with regard to the processing of personal data and on the free movement of\nsuch data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016,\np. 1).\n28European       Commission:       https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-does-\ngeneral-data-protection-regulation-gdpr-govern_en#references.\n29 http://www.consilium.europa.eu/en/policies\n30 FSI Insights, Emerging prudential approaches on outsourcing to the cloud: the case of insurance\ncompanies, forthcoming.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 32 of 36\n     •   Is there a (possible) conflict of interest if the cloud service provider decides to involve\n         itself in the insurance business?\n95.      It is also vital that supervisors have the same direct and immediate access to data stored\non the cloud as they have to data stored on an insurer or intermediaries own servers.\n  Germany\n  BaFin published in its Journal 04/2018 an article on “Cloud Computing: Compliance with the\n  supervisory requirements regarding rights of information and audit and ability to monitor”. 31\n  With regard to outsourcing to cloud service providers, BaFin also holds discussions with the\n  respective cloud service providers and insurers about the content of outsourcing contracts.\n  A circular to clarify supervisory requirements for IT in insurers (VAIT) has been published\n  (https://www.bafin.de/SharedDocs/Downloads/DE/Rundschreiben/dl_rs_1810_vait_va.html).\n31\nhttps://www.bafin.de/SharedDocs/Veroeffentlichungen/EN/Fachartikel/2018/fa_bj_1804_Cloud_Comput\ning_en.html.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 33 of 36\n5     Conclusion and recommendations\n96.      Digital innovations can change and potentially improve the customer experience and\nreduce insurers’ and intermediaries’ operating costs. However, digitalisation may have an\nimpact on consumer protection and the extent to which customers are treated fairly; from the\ndesign, underwriting and pricing of products, their marketing and distribution, through to claims\nprocessing and the ongoing management of customers. Therefore, in respect of product design,\nmarketing and sales, due attention needs to be given to achieving fair customer outcomes in\nterms of products that meet consumers’ needs, the design and use of algorithms and the use\nof customer data.\n97.      To adjust to the digital age and foster innovation, supervisors should consider how to\nensure that new innovation does not come at the expense of protections for policyholders and\nthe integrity of the insurance sector as a whole.\n98.      One of the key challenges to supervisors will be to consider a balanced approach to\nfacilitate innovations by insurers while maintaining the level of consumer protection as stipulated\nin its laws and regulations. To promote innovation consistent with consumer benefit and\nprotection, it is recommended that supervisors develop a thorough understanding of how\ninnovations work and are applied to ensure a proper assessment of new products and business\nmodels, and the design and functioning of the IT architecture, infrastructures and processes,\nand how this is catered for in the insurers’ risk management framework.\n99.      Supervisors may also need to develop new tools and skills for supervision of digitalised\ninsurers, enhancing cooperation with financial and other authorities, safeguarding the\nsupervisory perimeter to prevent regulatory arbitrage and enhancing information security.\n100. Supported by further material to be developed by the IAIS, supervisors should consider\nestablishing guidance for appropriate and responsible use of new technologies to safeguard the\nfair treatment of customers and – for example in terms of the use of AI and robo advice\nmechanisms – promote advice and services that are suitable and affordable for the customers.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 34 of 36\nAnnex: Digital technologies and alternative business models affecting\n      insurance business\nGeneral overview of significant innovations within the insurance industry as mentioned in the\nIAIS report “FinTech Innovations in the Insurance Industry”. 32\nDigital devices and the internet:\n 1. The changes addressed in this paper are facilitated by the proliferation of digital devices\n      (devices that contain a computer or microcontroller) such as smartphones, tablets and\n      \"wearables\". These devices are connected by the internet: a global network of computers\n      using standardised communication protocols.\nInternet of Things (IoT): 33\n 2. IoT involves the internetworking of physical devices, vehicles, buildings and other items\n      (also referred to as \"connected devices\" and \"smart devices\"), embedded with electronics,\n      software, sensors, actuators, and network connectivity that enable these objects to collect\n      and exchange data.\nTelematics / Telemetry:\n 3. In the context of IoT, telematics involves telecommunications, sensors and computer\n      science to allow sending, receiving, storing and processing data via telecommunication\n      devices, affecting or not control on remote objects. Telemetry involves the transmission of\n      measurements from the location of origin to the location of computing and consumption,\n      especially without affecting control on the remote objects. In the context of insurance, its\n      main applications are Connected Cars, Advanced Driver Assistance Systems (ADAS),\n      Health monitoring and Home monitoring.\nBig Data 34 and Data Analytics: 35\n 4. In the insurance market, Big Data and Data Analytics could be used in various processes,\n      such as product offerings, risk selection, pricing, cross selling, claims prediction and fraud\n      detection, for example to offer customised products.\nComparators and Robo advisers:\n 5. Online services that provide automated, algorithm-based product comparison and advice\n      without human intervention.\n32  21 February 2017; https://www.iaisweb.org/page/news/other-papers-and-reports/file/65625/report-on-fintech-\ndevelopments-in-the-insurance-industry\n33 The term IoT has been defined as a global infrastructure for the information society, enabling advanced services\nby interconnecting (physical and virtual) things based on existing and evolving interoperable information and\ncommunication technologies (source http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=y.2060)\n34 Big Data is the term used for the storage of data from different sources, in large volume and speed; IAIS, FinTech\nDevelopments in the Insurance Industry, 21 February 2017.\n35 Data Analytics is the process of inspecting, cleaning, transforming, and modelling data with the goal of discovering\nuseful information, suggesting conclusions, and supporting decision-making; IAIS, FinTech Developments in the\nInsurance Industry, 21 February 2017.\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                                Page 35 of 36\nMachine Learning (ML) and Artificial Intelligence (AI):\n 6. The use of ML and AI enables several insurance industry processes to use data in real time\n      and, especially, use events prediction (eg vehicles thefts, health problems and weather\n      events). There is a vast scope for AI, not only in a better pricing of risks, but also in fraud\n      prevention, automated underwriting, claims handling or in preventive counselling.\nDistributed Ledger Technology (DLT):\n 7. A distributed ledger is essentially an asset database that can be shared across a network\n      of multiple sites, geographies or institutions. The security and accuracy of the assets stored\n      in the ledger are maintained cryptographically through the use of “keys” and signatures to\n      control who did what within the shared ledger.\n     •    Blockchain:\n          This is a type of decentralised distributed ledger, comprised of unchangeable, digitally\n          recorded data in packages called “blocks” which are stored in a linear chain; and\n     •    Smart Contracts:\n          The novelty of DLT is that it is more than just a database – it can also set rules about a\n          transaction (business logic) that are tied to the transaction itself. Smart contract is a term\n          used to describe computer programme code that is capable of facilitating, executing,\n          and enforcing the negotiation or performance of an agreement using DLT.\nPlatform business models, Peer-to-peer, Usage Based, On-demand Insurance:\n 8. Emerging digital technologies are facilitating alternative business models, such as:\n     •    Platform business models: a \"platform\" is a business model that creates value by\n          facilitating exchanges between two or more independent groups, usually consumers and\n          producers. To make these exchanges happen, platforms harness and create large,\n          rapidly scalable networks of users and resources. Platforms don't own the means of\n          production – instead they create the means of connection. 36 Google, Apple, Facebook,\n          Amazon, Uber and Alibaba are all examples of platform business models;\n     •    Peer-to-Peer: a business model that allows the insured to pool their capital, self-organise\n          and self-administer their own insurance. Although it is not an innovative concept,\n          emerging technologies (like DLT) offer substantial benefits for implementing this model\n          in a broader scale;\n     •    Usage based insurance: a new business model introduced by insurers and\n          intermediaries that more closely aligns behaviours with premium rates. For example, in\n          auto insurance there are usage based insurance products where the customer only pays\n          for the actual distance driven and driver behaviours also impact price; and\n     •    On-demand insurance: a new business model that specialises in covering only those\n          risks faced at a certain moment.\n36 https://www.applicoinc.com/blog/what-is-a-platform-business-model/ (accessed 2 January 2018)\nIssues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes\nApproved by the IAIS Executive Committee on 7 November 2018                                          Page 36 of 36\n","cleanText":"issues paper increasing digitalisation insurance potential impact consumer outcomes november issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page iais international association insurance supervisors iais voluntary membership organisation insurance supervisors regulators jurisdictions mission iais promote effective globally consistent supervision insurance industry order develop maintain fair safe stable insurance markets benefit protection policyholders contribute global financial stability established iais international standard setting body responsible developing principles standards supporting material supervision insurance sector assisting implementation iais also provides forum members share experiences understanding insurance supervision insurance markets iais coordinates work international financial policymakers associations supervisors regulators assists shaping financial systems globally particular iais member financial stability board fsb member standards advisory council international accounting standards board iasb partner access insurance initiative a2ii recognition collective expertise iais also routinely called upon g20 leaders international standard setting bodies input insurance issues well issues related regulation supervision global financial sector issues papers provide background particular topics describe current practices actual examples case studies pertaining particular topic identify related regulatory supervisory issues challenges issues papers primarily descriptive meant create expectations supervisors implement supervisory material issues papers often form part preparatory work developing standards may contain recommendations future work iais international association insurance supervisors c bank international settlements ch basel switzerland tel fax www iaisweb org document prepared market conduct working group consultation iais members document available iais website www iaisweb org (c) international association insurance supervisors iais rights reserved brief excerpts may reproduced translated provided source stated issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page contents executive summary acronyms introduction product design digitalisation impact product design examples digitalisation impact product design background shared economy usage based insurance demand insurance marketing sales distribution marketing promotions benefits opportunities potential risks robo advice types advice benefits opportunities robo advice potential risks price comparison websites benefits opportunities potential risks disclosure informed decision making benefits opportunities potential risks supervisory issues conclusion recommendations annex digital technologies alternative business models affecting insurance business issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page executive summary digitalisation transforming insurance business examples mobile devices internet things iot big data artificial intelligence ai chat bots distributed ledger technology dlt robo advisors impact throughout insurance value chain design underwriting pricing products marketing distribution claims processing ongoing management customers purpose paper consider impact trend increasing digitalisation insurance consumer outcomes insurance supervision light insurance core principle conduct business focus product design underwriting along marketing sales distribution aspects insurance value chain recognised impact digitalisation may differ jurisdictions depending legal frameworks place respect product design digitalisation may affect nature insurance coverage example demand insurance usage based insurance insurance based consumer generated data vehicles homes wearable devices potentially service broader clientele including people currently un der served insurers able adapt evolving demands market data available insurers use example motor vehicles inform pricing product consumers need aware use risk pricing tailored use risk profile customer affect price required reserves insurers terms marketing promotions digitalisation impact information provided consumers regardless use digital technology information provided needs timely clear accurate misleading greater availability customer related data increased analytics enhanced digital deployment tools enable insurers intermediaries identify opportunities across insurance value chain reduce customer friction increase efficiencies improve overall customer experience digital technology use social media may enable insurers intermediaries better reach target markets may reduce marketing costs example improve customers' experience offering easier quicker ways insurer consumer communicate hand social media applications may transparent consumers result consumers \"nudged\" without aware - consumers confronted unsolicited offerings based use internet risk customers persuaded buying products add ons best interest specific emerging sales method use robo advice may improve accessibility products customer however require proper design underlying algorithms adequate availability use customer data also depending sophistication algorithm available datasources benefits face face interaction salesperson customer may succeed example identifying non verbal hesitation furthermore flaws design operation algorithm create risk selling products entirely interest particular customer another development promotion sales use price comparison websites pcws provide automated suggestions proposals products providers prices based input consumer increased accessibility comparability information issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page insurers products well easy use line systems benefits customer may however issues around transparency respect identity independence owner operator comparison website consumers may also risk selecting products less suitable needs solely focus price elements product coverage generally innovations influence presentation disclosure information offer potential provide relevant information consumers useable manner relevant time large volumes information may however difficult read understand instance using smart phones insurers mindful risk misconceptions consumers therefore flaws consent given digital means digitalisation changes way insurance products designed distributed supervisors monitor developments engage stakeholders within outside insurance industry consider new supervisory responses protect consumers' interests one key challenges supervisors consider balanced approach facilitate innovations maintaining level consumer protection stipulated laws regulations supervisors likely confronted new insurance market participants like start ups \"big tech\" firms entities may different perspectives consumer interest compliance culture traditional incumbent insurers supervisors cognisant may need take proactive approach including \"educating\" new market participants challenges supervisors face developing new tools skills supervision increasingly digitalised firms enhancing cooperation financial authorities safeguarding supervisory parameters prevent regulatory arbitrage enhancing information security supervisors consider taking appropriate steps issuing guidelines help promote responsible use new technologies insurers intermediaries safeguard fair treatment customers issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page acronyms acpr autorite de controle prudentiel et de resolution france ai artificial intelligence afm autoriteit financiele markten netherlands amf autorite des marches financiers quebec asic australian securities investments commission bafin bundesanstalt fur finanzdienstleistungsaufsicht germany boe bank england dlt distributed ledger technology fca financial conduct authority finma financial market supervisory authority switzerland fintech financial technology fsb financial stability board iais international association insurance supervisors icp insurance core principle insurtech insurance technology iot internet things information technology mas monetary authority singapore ml machine learning naic national association insurance commissioners usa pcw price comparison website regtech regulatory technology suptech supervisory technology ubi usage based insurance issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page introduction described observers \"fourth industrial revolution\" digitalisation rapidly transforming societies economies velocity scope change significant digitalisation potential fundamentally change almost every industry every country one industries insurance continues revolution insurance pricing marketing product design claims settlement resulting insurers use new technologies available data diagram illustrates rapid change evident throughout insurance value chain design underwriting pricing products marketing distribution claims processing ongoing management customer relationships examples digitalisation technologies - machine learning artificial intelligence distributed ledger technology eg blockchain - applications - telematics robo advisers peer peer platford business models - set diagram one varied described annex diagram digitalisation insurance value chain * customer specific \"targeted\" * automated including non * platform business models marketing human product service * degree view customer * robo advice ai chat centers using robo advice consultants bots chat bots ai * continuous real time data * internet sales price * big data enables ability enabling focus high value comparison websites predict customers want customers * social media smart need ask * unstructured data eg voice phone device channels analysis learning direct distribution * continuous real time customer communication u w marketing pricing product claims customer sales underwriting management handling interactions distribution * telemetrics - customers insurers * fraud detection using big data understand risk much better wearables iot blockchain smart phones apps * blockchain facilitating trust worthy * big data enabling granular timely claims information accurate pricing faster u w * ai drones assessing processes * blockchain technologies seamlessly * claims cost efficiencies online smart manage instantly verify data sources device claims lodgement ai automated * peer peer insurance models assessing optimised pay outs reduced * granular customer specific product labour costs offerings including usage based insurance * supply chain management efficiencies * genetic data - potential impact pricing vertically integrated claims processes availability see example schwab k \" fourth industrial revolution means respond\" https www weforum org agenda fourth industrial revolution means respond january schwab issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page availability data sources telematics wearable devices enable insurers design price products basis information consumer developments artificial intelligence machine learning enable provision automated advice facilitate fraud detection comparison websites provide consumers information range products available new technologies able speed processes claims handling lead efficiencies drive costs whilst digitalisation potential benefit consumers give rise risks could impact fair consumer outcomes considered supervisors light requirements conduct business insurance core principle icp include potential impacts reduced face face contact insufficient consumer understanding product service provider risks security potential misuse increasing amounts consumer data potential exclusion consumers collection data policyholders may enable granular risk categorisation could potentially affect risk pooling principles may lead issues around affordability certain insurance products possibly even leading exclusion supervisors therefore possibly new ways monitor consumer outcomes carefully ensure supervisory regimes continue facilitate benefits consumers technology innovation whilst safeguarding policyholder protection vital intent icp met purpose paper consider impact increasing use digital technology insurance consider consumer outcomes discuss digitalisation means insurance supervision recognised impact digitalisation may differ jurisdictions depending legal frameworks place distinction made applications data information applications use generate paper cover innovative applications - generally - use data separate paper discuss details use personal data conduct business perspective paper provides considerations context iais work fintech insurtech accordingly paper focuses product design underwriting along marketing sales distribution aspects insurance value chain impact increasing use digital technology aspects insurance value chain addressed iais material paper three sections * section considers impacts digitalisation product design underwriting sections also gives examples digitalisation impact product design * section robo advice price comparison websites considered illustrate digitalisation' impact marketing sale distribution products * finally section paper describes challenges faced supervisors responding developments paragraph iais report \"fintech developments insurance industry\" march issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page product design digitalisation impact product design digitalisation changing risk landscape creating need enabling development new types lines products consumer needs evolving digital age growing expectation accessibility services solutions time place variety ways consumer trends habits supported new technologies greater connectivity wearables smart phones smart homes greater optionality sharing economy impacting way insurers design products policyholders changes driven new technology create new opportunities also new challenges risks insurers intermediaries need new insurance coverages new products likely grow digitalisation brings opportunities better serve customers changing needs may also serve means better reach underserved markets digitalisation may also lead shift distribution focused product design supply driven consumer focused product design demand driven may provide great opportunity may challenge insurers meet consumers' growing need covering new risks covering different way future digitalisation impacting insurers develop design underwrite products advancement technology may enable development adaptable tailored products creation new insurance products * big data means data risk assessment enable underwriting based granular data may turn increase accuracy allow faster risk specific underwriting needs balanced privacy concerns individual * ai may create new possibilities risk assessment underwriting example insurers use algorithms combination ai uses customer' insurance history lifestyle information suggest insurance products onboarding * iot may create new products focusing prevention situational insurance example sensor able monitor household water consumptions patterns detecting potential leaks interrupting flow basement flooded thus preventing major damage costly claims tools improve interaction provide value customer though raise concerns data devices eg alerts used premium increases changes existing coverage * telematics context iot telematics involve telecommunications sensors computer science allow sending receiving storing processing data via telecommunication devices without interfering steering remote objects * dlt may able seamlessly manage instantly verify data sources smart contracts ie programmes automatically execute claim payment pre defined conditions stored blockchain potential fully digital fully automated products could case agricultural parametric index based insurance technology proves viable tool could transform insurance bain company digitalization insurance multibillion dollar opportunity henrik naujoks florian mueller nikos kotalakidis march geneva papers impact digitalization insurance value chain insurability risks martin eling martin lehman institute economics university st gallen issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page industry shared transparent record contract related information enabling parties immutable audit trail underpinning end end underwriting claims governance without need intermediary following section consider benefits risks context examples impacts digitalisation product design examples digitalisation impact product design background numerous examples digitalisation changing nature insurance products following section provides examples three widespread significant examples namely * shared economy * usage based insurance * demand insurance examples involve fundamental change design product also examples changes digitalisation facilitated small specific changes product features united kingdom uk based fintech start cuvva set address gap providing hourly car insurance infrequent drivers wanted borrow people' cars cuvva allows customers arrange cover via app seconds cuvva manages sale service first notification loss process mobile app since launching initial car sharing product cuvva since launched second proposition designed car seldom drive customers pay small amount insure car whilst driving pay additional amount via app hours drive netherlands clixx product dutch insurer ohra offers opportunity insure also borrowed car product bought per day clixx' premium lower borrowed car already fully insured compared borrowed car legally obligated liability insurance shared economy new sharing models creating unique challenge traditional insurance protection coverage may align needs approaches taken shared economy many insurance products currently offered based exclusive legal economic ownership good shared economy based shared use goods additionally traditional insurance products generally intended cover personal commercial use good designed cover part time business use whether compensated strategic risk europe digitalisation transform risk insurance industry dieter goebbels country manager germany regional manager central europe xl catlin issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page availability insurance coverage adapted needs participants shared economy important development sharing industry acceptance consumers grow shared economy adequately mitigate potential risk participants - providers users - need appropriate insurance coverage currently participants shared economy try obtain insurance coverage traditional means may faced impossibility taking coverage fully meets particular needs example drivers working ride sharing businesses uber homeowners participating host shared hosting services airbnb always able find adequate insurance coverage traditional protections covering vehicles homes generally extend new businesses sharing industry personal property used part time business insurance industry already developed new products meet need adapted coverage important consumers understand differences limitations insurance coverage acting either provider user shared economy offering products consumers taking part shared economy equally important insurers clear limitations risk disruption damage reputation insurance industry products designed meet shared economy deliver level consumer protection traditional insurance products usage based insurance digitalisation already used automobile insurance terms product design traditional motor vehicle insurance policy joined data collection analytics tool capture data generated vehicle cases ubi product design includes various forms real time fact feedback data transmission insurer consumers terms pricing insurers' model prices based vehicle generated data turn based use vehicle insured - including vehicle driven ubi also produces data used claim settlements discover corroborate damage event order obtain data use vehicle insurers mostly use telematics may identify granular driving habits eg distance travelled hard braking number trips destinations data allows insurers establish rate personalised individual customer telematics app based relying smartphone' sensors gps signal making functionality dependent underlying smartphone' capabilities however personalisation may limits operation vehicle person policyholder affect data calculation insurance premiums thus important consumers information need properly informed make sound decisions insurance products use ubi programme addition consumers aware whether participation programmes voluntary basis information may help inform consumers features ubi programme may include things * programme eligibility criteria see naic white paper insurance implications home sharing regulator insights consumer awareness http www naic org prod serv ihs op pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page * type data collected * use data eg part investigation settlement claim third parties access use data granted * insurer employees could access collected data * impact data insurance premium * period used insurance premium reviews quebec amf published notice expectations regarding ubi programmes initiative intended highlight insurers firms representatives offering non life insurance importance effectively managing risks associated data sent via ubi programmes used automobile insurance underwriting also underlined need act fairly dealings consumers participate programmes netherlands netherlands sharing economy marks trend offering services already include additional insurance example dutch initiative swapfiets meaning \"swap bike\" offers opportunity lease bicycle fixed amount money per month repairments included well getting new bike original one gets stolen customer arrange additional insurance product demand insurance emergence shared economy underpinned changing attitude behaviour new consumer groups millennials causing shift product lines insurers trying respond need self directed tech savvy hyper personalised products services historically insurance purchased fixed period coverage - typically six months year time insurers systems processes developed around type product coverage exceptions travel insurance response changing ownership models sharing economy consumer may use product good limited period time product changing consumer desires coverage limited precise time frames insuring bicycle used new market entrants incumbent insurers responding developing new products adapting existing product lines pricing customer service experience create demand insurance trov trov mobile app allows users collect store information possessions including value partners insurers enable users insure specific possessions specified durations users literally turn insurance coverage sliding appropriate option mobile phone example could choose insure mobile phone house key demand insurance temporal nature provides insurance coverage specific periods time turned users identify https lautorite qc ca fileadmin lautorite reglementation assurances inst depot notice automobile usage based pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page need insurance get coverage specific period meet need ability insure \"moments\" enables consumers tailor coverage pay coverage need quickly alter insurance coverage meet changing personal circumstances however users need constantly engaged actively turning coverage obtain benefits demand insurance failure constantly engage may result insured insurers cognisant build controls mitigate risks pose could include * proactive messages remind consumers coverage still active perhaps importantly inactive ai learning behavioural economics could used optimise messaging * systems enable customers turn coverage set periods reoccurring basis example could insurance mobile activated house use location tracking verify * inbuilt terms conditions provide back coverage circumstances customers inadvertently fail turn coverage issues discussed section notably issues around use data subject seprate paper issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page marketing sales distribution marketing promotions consistent icp conduct business insurance products must marketed sold manner pays due regard interests needs customers insurers intermediaries required provide timely clear adequate pre contractual contractual information customers supervisors apply digital insurance activities requirements transparency disclosure provide equivalent level protection customers applied insurance business conducted non digital means marketing advertising digital means offer new opportunities inform empower consumers may pose certain additional challenges insurance industry supervisors alike necessitate consideration terms specific regulatory requirements industry responses south africa insurance policyholder protection rules recently amended ensure rules relating advertising marketing would apply similarly irrespective medium used advertising definitions \"advertisement\" \"direct marketing\" clarified widened scope follows \"advertisement\" means communication published medium form together communication intended create public interest business policies related services insurer persuade public part thereof transact relation policy related service insurer manner purport provide detailed information specific policyholder regarding specific policy related service \"direct marketing\" means marketing policy behalf insurer way telephone internet digital application platform media insert direct electronic mail manner entails completion submission application proposal order instruction contractual information required insurer relation entering policy transaction relation policy related services excludes publication advertisement australia asic good practice guide advertising covers digital advertising including online advertisements video streaming social media microblogging points highlighted * particular impact advertising high trust environment need distinguish clearly advertisement content ie blogs * online advertising beneficial provides links additional information customers cannot make misleading impressions created initial ad need balance promotion standard guidance see http download asic gov au media rg234 pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page asic taken action number potentially misleading social media advertisements relating self managed super pension funds benefits opportunities insurers intermediaries increasingly focusing ways improve marketing sales distribution increase ability reach customers digitalisation technologies seen product design including telematics ai big data one example targeted marketing ability develop specific marketing messages individual customers potential customers digital marketing may reduce marketing costs insurer intermediary creating savings may passed customer use big data may result better understanding customers inform personalised marketing appropriate levels disclosures greater availability customer related data increased analytics enhanced digital deployment tools enable insurers intermediaries identify opportunities across insurance value chain reduce customer friction increase efficiencies improve overall customer experience digital technology insurers intermediaries use enhanced customer experience product differentiation marketing campaigns example quebec start covera based marketing strategy digital solution promises break standard insurance renewal process identified common painpoint customers use targeted social media campaigns relay promotional material common way targeting particular customers active social media platforms insurers tapping market using social media make marketing seem less like \"cold advertising\" like information sharing entertainment \"infotainment\" examples us include gecko allstate' mayhem progressive' flo whose promotional mascots instantly recognisable insurance customers social media presence overcome fragmented communication policyholder insurers intermediaries use digital devices internet connect consumers throughout life policy underwriting claim example insurers started provide customers prevention tools free water humidity detector sends alert notification text message email senses problem initiatives part new digital brand marketing strategy tools designed provide data determining premium coverage rather attract retain customers potential risks use social media platforms digital marketing campaigns well increased collection use data may increasingly lead customers \"nudged\" directed including advertising without aware example insurers intermediaries third party marketers may \"target customer specific example see http asic gov au asic media centre find media release releases 041mr asic stops potentially misleading smsf social media advertising https covera ai http www digitalistmag com customer experience social media insurance marketing today issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page search engines click sponsored links may channel customers highlighting limiting particular information produce certain actions customer often lack transparency existence purpose practices targeted marketing social media may cases become confusing customers may struggle distinguishing neutral opinions social media promotional material sponsored insurers digital marketing mobile based applications could also used respond real time individual circumstances including consumers' emotions individual' insecurity want heightened key life events context insurance intangible product intended mitigate personal fears type emotional framing may pose concern singapore example small specific changes singapore policypal start helps consumers organise understand purchase insurance policies digitally mobile app policypal graduated mas sandbox insurance broker registered mas australia example revelation australian media outlets may facebook disclosed major australian bank could exploit moods insecurities users potential benefit advertisers followed media reports facebook contributed published study proceedings national academy sciences usa showed via experiment users could make people positive negative process described emotional contagion - positive expressions reduced people produced fewer positive posts negative posts negative expressions reduced opposite pattern occurred france french r recommendation use social media business purposes remind professionals supervisory expectations explain rules apply use social media french autorite de controle prudentiel et de resolution acpr issued recommendation november applies banking insurance sectors october firstly advertising material issued social media fulfil applicable rules regarding information disclosed presentation information secondly professionals refrain unfair commercial practices using social media instance misleading opinions good bad issued professionals social media avoided well practice buying \"likes\" \"followers\" https www theguardian com technology may facebook advertising data insecure teens see http www pnas org content full https www mckinsey com industries financial services insights insurtech threat inspires issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page moreover according acpr recommendation professionals set procedures disclosure content social media promotion \"add \" insurance products sales another product supervisors' radar use digital means market sell insurance products nevertheless facilitate practices common example offering travel insurance online sales process airline tickets example whilst could customer interest informed travel insurance options timing promotion end sales process customer already bought airline ticket way message delivered could result customers believing purchase add insurance product required primary purchase airline ticket completed creates risk passive purchases purchase cover needed united kingdom fca conducted market study general insurance add ons market study found add distribution method real impact consumer behaviour affects consumer decision making consumers often focus sale primary product leading many purchase add products need understand fca also found consumers poor awareness products bought - unaware owned add ons considered market study findings indicate consumers' ability make choices often hindered insufficient information available quality price add ons information presented late buying process following findings fca implemented two remedies address specific issues * ban opt selling * improved information provision add buyers robo advice types advice robo advice essentially financial advice automated practice distinction made following types advice * full robo advice robo adviser completely takes work traditional financial adviser \"customer journey\" fully digitalised advice fully automated human role develop maintain robo advice system prevent malfunctions algorithm face face contact * partial robo advice advice fully automated traditional adviser still available answer questions * hybrid advice robo adviser human beings interact example \"customer journey\" fully digitalised advice still provided human possibly face face * traditional face face advice technology used additional tool example show graphs animations information see french https acpr banque france fr fileadmin user upload acp publications registre officiel annexe reco 2013 r 01 pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page benefits opportunities robo advice robo advice potential improve accessibility consistency financial advice accessibility means financial advice easily accessible majority consumers includes continuous availability advice one' home may also reduce costs consumer furthermore consistency advice improved use technology new financial products become available product conditions change algorithm instantly take changes account programmed correctly using sufficient accurate data robo advice consistently quality robo adviser may helpful overcoming cognitive bias insufficient competence part human adviser another potential benefit robo advice customers may find disclosing pre exiting conditions mental health easier machine human adviser could encourage disclosures leading customers receiving appropriate coverage robo advice considered another form distribution addition internet telephone based sales potentially without providing advice customer robo advice also part enhanced consumer communications throughout life product traditional advice robo advice solid audit trail nature robo advice produce audit trail technology used rather requiring completed manually provided technology delivers audit trail reliable good quality robo advice could therefore make easier deliver traceability auditability advice advice given provider robo advice able provide insight data used algorithms used information presented customer would make robo advice traceable reproducible allowing customer subsequent adviser supervisor check advice established robo advice netherlands netherlands robo advice available couple years different types financial products however last year robo advice available non complex products car insurance since robo advice also developed complex financial products disability insurance one challenges according developers making sure customers completely understand meant specific questions posed robo advisor since human advisor present answer questions potential risks according icp advice provided consumers take account customer' disclosed circumstances advice communicated clear accurate manner comprehensible customer advice provided communicated customer written format paper durable accessible medium record kept \"client file\" however may specific issues need addressed safeguard fair treatment customers use robo advice robo advice cannot solve every limitation traditional face face advice robo advice example overcome problems flowing guidance issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page limited selection available products overcome problems caused complexity products advice fully automated customer might opportunity ask questions unless programmed robo chat risk misunderstanding therefore present robo advice face face advice lack interaction humans might also lead reduced detection contradicting answers customers human adviser recognise customer doubt robo adviser may able however robo advice tools could programmed purpose - example algorithm could detect customer continuously clicks back forth pages prompt pop asking customer whether needs additional help explanations detecting doubt however one challenging aspects robo advice conventional advice fully automated advice process customer responsible subsequent decisions however case face face advice adviser discuss customer' hesitation follow advice customer possibly remove doubts concerns fully automated concept cannot therefore may merit limiting possible deviations advice fully automated process could prevent consumers making suboptimal choices exchange lower insurance premium incorrectly programmed algorithm far reaching consequences therefore important algorithm carefully developed tested used practice subsequently subject adequate maintenance design algorithm robo advice needs output treats customer fairly faulty algorithm defective ai tool lead inconsistent advice consistently bad improper advice tools data rely also potentially reinforce perpetuate existing biases price comparison websites pcw price comparison websites also known digital comparison tools insurance websites present multiple insurance products providers insurance thereby enabling principle consumers compare select products array insurers intermediaries selecting product purchase website could direct customer website insurer intermediary complete transaction pcws currently well established many jurisdictions many products services electricity utility air tickets also key distribution channel insurance markets whilst various methods remuneration exist pcws remunerated insurer intermediary successful transaction usually via fixed amount per policy pcw usually customer relationship significant difference types intermediation supervision pcws varies across jurisdictions depending activities performed pcw' business model may considered intermediaries jurisdictions pcws need comply insurance intermediary requirements issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page benefits opportunities bringing together product price information single place potential benefit pcws empower consumer quickly compare assess select among products available insurance market potential accomplished - reliable information objectively presented related educational tools - pcws promote competitive markets empowering consumers pcws facilitate consumer shopping accessible time anywhere although insurers offer websites products similar access another benefit pcws consumers ability enter personal information receive personally relevant products prices multiple vendors - clear advantage entering personal information every time shopping across different insurers websites respect pcws promote competition well reduce marketing underwrting costs could result lower insurance premiums potential risks risks - consumers self direct inform without assistance - common digital sales via website insurer intermediary however number particular risks example risk pcws non disclosed conflicts interest due compensation arrangements ownership structures particular providers shown web site conflicts interest cause website present products guide consumers products interest website interest consumer pcws subject specific disclosure requirements may lack transparency lack transparency may relate example potential conflicts interest compensation arrangements providers true identity pcw owner operator providers could affect adversely ability consumers make informed decisions consumers generally aware number suppliers consulted given product comparision criteria used establish recommendation pcw permitted make recommendations consumers may believe pcw providing objective complete information fact pcw providing limited biased information channels consumer specific product another major risk consumers focus price select product result adequately covered bought product meet needs also risk consumers buy unsuitable products based results provided pcw believed receiving form advice results shown met cover needs specific risks identified use pcws create harm consumers including unreliable performance disorderly failure example caused technology data failure due volume transactions generated relative small number pcws issue one pcw failure algorithm present correct results display inaccurate information product far reaching impact markets pcws play big role even create systemic issue across specific product line could expected one pcw fails others pick market segment however markets concentration certain segments product lines may create harm consumers lack availability pcws issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page netherlands afm issued press release quality pcws main findings * overall services pcws found interest customer based research five main pcws netherlands usually comparisons ranked price quality based preferences customer * signals overall comparison based payments insurers however often insurer would end top consumer able close product via pcws - turn possible commissions paid * main points improvement provision information way top constructed inclusion one discounts premium default preferences afm issued another press release services pcws * main finding pcws sometimes provide financial advice advertising services execution customer onboarding however suitable financial advice pcws based execution therefore contains limited set questions consumer however might get impression financial advice given * example pcw giving advice presenting \" top best suitable mortgages \" qualify financial advice would compliant advice rules adequate customer onboarding * q afm explained services pcws would qualify financial advice market players move execution towards robo advice next couple years disclosure informed decision making standard requires insurers intermediaries provide timely clear adequate pre contractual contractual information customers product disclosure key requirement needs adapt new digital channels habits benefits opportunities one advantages online services providers use visual information disclose features product example course premium time presented manner easily understandable easily adjustable customer enters new information example using graphs goes product features providers experiment best way disclose information customers maximise intelligibility thereof chatbots may also assist customer takes long scroll past certain section moves quickly across material term policy could indicate customer looking additional information explanation either bot adviser depending complexity policy computer programme designed simulate conversation human users especially internet issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page technologies may utilise customer data introduce relevant disclosures based information obtained customer different data sources examples \"virtual cognitive customer service representatives\" chatbots include uk based spixii \"speaks\" six languages flamingo' \"rosie\" australia \"learns business\" order respond customers united states us based insurer lemonade used two types artificial intelligence \"cognitive\" systems interface customers one called \"maya\" signs customers via mobile devices called \"jim\" finalises claims without assistance insurify another example uses evia \"expert virtual insurance agent\" uses natural language image recognition collect auto insurance quotes customers also engage evia comes clarification terms \"comprehension testing\" technology may assist obtaining certainty disclosed information adequate consequences thereof properly conveyed insured technology particularly machine learning chatbots used enabler customer comprehension online filter quick test questions may also assist gauging customer' understanding may need educate online users dedicate sufficient time adequate understanding contents agreement means presentation instance dedicated popup windows play important role ensuring proper understanding information customers obtaining explicit consents appropriate addition gamified product sales information - whereby information disclosed part game challenge - keep consumer interested engaged critical information might otherwise overlooked seen understood retained consumer potential risks time efficiencies instant gratification associated digitised transacting mean customers expect relatively quick transactional experience particularly instances smartphone applications used poses significant challenges insurers maintaining balance convenient seamless contracting versus risk inadequate disclosure material policy terms conditions netherlands netherlands supervision intelligibility products part product approval process also online services customers often obliged read information confirm information understood nevertheless remains risk customers fully understand details product providing confirmation supervisor therefore encourages parties write product conditions way complete easy read understand possible https www digitalpulse pwc com au insurtech make love insurer issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page contrast face face interaction digital interaction make difficult flag misunderstandings need explanations based non verbal communication digital context customers faced plethora information different sources difficult identify reliable product disclosures manner appropriately presented differentiate product disclosure marketing addition digital tools used highlight information relevant equally used downplay information way creates risk consumers choosing inappropriate product non digital information disclosure consumers face risks information overload many sources means ascertain accuracy legitimacy information issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page supervisory issues digitalisation transforming insurance industry society supervisors presents \"moving target moving environment\" digitalisation changes way insurance products designed marketed distributed supervisors monitor new developments engage stakeholders within outside insurance industry protect consumers' interests includes non traditional stakeholders cloud service providers data vendors short new developments shift risks require new supervisory responses delivered adequate way key challenges described consumer outcomes supervisors digitalised world crucial understand incumbent insurers intermediaries well newer insurance market participants including insurtech start ups big techs behaving impact outcomes consumers digitalisation use data potential benefit consumers also create risks unfair treatment discrimination concerns access exclusion insurance services measuring assessing outcomes challenging supervisors consider monitoring behaviour outcomes examining information multiple sources australia september asic launched data strategy tag line \"connecting dots achieve better regulatory outcomes \" purpose describe asic' vision data objectives approach improving captures shares uses data germany bafin launched internal project learn business model technological start ups fintech appearance market drawing expertise areas banking insurance securities supervision objective project group observe latest developments fintech market review whether bafin needed adjust processes view new developments area digitalisation result project bafin established innovation hub innovation hub analyses evaluates upcoming technological solutions new business models based solutions additionally innovation hub coordinates network experts various areas responsibility within bafin rates innovative business models regard regulatory requirements experts banking insurance securities regulators represented network well licensing pursuit unauthorised business departments combination experience expertise ongoing oversight review licensing requirements allows rapid assessment innovative business models processes may unique one department http download asic gov au media asic data strategy published september pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page quebec amf created fintech lab deepen amf' knowledge new business models underlying technologies explore current potential applications technologies explore amf use france french central bank launched lab open space discussions collaborative work lab links banque de france various initiators innovative projects - start ups fintechs institutional players universities - order experiment new concepts technologies connection activities institution lab working technologies blockchain madre project iot ia etc acpr developing new tool supervising business practices * database innovations insurance sector enables monitoring technological innovations new services well guarantees offered * web listening platform internal analysing tool capture online messages consumers concerning bad market practices march acpr launched task force tackle opportunities challenges raised ai financial sector task force tf composed banks insurance companies fintechs also includes authorities data protection authority primary goal tf consist issuing discussion paper end aiming summarising implications using ai technologies financial sector united kingdom fca launched innovate department committed encouraging innovation interests consumers innovate provides assistance firms using innovation improve consumer outcomes helps firms better understand fca' rules processes guidance innovate helps fca keep ahead developing trends potential harms market united states naic us state insurance supervisors launched three year strategic plan state ahead drive efforts resources attention meet ongoing challenges including rapidly evolving marketplace fuelled seismic shifts consumer behaviour huge technological advances part goal ensure consumer protection keeps pace changes marketplace one objective optimise use market data regulatory processes enhance consumer protections including * rebuilding naic market conduct annual statement mcas application well applications utilising mcas data cloud based solution * implementing business intelligence tool self service capabilities * creating enterprise market data strategy analytics data warehouse * rebuilding naic consumer information source cis application additionally us state insurance regulators trying gain good understanding new innovative insurance products services including manner impact consumers stakeholders insurance marketplace accordingly naic issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page established innovation technology task force help insurance regulators stay informed key developments including new products services start companies well established insurance industry players task force naic big data working group charged part assessing data needs required tools state insurance regulators appropriately monitor marketplace evaluate underwriting rating claims marketing practices assessment includes gaining better understanding currently available data tools well recommendations additional data tools appropriate based assessment working group propose means collect house analyse needed data need balance innovation conduct concerns digitalisation innovation enormous potential help insurers intermediaries build cultures compliance identify potential consumer harms improve outcomes consumers however could also pose significant risks could lead consumer harms properly managed could include technological exclusion discrimination accessibility affordability issues one key challenges supervisors consider balanced approach facilitate innovations insurers maintaining level consumer protection stipulated laws regulations australia asic' innovation hub drives much australian conduct regulator' support digitalisation engagement fintech insurtech companies innovation hub asic provides informal assistance insurtechs regulatory obligations overarching regulatory framework appropriate options relating asic' exemption powers germany bafin' innovation hub serves - besides responsibilities - communication platform incumbents start ups one main aims gather spread knowledge example special contact form bafin' webpage company founders fintechs submit preliminary inquiries concrete questions eg business models term contact form may seem bit old fashioned contributes efficiency communication serves quickly determine responsible section respective business model body within bafin - public authority employees decisive factor france acpr launched fintech innovation unit point entry financial start ups licensing process unit evaluates opportunities well risks related innovations financial industry gives recommendations adjustments need made current regulation supervision practices acpr coordinating actions securities markets authority smf launched fintech forum leading dialogue fintech professionals regarding regulation supervision https www naic org state ahead htm many issues addressed application paper use digital technology inclusive insurance november issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page quebec amf created external advisory committee technological innovation advisory committee mandate assist amf identifying analysing trends issues help ensure balance consumer protection market efficiency switzerland finma working challenges presented fintech regarding authorisation supervision regulation innovative trends ideas require solid framework within operate clients financial system whole need protection shift direction finma regards innovation key competitiveness switzerland' financial centre adopts essentially neutral approach certain business models technologies therefore reviewed whether specific provisions ordinances circulars disadvantaged technologies concluded obstacles existed increasing number financial intermediaries interact clients via internet mobile devices finma therefore enhancing regulatory framework facilitate client onboarding via digital channels new circular anti money laundering due diligence requirements explained context digitalisation financial services need technology neutral regulation particularly respect video identification circular came force march launching operations fintech companies must establish whether subject anti money laundering authorisation requirements general authorisation insurance required risks dangers clients insured services rendered voluntarily without contractual obligation authorisation might required united kingdom fca launched regulatory sandbox regulatory sandbox space firms apply test innovative propositions live environment oversight input fca firms testing sandbox required meet relevant regulatory requirements bespoke safeguards mitigants built testing plan sandbox aims enable firms get market test propositions faster speed reduced cost also gives fca understanding opportunities potential harms innovation create market almost firms supported across first four cohorts fca' sandbox including significant number insurance sector supervisory skills tools supervisors become \"data driven\" \"digital intelligence led\" supervising market conduct digital world requires different skill sets addition lawyers economists actuaries mathematicians data scientists interdisciplinary supervisory teams vital digitalised world supervisors technologically numerically literate understand risks associated data supervisors need new skills identify monitor assess new applications technologies understanding market structures activities new participants understanding consumer outcomes arrange knowledge skill wihin staff - efficient - use third party providers lawyers economists actuaries mathematicians data scientists need work together supervise insurance issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page markets respect supervisory authorities need reconsider qualifications skill sets need become \"fit future\" continue compete talent within industry germany specialists part supervising teams order better prepare challenges posed inter alia cyber risks bafin set separate organisational unit supervision financial sector january quebec amf created dedicated internal working group experts fintech involving employees working cross functional teams france alongside creation dedicated teams hubs central bank appointed chief digital officer cdo charge digital transformation institution also chairing innovation lab united states part naic us state insurance supervisors state ahead strategic plan theme consumer protection education recognises need stay abreast new developments area innovation emerging technology need become engaged areas insurtech regtech opportunities explored objective include providing forums programmes state insurance regulator education discussion regarding changes insurance marketplace including innovation technology convening autonomous vehicle insurance forum stakeholders discuss insurance regulatory issues related autonomous vehicles considering creation cybersecurity insurance institute additionally naic innovation technology task force charged part discussing innovation technology developments insurance sector including collection use data insurers state insurance regulators - well new products services distribution platforms - order educate state insurance regulators developments impact consumer protection privacy insurer producer oversight marketplace dynamics state based insurance regulatory framework particular supervisors need skills understand digitalisation result consumer harm example supervisors may encounter challenges supervising self learning algorithms underlie automated decisions made digitalised world problematic consumer protection perspective also risk management perspective issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page robo advice robo advice provides useful case study many issues pertinent supervisors result many supervisors recently published guidelines approaching robo advice including * germany https www bafin de en aufsicht fintech anlageberatung anlageberatung de en html * australia https asic gov au regulatory resources find document regulatory guides rg providing digital financial product advice retail clients * netherlands https www afm nl profmedia files onderwerpen roboadvies sav view robo advice pdf * united kingdom uk' fca launched advice unit may provide regulatory feedback firms developing automated advice guidance models across range sectors including insurance feedback focuses helping firm understand regulatory implications model fca also publishes tools resources firms developing automated advice guidance propositions based experiences individual firms developing guidelines supervisors needed consider quality advice provided measured verified supervisors directly supervise algorithm supervisors supervise monitor outputs - ie advice supervisors require insurers intermediaries self audit provide annual assurances advice robo advisers providing appropriate require insurers intermediaries engage external experts conduct audits depending address questions supervisors may need establish dedicated teams address technical matters involving specialists required knowledge non digital advice supervisors need mandate insurers intermediaries adopt appropriate document management strategies includes retaining versions algorithm robo adviser needs save algorithm used data information advice provided customer supervisory authorities consider embrace new technologies help carry supervision also referred suptech solutions germany first half bafin published report big data ai together partnerschaft deutschland fraunhofer institute intelligent analysis information systems boston consulting group https www bafin de shareddocs downloads en dl bdai studie en html advancements key technologies increasing data availability decreasing entry barriers usage big data ai solutions incumbent institutions new suptech use technological innovations fintech supervisory authorities regtech use technological innovations fintech compliance purposes reporting regulated financial institutions issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page market players able structure work processes efficiently develop new business models supervisors however facing new challenges use big data ai impacts financial markets aim report gain better understanding challenges report highlights implications technology driven market developments various regulatory supervisory perspectives six main observations implications summarised follows financial data innovation race already underway systematic dependencies big data ai companies could develop outside regulatory framework financial system black box modelling spreading black box modelling must never stand way proper business organisation big data ai accelerate automation process responsibility always remain senior management \"transparent customer\" phrase age big data ai big data ai must used customers consumer confidence catalyst big data ai innovations anchor stability integrity financial system speed innovation increasingly tests limits regulatory framework' adaptability level playing field age big data ai means speed innovation met agile oversight technology neutral regulation united kingdom fca worked ing commonwealth bank australia pinsent masons test possibilities using natural processing language ai technologies interpret markets financial instruments directive ii regulations automatically build manage compliance programme different entities supervisors also need deal non incumbent firms different entity structures approaches consumer related risk incumbents supervisors need engage new entrants insurance financial services may experience knowledge financial services regulation new entrants may different entity structures approaches consumer related risk incumbents historically monitored supervisors unlike incumbents general compliance awareness risk culture ability comply regulatory requirements may differ significantly non traditional firms may require proactive strategy outreach engagement new entrants inform \"educate\" relevant supervisory matters proper compliance attitude well capitalised \"bigtech\" platform businesses may move distribution markets small nimble lowly capitalised insurtech start ups may also look enter insurance markets supervisors need understand different challenges posed new entities may lack knowledge regulatory reuqirements may experience engaging supervisors supervisory cooperation cooperation financial supervisory authorities crucial digitalised world digital innovations risks stop border jurisdictional supervisors coordinate authorities jurisdictions meet challenges supervisors proactively work together across jurisdictional subject matter boundaries identify emerging trends develop implement solutions includes collaboration market conduct regulators prudential privacy competition issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page regulators given implications digital technology consumer outcomes significant number solutions focused marketing sales distribution end value chain regular going interaction supervisors crucial iais continues support facilitate discussions including fintech forum france acpr signed agreement authority supervising security information systems anssi authority responsible responding threats targeting public authorities private sector particular vital information systems coordinates government action area defence systems regulatory arbitrage supervisors also need cognisant emergence product types effect insurance structured way falls outside legal definition regulated insurance product would enable product providers avoid regulatory requirements consumers means would able access compensation policyholder protection schemes product manufacturers unable meet claim costs arbitrage take two forms * jurisdictional product falls outside jurisdictional power regulator despite available customers jurisdiction * definitional product legal characteristics insurance product although effects one addition digitalisation new technologies may increase potential regulatory arbitrage created products bundle insurance non insurance products services choice consumer limited purchasing purchasing package information security storage protection third party use customer data insights gathered also issue cyber risks data protection questions become vital importance steady rise digitalisation working together competent authorities issues utmost importance world wide wannacry \"petya\" ransomware outbreaks highlight cyber risks rise data customer specific behavioural insights insurers hold would high value context particularly relevant given order drive efficiencies reduce costs many insurers store data insights cloud share data insights third parties many shore cloud services probably focus hackers cases much better protected premise installations run incumbents hand cloud service successfully attacked outcomes worse even systemic customers need know data insights specific derived data secure corrupted tainted clearly access data challenge privacy regulators also financial services regulators issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page eu eu reformed data protection rules simplify use big data businesses set high standards data protection may entry application general data protection regulation one set data protection rules companies operating eu wherever based also eu aims strengthen cybersecurity regulation cope growing threat cyberattacks take advantage opportunities offered new digital era october european council called common approach cybersecurity line european commission september reform package united states naic big data working group charged part reviewing current regulatory frameworks used oversee insurers use consumer non insurance data appropriate recommend modifications model laws regulations regarding marketing rating underwriting claims regulation data vendors brokers regulatory reporting requirements consumer disclosure requirements use cloud computing increasing insurance industry need supervisors enhance regulatory frameworks supervisory practices effectively capture risks supervisors typically apply existing frameworks outsourcing governance risk management internal control information security insurers' cloud computing activities others issued cloud specific recommendations expectations meanwhile authorities put place formal informal communication arrangements reinforcing supervisory processes better monitor assess risks cloud computing cloud services give rise specific questions example * country data stored verified * access data * key controls provided * danger risk concentration many cloud service providers * possible conflict interest data different insurers stored server information commissioner office big data artificial intelligence machine learning data protection version regulation eu european parliament council april protection natural persons regard processing personal data free movement data repealing directive ec general data protection regulation oj l p 28european commission https ec europa eu info law law topic data protection reform general data protection regulation gdpr govern en references http www consilium europa eu en policies fsi insights emerging prudential approaches outsourcing cloud case insurance companies forthcoming issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page * possible conflict interest cloud service provider decides involve insurance business also vital supervisors direct immediate access data stored cloud data stored insurer intermediaries servers germany bafin published journal article \"cloud computing compliance supervisory requirements regarding rights information audit ability monitor\" regard outsourcing cloud service providers bafin also holds discussions respective cloud service providers insurers content outsourcing contracts circular clarify supervisory requirements insurers vait published https www bafin de shareddocs downloads de rundschreiben dl rs 1810 vait va html https www bafin de shareddocs veroeffentlichungen en fachartikel fa bj 1804 cloud comput ing en html issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page conclusion recommendations digital innovations change potentially improve customer experience reduce insurers' intermediaries' operating costs however digitalisation may impact consumer protection extent customers treated fairly design underwriting pricing products marketing distribution claims processing ongoing management customers therefore respect product design marketing sales due attention needs given achieving fair customer outcomes terms products meet consumers' needs design use algorithms use customer data adjust digital age foster innovation supervisors consider ensure new innovation come expense protections policyholders integrity insurance sector whole one key challenges supervisors consider balanced approach facilitate innovations insurers maintaining level consumer protection stipulated laws regulations promote innovation consistent consumer benefit protection recommended supervisors develop thorough understanding innovations work applied ensure proper assessment new products business models design functioning architecture infrastructures processes catered insurers' risk management framework supervisors may also need develop new tools skills supervision digitalised insurers enhancing cooperation financial authorities safeguarding supervisory perimeter prevent regulatory arbitrage enhancing information security supported material developed iais supervisors consider establishing guidance appropriate responsible use new technologies safeguard fair treatment customers - example terms use ai robo advice mechanisms - promote advice services suitable affordable customers issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page annex digital technologies alternative business models affecting insurance business general overview significant innovations within insurance industry mentioned iais report \"fintech innovations insurance industry\" digital devices internet changes addressed paper facilitated proliferation digital devices devices contain computer microcontroller smartphones tablets wearables devices connected internet global network computers using standardised communication protocols internet things iot iot involves internetworking physical devices vehicles buildings items also referred connected devices smart devices embedded electronics software sensors actuators network connectivity enable objects collect exchange data telematics telemetry context iot telematics involves telecommunications sensors computer science allow sending receiving storing processing data via telecommunication devices affecting control remote objects telemetry involves transmission measurements location origin location computing consumption especially without affecting control remote objects context insurance main applications connected cars advanced driver assistance systems adas health monitoring home monitoring big data data analytics insurance market big data data analytics could used various processes product offerings risk selection pricing cross selling claims prediction fraud detection example offer customised products comparators robo advisers online services provide automated algorithm based product comparison advice without human intervention february https www iaisweb org page news papers reports file report fintech developments insurance industry term iot defined global infrastructure information society enabling advanced services interconnecting physical virtual things based existing evolving interoperable information communication technologies source http www itu int itu recommendations rec aspx rec big data term used storage data different sources large volume speed iais fintech developments insurance industry february data analytics process inspecting cleaning transforming modelling data goal discovering useful information suggesting conclusions supporting decision making iais fintech developments insurance industry february issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page machine learning ml artificial intelligence ai use ml ai enables several insurance industry processes use data real time especially use events prediction eg vehicles thefts health problems weather events vast scope ai better pricing risks also fraud prevention automated underwriting claims handling preventive counselling distributed ledger technology dlt distributed ledger essentially asset database shared across network multiple sites geographies institutions security accuracy assets stored ledger maintained cryptographically use \"keys\" signatures control within shared ledger * blockchain type decentralised distributed ledger comprised unchangeable digitally recorded data packages called \"blocks\" stored linear chain * smart contracts novelty dlt database - also set rules transaction business logic tied transaction smart contract term used describe computer programme code capable facilitating executing enforcing negotiation performance agreement using dlt platform business models peer peer usage based demand insurance emerging digital technologies facilitating alternative business models * platform business models platform business model creates value facilitating exchanges two independent groups usually consumers producers make exchanges happen platforms harness create large rapidly scalable networks users resources platforms means production - instead create means connection google apple facebook amazon uber alibaba examples platform business models * peer peer business model allows insured pool capital self organise self administer insurance although innovative concept emerging technologies like dlt offer substantial benefits implementing model broader scale * usage based insurance new business model introduced insurers intermediaries closely aligns behaviours premium rates example auto insurance usage based insurance products customer pays actual distance driven driver behaviours also impact price * demand insurance new business model specialises covering risks faced certain moment https www applicoinc com blog platform business model accessed january issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page","NLP":"issues paper increasing digitalisation insurance potential impact consumer outcomes november issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page iais international association insurance supervisors iais voluntary membership organisation insurance supervisors regulators jurisdictions mission iais promote effective globally consistent supervision insurance industry order develop maintain fair safe stable insurance markets benefit protection policyholders contribute global financial stability established iais international standard setting body responsible developing principles standards supporting material supervision insurance sector assisting implementation iais also provides forum members share experiences understanding insurance supervision insurance markets iais coordinates work international financial policymakers associations supervisors regulators assists shaping financial systems globally particular iais member financial stability board fsb member standards advisory council international accounting standards board iasb partner access insurance initiative a2ii recognition collective expertise iais also routinely called upon g20 leaders international standard setting bodies input insurance issues well issues related regulation supervision global financial sector issues papers provide background particular topics describe current practices actual examples case studies pertaining particular topic identify related regulatory supervisory issues challenges issues papers primarily descriptive meant create expectations supervisors implement supervisory material issues papers often form part preparatory work developing standards may contain recommendations future work iais international association insurance supervisors c bank international settlements ch basel switzerland tel fax www iaisweb org document prepared market conduct working group consultation iais members document available iais website www iaisweb org (c) international association insurance supervisors iais rights reserved brief excerpts may reproduced translated provided source stated issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page contents executive summary acronyms introduction product design digitalisation impact product design examples digitalisation impact product design background shared economy usage based insurance demand insurance marketing sales distribution marketing promotions benefits opportunities potential risks robo advice types advice benefits opportunities robo advice potential risks price comparison websites benefits opportunities potential risks disclosure informed decision making benefits opportunities potential risks supervisory issues conclusion recommendations annex digital technologies alternative business models affecting insurance business issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page executive summary digitalisation transforming insurance business examples mobile devices internet things iot big data artificial intelligence ai chat bots distributed ledger technology dlt robo advisors impact throughout insurance value chain design underwriting pricing products marketing distribution claims processing ongoing management customers purpose paper consider impact trend increasing digitalisation insurance consumer outcomes insurance supervision light insurance core principle conduct business focus product design underwriting along marketing sales distribution aspects insurance value chain recognised impact digitalisation may differ jurisdictions depending legal frameworks place respect product design digitalisation may affect nature insurance coverage example demand insurance usage based insurance insurance based consumer generated data vehicles homes wearable devices potentially service broader clientele including people currently un der served insurers able adapt evolving demands market data available insurers use example motor vehicles inform pricing product consumers need aware use risk pricing tailored use risk profile customer affect price required reserves insurers terms marketing promotions digitalisation impact information provided consumers regardless use digital technology information provided needs timely clear accurate misleading greater availability customer related data increased analytics enhanced digital deployment tools enable insurers intermediaries identify opportunities across insurance value chain reduce customer friction increase efficiencies improve overall customer experience digital technology use social media may enable insurers intermediaries better reach target markets may reduce marketing costs example improve customers' experience offering easier quicker ways insurer consumer communicate hand social media applications may transparent consumers result consumers \"nudged\" without aware - consumers confronted unsolicited offerings based use internet risk customers persuaded buying products add ons best interest specific emerging sales method use robo advice may improve accessibility products customer however require proper design underlying algorithms adequate availability use customer data also depending sophistication algorithm available datasources benefits face face interaction salesperson customer may succeed example identifying non verbal hesitation furthermore flaws design operation algorithm create risk selling products entirely interest particular customer another development promotion sales use price comparison websites pcws provide automated suggestions proposals products providers prices based input consumer increased accessibility comparability information issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page insurers products well easy use line systems benefits customer may however issues around transparency respect identity independence owner operator comparison website consumers may also risk selecting products less suitable needs solely focus price elements product coverage generally innovations influence presentation disclosure information offer potential provide relevant information consumers useable manner relevant time large volumes information may however difficult read understand instance using smart phones insurers mindful risk misconceptions consumers therefore flaws consent given digital means digitalisation changes way insurance products designed distributed supervisors monitor developments engage stakeholders within outside insurance industry consider new supervisory responses protect consumers' interests one key challenges supervisors consider balanced approach facilitate innovations maintaining level consumer protection stipulated laws regulations supervisors likely confronted new insurance market participants like start ups \"big tech\" firms entities may different perspectives consumer interest compliance culture traditional incumbent insurers supervisors cognisant may need take proactive approach including \"educating\" new market participants challenges supervisors face developing new tools skills supervision increasingly digitalised firms enhancing cooperation financial authorities safeguarding supervisory parameters prevent regulatory arbitrage enhancing information security supervisors consider taking appropriate steps issuing guidelines help promote responsible use new technologies insurers intermediaries safeguard fair treatment customers issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page acronyms acpr autorite de controle prudentiel et de resolution france ai artificial intelligence afm autoriteit financiele markten netherlands amf autorite des marches financiers quebec asic australian securities investments commission bafin bundesanstalt fur finanzdienstleistungsaufsicht germany boe bank england dlt distributed ledger technology fca financial conduct authority finma financial market supervisory authority switzerland fintech financial technology fsb financial stability board iais international association insurance supervisors icp insurance core principle insurtech insurance technology iot internet things information technology mas monetary authority singapore ml machine learning naic national association insurance commissioners usa pcw price comparison website regtech regulatory technology suptech supervisory technology ubi usage based insurance issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page introduction described observers \"fourth industrial revolution\" digitalisation rapidly transforming societies economies velocity scope change significant digitalisation potential fundamentally change almost every industry every country one industries insurance continues revolution insurance pricing marketing product design claims settlement resulting insurers use new technologies available data diagram illustrates rapid change evident throughout insurance value chain design underwriting pricing products marketing distribution claims processing ongoing management customer relationships examples digitalisation technologies - machine learning artificial intelligence distributed ledger technology eg blockchain - applications - telematics robo advisers peer peer platford business models - set diagram one varied described annex diagram digitalisation insurance value chain * customer specific \"targeted\" * automated including non * platform business models marketing human product service * degree view customer * robo advice ai chat centers using robo advice consultants bots chat bots ai * continuous real time data * internet sales price * big data enables ability enabling focus high value comparison websites predict customers want customers * social media smart need ask * unstructured data eg voice phone device channels analysis learning direct distribution * continuous real time customer communication u w marketing pricing product claims customer sales underwriting management handling interactions distribution * telemetrics - customers insurers * fraud detection using big data understand risk much better wearables iot blockchain smart phones apps * blockchain facilitating trust worthy * big data enabling granular timely claims information accurate pricing faster u w * ai drones assessing processes * blockchain technologies seamlessly * claims cost efficiencies online smart manage instantly verify data sources device claims lodgement ai automated * peer peer insurance models assessing optimised pay outs reduced * granular customer specific product labour costs offerings including usage based insurance * supply chain management efficiencies * genetic data - potential impact pricing vertically integrated claims processes availability see example schwab k \" fourth industrial revolution means respond\" https www weforum org agenda fourth industrial revolution means respond january schwab issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page availability data sources telematics wearable devices enable insurers design price products basis information consumer developments artificial intelligence machine learning enable provision automated advice facilitate fraud detection comparison websites provide consumers information range products available new technologies able speed processes claims handling lead efficiencies drive costs whilst digitalisation potential benefit consumers give rise risks could impact fair consumer outcomes considered supervisors light requirements conduct business insurance core principle icp include potential impacts reduced face face contact insufficient consumer understanding product service provider risks security potential misuse increasing amounts consumer data potential exclusion consumers collection data policyholders may enable granular risk categorisation could potentially affect risk pooling principles may lead issues around affordability certain insurance products possibly even leading exclusion supervisors therefore possibly new ways monitor consumer outcomes carefully ensure supervisory regimes continue facilitate benefits consumers technology innovation whilst safeguarding policyholder protection vital intent icp met purpose paper consider impact increasing use digital technology insurance consider consumer outcomes discuss digitalisation means insurance supervision recognised impact digitalisation may differ jurisdictions depending legal frameworks place distinction made applications data information applications use generate paper cover innovative applications - generally - use data separate paper discuss details use personal data conduct business perspective paper provides considerations context iais work fintech insurtech accordingly paper focuses product design underwriting along marketing sales distribution aspects insurance value chain impact increasing use digital technology aspects insurance value chain addressed iais material paper three sections * section considers impacts digitalisation product design underwriting sections also gives examples digitalisation impact product design * section robo advice price comparison websites considered illustrate digitalisation' impact marketing sale distribution products * finally section paper describes challenges faced supervisors responding developments paragraph iais report \"fintech developments insurance industry\" march issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page product design digitalisation impact product design digitalisation changing risk landscape creating need enabling development new types lines products consumer needs evolving digital age growing expectation accessibility services solutions time place variety ways consumer trends habits supported new technologies greater connectivity wearables smart phones smart homes greater optionality sharing economy impacting way insurers design products policyholders changes driven new technology create new opportunities also new challenges risks insurers intermediaries need new insurance coverages new products likely grow digitalisation brings opportunities better serve customers changing needs may also serve means better reach underserved markets digitalisation may also lead shift distribution focused product design supply driven consumer focused product design demand driven may provide great opportunity may challenge insurers meet consumers' growing need covering new risks covering different way future digitalisation impacting insurers develop design underwrite products advancement technology may enable development adaptable tailored products creation new insurance products * big data means data risk assessment enable underwriting based granular data may turn increase accuracy allow faster risk specific underwriting needs balanced privacy concerns individual * ai may create new possibilities risk assessment underwriting example insurers use algorithms combination ai uses customer' insurance history lifestyle information suggest insurance products onboarding * iot may create new products focusing prevention situational insurance example sensor able monitor household water consumptions patterns detecting potential leaks interrupting flow basement flooded thus preventing major damage costly claims tools improve interaction provide value customer though raise concerns data devices eg alerts used premium increases changes existing coverage * telematics context iot telematics involve telecommunications sensors computer science allow sending receiving storing processing data via telecommunication devices without interfering steering remote objects * dlt may able seamlessly manage instantly verify data sources smart contracts ie programmes automatically execute claim payment pre defined conditions stored blockchain potential fully digital fully automated products could case agricultural parametric index based insurance technology proves viable tool could transform insurance bain company digitalization insurance multibillion dollar opportunity henrik naujoks florian mueller nikos kotalakidis march geneva papers impact digitalization insurance value chain insurability risks martin eling martin lehman institute economics university st gallen issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page industry shared transparent record contract related information enabling parties immutable audit trail underpinning end end underwriting claims governance without need intermediary following section consider benefits risks context examples impacts digitalisation product design examples digitalisation impact product design background numerous examples digitalisation changing nature insurance products following section provides examples three widespread significant examples namely * shared economy * usage based insurance * demand insurance examples involve fundamental change design product also examples changes digitalisation facilitated small specific changes product features united kingdom uk based fintech start cuvva set address gap providing hourly car insurance infrequent drivers wanted borrow people' cars cuvva allows customers arrange cover via app seconds cuvva manages sale service first notification loss process mobile app since launching initial car sharing product cuvva since launched second proposition designed car seldom drive customers pay small amount insure car whilst driving pay additional amount via app hours drive netherlands clixx product dutch insurer ohra offers opportunity insure also borrowed car product bought per day clixx' premium lower borrowed car already fully insured compared borrowed car legally obligated liability insurance shared economy new sharing models creating unique challenge traditional insurance protection coverage may align needs approaches taken shared economy many insurance products currently offered based exclusive legal economic ownership good shared economy based shared use goods additionally traditional insurance products generally intended cover personal commercial use good designed cover part time business use whether compensated strategic risk europe digitalisation transform risk insurance industry dieter goebbels country manager germany regional manager central europe xl catlin issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page availability insurance coverage adapted needs participants shared economy important development sharing industry acceptance consumers grow shared economy adequately mitigate potential risk participants - providers users - need appropriate insurance coverage currently participants shared economy try obtain insurance coverage traditional means may faced impossibility taking coverage fully meets particular needs example drivers working ride sharing businesses uber homeowners participating host shared hosting services airbnb always able find adequate insurance coverage traditional protections covering vehicles homes generally extend new businesses sharing industry personal property used part time business insurance industry already developed new products meet need adapted coverage important consumers understand differences limitations insurance coverage acting either provider user shared economy offering products consumers taking part shared economy equally important insurers clear limitations risk disruption damage reputation insurance industry products designed meet shared economy deliver level consumer protection traditional insurance products usage based insurance digitalisation already used automobile insurance terms product design traditional motor vehicle insurance policy joined data collection analytics tool capture data generated vehicle cases ubi product design includes various forms real time fact feedback data transmission insurer consumers terms pricing insurers' model prices based vehicle generated data turn based use vehicle insured - including vehicle driven ubi also produces data used claim settlements discover corroborate damage event order obtain data use vehicle insurers mostly use telematics may identify granular driving habits eg distance travelled hard braking number trips destinations data allows insurers establish rate personalised individual customer telematics app based relying smartphone' sensors gps signal making functionality dependent underlying smartphone' capabilities however personalisation may limits operation vehicle person policyholder affect data calculation insurance premiums thus important consumers information need properly informed make sound decisions insurance products use ubi programme addition consumers aware whether participation programmes voluntary basis information may help inform consumers features ubi programme may include things * programme eligibility criteria see naic white paper insurance implications home sharing regulator insights consumer awareness http www naic org prod serv ihs op pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page * type data collected * use data eg part investigation settlement claim third parties access use data granted * insurer employees could access collected data * impact data insurance premium * period used insurance premium reviews quebec amf published notice expectations regarding ubi programmes initiative intended highlight insurers firms representatives offering non life insurance importance effectively managing risks associated data sent via ubi programmes used automobile insurance underwriting also underlined need act fairly dealings consumers participate programmes netherlands netherlands sharing economy marks trend offering services already include additional insurance example dutch initiative swapfiets meaning \"swap bike\" offers opportunity lease bicycle fixed amount money per month repairments included well getting new bike original one gets stolen customer arrange additional insurance product demand insurance emergence shared economy underpinned changing attitude behaviour new consumer groups millennials causing shift product lines insurers trying respond need self directed tech savvy hyper personalised products services historically insurance purchased fixed period coverage - typically six months year time insurers systems processes developed around type product coverage exceptions travel insurance response changing ownership models sharing economy consumer may use product good limited period time product changing consumer desires coverage limited precise time frames insuring bicycle used new market entrants incumbent insurers responding developing new products adapting existing product lines pricing customer service experience create demand insurance trov trov mobile app allows users collect store information possessions including value partners insurers enable users insure specific possessions specified durations users literally turn insurance coverage sliding appropriate option mobile phone example could choose insure mobile phone house key demand insurance temporal nature provides insurance coverage specific periods time turned users identify https lautorite qc ca fileadmin lautorite reglementation assurances inst depot notice automobile usage based pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page need insurance get coverage specific period meet need ability insure \"moments\" enables consumers tailor coverage pay coverage need quickly alter insurance coverage meet changing personal circumstances however users need constantly engaged actively turning coverage obtain benefits demand insurance failure constantly engage may result insured insurers cognisant build controls mitigate risks pose could include * proactive messages remind consumers coverage still active perhaps importantly inactive ai learning behavioural economics could used optimise messaging * systems enable customers turn coverage set periods reoccurring basis example could insurance mobile activated house use location tracking verify * inbuilt terms conditions provide back coverage circumstances customers inadvertently fail turn coverage issues discussed section notably issues around use data subject seprate paper issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page marketing sales distribution marketing promotions consistent icp conduct business insurance products must marketed sold manner pays due regard interests needs customers insurers intermediaries required provide timely clear adequate pre contractual contractual information customers supervisors apply digital insurance activities requirements transparency disclosure provide equivalent level protection customers applied insurance business conducted non digital means marketing advertising digital means offer new opportunities inform empower consumers may pose certain additional challenges insurance industry supervisors alike necessitate consideration terms specific regulatory requirements industry responses south africa insurance policyholder protection rules recently amended ensure rules relating advertising marketing would apply similarly irrespective medium used advertising definitions \"advertisement\" \"direct marketing\" clarified widened scope follows \"advertisement\" means communication published medium form together communication intended create public interest business policies related services insurer persuade public part thereof transact relation policy related service insurer manner purport provide detailed information specific policyholder regarding specific policy related service \"direct marketing\" means marketing policy behalf insurer way telephone internet digital application platform media insert direct electronic mail manner entails completion submission application proposal order instruction contractual information required insurer relation entering policy transaction relation policy related services excludes publication advertisement australia asic good practice guide advertising covers digital advertising including online advertisements video streaming social media microblogging points highlighted * particular impact advertising high trust environment need distinguish clearly advertisement content ie blogs * online advertising beneficial provides links additional information customers cannot make misleading impressions created initial ad need balance promotion standard guidance see http download asic gov au media rg234 pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page asic taken action number potentially misleading social media advertisements relating self managed super pension funds benefits opportunities insurers intermediaries increasingly focusing ways improve marketing sales distribution increase ability reach customers digitalisation technologies seen product design including telematics ai big data one example targeted marketing ability develop specific marketing messages individual customers potential customers digital marketing may reduce marketing costs insurer intermediary creating savings may passed customer use big data may result better understanding customers inform personalised marketing appropriate levels disclosures greater availability customer related data increased analytics enhanced digital deployment tools enable insurers intermediaries identify opportunities across insurance value chain reduce customer friction increase efficiencies improve overall customer experience digital technology insurers intermediaries use enhanced customer experience product differentiation marketing campaigns example quebec start covera based marketing strategy digital solution promises break standard insurance renewal process identified common painpoint customers use targeted social media campaigns relay promotional material common way targeting particular customers active social media platforms insurers tapping market using social media make marketing seem less like \"cold advertising\" like information sharing entertainment \"infotainment\" examples us include gecko allstate' mayhem progressive' flo whose promotional mascots instantly recognisable insurance customers social media presence overcome fragmented communication policyholder insurers intermediaries use digital devices internet connect consumers throughout life policy underwriting claim example insurers started provide customers prevention tools free water humidity detector sends alert notification text message email senses problem initiatives part new digital brand marketing strategy tools designed provide data determining premium coverage rather attract retain customers potential risks use social media platforms digital marketing campaigns well increased collection use data may increasingly lead customers \"nudged\" directed including advertising without aware example insurers intermediaries third party marketers may \"target customer specific example see http asic gov au asic media centre find media release releases 041mr asic stops potentially misleading smsf social media advertising https covera ai http www digitalistmag com customer experience social media insurance marketing today issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page search engines click sponsored links may channel customers highlighting limiting particular information produce certain actions customer often lack transparency existence purpose practices targeted marketing social media may cases become confusing customers may struggle distinguishing neutral opinions social media promotional material sponsored insurers digital marketing mobile based applications could also used respond real time individual circumstances including consumers' emotions individual' insecurity want heightened key life events context insurance intangible product intended mitigate personal fears type emotional framing may pose concern singapore example small specific changes singapore policypal start helps consumers organise understand purchase insurance policies digitally mobile app policypal graduated mas sandbox insurance broker registered mas australia example revelation australian media outlets may facebook disclosed major australian bank could exploit moods insecurities users potential benefit advertisers followed media reports facebook contributed published study proceedings national academy sciences usa showed via experiment users could make people positive negative process described emotional contagion - positive expressions reduced people produced fewer positive posts negative posts negative expressions reduced opposite pattern occurred france french r recommendation use social media business purposes remind professionals supervisory expectations explain rules apply use social media french autorite de controle prudentiel et de resolution acpr issued recommendation november applies banking insurance sectors october firstly advertising material issued social media fulfil applicable rules regarding information disclosed presentation information secondly professionals refrain unfair commercial practices using social media instance misleading opinions good bad issued professionals social media avoided well practice buying \"likes\" \"followers\" https www theguardian com technology may facebook advertising data insecure teens see http www pnas org content full https www mckinsey com industries financial services insights insurtech threat inspires issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page moreover according acpr recommendation professionals set procedures disclosure content social media promotion \"add \" insurance products sales another product supervisors' radar use digital means market sell insurance products nevertheless facilitate practices common example offering travel insurance online sales process airline tickets example whilst could customer interest informed travel insurance options timing promotion end sales process customer already bought airline ticket way message delivered could result customers believing purchase add insurance product required primary purchase airline ticket completed creates risk passive purchases purchase cover needed united kingdom fca conducted market study general insurance add ons market study found add distribution method real impact consumer behaviour affects consumer decision making consumers often focus sale primary product leading many purchase add products need understand fca also found consumers poor awareness products bought - unaware owned add ons considered market study findings indicate consumers' ability make choices often hindered insufficient information available quality price add ons information presented late buying process following findings fca implemented two remedies address specific issues * ban opt selling * improved information provision add buyers robo advice types advice robo advice essentially financial advice automated practice distinction made following types advice * full robo advice robo adviser completely takes work traditional financial adviser \"customer journey\" fully digitalised advice fully automated human role develop maintain robo advice system prevent malfunctions algorithm face face contact * partial robo advice advice fully automated traditional adviser still available answer questions * hybrid advice robo adviser human beings interact example \"customer journey\" fully digitalised advice still provided human possibly face face * traditional face face advice technology used additional tool example show graphs animations information see french https acpr banque france fr fileadmin user upload acp publications registre officiel annexe reco 2013 r 01 pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page benefits opportunities robo advice robo advice potential improve accessibility consistency financial advice accessibility means financial advice easily accessible majority consumers includes continuous availability advice one' home may also reduce costs consumer furthermore consistency advice improved use technology new financial products become available product conditions change algorithm instantly take changes account programmed correctly using sufficient accurate data robo advice consistently quality robo adviser may helpful overcoming cognitive bias insufficient competence part human adviser another potential benefit robo advice customers may find disclosing pre exiting conditions mental health easier machine human adviser could encourage disclosures leading customers receiving appropriate coverage robo advice considered another form distribution addition internet telephone based sales potentially without providing advice customer robo advice also part enhanced consumer communications throughout life product traditional advice robo advice solid audit trail nature robo advice produce audit trail technology used rather requiring completed manually provided technology delivers audit trail reliable good quality robo advice could therefore make easier deliver traceability auditability advice advice given provider robo advice able provide insight data used algorithms used information presented customer would make robo advice traceable reproducible allowing customer subsequent adviser supervisor check advice established robo advice netherlands netherlands robo advice available couple years different types financial products however last year robo advice available non complex products car insurance since robo advice also developed complex financial products disability insurance one challenges according developers making sure customers completely understand meant specific questions posed robo advisor since human advisor present answer questions potential risks according icp advice provided consumers take account customer' disclosed circumstances advice communicated clear accurate manner comprehensible customer advice provided communicated customer written format paper durable accessible medium record kept \"client file\" however may specific issues need addressed safeguard fair treatment customers use robo advice robo advice cannot solve every limitation traditional face face advice robo advice example overcome problems flowing guidance issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page limited selection available products overcome problems caused complexity products advice fully automated customer might opportunity ask questions unless programmed robo chat risk misunderstanding therefore present robo advice face face advice lack interaction humans might also lead reduced detection contradicting answers customers human adviser recognise customer doubt robo adviser may able however robo advice tools could programmed purpose - example algorithm could detect customer continuously clicks back forth pages prompt pop asking customer whether needs additional help explanations detecting doubt however one challenging aspects robo advice conventional advice fully automated advice process customer responsible subsequent decisions however case face face advice adviser discuss customer' hesitation follow advice customer possibly remove doubts concerns fully automated concept cannot therefore may merit limiting possible deviations advice fully automated process could prevent consumers making suboptimal choices exchange lower insurance premium incorrectly programmed algorithm far reaching consequences therefore important algorithm carefully developed tested used practice subsequently subject adequate maintenance design algorithm robo advice needs output treats customer fairly faulty algorithm defective ai tool lead inconsistent advice consistently bad improper advice tools data rely also potentially reinforce perpetuate existing biases price comparison websites pcw price comparison websites also known digital comparison tools insurance websites present multiple insurance products providers insurance thereby enabling principle consumers compare select products array insurers intermediaries selecting product purchase website could direct customer website insurer intermediary complete transaction pcws currently well established many jurisdictions many products services electricity utility air tickets also key distribution channel insurance markets whilst various methods remuneration exist pcws remunerated insurer intermediary successful transaction usually via fixed amount per policy pcw usually customer relationship significant difference types intermediation supervision pcws varies across jurisdictions depending activities performed pcw' business model may considered intermediaries jurisdictions pcws need comply insurance intermediary requirements issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page benefits opportunities bringing together product price information single place potential benefit pcws empower consumer quickly compare assess select among products available insurance market potential accomplished - reliable information objectively presented related educational tools - pcws promote competitive markets empowering consumers pcws facilitate consumer shopping accessible time anywhere although insurers offer websites products similar access another benefit pcws consumers ability enter personal information receive personally relevant products prices multiple vendors - clear advantage entering personal information every time shopping across different insurers websites respect pcws promote competition well reduce marketing underwrting costs could result lower insurance premiums potential risks risks - consumers self direct inform without assistance - common digital sales via website insurer intermediary however number particular risks example risk pcws non disclosed conflicts interest due compensation arrangements ownership structures particular providers shown web site conflicts interest cause website present products guide consumers products interest website interest consumer pcws subject specific disclosure requirements may lack transparency lack transparency may relate example potential conflicts interest compensation arrangements providers true identity pcw owner operator providers could affect adversely ability consumers make informed decisions consumers generally aware number suppliers consulted given product comparision criteria used establish recommendation pcw permitted make recommendations consumers may believe pcw providing objective complete information fact pcw providing limited biased information channels consumer specific product another major risk consumers focus price select product result adequately covered bought product meet needs also risk consumers buy unsuitable products based results provided pcw believed receiving form advice results shown met cover needs specific risks identified use pcws create harm consumers including unreliable performance disorderly failure example caused technology data failure due volume transactions generated relative small number pcws issue one pcw failure algorithm present correct results display inaccurate information product far reaching impact markets pcws play big role even create systemic issue across specific product line could expected one pcw fails others pick market segment however markets concentration certain segments product lines may create harm consumers lack availability pcws issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page netherlands afm issued press release quality pcws main findings * overall services pcws found interest customer based research five main pcws netherlands usually comparisons ranked price quality based preferences customer * signals overall comparison based payments insurers however often insurer would end top consumer able close product via pcws - turn possible commissions paid * main points improvement provision information way top constructed inclusion one discounts premium default preferences afm issued another press release services pcws * main finding pcws sometimes provide financial advice advertising services execution customer onboarding however suitable financial advice pcws based execution therefore contains limited set questions consumer however might get impression financial advice given * example pcw giving advice presenting \" top best suitable mortgages \" qualify financial advice would compliant advice rules adequate customer onboarding * q afm explained services pcws would qualify financial advice market players move execution towards robo advice next couple years disclosure informed decision making standard requires insurers intermediaries provide timely clear adequate pre contractual contractual information customers product disclosure key requirement needs adapt new digital channels habits benefits opportunities one advantages online services providers use visual information disclose features product example course premium time presented manner easily understandable easily adjustable customer enters new information example using graphs goes product features providers experiment best way disclose information customers maximise intelligibility thereof chatbots may also assist customer takes long scroll past certain section moves quickly across material term policy could indicate customer looking additional information explanation either bot adviser depending complexity policy computer programme designed simulate conversation human users especially internet issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page technologies may utilise customer data introduce relevant disclosures based information obtained customer different data sources examples \"virtual cognitive customer service representatives\" chatbots include uk based spixii \"speaks\" six languages flamingo' \"rosie\" australia \"learns business\" order respond customers united states us based insurer lemonade used two types artificial intelligence \"cognitive\" systems interface customers one called \"maya\" signs customers via mobile devices called \"jim\" finalises claims without assistance insurify another example uses evia \"expert virtual insurance agent\" uses natural language image recognition collect auto insurance quotes customers also engage evia comes clarification terms \"comprehension testing\" technology may assist obtaining certainty disclosed information adequate consequences thereof properly conveyed insured technology particularly machine learning chatbots used enabler customer comprehension online filter quick test questions may also assist gauging customer' understanding may need educate online users dedicate sufficient time adequate understanding contents agreement means presentation instance dedicated popup windows play important role ensuring proper understanding information customers obtaining explicit consents appropriate addition gamified product sales information - whereby information disclosed part game challenge - keep consumer interested engaged critical information might otherwise overlooked seen understood retained consumer potential risks time efficiencies instant gratification associated digitised transacting mean customers expect relatively quick transactional experience particularly instances smartphone applications used poses significant challenges insurers maintaining balance convenient seamless contracting versus risk inadequate disclosure material policy terms conditions netherlands netherlands supervision intelligibility products part product approval process also online services customers often obliged read information confirm information understood nevertheless remains risk customers fully understand details product providing confirmation supervisor therefore encourages parties write product conditions way complete easy read understand possible https www digitalpulse pwc com au insurtech make love insurer issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page contrast face face interaction digital interaction make difficult flag misunderstandings need explanations based non verbal communication digital context customers faced plethora information different sources difficult identify reliable product disclosures manner appropriately presented differentiate product disclosure marketing addition digital tools used highlight information relevant equally used downplay information way creates risk consumers choosing inappropriate product non digital information disclosure consumers face risks information overload many sources means ascertain accuracy legitimacy information issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page supervisory issues digitalisation transforming insurance industry society supervisors presents \"moving target moving environment\" digitalisation changes way insurance products designed marketed distributed supervisors monitor new developments engage stakeholders within outside insurance industry protect consumers' interests includes non traditional stakeholders cloud service providers data vendors short new developments shift risks require new supervisory responses delivered adequate way key challenges described consumer outcomes supervisors digitalised world crucial understand incumbent insurers intermediaries well newer insurance market participants including insurtech start ups big techs behaving impact outcomes consumers digitalisation use data potential benefit consumers also create risks unfair treatment discrimination concerns access exclusion insurance services measuring assessing outcomes challenging supervisors consider monitoring behaviour outcomes examining information multiple sources australia september asic launched data strategy tag line \"connecting dots achieve better regulatory outcomes \" purpose describe asic' vision data objectives approach improving captures shares uses data germany bafin launched internal project learn business model technological start ups fintech appearance market drawing expertise areas banking insurance securities supervision objective project group observe latest developments fintech market review whether bafin needed adjust processes view new developments area digitalisation result project bafin established innovation hub innovation hub analyses evaluates upcoming technological solutions new business models based solutions additionally innovation hub coordinates network experts various areas responsibility within bafin rates innovative business models regard regulatory requirements experts banking insurance securities regulators represented network well licensing pursuit unauthorised business departments combination experience expertise ongoing oversight review licensing requirements allows rapid assessment innovative business models processes may unique one department http download asic gov au media asic data strategy published september pdf issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page quebec amf created fintech lab deepen amf' knowledge new business models underlying technologies explore current potential applications technologies explore amf use france french central bank launched lab open space discussions collaborative work lab links banque de france various initiators innovative projects - start ups fintechs institutional players universities - order experiment new concepts technologies connection activities institution lab working technologies blockchain madre project iot ia etc acpr developing new tool supervising business practices * database innovations insurance sector enables monitoring technological innovations new services well guarantees offered * web listening platform internal analysing tool capture online messages consumers concerning bad market practices march acpr launched task force tackle opportunities challenges raised ai financial sector task force tf composed banks insurance companies fintechs also includes authorities data protection authority primary goal tf consist issuing discussion paper end aiming summarising implications using ai technologies financial sector united kingdom fca launched innovate department committed encouraging innovation interests consumers innovate provides assistance firms using innovation improve consumer outcomes helps firms better understand fca' rules processes guidance innovate helps fca keep ahead developing trends potential harms market united states naic us state insurance supervisors launched three year strategic plan state ahead drive efforts resources attention meet ongoing challenges including rapidly evolving marketplace fuelled seismic shifts consumer behaviour huge technological advances part goal ensure consumer protection keeps pace changes marketplace one objective optimise use market data regulatory processes enhance consumer protections including * rebuilding naic market conduct annual statement mcas application well applications utilising mcas data cloud based solution * implementing business intelligence tool self service capabilities * creating enterprise market data strategy analytics data warehouse * rebuilding naic consumer information source cis application additionally us state insurance regulators trying gain good understanding new innovative insurance products services including manner impact consumers stakeholders insurance marketplace accordingly naic issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page established innovation technology task force help insurance regulators stay informed key developments including new products services start companies well established insurance industry players task force naic big data working group charged part assessing data needs required tools state insurance regulators appropriately monitor marketplace evaluate underwriting rating claims marketing practices assessment includes gaining better understanding currently available data tools well recommendations additional data tools appropriate based assessment working group propose means collect house analyse needed data need balance innovation conduct concerns digitalisation innovation enormous potential help insurers intermediaries build cultures compliance identify potential consumer harms improve outcomes consumers however could also pose significant risks could lead consumer harms properly managed could include technological exclusion discrimination accessibility affordability issues one key challenges supervisors consider balanced approach facilitate innovations insurers maintaining level consumer protection stipulated laws regulations australia asic' innovation hub drives much australian conduct regulator' support digitalisation engagement fintech insurtech companies innovation hub asic provides informal assistance insurtechs regulatory obligations overarching regulatory framework appropriate options relating asic' exemption powers germany bafin' innovation hub serves - besides responsibilities - communication platform incumbents start ups one main aims gather spread knowledge example special contact form bafin' webpage company founders fintechs submit preliminary inquiries concrete questions eg business models term contact form may seem bit old fashioned contributes efficiency communication serves quickly determine responsible section respective business model body within bafin - public authority employees decisive factor france acpr launched fintech innovation unit point entry financial start ups licensing process unit evaluates opportunities well risks related innovations financial industry gives recommendations adjustments need made current regulation supervision practices acpr coordinating actions securities markets authority smf launched fintech forum leading dialogue fintech professionals regarding regulation supervision https www naic org state ahead htm many issues addressed application paper use digital technology inclusive insurance november issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page quebec amf created external advisory committee technological innovation advisory committee mandate assist amf identifying analysing trends issues help ensure balance consumer protection market efficiency switzerland finma working challenges presented fintech regarding authorisation supervision regulation innovative trends ideas require solid framework within operate clients financial system whole need protection shift direction finma regards innovation key competitiveness switzerland' financial centre adopts essentially neutral approach certain business models technologies therefore reviewed whether specific provisions ordinances circulars disadvantaged technologies concluded obstacles existed increasing number financial intermediaries interact clients via internet mobile devices finma therefore enhancing regulatory framework facilitate client onboarding via digital channels new circular anti money laundering due diligence requirements explained context digitalisation financial services need technology neutral regulation particularly respect video identification circular came force march launching operations fintech companies must establish whether subject anti money laundering authorisation requirements general authorisation insurance required risks dangers clients insured services rendered voluntarily without contractual obligation authorisation might required united kingdom fca launched regulatory sandbox regulatory sandbox space firms apply test innovative propositions live environment oversight input fca firms testing sandbox required meet relevant regulatory requirements bespoke safeguards mitigants built testing plan sandbox aims enable firms get market test propositions faster speed reduced cost also gives fca understanding opportunities potential harms innovation create market almost firms supported across first four cohorts fca' sandbox including significant number insurance sector supervisory skills tools supervisors become \"data driven\" \"digital intelligence led\" supervising market conduct digital world requires different skill sets addition lawyers economists actuaries mathematicians data scientists interdisciplinary supervisory teams vital digitalised world supervisors technologically numerically literate understand risks associated data supervisors need new skills identify monitor assess new applications technologies understanding market structures activities new participants understanding consumer outcomes arrange knowledge skill wihin staff - efficient - use third party providers lawyers economists actuaries mathematicians data scientists need work together supervise insurance issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page markets respect supervisory authorities need reconsider qualifications skill sets need become \"fit future\" continue compete talent within industry germany specialists part supervising teams order better prepare challenges posed inter alia cyber risks bafin set separate organisational unit supervision financial sector january quebec amf created dedicated internal working group experts fintech involving employees working cross functional teams france alongside creation dedicated teams hubs central bank appointed chief digital officer cdo charge digital transformation institution also chairing innovation lab united states part naic us state insurance supervisors state ahead strategic plan theme consumer protection education recognises need stay abreast new developments area innovation emerging technology need become engaged areas insurtech regtech opportunities explored objective include providing forums programmes state insurance regulator education discussion regarding changes insurance marketplace including innovation technology convening autonomous vehicle insurance forum stakeholders discuss insurance regulatory issues related autonomous vehicles considering creation cybersecurity insurance institute additionally naic innovation technology task force charged part discussing innovation technology developments insurance sector including collection use data insurers state insurance regulators - well new products services distribution platforms - order educate state insurance regulators developments impact consumer protection privacy insurer producer oversight marketplace dynamics state based insurance regulatory framework particular supervisors need skills understand digitalisation result consumer harm example supervisors may encounter challenges supervising self learning algorithms underlie automated decisions made digitalised world problematic consumer protection perspective also risk management perspective issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page robo advice robo advice provides useful case study many issues pertinent supervisors result many supervisors recently published guidelines approaching robo advice including * germany https www bafin de en aufsicht fintech anlageberatung anlageberatung de en html * australia https asic gov au regulatory resources find document regulatory guides rg providing digital financial product advice retail clients * netherlands https www afm nl profmedia files onderwerpen roboadvies sav view robo advice pdf * united kingdom uk' fca launched advice unit may provide regulatory feedback firms developing automated advice guidance models across range sectors including insurance feedback focuses helping firm understand regulatory implications model fca also publishes tools resources firms developing automated advice guidance propositions based experiences individual firms developing guidelines supervisors needed consider quality advice provided measured verified supervisors directly supervise algorithm supervisors supervise monitor outputs - ie advice supervisors require insurers intermediaries self audit provide annual assurances advice robo advisers providing appropriate require insurers intermediaries engage external experts conduct audits depending address questions supervisors may need establish dedicated teams address technical matters involving specialists required knowledge non digital advice supervisors need mandate insurers intermediaries adopt appropriate document management strategies includes retaining versions algorithm robo adviser needs save algorithm used data information advice provided customer supervisory authorities consider embrace new technologies help carry supervision also referred suptech solutions germany first half bafin published report big data ai together partnerschaft deutschland fraunhofer institute intelligent analysis information systems boston consulting group https www bafin de shareddocs downloads en dl bdai studie en html advancements key technologies increasing data availability decreasing entry barriers usage big data ai solutions incumbent institutions new suptech use technological innovations fintech supervisory authorities regtech use technological innovations fintech compliance purposes reporting regulated financial institutions issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page market players able structure work processes efficiently develop new business models supervisors however facing new challenges use big data ai impacts financial markets aim report gain better understanding challenges report highlights implications technology driven market developments various regulatory supervisory perspectives six main observations implications summarised follows financial data innovation race already underway systematic dependencies big data ai companies could develop outside regulatory framework financial system black box modelling spreading black box modelling must never stand way proper business organisation big data ai accelerate automation process responsibility always remain senior management \"transparent customer\" phrase age big data ai big data ai must used customers consumer confidence catalyst big data ai innovations anchor stability integrity financial system speed innovation increasingly tests limits regulatory framework' adaptability level playing field age big data ai means speed innovation met agile oversight technology neutral regulation united kingdom fca worked ing commonwealth bank australia pinsent masons test possibilities using natural processing language ai technologies interpret markets financial instruments directive ii regulations automatically build manage compliance programme different entities supervisors also need deal non incumbent firms different entity structures approaches consumer related risk incumbents supervisors need engage new entrants insurance financial services may experience knowledge financial services regulation new entrants may different entity structures approaches consumer related risk incumbents historically monitored supervisors unlike incumbents general compliance awareness risk culture ability comply regulatory requirements may differ significantly non traditional firms may require proactive strategy outreach engagement new entrants inform \"educate\" relevant supervisory matters proper compliance attitude well capitalised \"bigtech\" platform businesses may move distribution markets small nimble lowly capitalised insurtech start ups may also look enter insurance markets supervisors need understand different challenges posed new entities may lack knowledge regulatory reuqirements may experience engaging supervisors supervisory cooperation cooperation financial supervisory authorities crucial digitalised world digital innovations risks stop border jurisdictional supervisors coordinate authorities jurisdictions meet challenges supervisors proactively work together across jurisdictional subject matter boundaries identify emerging trends develop implement solutions includes collaboration market conduct regulators prudential privacy competition issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page regulators given implications digital technology consumer outcomes significant number solutions focused marketing sales distribution end value chain regular going interaction supervisors crucial iais continues support facilitate discussions including fintech forum france acpr signed agreement authority supervising security information systems anssi authority responsible responding threats targeting public authorities private sector particular vital information systems coordinates government action area defence systems regulatory arbitrage supervisors also need cognisant emergence product types effect insurance structured way falls outside legal definition regulated insurance product would enable product providers avoid regulatory requirements consumers means would able access compensation policyholder protection schemes product manufacturers unable meet claim costs arbitrage take two forms * jurisdictional product falls outside jurisdictional power regulator despite available customers jurisdiction * definitional product legal characteristics insurance product although effects one addition digitalisation new technologies may increase potential regulatory arbitrage created products bundle insurance non insurance products services choice consumer limited purchasing purchasing package information security storage protection third party use customer data insights gathered also issue cyber risks data protection questions become vital importance steady rise digitalisation working together competent authorities issues utmost importance world wide wannacry \"petya\" ransomware outbreaks highlight cyber risks rise data customer specific behavioural insights insurers hold would high value context particularly relevant given order drive efficiencies reduce costs many insurers store data insights cloud share data insights third parties many shore cloud services probably focus hackers cases much better protected premise installations run incumbents hand cloud service successfully attacked outcomes worse even systemic customers need know data insights specific derived data secure corrupted tainted clearly access data challenge privacy regulators also financial services regulators issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page eu eu reformed data protection rules simplify use big data businesses set high standards data protection may entry application general data protection regulation one set data protection rules companies operating eu wherever based also eu aims strengthen cybersecurity regulation cope growing threat cyberattacks take advantage opportunities offered new digital era october european council called common approach cybersecurity line european commission september reform package united states naic big data working group charged part reviewing current regulatory frameworks used oversee insurers use consumer non insurance data appropriate recommend modifications model laws regulations regarding marketing rating underwriting claims regulation data vendors brokers regulatory reporting requirements consumer disclosure requirements use cloud computing increasing insurance industry need supervisors enhance regulatory frameworks supervisory practices effectively capture risks supervisors typically apply existing frameworks outsourcing governance risk management internal control information security insurers' cloud computing activities others issued cloud specific recommendations expectations meanwhile authorities put place formal informal communication arrangements reinforcing supervisory processes better monitor assess risks cloud computing cloud services give rise specific questions example * country data stored verified * access data * key controls provided * danger risk concentration many cloud service providers * possible conflict interest data different insurers stored server information commissioner office big data artificial intelligence machine learning data protection version regulation eu european parliament council april protection natural persons regard processing personal data free movement data repealing directive ec general data protection regulation oj l p 28european commission https ec europa eu info law law topic data protection reform general data protection regulation gdpr govern en references http www consilium europa eu en policies fsi insights emerging prudential approaches outsourcing cloud case insurance companies forthcoming issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page * possible conflict interest cloud service provider decides involve insurance business also vital supervisors direct immediate access data stored cloud data stored insurer intermediaries servers germany bafin published journal article \"cloud computing compliance supervisory requirements regarding rights information audit ability monitor\" regard outsourcing cloud service providers bafin also holds discussions respective cloud service providers insurers content outsourcing contracts circular clarify supervisory requirements insurers vait published https www bafin de shareddocs downloads de rundschreiben dl rs 1810 vait va html https www bafin de shareddocs veroeffentlichungen en fachartikel fa bj 1804 cloud comput ing en html issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page conclusion recommendations digital innovations change potentially improve customer experience reduce insurers' intermediaries' operating costs however digitalisation may impact consumer protection extent customers treated fairly design underwriting pricing products marketing distribution claims processing ongoing management customers therefore respect product design marketing sales due attention needs given achieving fair customer outcomes terms products meet consumers' needs design use algorithms use customer data adjust digital age foster innovation supervisors consider ensure new innovation come expense protections policyholders integrity insurance sector whole one key challenges supervisors consider balanced approach facilitate innovations insurers maintaining level consumer protection stipulated laws regulations promote innovation consistent consumer benefit protection recommended supervisors develop thorough understanding innovations work applied ensure proper assessment new products business models design functioning architecture infrastructures processes catered insurers' risk management framework supervisors may also need develop new tools skills supervision digitalised insurers enhancing cooperation financial authorities safeguarding supervisory perimeter prevent regulatory arbitrage enhancing information security supported material developed iais supervisors consider establishing guidance appropriate responsible use new technologies safeguard fair treatment customers - example terms use ai robo advice mechanisms - promote advice services suitable affordable customers issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page annex digital technologies alternative business models affecting insurance business general overview significant innovations within insurance industry mentioned iais report \"fintech innovations insurance industry\" digital devices internet changes addressed paper facilitated proliferation digital devices devices contain computer microcontroller smartphones tablets wearables devices connected internet global network computers using standardised communication protocols internet things iot iot involves internetworking physical devices vehicles buildings items also referred connected devices smart devices embedded electronics software sensors actuators network connectivity enable objects collect exchange data telematics telemetry context iot telematics involves telecommunications sensors computer science allow sending receiving storing processing data via telecommunication devices affecting control remote objects telemetry involves transmission measurements location origin location computing consumption especially without affecting control remote objects context insurance main applications connected cars advanced driver assistance systems adas health monitoring home monitoring big data data analytics insurance market big data data analytics could used various processes product offerings risk selection pricing cross selling claims prediction fraud detection example offer customised products comparators robo advisers online services provide automated algorithm based product comparison advice without human intervention february https www iaisweb org page news papers reports file report fintech developments insurance industry term iot defined global infrastructure information society enabling advanced services interconnecting physical virtual things based existing evolving interoperable information communication technologies source http www itu int itu recommendations rec aspx rec big data term used storage data different sources large volume speed iais fintech developments insurance industry february data analytics process inspecting cleaning transforming modelling data goal discovering useful information suggesting conclusions supporting decision making iais fintech developments insurance industry february issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page machine learning ml artificial intelligence ai use ml ai enables several insurance industry processes use data real time especially use events prediction eg vehicles thefts health problems weather events vast scope ai better pricing risks also fraud prevention automated underwriting claims handling preventive counselling distributed ledger technology dlt distributed ledger essentially asset database shared across network multiple sites geographies institutions security accuracy assets stored ledger maintained cryptographically use \"keys\" signatures control within shared ledger * blockchain type decentralised distributed ledger comprised unchangeable digitally recorded data packages called \"blocks\" stored linear chain * smart contracts novelty dlt database - also set rules transaction business logic tied transaction smart contract term used describe computer programme code capable facilitating executing enforcing negotiation performance agreement using dlt platform business models peer peer usage based demand insurance emerging digital technologies facilitating alternative business models * platform business models platform business model creates value facilitating exchanges two independent groups usually consumers producers make exchanges happen platforms harness create large rapidly scalable networks users resources platforms means production - instead create means connection google apple facebook amazon uber alibaba examples platform business models * peer peer business model allows insured pool capital self organise self administer insurance although innovative concept emerging technologies like dlt offer substantial benefits implementing model broader scale * usage based insurance new business model introduced insurers intermediaries closely aligns behaviours premium rates example auto insurance usage based insurance products customer pays actual distance driven driver behaviours also impact price * demand insurance new business model specialises covering risks faced certain moment https www applicoinc com blog platform business model accessed january issues paper increasing digitalisation insurance potential impact consumer outcomes approved iais executive committee november page","text_word_count":15393,"text_unique_words":3684,"_deepnote_index_column":3},{"Name":"./sampledocs/sigma1_2020_en.pdf","Type":"pdf","Text":"                             No 1 /2020\nData-driven insurance:       01 Executive summary\n                             02\t\u0007The battle for customer\nready for the next frontier?     touchpoints\n                             15 The digital insurer\n                             23\t\u0007Marketing, distribution\n                                 and servicing\n                             28\t\u0007Product development\n                                 and underwriting\n                             38 Claims management\n                             43 Outlook\n                             47 Conclusion\nExecutive summary\nDigital technology will direct change    The rapid spread of internet-enabled devices and universal connectivity has\nacross the insurance value chain.        changed consumer behaviours and expectations across all industries, particularly\n                                         among the younger generations. The digital age has also brought an explosion of\n                                         heterogeneous data from different sources and platforms, which providers of risk\n                                         protection solutions can use to broaden the reach and boundaries of insurability. As\n                                         their value chain becomes more digitally connected, insurers will be able to better\n                                         understand customer segments and partners and adapt in near real-time.\nInsurers will become hyper-aware of      In the short-term the digital insurance consumers will likely be young, educated and\ncustomer needs and preferences...        with higher levels of income. Over time, innovative digital cover options for all\n                                         consumers will become increasingly available, making income and education less\n                                         relevant factors in purchasing decisions. With advanced analytics capabilities, the\n                                         insurer of the future will be very aware of customer needs and preferences, and\n                                         provide personalised and real-time service, with flexible product offerings. Artificial\n                                         intelligence (AI) will be used to interact and build understanding of the customer,\n                                         and servicing will be through (personal) virtual assistance, 24/7.\n...and to meet customer expectations,    Insurers will be able to move away from a product-focused sales approach to a one\nthey will need to adapt to provide       closely tied to the broader needs across the life time of a customer, with greater\ncoverage across life-cycle stages.       focus on the human experience. New generations of systems will deliver\n                                         unprecedented levels of proximity and influence on customers, and insurers will go\n                                         beyond mere channel management to optimising interaction across a diverse range\n                                         of customer touchpoints. This will have implications for business models, how\n                                         insurers interact with their customers, and the nature of the services they provide.\nDigitalisation will also help create new Over time, Big Data and sophisticated models will allow risk pricing at increasingly\nunderwriting and portfolio risk          granular level. Emergence of new risks will create new underwriting and portfolio\nmanagement techniques.                   risk management techniques. Insurers will create early-warning systems to gather\n                                         practical insights that prevent incidents to simplify and accelerate claims processing.\n                                         Data-enabled processes will minimise friction and streamline the customer\n                                         insurance journey, from request for coverage to claim. Digitalisation will thus help\n                                         improve the customer experience and also the efficacy of back office processes.\nAt the same time, using data from        Further, digitalisation will enable development of new data-driven business models\nmultiple providers could drive new       impacting the entire insurance value chain. Access to data and the capability to\nbusiness models...                       model risks will be key. True leverage will come from utilising other assets, such as\n                                         key data supplier partners, entry points to ecosystems, and the know how to\n                                         generate customer insights. Insurers will need to decide whether to be suppliers of\n                                         coverage, or to collaborate with and/or own new areas of business operation.\n...and value propositions in insurance.  Technology could foster just ongoing incremental industry change by broadening\n                                         the scope and affordability of insurance. Alternatively, potential for more radical\n                                         transformation in the provision of risk protection services to households and\n                                         businesses is up for grabs if some typical hurdles to innovation can be overcome.\n                                                                                                       Swiss Re sigma No 1/2020 1\nThe battle for customer touchpoints\nDigital customers expect different levels of service. What is second nature to digital natives presents a new world of\nopportunity for incumbent insurers still operating paper systems. Combining digital solutions with advanced analytics\nwill deliver new insights, allowing insurers to provide deep, holistic engagement across customer lifetimes.\n                                                 Digital interaction: it’s life\nDigital technology has already                   Digital interaction has become a norm of daily life. Across the world, of the\ntransformed many industries…                     approximately 4 billion people connected online, more than 90% use mobile devices\n                                                 and spend several hours in the virtual world each day.1 New technologies and digital\n                                                 facilities are being developed for consumers, including, for example, location and\n                                                 other sensor-based services (for smart homes, cars and factories). Developments in\n                                                 cognitive systems and artificial intelligence (AI) are also creating innovation\n                                                 opportunities (see Figure 1), including for insurers.\n  Figure 1\n  New technologies impacting insurance\n   Impact on business\n   Revolutionary                                     ● Full life-cycle             ● Digital business                 ● Intelligent process\n                                                         API management               technology platforms                automation\n                                                                                   ● IoT platforms\n                                                                                   ● Digital experience\n                                                                                      platforms\n   High                                              ● Next generation             ● Conversational                   ● Digitally-engineered\n                                                         personal health              platforms                           underwriting\n                                                         records (PHR)             ● Behavioural analytics            ● Autonomous vehicles\n                                                                                   ● Electronic health\n                                                                                      Record data extraction\n                                                                                   ● Genomics and\n                                                                                      epigenetics\n   Medium              ● Insurance wallets           ● Digital advisors            ● Administration/\n                       ● Reward and loyalty          ● Advanced analytics             management SaaS\n                          platforms                      solutions\n                               0 to 2 years                   2 to 5 years                5 to 10 years                         10+ years\n                                                                                                                                  Time to impact\n  ● P&C related technologies            ● Sensing & analytics related technologies             ● Productivity related technologies\n  ● L&H related technologies            ● Customer interaction related technologies            ● Customer loyalty/wallet related technologies\n  Source: Swiss Re Institute, adapted from Gartner’s Hype Cycle for Digital and P&C Insurance, 2019\n                                                 1  Connected Commerce: Connectivity is Enabling Lifestyle Evolution, Nielsen,19 November 2018.\n2   Swiss Re sigma No 1/2020\n                                       New technologies are impacting engagement and insight generation\n… and will change how insurers         The rapid spread of internet-enabled wearable devices and ubiquitous connectivity\nacquire and service customers.         are enabling new ways of communication and information sharing. This has led to\n                                       exponential growth in the volume of digital data being generated automatically,\n                                       cheaply and non-intrusively. International Data Corp estimates that there will be 41.6\n                                       billion IoT devices around the world by 2025, each generating data (see Figure 2).2\n                                       New tools to analyse the data and extract useful insights are also proliferating, and\n                                       will change the way insurers interact with consumers.\n  Figure 2\n  Forecast growth of digital data       200\n                                                   CAGR (2019–25)\n                                                   Real-time data 39%\n                                        150        Non-real time data:24%\n                                        100\n                                         50\n                                           0\n                                             2010        2012        2014       2016        2018         2020      2022      2024\n                                                 Real time (zettabytes)             Non real time (zettabytes)\n                                       Source: IDC, Swiss Re Institute\nThe task facing insurers is to upgrade Consumer preferences and buying behaviours are also changing rapidly, and many\nthe online experience for their        industries have adopted more customer-centric business models. Insurers will have\ncustomers.                             to work hard to keep customers loyal. A recent survey found that while insurers\n                                       remain the most trusted source for new risk coverage solutions, consumers are\n                                       switching carriers more often than they used to, and are more open to new entrants,\n                                       including from InsurTech and Big Tech.3\n                                       2  The Growth in Connected IoT Devices Is Expected to Generate 79.4ZB of Data in 2025, International\n                                          Data Corporation, 18 June 2019.\n                                       3  Insurers: How to Lead in the New Era of Connectivity, Bain and Company, 26 September 2019.\n                                                                                                               Swiss Re sigma No 1/2020 3\nThe battle for customer touchpoints\n                                         Digital insights into life events, new insurance opportunities\nInsights from data analytics can boost   Figure 3 shows the cycles of customer acquisition, and also what is involved in\ncustomer retention...                    customer retention. Digitalisation can support these cycles by capturing insights\n                                         from the end-to-end consumer experience.\n    Figure 3\n    Customer acquisition and                        Prospect customer experience                       Existing customer experience\n    loyalty lifecycle                                    Finding “good” customers                      Maximising customer life time value\n                                                                 Advise                                              Advise\n                                                                                 Purchase         Purchase/\n                                           Evaluation                                                use                               Evaluation\n                                                                      Decision                                Decision\n                                                                   buy/not buy                                ie, upselling, service\n                                                                                                              use, etc\n                                                                                   N                    ive\n                                                                                  ex ega             sit ce\n                                         Conside-                      Trigger      pe tiv\n                                                                                      rie e       Po rien        Trigger                     Conside-\n                                                                                         nc      e pe\n                                           ration               ie, life event              e   ex\n                                                                                                ienc             ie, service use             ration\n                                                                                          Ba\n                                                                                         expde\n                                                                                               r\n                                                                                        Exit\n                                                             Research                                                 Research\n                                                        Customer acquisition cycle                     Customer servicing cycle\n                                                             Evaluating other offers –                 Continuous engagement –\n                                                        competitors have to put very                   Customer pays very little attention\n                                                         little effort to gain attention               to competitors offerings. Improves\n                                                                                                       customer life time value\n                                         Source: Swiss Re Institute\n... by triggering actions that enable    Sources of digital data can also provide information on what has changed in a\nnew sales or servicing opportunities.    customer’s life (eg, family, location or job change, see Figure 4). This helps insurers\n                                         better understand life events, knowledge they can use to develop personalised\n                                         marketing strategies and guidance on next best actions (both predictive and\n                                         prescriptive) for individual customers. This can entail cross-selling of other risk\n                                         mitigation and value-added services, in addition to traditional insurance cover.\n4    Swiss Re sigma No 1/2020\nFigure 4\nAccess to life events enabled by digital interactions\n                                                                                   Data                4) Predictive: The aim of\n                                                                                 sources               predictive analytics is to detect\n                                                                                                       problems before they occur.\n                                                                                                       5) Prescriptive: Prescriptive\n                                                                                                       analytics takes predictive analytics\n                                                                                                       one step further by offering specific\n                                                                                                       and actionable next steps on how\n                                                                      3) Identify and communicate      to solve the issues brought up in\n                                                                           the next best action        the predictive data analysis.1\n                                                                                                       6) Customer online interaction\n                                                2) Social media\n                                                     interaction\n                                                                                                       Insurance event trigger\n                                                                                                       Insurance Prevention Value add\n                                                                                               4)\n Life event trigger\n                                Love                        2.Kid                      1) Pregnancy\n                                Marriage                    Kids leaving\n         Family-                Pregnancy                   Separation                 6)\n     related events             1. Kid                      Death\n                                                                                                    5)\n                                First apartment             Emigration\n                                Same apartment              House purchase\n              Life              Relocation                  House sale\n       related events\n                                Education                   Self-employment\n                                Re-skilling                 Workload\n                                (post graduate)             eduction\n     Education/job-\n                                Job change                  Retirement\n       related events\n                                Hobbies/interest            Food/nutrition\n                                Travel                      Culture\n                                Media &                     Commerce\n          Free time-\n                                entertainment\n        related events\n                               On-demand                    Car/bike/\n                               mobility                     purchase/sale\n                               Daily commute                Business travel\n         Mobility-             Ride hailing                 On-demand\n      related events                                        micromobility\n1\n  Source: https://www.proponent.com/predictive-analytics-vs-prescriptive-analytics/\nSource: Swiss Re Institute\n                                                                                                                   Swiss Re sigma No 1/2020 5\nThe battle for customer touchpoints\nNew actors, not insurers, own many     However, these digital signals are not embedded in the environments that insurers\ntouchpoints.                           naturally access, not least because insurance is a low-touch business. For example, it\n                                       has been estimated that buyers of insurance in the US average just 2.7 information\n                                       interactions with their provider per year.4 Data on consumer actions is more readily\n                                       available from other agencies. For instance, data on mobility behaviour is owned by\n                                       mobility companies, not insurers. And operators of smart-home digital platforms\n                                       know more about insurers’ customers properties than insurers do. The more urban\n                                       the area of operation, the greater the likelihood that insights are generated by non-\n                                       insurers. What insurers need to do is embed themselves as providers of risk\n                                       protection within the broader ecosystems of digital touch points with consumers.\n                                       Optimising interaction across multiple touchpoints\nInsurers need to access both owned     To achieve this, insurers need to optimise their interaction with customers across\nand not-owned touchpoints.             multiple providers and lifestyle areas like health, education, mobility and leisure. In a\n                                       single journey, customers switch touchpoints to fulfil various information needs.\n                                       Insurers need to access insights across the different types of touchpoints, both\n                                       owned and not owned (paid, earned and social, see arrows in Figure 5). Top-\n                                       performing insurers will operate across touchpoints as a unified customer-facing\n                                       brand, with consistent messaging and experience to provoke recall.\n  Figure 5\n  Dynamic and personalised protection enabled by digital-touchpoint insights and digitally-augmented assistants\n                                                                                                          Sensors and data sources\n                                                      Customer journey                                    Context & relevance\n                                                                                                          Digital augmented\n                                                     Customer lifestyle                                   interface\n           Health                                                                                         Lifestyle clusters\n                                                                                                          Digital augmented\n           Education/working\n                                                                                                          assistant\n           Living                                                                                         Dynamic, personalised,\n                                                                                                          and augmented\n                                                                                                          protection\n           Free time\n                                                                                                          Consumers touchpoints:\n                                                                                                          Owned\n           Mobility\n                                                                                                          Consumers touchpoints:\n                                                  Risk, asset & liability                                 Not owned (paid,\n                                                     management                                           earned, social)\n  Source: Swiss Re Institute\n                                       4  Customers Know What They Want. Are Insurers Listening? Bain and Company, 10 October 2018.\n6  Swiss Re sigma No 1/2020\n                                                    ̤̤ Owned touchpoints: The insurer invests and controls these consumer\n                                                       touchpoints. They include agents, websites, apps. The goal is long-term\n                                                       relationship building; the benefits are cost efficiency and control.\n                                                    ̤̤ Paid touchpoints: The insurer pays for the digital touchpoint to reach the\n                                                       customer, such as through paid searches and sponsorships. Paid touchpoints can\n                                                       include various channels like bancassurance and agents.\n                                                    ̤̤ Earned touchpoints: Here the customer becomes the touchpoint. This is when\n                                                       satisfied customers become social media influencers, and spread awareness.\n                                                    ̤̤ Social touchpoints: An insurer interacts via third party channels but uses its own\n                                                       profile, such as on Facebook or Twitter. Social media is forecast to be the fastest-\n                                                       growing digital advertising channel globally over the next five years.5\n                                                    The changing nature of insurance touchpoints\n                                                    With the growth of multiple touchpoints, industry susceptibility to disruption is rising\n                                                    (see Figure 6). We expect insurer interactions will – necessarily – increasingly begin\n                                                    to cross conventional boundaries. Big Tech and digital ecosystems (eg, Grab, GoJek,\n                                                    ecommerce marketplaces) are changing the touchpoints in the insurance context.\n                                                    For example, in the UK four of the leading insurance providers are retailers selling\n                                                    “white-label” insurance.6 Some insurers will orchestrate digital and physical\n                                                    ecosystems by combining, in one offer, services that are normally delivered by\n                                                    different providers.\nFigure 6\nSpeed of disruption across sectors: insurance to be more disrupted and volatile\nCurrent level of disruption vs. susceptibility to future disruption                                                           Current level of disruption\n18 industry sectors                                                                                                                              0–1 scale\n                                Viability                                                                          Volatility\n                                                                                                                                      More disrupted\n                                      Comms & media\n                                                                                     Infrastructure & transposrtation\n                                                                           Retail\n                                                                                                 services\n                                                           High-tech               CG & S                        Insurance\n             Software & platform\n                                                                                                   Banking\n                                        Life sciences                  Travel\n                                                                   Automotive\n                                                                   Energy Utilities\n                                                                                                           Capital markets\n                                                            Health          Chemicals                     Natural resources\n                                                                                                  Industrial equipment & machinery\n                                                                                                                                      Less disrupted\n                               Durability                                                                       Vulnerability\n      Less susceptible                                   Susceptibility to future disruption                                     More susceptible\n                                                                        0–1 scale\n                                     Less susceptible yet more disrupted                             Less susceptible and less disrupted\nSource: V. Savic and B. Moussavi, Good things come to those who don’t wait, Accenture, 2019, Swiss Re Institute\n                                                    5  Social Will Be The Fastest-Growing Digital Advertising Channel Globally, Forrester, 15 August 2019.\n                                                    6  Bain and Company, 10 October 2018, op. cit.\n                                                                                                                              Swiss Re sigma No 1/2020 7\nThe battle for customer touchpoints\nFront-runners are set to outperform.             An early-mover strategy into new technology could prove expensive in the medium\n                                                 term, but deliver strong returns over the longer run. A BCG MIT survey found that\n                                                 65% of executives across industries do not yet see value from AI investments made\n                                                 in recent years.7 Activities like collecting and curating data, building a knowledge\n                                                 base that is specific to the company, training systems, getting employees to\n                                                 augment them and developing strong governance, takes years. These are rarely out-\n                                                 of-the-box solutions that can be rapidly implemented. Figure 7 demonstrates one\n                                                 future pay-back scenario: companies that invest early will burn cash for a few years,\n                                                 but then begin to rapidly see benefits of accumulated learning and investments.\n                                                 Those players adopting the “fast follower” strategy that has worked with other\n                                                 technologies, could find themselves long-term laggards.8\n  Figure 7\n  Relative changes in cash flow by AI adoption cohort\n  140% % change per cohort, cumulative\n  120%                                                                                             Front-runners\n  100%                                                                                             (absorb within 5–7 years)\n    80%\n    60%\n    40%\n    20%                                                                                            Followers\n     0%                                                                                            (absorb by 2030)\n  –20%\n                                                                                                   Laggards\n  –40%                                                                                             (do not absorb by 2030)\n       2017               2020                            2025                            2030\n  Source: Jacques Bughin et al., Notes from the AI frontier: Modeling the impact of AI on the world economy, McKinsey Global Institute,\n  2018, and Swiss Re Institute\n                                                 7   MIT Sloan Management Review and Boston Consulting Group surveyed ~2,500 executives, See\n                                                     Winning With AI: Pioneers Combine Strategy, Organizational Behavior, and Technology, MIT Sloan\n                                                     Management Review, October 2019.\n                                                 8   V. Mahidhar,T. H. Davenport, Why Companies That Wait to Adopt AI May Never Catch Up, Harvard\n                                                     Business Review, 6 December 2018.\n8  Swiss Re sigma No 1/2020\n                                                 The digital consumer – today and the future\nMillennials are the typical digital              Based on a sample from three key markets where digital insurance is making\ninsurance consumers of today.                    headway– the US, Sweden and China – today’s digital consumers are typically\n                                                 between 20 and 36 years old, affluent and educated.9,10,11,12,13 The digital generation\n                                                 (Generation Y (born between the early 1980s and late 1990s) and subsequent ones)\n                                                 expect rapid access to information, and clarity around the value proposition.\n                                                 Proactive servicing is key to maintaining loyalty and to maximise customer lifetime\n                                                 value among these clients.\nMost have above-average incomes                  Higher-income consumers can afford insurance. There is room for growth in lower-\nand education levels.                            income segments, but value propositions need to be adapted. In the sample markets,\n                                                 younger consumers with higher incomes and education tend to make more online\n                                                 purchases than those with low- and mid-level incomes, at least in China and the US.\n                                                 In China, 44% of consumers with incomes in excess of RMB 20 000 use online tools\n                                                 to buy health insurance (including medical expenditure and critical illness). Only 30%\n                                                 of low-income consumers (< RMB 5 000) do so (see Figure 8). Along similar lines,\n                                                 45% of consumers with doctorate degrees buy health insurance digitally compared\n                                                 with 34% with technical/vocational training.14 The same can be seen in the US. For\n                                                 example, the median income of consumers buying insurance from Haven Life is USD\n                                                 80 000, and more than 80% have (at least) a college degree.15\n  Figure 8\n  Digital purchases of health insurance products across income and education levels in China, 2018\n  Income level                                                               Education level\n  50%      % of consumers using digital purchase methods                     50%         % of consumers using digital purchase methods\n                                                              44%                                                                 44%             45%\n  40%                               40%                                      40%                             41%\n                                                                                           34%\n  30%        30%                                                             30%\n  20%                                                                        20%\n   10%                                                                       10%\n    0%                                                                         0%\n        Low income              Mid income              High income                     Technical/       Bachelor/              Master         Doctorate/\n       (<RMB 5 000)         (RMB 5 001 – 20 000)      (>RMB 20 000)                     vocational       associate              degree        professional\n                                                                                         training          degree                                degree\n  Source: Swiss Re Institute\n                                                 9  At Avanza, a Swedish digital life insurer; most customers were between 20 to 30 years old. See\n                                                    Annual Report 2016 – Sweden’s most satisfied savings customers 7 years in a row”, Avanza, 2016.\n                                                 10 In China, the majority who signed up for Alipay’s digital critical illness product were born\n                                                    post- 1990. See Online Insurance in China, General Reinsurance AG, 2017.\n                                                 11 The majority of Haven Life customers are millennials (21 to 36). See Happy Birthday to us, Haven Life,\n                                                    16 May 2018.\n                                                 12 81% of customers at US insurer Lemonade are between 24 to 44. See S. Wininger, “Lemonade’s First\n                                                    Quarter in Market”, lemonade.com, 18 January 2017.\n                                                 13 The majority at ZhongAn, a digital P&C insurer in China, are aged 18 to 39. What can insurers learn\n                                                    from insurtech giant Zhong An, Accenture, 15 January 2019.\n                                                 14 Chinese non-life personal lines: consumer perspectives, Swiss Re Institute, 2019.\n                                                 15 Haven Life, 16 May 2018, op. cit.\n                                                                                                                                 Swiss Re sigma No 1/2020 9\nThe battle for customer touchpoints\n                                                  Why do differences in online buying behaviour exist across markets?\nMany use the internet to research                 The internet is the most used medium to research L&H insurance in China (59%)16\ninsurance but penetration of online               and Sweden (68%).17 However, the penetration of online insurance purchases is still\npurchases is low.                                 uneven in some countries. For example, online L&H insurance distribution accounts\n                                                  for less than 8% in the US18,19 and China20 but already above 30% in Sweden.21 The\n                                                  findings are similar for P&C insurance. In both Sweden and China, around 10% of\n                                                  P&C premiums written were distributed online in 2016. 22, 23 In other markets, levels\n                                                  of digital insurance purchases are much lower despite the internet being used by\n                                                  many as a research tool. For example, in Spain, 52% use the internet to research L&H\n                                                  insurance,24 but online penetration of life insurance is only around 0.02%,25 and only\n                                                  2%26 of P&C premiums came from online channels in 2015 (see Figure 9). For some\n                                                  commoditised lines such as motor, online purchases are higher: 25% of consumers\n                                                  in the US,27 11% of consumers in China28 and 5% of consumers in Spain29 purchased\n                                                  their motor insurance online in 2015/2016.\n  Figure 9\n  Online insurance purchases\n  Online distribution of life insurance in 2017                               P&C online distribution in 2015/2016\n           % online distribution of annual\n  35%      life insurance premiums written                                    12%         Online distribution of premiums written in %\n              32.4%\n  30%                                                                         10%             10%                       10%\n  25%\n                                                                               8%\n  20%\n                                                                               6%\n  15%\n                                                                               4%\n  10%\n                             <7.6%           6.5%                                                                                              2.2%\n   5%                                                                          2%\n   0%                                                         >1%              0%\n            Sweden            US            China           Spain                         Sweden                       China                  Spain\n  Source: Verdict, 2015; China Statistic Press, 2018; GenRe, 2017; Axco, 2019; Statista, 2019, Swiss Re Institute\n                                                  16 Chinese non-life personal lines: Consumer perspectives, Swiss Re Institute, 2019.\n                                                  17 European Insurance Report, Swiss Re, 2015.\n                                                  18 Online distribution includes life, health and accident insurance.\n                                                  19 United States Special InsurTech: Online Distribution, Statista, 2019.\n                                                  20 Insurance in China, forty years of reform and opening, China Statistics Press, 2018.\n                                                  21 Sweden Distribution Channels Life, axco.co.uk\n                                                  22 Sweden Distribution Channels Non-life, axco.co.uk\n                                                  23 “How Tencent and Ant Financial are rushing into China’s insurance industry”, cbinsights.com,\n                                                     23 October 2017.\n                                                  24 European Insurance Report, Swiss Re, 2015.\n                                                  25 Spain Distribution Channels Life, axco.co.uk\n                                                  26 Spain Distribution Channels Non-life, axco.co.uk\n                                                  27 US Insurance Shopping Study, J.D. Power, 2016.\n                                                  28 sigma 3/2017 – World insurance in 2016: The China growth engine steams ahead, Swiss Re Institute.\n                                                  29 Ibid.\n10  Swiss Re sigma No 1/2020\nOne reason is that availability of online One reason for the low penetration of online life cover could be that many insurers do\npurchase tools is limited.                not offer the tools to buy online. Looking at provider websites, we found that in\n                                          Spain, neither the biggest insurers nor InsurTechs sell life insurance online directly to\n                                          consumers. Similarly, in China and the US, very few large insurers sell life insurance\n                                          policies digitally.30, 31 However, in Sweden, six of the biggest financial institutions do\n                                          so, which could explain the higher purchase penetration there (32%). 32\nOnline purchases are higher in markets    Nevertheless, equal access to online purchasing facilities may not necessarily lead to\nwith low power concentration,             similar penetration across markets. Cultural characteristics based on the Hofstede\nhigh individualism, low uncertainty       Cross Cultural Index33 and preferred leisure activities may also have an impact. This\navoidance…                                comes through when comparing online insurance penetration to more general\n                                          e-commerce penetration. We use e-commerce buyer penetration as a proxy for\n                                          online insurance purchases because countries with a higher e-shopping penetration\n                                          also demonstrate higher online distribution of life insurance. One reason might be\n                                          that consumers who use the internet often for different purposes, including online\n                                          shopping, feel comfortable going online to buy insurance too.34, 35\n                                          ̤̤ Country-specific cultural characteristics such as low “power distance”, defined as\n                                             the degree to which individuals expect and accept that power is distributed\n                                             unequally,36 high individualism and low uncertainty avoidance could influence the\n                                             adoption of technology.37, 38, 39, 40\n                                          ̤̤ Countries with low power concentration have flatter hierarchies and people make\n                                             decisions by themselves, contrary to hierarchical societies where it is\n                                             unacceptable to disagree with decisions of a superior.41 Further, in individualistic\n                                             cultures people prefer to collect information on their own from direct sources such\n                                             as the internet, while in collectivist cultures people rely on subjective advice from\n                                             family and friends.42 And finally, in countries with lower uncertainty avoidance,\n                                             people feel comfortable trying new things, including new methods of buying.43, 44\n                                          30 The insurers in China are AnBong Life and PingAn; Insurtechs are Bihubao, HeTai and Xiaoyusan.\n                                          31 The US insurers include Massachusetts Mutual; Insurtechs are Ladder Life, Haven Life, Spot Life, Ethos.\n                                          32 The insurers in Sweden are Folksam, Scandia, Avanza, Nordea Liv, Nordnet and Idun Liv.\n                                          33 The Hofstede Cross Cultural Index analyses the national culture of a country based on six cultural\n                                             dimensions relative to other countries.\n                                          34 Bellman, S. et al., “Predictors of online buying behavior”, Communications of the ACM, 1992.\n                                          35 Lee, H. et al., “Consumer Lifestyles and Adoption of High-Technology Products: A Case of South Korea”,\n                                             Journal of International Consumer Marketing, Vol 21, 2009.\n                                          36 See Hofstede Cross Cultural Index at https://www.hofstede-insights.com/models/national-culture/\n                                          37 Matusitz, M. et al., “Power Distance, Uncertainty Avoidance, and Technology: Analyzing Hofstede’s\n                                             Dimensions and Human Development Indicators”, Journal of Technology in Human Services, 2013.\n                                          38 Özbilen, P., “The Impact of Natural Culture on New Technology Adoption by Firms: A Country Level\n                                             Analysis”, International Journal of Innovation and Technology, Vol 8, 2017.\n                                          39 Peterson, M. et al., “The Influence of National Culture in Information Technology Product Adoption”,\n                                             AMCIS 2003 Proceedings, Paper 119, 2003.\n                                          40 Lee, S. “The impact of cultural differences on technology adoption”, Journal of World Business, 2013.\n                                          41 Zakour, A., 2004, Cultural differences and information technology acceptance, Proceedings of the\n                                             7th Annual Conference of the Southern Association for Information Systems.\n                                          42 Lee, S., 2013, The impact of cultural differences on technology adoption, Journal of World Business,\n                                             Vol 48.\n                                          43 Laukkanen, T., “How Uncertainty Avoidance Affects Innovation Resistance in Mobile Banking?” 48th\n                                             Hawaii International Conference on System Science, 2015.\n                                          44 Garbarino, E. et al., “How cultural differences in uncertainty avoidance affect product perceptions”,\n                                             International Marketing Review, Vol 24, 2007.\n                                                                                                                      Swiss Re sigma No 1/2020 11\nThe battle for customer touchpoints\n                                                   ̤̤ Every country has a unique combination of cultural characteristics. China’s society\n                                                      is highly collectivist and there is high-power distance, indicating that inequalities\n                                                      are accepted. Yet the Chinese also appear comfortable with uncertainty, much like\n                                                      the Swedes. This could explain why China has a high e-commerce buyer\n                                                      penetration. So too could what count as popular leisure activities. Consumers that\n                                                      engage with technology more frequently in their daily lives and free-time also\n                                                      shop more online.45, 46 Noteworthy is that web surfing is a very popular free-time\n                                                      activity in China (ranked first)47 and Sweden (ranked third)48; not so in Spain.49\n…and where surfing the web is a                    All told, we believe that more insurance will be bought online once consumers have\npopular leisure activity.                          easier access. The degree of online insurance purchases will grow faster in markets\n                                                   with certain cultural characteristics (low power distance, high individualism, low\n                                                   uncertainty avoidance), and where use of the internet is a preferred leisure activity.\n                                                   To this end, we see more opportunities for online insurance purchases in markets like\n                                                   Sweden, the US, the UK, Germany and China (see Figure 10).\n Figure 10\n Culture and leisure across markets\n  Factor                                       Sweden           US             UK          Germany         China          Spain          Italy       Turkey\n            High individualism                    ✔              ✔             ✔              ✔               ✘             ✘             ✔            ✔\n  Culture Low power concentration                 ✔              ✔             ✔              ✔               ✘             ✘              ✘           ✘\n            Low uncertainty avoidance             ✔              ✔             ✔              ✘               ✔             ✘              ✘           ✘\n  Leisure activities*                             ✔              ✔             ✔              ✔               ✔             ✘              ✘           ✘\n  E-commerce consumption                         80%           78%            89%            82%             71%          59%            53%          33%\n Note: Digital e-commerce buyers are defined as consumers who use the Internet and have made at least one purchase online in 2017;\n * 1 of 3 most preferred leisure activities includes web surfing.\n Source: Swiss Re Institute\n                                                   45 Li, H. et al., “The impact of perceived channel utilities, shopping orientations, and demographics on the\n                                                      consumers online buying behavior”, Journal of Computer-Mediated Communication, Vol 5, 1999.\n                                                   46 Bellman, S. et al, 1999, op. cit.\n                                                   47 Chen, M. et al., “Leisure activities and leisure motivations of Chinese residents”, PLoS ONE, Vol 13,\n                                                      2018.\n                                                   48 En undersökning om svenska folkets tidsanvändning år 2010/11, Statistiska CentralByrån, 2011.\n                                                   49 2009-2010 Time Use Survey, Instituto National de Estadistica, 2011.\n12 Swiss Re sigma No 1/2020\n                                           What drives online purchasing behaviour if access is provided?\nA theoretical framework helps              Models based on consumer value and need theories can help answer the above\nunderstanding of what affects the          question.50, 51, 52 Figure 11 depicts a theoretical model tailored to online life insurance\ndecision to buy cover online.              buying behaviour. It consists of three value drivers: functionality, emotions and\n                                           personal growth. To date, insurance product design has mostly focussed on\n                                           functionality, with little consideration of emotional and personal growth values.\n  Figure 11\n  Schematic showing of drivers of insurance purchasing behaviour model\n   Functionality                            Emotions                                              Personal growth\n   ̤̤ Ease of purchase process              ̤̤ Low pressure                                       ̤̤ Empowerment\n   ̤̤ Convenience                           ̤̤ Enjoyment                                          ̤̤ Epistemic value\n   ̤̤ Immediate and fast purchase process   ̤̤ Painless process\n   ̤̤ Better price                          ̤̤ Feeling of relief\n                                            ̤̤ Feeling of comfortability\n                                                       Consumption behaviour\n  Source: Swiss Re Institute\nConsumers value simplicity and want        Analysing 400 consumer reviews of two US digital life insurers (Ladder Life and\nto be emotionally fulfilled.               Haven Life), we find that when consumers buy cover online, they value functional but\n                                           also emotional and personal growth elements (eg, empowerment).53 The most\n                                           commonly mentioned emotions are “no pain” and feeling of “happiness” and\n                                           “delight”; 23% said they find buying online “painless” while 22% said they feel\n                                           “great” to buy online. Another 20% of consumers said they “enjoy” completing their\n                                           application and purchase online. Nearly 17% said they feel lower pressure not being\n                                           in contact with an insurance agent.\nThey enjoy products that address           The personal growth values perceived by consumers mostly relate to empowerment.\npersonal growth values.                    Consumers value the ability to acquire information on product prices and terms, and\n                                           to be able to decide by themselves which policy best fits their needs. One consumer\n                                           stated, for instance, “it was great (…) to get the information to make a decision\n                                           yourself”.54 These empowered consumers are no longer passive shoppers\n                                           dependent on an agent’s advice. They are active participants who want information\n                                           to make their own decisions.55\n                                           50 M. Holbrook, Customer Value: A framework for analysis and research, Routledge, 1999.\n                                           51 J. Sheth, et al, “Why We Buy What We Buy: A theory of consumption value”, Journal of Business\n                                               Research, Vol 22, 1991.\n                                           52 A. Maslow, “A Theory of Human Motivation”, Psychological Review, Vol 50, 1943.\n                                           53 “Reviews”, trustpilot.com, and consumeraffairs.com last accessed 22 August 2019.\n                                           54 Consumer review quoted from consumeraffairs.com, 1 February 2018.\n                                           55 P. Bühler, P. Maas, “Consumer empowerment in insurance”, International Journal of Bank Marketing,\n                                               Vol 36, 2018.\n                                                                                                                  Swiss Re sigma No 1/2020 13\nThe battle for customer touchpoints\nInsurers need to better relate with                These value drivers play an important role in understanding consumers’ choice\nconsumers on higher-level emotional                behaviour and future purchasing intentions.56, 57 For example, Alibaba’s critical illness\nattributes.                                        insurance58 and Lemonade’s home insurance59, 60 offer a simple and immediate\n                                                   purchase process, and also address emotional and personal growth values (see\n                                                   Figure 12). In the case of Alibaba, consumers emotionally experience insurance as a\n                                                   community product since they only contribute to a pay-out when someone is ill. The\n                                                   personal growth value (empowerment) is fulfilled by allowing a large group of pre-\n                                                   approved consumers decide through a collective voting system whether a pay-out\n                                                   should be approved.61, 62\n  Figure 12\n  Examples of insurance products based on the three value drivers\n   Cases: designing insurance based on functional, emotional and personal growth values\n                                 Case 1                                                        Case 2\n                                 Alibaba's critical illness insurance                          Lemonade's home insurance\n   Functional value              The purchase process is simple and immediate via              The purchase process is simple and immediate via\n                                 the transaction app Alipay.                                   the Lemonade app or a laptop/tablet.\n   Emotional value               Consumers experience insurance as a community                 Customers choose a non-profit organisation for the\n                                 product since they only collectively contribute to            leftover money from the monthly flat fee and join a\n                                 the payout when someone is ill.                               community that believes in the same cause.\n   Personal growth value         (Empowerment): Preapproved consumers decide                   (Empowerment): consumers are empowered by\n   (empowerment)                 themselves through a collective voting system                 deciding which social cause they want to choose\n                                 whether a payout for claimants should be                      for their leftover money.\n                                 approved.\n  Source: Swiss Re Institute, based on information from company websites\nIn the future, all ages will be attracted          In the next three to five years, the digital insurance consumer will likely remain the\nto digital insurance.                              Millennials, with higher levels of income and education. In the longer-term, as\n                                                   millennials become seniors, all age groups will be attracted to digital insurance, with\n                                                   income and education less relevant factors in purchase decisions. New, innovative\n                                                   digital cover options for consumers with lower income are already available. For\n                                                   instance, micro L&H insurance products offered by China Life63, PROSUR64 or\n                                                   Jubilee,65 and also Alibaba’s critical illness product “Xiang Hu Bao” attract\n                                                   consumers from all backgrounds, irrespective of income and education levels.66\n                                                   56 J. Sheth, et al, “Why We Buy What We Buy: A theory of consumption value”, Journal of Business\n                                                        Research, Vol 22, 1991.\n                                                   57 M. Holbrook, Customer Value: A framework for analysis and research, Routledge, 1999.\n                                                   58 “Alipay Health Plan Aiming For 300m Users”, alizila.com, 11 April 2019.\n                                                   59 “Instant everything” on Lemonade.com\n                                                   60 “The Lemonade Giveback” on Lemonade.com\n                                                   61 Ant Financial Aims to Disrupt Health Insurance with New Chinese Health ‘Collective’,\n                                                        insurancejournal.com, 11 April 2019.\n                                                   62 “Jack MA is Selling Cancer Coverage for Pennies a Month in China”, bloomberg.com, 20 May 2019.\n                                                   63 Microinsurance and rural development in China: A Q&A with Associate Professor Yi Yao (Kitty) of\n                                                        Peking University, MicroInsurance Center at at Milliman, 21 March 2019.\n                                                   64 “POSUR Micro-Insurance” on prosur.com.kh/\n                                                   65 “Micro Insurance: Enabling people to overcome uncertainty”, jubilee.com, 23 August 2014.\n                                                   66 insurancejournal.com, 11 April 2019, op. cit.\n14   Swiss Re sigma No 1/2020\nThe digital insurer\nThe future insurer will be hyper-aware of customers needs and preferences, and offer personalised, flexible products\nand services in real time. Data-enabled processes will minimise friction and streamline the customer insurance\njourney, from request for coverage to claim. Artificial intelligence will be used to interact and build understanding of\nthe customer, and servicing will be through (personal) virtual assistance, 24/7.\n                                         The offering\n                                         Personalised, social, sustainable and engaging\nDigitisation can provide more timely     Digitalisation can provide far more timely feedback to insurers by leveraging a range\nfeedback loops.                          of daily interactions, for example across mobility, health and recreational behaviour.\n                                         Feedback loops have always existed in insurance, influencing purchasing patterns,\n                                         insureds’ risk behaviours and how they make claims. The feedback has traditionally\n                                         been ex-post, and that has sometimes meant a long time lag.\n                                         Balance autonomous insights and empathetic care\nThese constant feedback-based            Digitalisation goes beyond data collection. It also gives insurers power to understand\nprocesses provide better insights for    risks more dynamically.67 Insurers may discover uncovered exposures that they can\nrisk management…                         seek to close (see Figure 13). For example, combining traffic, weather patterns and\n                                         other factors, insurers may determine that a driver spends the majority of time on\n                                         higher-risk roads. Insurers are taking first steps in engaging customers in an\n                                         empathetic manner based on such observed behaviours. For example, Generali has\n                                         a real-time coaching tool for motor insurance clients, helping them drive better and\n                                         work on accident prevention. The focus is currently on mobility services, with plans\n                                         to expand to other areas.68\n                                         Access risk insights across new touchpoints and channel formats\n…and allow insurers to propose           With digitalisation, insurers can obtain risk-related data from a variety of touchpoints,\ninnovative coverages quicker.            and can identify and monitor existing and new risk exposures. New data sources and\n                                         analytical capabilities can provide instant in-depth view of risks across touchpoints,\n                                         which facilitates quicker risk assessment and allows insurers to propose innovative\n                                         and relevant coverage. Pre-digital, insurers have had to wait till loss data is available\n                                         to assess if a customer is adequately covered.\n                                         67 For example, when someone stops working due to a disability, insurers can use data to suggest ways to\n                                            help the customer recover more quickly. See “MetLife hooks into external data feeds to help curb\n                                            customer risk”, CIO.com, 15 October 2018.\n                                         68 “We innovate for our customers”, on generali.com\n                                                                                                                 Swiss Re sigma No 1/2020 15\nThe digital insurer\n Figure 13\n Insights improve the existing value chain\n                                                                    Smart interactions\n                                                                      Smart mobility\n                                                                         living / workin\n                                                                  Smart                 g\n                                                                       Smart health\n                                                           Active break\n                                                      Start work\n                                                       Park & ride\n                                                       (mobility)                         End work\n                                                            Breakfast with             Outdoor\n                                                            family                     activity\n                                                                 Sleep         Shopping\n                                                                        Family\n                   Risk-related interactions\n                   Digital risk-related\n                   interactions\n         Identification/monitoring of existing                                               Identification /monitoring of new/\n         and clients risk exposure                                                           future risk exposure\n         Reducing /adapting existing protection gap                                          Generating new protection related products\n             growth / profitability adoption on existing                                     and services (value propositions)\n         and new customers\n           Product                      UWR                         Marketing                   Claims                  Customer\n           management                   pricing                     & sales                     management              service\n                       Retail business                                                                   Corporate business\n                                                                         Risk exposure\n                                                                         Over-insurance\n            L           H            P          C                        Under-insurance           L          H           P        C\n                                                                         (protection gap)\n Source: Swiss Re Institute\n16 Swiss Re sigma No 1/2020\n                                                     Shift from product to service orientation\nInsurance will thus become more                      Digital interactions will enable insurers to create personalised insurance and new\nproactive.                                           protection-related services. Traditional risk selection will remain a core activity, but\n                                                     will be faster, simpler and based on real-time data. Greater volumes of digital data\n                                                     will help quantify different kinds of exposure, (eg, behavioural, external and internal,\n                                                     see Figure 14).\n                                                     ̤̤ Behavioural exposure is the risk of aberrant actions, such as driver speeding,\n                                                        failing to maintain safe distance, or choosing dangerous routes.\n                                                     ̤̤ Internal risk exposure covers the inherent risk of each insured or asset (eg, quality\n                                                        of construction for a property).\n                                                     ̤̤ External risk exposure covers environmental factors like catastrophe risks for\n                                                        property, or bad weather and street conditions for drivers.\n  Figure 14\n  Representation of an ideal state of risk quantification\n                    Life & health insurance               Property insurance              Automotive insurance        Classical lines\n                                                                                                                      of business\n                                                                                                         offline\n    Customer journey\n                                                                                                         online\n     Data\n   collection\n                         Risk exposure:\n              external                   internal                                                                     Sensor\n                          behavioural\n                Identification of risk exposure: contextual risk exposure\n                (evidence based analytics and decisions – timing – relevance)\n                Relevant product/service offering: contextual content, products and services\n                                                                                                                      Risk-related offerings\n                (risk transfer, risk mitigation and preventive services, value adding services)\n                Partner and community integration/intervention: access/sharing/steering\n                (to the support community, partner network)\n                                                                                                                      Life area clusters\n                             Health      Education/working       Living         Free time       Mobility\n  Source: Swiss Re Institute\n                                                                                                                    Swiss Re sigma No 1/2020 17\nThe digital insurer\nThis ideal state is still a way off. The key task for insurers is to develop their digital capabilities to access relevant\n                                     information about changes in risk exposures, convert them into actionable insights\n                                     and then package and communicate these to customers in a user-friendly manner.\n                                     To do this, insurers need to upgrade digital capabilities across three areas.\n                                     1\t\u0007Identify true risk exposures: Not all data can be used for risk exposure\n                                          identification. Insurers need to understand risk exposure (evidence-based\n                                          analytics and decisions), based on timing, relevance and true signalling power.\n                                     2 C  \u0007 reate relevant product/service offerings: Insurance cover should adapt to\n                                          changing needs, ensuring that cover is appropriate across different life stages. To\n                                          this end, insurers will need to be able to contextualise products and services (risk\n                                          mitigation and preventive services, value adding services).\n                                     3\t\u0007Partner and community integration/intervention: Insurers can steer the\n                                          support community and partner network. With advanced analytics, they will be\n                                          able to identify customers most likely to buy based on risk sub-segments.\nHigher-quality service will lead to  As insurers refine these capabilities, they will be able to provide higher-quality\ngreater loyalty.                     service interactions, which should lead to greater loyalty. For example, if a property\n                                     insurer can access and analyse data on changes in risk exposure, whether from\n                                     owned or third-party data, it can enable safety and security services to prevent\n                                     damage and losses. It could also advise on real estate purchases across hundreds of\n                                     relevant risk variables. For example, one insurer is exploring how to engage with\n                                     customers on predictive indicators such as if a property is in public transit corridors,\n                                     the number of emergency calls made from the property, hygiene of nearby\n                                     restaurants, and infrastructure quality in the area.69\nInsurers in emerging markets are     Technology is accelerating the development of insurance in emerging markets. What\nleapfrogging peers in advanced       took many years of development in advanced markets is being compressed into just\nmarkets in some innovative areas...  a few years in the emerging territories. This is “leapfrogging”, where some\n                                     technology steps are skipped. These disruptive technologies are facilitating\n                                     convenient access to insurance products and services as part of local requirements.\n...and are working with digital      Insurers in emerging markets are taking the lead in leveraging new digital\necosystems to extend their reach.    ecosystems; these offer multiple touchpoints to capture user attention, and often\n                                     evolve from their original businesses, to move into new domains. Such as in the case\n                                     of Grab based in Asia, which started with passenger mobility, then entered food and\n                                     goods delivery, and is now expanding into healthcare.70 Insurers in Asia are forming\n                                     partnerships with such firms: the insurer brings the underwriting expertise and the\n                                     partner firm an entry point to ecosystems, allowing the insurer to extend its reach,\n                                     target specific segments and mine user behaviour. Along these lines, in China, Aviva\n                                     has partnered with internet major Tencent to sell critical illness policies online. In\n                                     another example, in Africa insurers are selling insurers through ride sharing drivers in\n                                     mobility ecosystems.71\n                                     69 “MetLife hooks into external data feeds to help curb customer risk”, CIO Magazine, 15 October 2018.\n                                     70 “Ping An Good Doctor and Grab Form Joint Venture to Deliver Transformative O2O Healthcare Solutions\n                                        in Southeast Asia”, grab.com, 16 August 2018.\n                                     71 “How Nairobi-Based Ridesharing Service Little Competes With Uber in Africa”, skift.com,\n                                        19 June 2018.\n18 Swiss Re sigma No 1/2020\n                                                            Insurers can use digital platforms that are modern and flexible\nFlexible and modern platforms can                           Insurers can leverage flexible, product-agnostic and fully-integrated digital platforms\nhelp insurers enter new markets.                            to engage with customers through the buying journey. Core value chain components\n                                                            like underwriting can be transformed into insurance as a service (see Figure 15). For\n                                                            example, Swiss Re has a partnering solution, iptiQ, that allows insurers to harness\n                                                            the power of cloud applications and data analytics to make buying cover easier.72\nThey also connect market services for                       Platforms can also act as intermediaries connecting market services for different\ndifferent parties.                                          parties, (eg, enabling brokers to manage and deliver cross-border programs from a\n                                                            single online platform).73 Two types of platforms have been established. First, core\n                                                            service platforms. which enable part or the whole value chain of an established\n                                                            industry as a service (IaaS – Insurance as a Service solutions). Second, service\n                                                            platforms, which act as intermediaries between supply and demand. In rare cases a\n                                                            combination of both platform types may be established.74\n Figure 15\n Depiction of opportunities for platforms in insurance\n   Customers               Distribution                Primary                    Distribution             Reinsurance       Distribution              Retro-\n                              partner                 insurance                      partner                                    partner               cessions\n                 B2C\n      Retail                                  B2B                       B2B                         B2B                 B2B                   B2B\n                  B2B\n    Corporate\n       Core                             *)\n    service\n   platform\n    Market\n    service\n   platform\n                                                        Product          UWR         Marketing &     Claims    Customer\n                                                      management        pricing          sales    management    service\n              Service interaction platform (services)\n              Process platform (value chain)                      *) Industry, insurance, banks,\n              Technology platform (enablement)                       intermediaries, aggregators,\n              Platform management (coordination & governance)        brokers, platforms, ecosystems\n Source: Swiss Re Institute\n                                                            72 Swiss Re's partnering solution iptiQ allows insurers to harness the power of cloud-based applications\n                                                                and data analytics to make buying insurance easier, and to help more people to become insured. See\n                                                                iptiQ: The technology making it easier to buy insurance, Swiss Re, 7 November 2018.\n                                                            73 Swiss Re Corporate Solutions announces collaboration to build a multinational insurance program\n                                                                management platform for Brokerslink, Swiss Re Corporate Solutions, 18 October 2019.\n                                                            74 Starling’s Marketplace delivers an ‘app store’ experience for financial services, NS Banking,\n                                                                27 February 2019.\n                                                                                                                                       Swiss Re sigma No 1/2020 19\nThe digital insurer\nInsurers can use expose these                 The benefit of these platforms is that insurers can use APIs to integrate with external\nplatforms to influence brokers to             partners, eg, ecommerce to bring significant efficiency across the value chain due to\npartner with them. This allows a              seamless information exchange. Different components (from need analysis to policy\nseamless customer journey across the          issue) can be customized and co-branded for these distribution partners who can\nvalue chain.                                  choose which products they want to offer to consumers. This allows a seamless\n                                              customer journey across the value chain by virtue of remaining on single platform.\n                                              These platforms can be multi-device (optimised for desktop, tablet and smart\n                                              phone), and multi-channel, ie, customers can switch between face-to-face with an\n                                              agent to self-service and can even work with an agent in a call centre with their data\n                                              being saved and shared.\n                                              The value chain will become more integrated\nTo iterate quickly, processes will            The first wave of digitalisation has made the insurance value chain more efficient\nbecome flexible and open to                   (see Figure 16). However, multiple digital silos remain unconnected and information\ninnovation.                                   is compartmentalised. In the future, critical processes will be connected, and\n                                              insurers will progress from being mere “digitalised insurance providers” to “digital\n                                              insurers”. The potential for transformation does not end there. Looking further ahead,\n                                              as data quality and algorithms improve, the next generation will be AI insurers with\n                                              value chains that “learn” from data generated by consumers, ecosystems and\n                                              governments. Developments in cognitive technologies will help such insurers\n                                              integrate learning to adapt value propositions in real-time, thereby providing a\n                                              holistic and unique customer experience.\n  Figure 16\n  Insurance products growing into a comprehensive risk service\n  Involved stakeholder        Value chain (example)\n                                     Product               UWR              Marketing &          Claims           Customer\n    Insurance       (Icon)\n                                   management             pricing              sales          management            service\n   Digitalised                       Product               UWR              Marketing &          Claims           Customer\n                    (Icon)\n    insurance                      management             pricing              sales          management            service\n       Digital                       Product               UWR              Marketing &          Claims           Customer\n                    (Icon)\n    insurance                      management             pricing              sales          management            service\n   Data & AI-\n        driven      (Icon)                                               Product = service\n    insurance\n  Source: Swiss Re Institute\n20 Swiss Re sigma No 1/2020\nBetter data and digital triggers will  The availability of such high-quality underlying data will also make it easier to create\naccelerate the adoption of parametric  products with digital triggers, and will speed up the adoption of parametric\ninsurance.                             insurance. Property-level flood insurance that utilises data-feeds from sensors in\n                                       homes and businesses for parametric triggers is already available.75 Basis risk should\n                                       be much lower because the trigger is based on a data source in the property itself.\n                                       Over time this will virtualise the value chain and lead to products becoming more\n                                       parametric in nature (see Smart contracts).\n                                       Smart contracts\nSmart contracts can automatically      The concept of smart contracts aligns with the idea of an AI-driven insurer\nrecognise that the terms of an         constructed on an interplay between multiple value-chain components. Smart\ninsurance policy have been met.        contracts are self-executing software programs that, for instance, can make a\n                                       payment when triggered by occurrence of a risk event. Legal (contract) logic is put\n                                       into computational logic, such that a string of computer code recognises that the\n                                       terms of a contract or insurance policy have been met. It automatically transfers\n                                       funds (insurance payouts) at the agreed time and registers those transfers.\nHowever, as of yet there is no         Existing insurance contracts need to be translated into computational logic. So far,\ninternational regulation specific to   the most promising cases have been in parametric insurance. Lines of business\nsmart contracts.                       where telematics can be used, such as motor, aviation, cargo and agriculture, offer\n                                       potential. However, certain challenges stand in the way of wider adoption of smart\n                                       contracts. Notably, as of yet there is no international regulation specific to smart\n                                       contracts. As Lloyds recently pointed out, it is unclear how policyholders who\n                                       disagree with an automatic decision not to trigger cover can defend themselves.76\n                                       Challenges to digitalisation\n                                       Regulation\nMonitoring yet facilitating innovation The regulatory framework will play an important role in shaping the integration of\nis a priority for regulators.          new technology into the insurance space. In monetising the potential of\n                                       digitalisation, insurers could face regulatory challenges on data protection and\n                                       privacy, providing incorrect advice, and records retention. Errors or bias in algorithms\n                                       that might contribute to systemic risk or prompt inappropriate insurance decisions is\n                                       also an area of regulatory scrutiny. Likewise, it might be difficult for regulators to\n                                       understand why a complex and proprietary algorithm decided to deny coverage or\n                                       reject a claim, undermining their ability to fulfil their supervisory and consumer\n                                       protection tasks.\nInsurers could also face country-      Regulators are also wary of unsolicited use of data. Insurers also face country-\nspecific regulatory limitations.       specific regulatory limitations and high implementation efforts for product\n                                       development and modifications. They will have to construct a robust system of data\n                                       governance to win both customer trust and abide by regulation. There may be\n                                       additional legal and compliance challenges, such as integrating multiple vendors into\n                                       one proposition.\n                                       75 JBA & FloodFlash launch sensor-based parametric flood cover first, Artemis, 27 June 2019.\n                                       76 Triggering innovation, Lloyds, 2019.\n                                                                                                               Swiss Re sigma No 1/2020 21\nThe digital insurer\n                                    Ownership of data and willingness to share\nConsumer reticence to share data    As data become easier to collect, an increasingly important consideration will be\nvaries by country and there may be  ownership. For example, ownership of images of properties surveyed by drones may\nresistance in some contexts.        rest with clients, not insurers. Consumer reticence to share data varies from country-\n                                    to-country, and there may be resistance to do so in some contexts. For example,\n                                    some employers that offer workers boots, gloves and vests with wearables have\n                                    found some employees refuse to be monitored.77 This behaviour differs by line of\n                                    business; a recent survey found US consumers more willing to share health-related\n                                    data than home-related data.78\nThe focus should be on loss         Another survey found that consumers are cautious about sharing data, even when\nprevention rather than personalised they know it will lead to a more personalised experience.79 However, an Accenture\npremiums.                           survey found that eight of 10 would share personal data (eg, on income, location and\n                                    lifestyle habits) with insurers if it lowers risk of injury or loss.80 This suggests\n                                    messaging around data sharing is more effective if the focus is more on loss\n                                    prevention rather than personalised premiums.\n                                    Data availability\nBetter to ask fewer, more focused   Data is not easily available in the required depth and detail. Insurers will have to strike\nquestions.                          a balance between more and relevant data. Data can be easily misinterpreted if the\n                                    background context is not fully understood. For example, a non-smoker who on\n                                    occasion buys cigarettes for a parent could be identified as a casual smoker, and\n                                    could be subject to higher premium rates when buying health insurance. Does more\n                                    data help or confuse? In some cases, asking fewer but more focused questions could\n                                    lead to more precise answers.\n                                    Consumer wariness about automation\nBut some policyholders may be less  Some consumers may be less comfortable in an automated world. For example,\ncomfortable with digitisation.      48% of surveyed consumers said they worry about mistakes while filling out\n                                    insurance forms, and 46% fear that claims pay-outs might not be accurately\n                                    calculated by machines.81 Policyholders may be divided in their views on\n                                    digitalisation. It is vital to win customer confidence by reassuring them about\n                                    machine accuracy and developing checkpoints to help them avoid mistakes.\n                                    77 “The Exciting, New Ways Technology Is Streamlining the Claims Management Process”,\n                                        Risk&Insurance, 5 August 2019.\n                                    78 U.S. Consumer Survey: A Connected-Devices Insurance Roadmap, Aite Group, 20 May 2019.\n                                    79 “Promise of personalization has little impact on consumer willingness to share data, study reveals”,\n                                        Marketing Dive, 14 August 2019.\n                                    80 64% of consumers are willing to share data in exchange for adjusted car insurance premiums based on\n                                        safe driving and 52% are willing to share data in exchange for life insurance premiums tied to a healthy\n                                        lifestyle. See Six in Ten Consumers Willing to Share Significant Personal Data with Banks and Insurers\n                                        in Exchange for Lower Pricing, Accenture Study Finds, Accenture, 14 March 2019.\n                                    81 “Future of Claims: Customers Still Want Humans In Process – But Not Too Many”, Carrier Management,\n                                        24 March 2019.\n22 Swiss Re sigma No 1/2020\nMarketing, distribution and servicing\nInsurers are moving away from a product-focused sales approach to one closely tied to the broader needs of the\ncustomer, with greater focus on the human experience. New generations of systems will grant unprecedented levels\nof proximity and influence on customers, and insurers will be able to optimise interaction across a diverse range of\ncustomer touchpoints.\n                                                Marketing to become more granular\nInsurers can use new data to build a            Historically, insurance marketing has been about aggregating prospective buyers\nconsumer journey based on human                 into groups likely to have common needs and respond similarly to marketing. Typical\nexperience.                                     attributes include geography, demographics, psychographics (eg, values, attitudes,\n                                                lifestyles) and behavioural aspects (eg, price sensitivity). This approach leads to a\n                                                relative broad spectrum of consumers in a segment, which can limit effective\n                                                targeting. With new sources of digital data on life events such as purchase histories,\n                                                travel behaviour, customer service records, data from devices, wearables and also\n                                                social networks, insurers can employ advanced analytics techniques to yield more\n                                                granular classification of existing and prospective customers (see Table 1).82, 83\n  Table 1\n  Evolution of marketing and distribution innovation timeline\n   Marketing and\n   distribution journey      Past                         Present                        Future                          Key enablers\n   Customer experience       Fragmented, limited          Multichannel, somewhat         Seamless, continuous            Platform economy,\n                             channels                     integrated                                                     insights available\n   Market size/sentiment     Time consuming,              Tech-based, still takes        Tremendous efficiency           Online surveys, digital\n   analysis                  Physical                     time                           and accuracy gains              behavior\n   Personalisation           Not possible, very           Possible, limited scope        Complete customisation          Shorter development\n                             expensive                                                                                   time, data insights\n   Channel evolution         Agent-driven, door-to-       Direct (online, contact        Ecosystem based with            Chatbots and machine\n                             door                         center, bots), Tech            virtual connect                 learning\n                                                          enabled agents\n   Fulfillment               Exhausting & time            Direct to home                 Needs-based “just in\n                             consuming                                                   time”\n  Source: Swiss Re Institute\n                                                82 sigma 6/2015, Life insurance in the digital age: fundamental transformation ahead, Swiss Re,\n                                                    December 2015\n                                                83 Ibid.\n                                                                                                                        Swiss Re sigma No 1/2020 23\nMarketing, distribution and servicing\n                                                   Smart interaction: the future of marketing\nThis can boost customer loyalty.                   This personalisation can improve loyalty management and impact customer lifetime\n                                                   value (CLTV).84 Historically the focus of insurer engagement with customers has\n                                                   been on sales and claims. With new data sources and advanced analytics, insurers\n                                                   can interact with customers over their lifetime on a wide range of relevant issues,\n                                                   developing systems that help them listen, engage and act both proactively and\n                                                   reactively (see Figure 17).\n Figure 17\n Schematic showing how smart interaction increases loyalty and customer equity\n     Without/only limited interaction management                             With intensive interaction management\n     CLTV segment                                                            CLTV segment\n                              Customer journey\n                                                                                                               Customer journey\n                    Invoice       Mailing      Claim\n                                                                                  Potential\n                         Historical focus only on\n                                                                                                 Listen            Engage         Act\n                      sales (distribution) and claims\n       CLTV                                                    CLTV             CLTV                                                               CLTV\n     Owned                                                                    Owned\n                                                                                                   Proactive                      Reactive\n     Paid                                                                     Paid\n     Earned                                                                   Earned\n     Social                                                                   Social\n                                                                                                      Value for the customer\n     Content                                            Collaboration         Content                                                        Collaboration\n                                 Interaction\n     Communication                                        Community           Communication                                                   Community\n                                                                                              Insurance products and services\n Source: Swiss Re Institute\nLifestyle related interactions will help           The more high-quality interactions there are, the more loyal customers will be. To this\nbuild deeper one-to-one relationships,             end, insurers can pro-actively use digital engagement to reduce policyholder churn.\nat scale.                                          For example, recently a major P&C carrier ran a three-month pilot, sending\n                                                   personalised automated texts to delinquent customers and followed up with a phone\n                                                   call, creating a new level of proactive digital and human engagement. Less than 2%\n                                                   of customers opted out, and a large majority of delinquent customers were retained.\n                                                   There was less manual work: the approach yielded results with 50% fewer outbound\n                                                   calls.85 Insurers can broaden the scope of these interactions with policyholders to a\n                                                   wider sets of prospects. For example, some insurers have decoupled insurance from\n                                                   their telematics apps. The app is available to all drivers rather than insureds alone.86\n                                                   84 Customer lifetime value or CLTV is a measure of net profit attributed to the entire future relationship.\n                                                   85 Strategy Meets Action. Proactive Digital Engagement: From Reactive to Proactive: A Paradigm Shift for\n                                                      Insurers, Hearsay Systems, August 2019.\n                                                   86 Insurance Innovation of the Month: AIG on the Go, EFMA, 15 January 2019.\n24    Swiss Re sigma No 1/2020\n                                         Distribution: traditional touchpoints will adapt\nCapitalising on unique moments will      With digitalisation, insurers will be better able to identify where human interactions\nbe key to increasing loyalty and         play a significant role. Capitalising on unique moments will be key to increase CLTV.\ncustomer lifetime value.                 Currently digital interactions are mostly transactional (post-sale interactions focused\n                                         on billing and claims handling), and miss opportunities for insurers to address the full\n                                         range of customer needs. Insurers need to understand where and how empathy is an\n                                         important component of brand value by offering a unified customer experience.\n                                         Automated data capture and synchronisation with customer relationship\n                                         management (CRM) systems offer actionable insights. For example, agents are\n                                         alerted when a customer files a digital notice of loss claim and can use that event as\n                                         an opportunity to call on the same day to better understand other life events (eg, a\n                                         baby on the way) and, if appropriate within the contextual setting, cross-sell.87\n                                         Intermediaries: their role will change\nAgents will act as risk consultants with As customers become more comfortable with buying insurance online, the role of\na fuller view of customers' life events. intermediaries will evolve. Agents will act as risk consultants with an overview of\n                                         what’s happening with each customer across the value chain. Insurers and agents\n                                         will also need to rethink traditional cooperation models, including how intermediaries\n                                         are compensated. For example, a client acquired online may require advice from an\n                                         agent. A compensation model that rewards agents for these services via advisory\n                                         fees that consumers are willing to pay, may help alleviate channel conflicts that could\n                                         arise within a pure commissions-based system. 88\nIn commercial, small and medium          New distribution technology has not impacted wholesale commercial insurance\nsized business are also becoming         markets as much as retail/personal lines. However, there are some initiatives to\nmore open to digital distribution.       simplify parts of the value chain in commercial insurance. Lloyd’s of London for\n                                         example, mandated its syndicates to use electronic placements for no less than 30%\n                                         of their written risks by the end of 2018.89 Online brokers for small and medium\n                                         businesses (SME) are also seeing traction. For instance, Embroker and CoverHound\n                                         use data verification technology to obtain immediately bindable quotes, allowing\n                                         customers to complete the process almost seamlessly online.90 These platforms offer\n                                         additional services (eg, help SMEs upload and compare policies, generate vendor\n                                         certificates, and asset tracking), which can strengthen customer loyalty.\n                                         87 [Insurance] The Next-Gen Customer Experience – A Dreamforce Case Study, Hearsay Systems,\n                                            16 October 2018.\n                                         88 sigma 6/2015 op. cit.\n                                         89 “Lloyd’s issues electronic placement mandate”, Lloyds.com, 20 March 2018.\n                                         90 See Embroker, CoverHound.\n                                                                                                               Swiss Re sigma No 1/2020 25\nMarketing, distribution and servicing\n                                        Digitally-augmented channels on the rise\nDigitally-augmented channels,           New digital interactions will be driven by next-generation empathetic advisory tools,\ncombining man and machine will          augmented by AI (see Figure 18). Agents and brokers will leverage these to refine\nbecome more common.                     their communication to adapt to the customer’s emotional, situational and personal\n                                        contexts. The power is in combining both functional and empathetic interfaces. The\n                                        functional interface performs the analytics and provides the intelligence, such as\n                                        next best action. The agents/brokers provide the human or empathetic interface.\n Figure 18\n Data and AI-driven augmented\n advisors\n                                                Retail customer/\n                                          Commercial customer\n                                        Intermediaries                                                               Provider\n           Smart-augmented            Platform/ecosytem                                                                Bank\n           platform                    Broker/3rd Party                                                          Insurer/reinsurer\n                                          Marketplace\n                                          Aggregator\n           Digitally-augmented\n           empathic human interface\n                                                                               Interactions\n           Digitally-augmented\n           digital interface\n                                        Source: Swiss Re Institute\nBig Tech, intermediaries and insurers   We believe the use of digital advisors which combine these two capabilities will\nwill invest in different interaction    drive digital interactions in a targeted manner in the future. Generating this\nchannels.                               intelligence will become a core capability, and not just for insurers. Depending on\n                                        who the provider is, there could be three types or levels of digital interaction channel.\n                                        These could work in co-existence, although the one perceived by consumers as\n                                        most trustworthy and unbiased will likely hold a distinctive competitive advantage.\n                                        ̤̤ “Direct” customer advisors (insurer agnostic), offered by technology companies.\n                                           For example, these could be enhanced versions of today’s Siri/Alexa/Google\n                                           duplex assistants.\n                                        ̤̤ The “intermediary advisor”, which feeds intermediaries with better insights so that\n                                           back-office responsibilities are minimised, and agents can focus on human\n                                           relations and empathy, augmented by better risk insights and data.\n                                        ̤̤ The “product advisor”, which is fully insurer-owned. Such as, for example, GEICO’s\n                                           mobile virtual assistant Kate, which intuitively guides customers to the relevant\n                                           information about their policy and helps in self-service tasks.\n                                        Customers may use all three set ups. For example, for L&H insurance they may go to\n                                        an intermediary, for home insurance direct to an insurer, and for mobility and travel\n                                        insurance use an enhanced version of customer advisors like Google Assistant.\n                                        Intermediaries will continue to play an important role, although human interaction\n                                        may in some cases become less part of the standard process. Where human\n                                        involvement can be of added value is where the emphasis is on contextual “empathic\n                                        interactions”. In the new competitive landscape, insurers will need to balance these\n                                        customer interactions in the right manner using new loyalty and interaction systems.\n26 Swiss Re sigma No 1/2020\n                                       Servicing: loyalty and new risk management needs\nAdditional digital tools and value-    Servicing is all about building deeper 1-2-1 relationship with customers at scale\nadded services will help brokers push/ through risk prevention and value adding services (VAS).91 This is especially critical\ndistribute to customers.               in commercial lines where specialised knowledge can be leveraged. Whichever\n                                       party along the value chain possesses specialised knowledge and provides robust\n                                       engagement will control servicing.\n                                       There is significant scope for insurers to enable agents and brokers with digital tools\n                                       and value-added services. J.D. Power measures satisfaction scores among business\n                                       relationships, and finds that insurance agents currently report low satisfaction with\n                                       the service they receive from personal lines and commercial lines insurers.92 The\n                                       advent of digital engagement in servicing has several implications for insurers:\n                                       ̤̤ The boundaries of traditional insurance are becoming blurred, and distribution is\n                                          moving towards an integrated ecosystem focusing on an end-to-end buying\n                                          experience. A survey by Accenture found that three in five insurers (59%) are\n                                          forming relationships with non-traditional partners to reach customers in new\n                                          ways and create new value.93 This will have an impact on all aspects of the value\n                                          chain. To fully realise the benefits of these partnerships, insurers will need to\n                                          upgrade their predictive analytics capabilities, obtaining risk-related data from a\n                                          variety of touchpoints and providing tailored content.\n                                       ̤̤ Customers want a positive experience across multiple channels and touchpoints.\n                                          Gartner believes that in the coming five years, event-triggered and real-time\n                                          marketing will make the biggest impact across industries.94 However loyalty will\n                                          be hard to come by. Digital-only customers still give their insurers lower loyalty\n                                          scores than do multi-channel customers.95 There is a misalignment between what\n                                          customers expect from digital channels and what insurers provide. For example,\n                                          some insurers fail to replicate certain service components, such as their Contact\n                                          Us page, from their web to mobile sites.96\n                                       ̤̤ Shifts toward platform plays and ecosystems appear inevitable, in both personal\n                                          and commercial lines. E-commerce firms, retailers, automotive OEMs and other\n                                          non-traditional players are developing, owning and offering new and unique\n                                          product propositions to customers. They build platforms based on plug-and-pay\n                                          principles to allow third-party connections. Insurers need to be more cognizant\n                                          that different elements in the value chain own different customer touchpoints.\n                                       91 Srikanth Madani, Hypotheses on Innovation Direction in Insurance, St Gallen Risk & Finance, 2019.\n                                       92 “Insurers Come Up Short for Independent Agents Despite Critical Role Agents Play in Driving Business,\n                                          J.D. Power Finds”, prnewswire.com, 31 January 2019.\n                                       93 Accenture, Technology Vision for Insurance 2019, Accenture, 10 April 2019.\n                                       94 Gartner Identifies Four Emerging Trends That Will Transform How Marketers Run Their Technology\n                                          Ecosystems, Gartner, 29 August 2019.\n                                       95 Bain and Company, 10 October 2018. op. cit.\n                                       96 Gartner Says U.S. Insurance Brands Underperform in Digital, Despite Customers’ Growing Willingness\n                                          to Provide Data, Gartner, 10 July 2019.\n                                                                                                               Swiss Re sigma No 1/2020 27\nProduct development and underwriting\nFrom bikes to trainers, customers like tailored rather than off-the-shelf products. The same will become increasingly\nprevalent in insurance. Over time Big Data and sophisticated models will allow risk pricing at increasingly granular\nlevels. Emergence of new risks will create new underwriting and portfolio risk management techniques.\n                                                 Digital-enabled product development\n                                                 Real-time demand assessment\nDigitisation will enable direct                  Increasing digitalisation will present opportunities to connect directly with\ntouchpoints with customers and                   customers and assess product demand in real-time, rather than rely solely on\nreal-time demand assessment.                     channel partners (see Table 2). For example, in-app surveys and tracking search\n                                                 behaviour will provide clues about unmet customer needs. Based on granular\n                                                 customer data, insurers can more precisely segment customers and develop tailored\n                                                 products. Moreover, timing of product searches, when correlated with personal data,\n                                                 may provide the context in which a product is demanded. Such constant feedback\n                                                 will help insurers develop new products but also refine/adjust existing ones.97\n  Table 2\n  Impact of digitisation on product development journey\n   Product development\n   journey                   Past                           Present                        Future                        Key enablers\n   Demand assessment         Time consuming,                More efficient, based on        Real -time, based on         Online surveys, digital\n                             dependent on channel           limited customer data           changing customer life       behavior tracking\n                             partners                       collection                      stages\n   Coverage design           Fixed coverage for longer      Partially modular               Completely modular and       Hyper connectivity\n                             duration                       coverage for shorter            real-time coverage           enabled by sensors, 5G,\n                                                            duration                                                     LPWAN95 and satellites\n   Pricing                   Fixed pricing for defined      Short-period pricing           Dynamic pricing based\n                             risk categories                based on limited               on real time behavioral\n                                                            behavioral data                data\n   Partnership strategy      Mainly distribution-           Extended partnerships          Ecosystem partnerships        APIs96, cross industry\n                             based partnerships             across insurance value         driven by cross-industry      ecosystems\n                                                            chain                          value chains\n   Regulatory approach       Finite experimentation         Sandbox                         Approval for wide range      Sandboxes and\n                             due to lack of data            experimentation on              of data in risk modelling    innovation hubs\n                                                            data-based evidence\n   Customer feedback         Almost non-existent            Limited feedback based          Continuous feedback at       End to end digital\n                                                            mainly on claims                multiple touchpoints         delivery of products\n                                                            experience\n  Source: Swiss Re Institute\n                                                 9899\n                                                 97 K. Cool, C. Angoulvant, et.al., “ZhongAn’s Micropremium Model: The Future of Insurance?”,\n                                                    knowlege.insead.edu, 18 May 2018.\n                                                 98 Low Powered Wide Area Network, a low bandwidth wireless technology covering wide areas.\n                                                 99 An interface or communication protocol that allows two software programs to talk to each other.\n28 Swiss Re sigma No 1/2020\n                                         Data-driven and granular insights to facilitate new risk covers\nDigitisation has enabled coverage        At present, digitalisation drives modest coverage customisation in products. Such\ncustomisation, while preserving base     incremental innovation preserves the basic product structure, while using short time\nproduct structure.                       scales to price short duration covers and add-ons to provide coverage carve outs.\n                                         These solutions are either white labelled or written directly by insurers.100 Most of\n                                         the covers are fixed for shorter durations. Products may use sensors to track asset\n                                         usage or movement of individuals to activate coverage or offer minor premium\n                                         credits, but this information is not widely used for real-time underwriting and pricing.\nReal-time risk data will facilitate more In the sharing economy, modularisation and real-time coverage adjustment will\ninnovative insurance solutions.          enable new products to adapt to the frequently changing risk profiles.101\n                                         Modularisation means the de- or re-coupling of coverage and services to suit the\n                                         changing needs of a customer.102 Personal risk exposures will be tracked and\n                                         modelled more easily so that insurers can more confidently de-couple coverage\n                                         elements within existing monolithic products. Commercial risks are more complex\n                                         and do not change as frequently. Digitalisation can help in coverage re-coupling\n                                         across products to reduce protection gaps and achieve price efficiencies. In this\n                                         dynamic environment, product life cycles will get shorter and speed-to-market will\n                                         be crucial.103\nBetter data quality is facilitating      Some innovative products for new risk pools are emerging with granular data\nconstruction of new risk indices and     collection and analysis enabled by digitalisation. For example, we see first solutions\ncreating new markets.                    emerging for personal cyber, use of cryptocurrency exchanges, or IoT infrastructure\n                                         risks.104 Better data quality also facilitates construction of new risk indices and\n                                         parametric products, such as covers to protect against economic impacts of\n                                         infectious disease outbreaks. These use pathogen sentiment indices from digital\n                                         sources to gauge public fear and behavioural changes to measure the cost of\n                                         epidemics.105 In the future, we think algorithmic risk products will become\n                                         mainstream, with assets and processes becoming more autonomous and intelligent.\n                                         100 Toffee insurance, Trov and Element are examples of agencies providing pay-as-you go insurance via\n                                             online portals. Metromile and Lemonade are licensed insurers with usage-based products.\n                                         101 Traditional asset insurance mandated a clear commercial or personal insurable interest. However, this\n                                             classification is blurring with the sharing economy allowing individuals to put personal assets to\n                                             commercial use via online platforms.\n                                         102 See Mobility ecosystems striving towards a seamless interface for customers. Swiss Re, 2018.\n                                         103 Current average speed-to-market is eight months for new Life and Annuity products, and four months\n                                             for modifications. For Property and Casualty insurers, it is seven months for new products and three\n                                             months for modifications. See Speed to market for life/annuity insurers, Novarica, March 2019 and\n                                             Speed to market for property/casualty insurers, Novarica, March 2019.\n                                         104 Bitflyer and Mitsui Sumitomo are selling insurance for users of Cryptocurrency, while Munich Re and\n                                             Relayr have developed customized insurance products to facilitate IoT infrastructure investments.\n                                         105 Andrew W. Singer, “The Evolution of Parametric Insurance”, rmmagazine.com, 1 April 2019.\n                                                                                                                     Swiss Re sigma No 1/2020 29\nProduct development and underwriting\n                                                   Dynamic risk pricing to drive behavioural change\nPricing based on granular data will                Insurers will be able to micro-segment risk pools, thus accurately reflecting the risk of\ngenerate premium efficiencies and                  each unit within an insured pool. For example, Zurich Insurance in Spain partners\nmould consumer behaviour.                          with Klinc to offer coverage that can be turned on and off. This is for personal devices\n                                                   and other items from a catalogue of over 2 000 products, and there are plans to\n                                                   expand into categories like auto and home, all enabled by availability of granular\n                                                   behavioural data and advanced modelling.106, 107 Such pricing may change\n                                                   dynamically, based on changing risk profiles.108 Going forward, cross-industry\n                                                   platforms and seamless insurance portability can integrate all the risks, exposures\n                                                   and covers for an insured under a universal insurance policy. This policy will be\n                                                   customised according to the insured and it will adapt the coverage in real-time on\n                                                   the basis of a change in the insured’s risk profile (see Figure 19).\n Figure 19\n Tech-enabled insurance product and service opportunities\n Off-the-shelf coverage                                                         Digitalisation-enabled personalised coverage\n     Risk levels                                                                    Risk levels\n                                                   Policy period                                                                       Policy period\n      Static pricing level\n      Exposure inefficiency for insurer                                              Coverage and pricing\n      Insured risk profile                                                           Insured risk profile\n      Coverage and pricing inefficiences for insured                                 Coverage and exposure efficiences for both insurer and insured\n                                                   106 Zurich’s innovation efforts recognized with Global Innovator Gold Award; Klinc takes top award in\n                                                       Products & Services, Zurich, 25 June 2019.\n                                                   107 In absence of granular data, insurers use proxies such as driver age, type of car or gender to assess\n                                                       driving behaviour.\n                                                   108 Products like Pay as You Live (Health Insurance) or Pay as You Drive (Auto Insurance) rely not on the\n                                                       dynamic pricing, but short-period pricing based on limited behavioral data, which is a finer version of\n                                                       static pricing.\n30 Swiss Re sigma No 1/2020\nTech-enabled insurance product and service opportunities (cont.)\n Modularisation global insurance coverage in retail business\n                               Travel                                                                     Travel\n                    Cyber                                                                     Cyber\n                                                Liability                                                                Liability\n    Home                                                                       Home\n                                                           Mobility                                                                 Mobility\n                              25-year                                                                    Family,\n                               adult                                                                    two kids\n   NatCat                                                 Life                NatCat                                               Life\n                               Health                                                                    Health         Over-exposure\n                                                                                                                        (inefficiency for insurer)\nModularisation and global insurance coverage in commercial insurance\n                  Cyber                      Liability                                    Cyber                      Liability\n  Marine                                                                     Marine\n                                                             Fire                                                                     Fire\n                               SMEs                                                                   Mid/large\n                                                                                                      corporates\n     NatCat                                                                    NatCat\n                                                     Engineering                                                             Engineering\n                   Business interruption                                                    Business interruption\n              Risk exposure             Risk coverage and price\n                                        Under-insurance (protection gap)\n                                        No insurance coverage\nNote: The green (risk exposure) line in the circular charts indicates an insured’s dynamic risk profile revealed by processing of granular\ninsured data. The segments and blocks in the same chart indicate the extent to which coverage can be modularized across exposures under\ndifferent risk areas (eg, cyber, fire etc.)\nSource: Swiss Re Institute\n                                                                                                                     Swiss Re sigma No 1/2020 31\nProduct development and underwriting\n                                                  Extending insurance reach with preventive and value-added services\nInsurers may need to focus on                     Insurers with strong distribution networks, brand and ability to adapt to meet\npreventing accidents and improving                customer needs (eg, simplifying tasks, offering outcomes that reduce pain/improve\nquality of life.                                  gain) could develop new relationships based on trust and value add. They will\n                                                  reinvent themselves from risk protection to value-add and preventive service\n                                                  providers. This will help counter any decline in premiums caused by reduction in loss\n                                                  incidences due to various safety features enabled by technology. Prevention could\n                                                  contain future claims while value add services build loyalty. These services (see\n                                                  Table 3) will grow more sophisticated as insurers harness network effects within\n                                                  cross-industry ecosystems, and should also increase customer stickiness through\n                                                  increasing touchpoints.109\nA wide range of services could be                 In commercial lines, insurers could partner with providers of technology to monitor\nprovided as part of the overall                   industrial assets to improve efficiency, safety and early warnings. For example, one\ninsurance package.                                insurer offers a free online tool that helps SMEs assess risk of cyberattacks and\n                                                  become more proactive on security.110 In personal lines, insurers can enable access\n                                                  to technologies for coping with or mitigating morbidity. For example in Italy, Axa has\n                                                  prototyped a cancer profiling service that allows patients with advanced and/or\n                                                  metastatic tumours to access personalised cancer care.111 And South African insurer\n                                                  Discovery has a service that provides additional pay-outs to insureds stricken with\n                                                  permanent disabilities to help them adjust to a new way of life.112\n  Table 3\n  Examples of value-added and preventive services\n   Service Type               Auto                            Property                       Life                           Health\n   Preventive services        ̤̤ Safe driving alerts          ̤̤ Remote monitoring           ̤̤ Online diagnostics          ̤̤ Medical consultations\n                              ̤̤ Maintenance rewards             and alerts                  ̤̤ Help ﬁnding doctors         ̤̤ Screening and\n                              ̤̤ Anti-theft and               ̤̤ Flood monitoring in         ̤̤ Scheduling                     counselling\n                                 breakdown alerts                home basements                 appointments                ̤̤ Behavioural\n                              ̤̤ Geo fencing (eg, alert if    ̤̤ Automatic shutdowns                                           assessments\n                                 driven outside of safe          of gadgets during ﬁres\n                                 neighbourhood)\n   Value added services       ̤̤ Assistance to buy car        ̤̤ Property security           ̤̤ Meal and fitness            ̤̤ Digital health records\n                              ̤̤ Concierge services              advice                         vouchers                    ̤̤ Fitness club\n                              ̤̤ Alert when vehicle is        ̤̤ Facility maintenance        ̤̤ Retirement, financial          memberships and\n                                 moved, towed or hit          ̤̤ Emergency repair               planning                       discounts\n                                 when parked                     services                    ̤̤ Healthcare services\n  Source: Compilation of value-added and preventive services by Swiss Re Institute\n                                                  109 Digital ecosystems: extending the boundaries of value creation in insurance, Swiss Re, 2019.\n                                                  110 “Cyber Insurance” on armourinsurance.ca\n                                                  111 Efma and Accenture Reveal Winners of Innovation in Insurance Awards 2019, Accenture, 25 June\n                                                      2019.\n                                                  112 The Purple Life Plan, Discovery, 2019.\n32   Swiss Re sigma No 1/2020\n                                                  Digitalisation and underwriting\nDigitisation will enable more forward-            Digitalisation of external and internal data will improve risk selection and pricing,\nlooking underwriting.                             while automation will streamline manual submissions, triaging and binding (see\n                                                  Table 4). Overall, these developments should reduce underwriting costs, loss ratios\n                                                  and free up time for underwriters to engage in softer aspects of business (ie,\n                                                  negotiation and relationship building). Also, developing repositories of digitised risk\n                                                  information will enable more efficient underwriting than decisions based on limited\n                                                  claims experience data. For example, Helvetia Insurance enables logistics\n                                                  companies to purchase transport insurance for their customers through a\n                                                  straightforward online app in less than two minutes.113\n  Table 4\n  Impact of digitisation on underwriting value chain\n   Value chain               Past                             Present                         Future                        Key enablers\n   Submissions               Lengthy proposal forms           Proposal auto-filling           Fraud detection,              ̤̤ Advanced text and\n                             and human interactions.          through third party data.       behaviour analysis for            speech analysis (AI/\n                             No data collection for           Chatbot interaction but         spotting dishonest                ML)\n                             post submission analysis         limited data collection         disclosures. Complete         ̤̤ Efficient information\n                                                                                              data collection                   storage systems\n   Triaging                  Delays due to inefficient        Better quote to market          Submission ranking            ̤̤ Intelligent business\n                             workflows and time-              speed on some LOBs due          based on conversion               workflows (AI / NLP)\n                             consuming decisions on           to auto-referrals to            probability and auto          ̤̤ Better readability of\n                             referrals                        appropriate authority           suggestions on capacity           digitized information\n                                                                                              management                        (OCR)\n   Risk assessment           Manual, time consuming           Usage of third-party data       Evolution into a risk         ̤̤ Digitisation of asset and\n                             and limited at the point of      for external risk features,     foresight activity with           biological data\n                             underwriting                     and automated report            frequent touch-points         ̤̤ Text and image analysis\n                                                              interpretation                  and real-time sensor data     ̤̤ Processing of sensor\n                                                                                                                                data\n   Coverage and pricing      Coverage governed                Flexible cover and              Real-time coverage and        ̤̤ Real-time risk modelling\n                             strictly by insurability,        pricing for some LOBs,          pricing adjustments,          ̤̤ Capacity sharing (P2P,\n                             while pricing based on           driven by data on asset         Seamless cover                    Digital risk consortiums)\n                             static risk matrix               usage and individual            portability across            ̤̤ Connected sensors\n                                                              movements                       insurers\n    Binding                  Delayed binding due to           Reduction in binding            Instant binding on the        ̤̤ Same as submission\n                             data entry of paper-             time and errors due to          basis of end to end               and triaging\n                             based information in             higher flow of digitised        digitised submission and\n                             multiple systems                 submission data in to           risk assessment data\n                                                              binding process\n  Source: Swiss Re Institute\n                                                  Submissions triage and routing to become more automated efficient\nSubmission level accuracy will be                 Increasing digitalisation of geographical, personal and asset information will be\nmuch improved with availability of                leveraged by insurers to auto-fill proposals, offer risk scores, and even identify more\nmore granular customer data.                      honest brokers.114 Already pin-code and license plate numbers reveal location\n                                                  specific hazards and vehicle details, while business registration numbers linked with\n                                                  municipal data are used to provide occupancy and previous loss history for\n                                                  commercial clients. For example, about 85% of submissions for Chubb’s business\n                                                  113 Helvetia transforms logistics and freight companies into insurance professionals, Helvetia,\n                                                      7 October 2019.\n                                                  114 Berkshire Hathway Guard Insurance now obtains needed information based on data available online\n                                                      and offline, which has reduced time from submission to quote. See “Berkshire Hathaway GUARD\n                                                      Insurance Companies partners with Planck to create full digital underwriting for their commercial lines”,\n                                                      prnewswire.com, 7 March 2019.\n                                                                                                                            Swiss Re sigma No 1/2020 33\nProduct development and underwriting\n                                                                                    owners policy for SMEs can now flow straight through without any human\n                                                                                    intervention on Chubb’s part.115 In the future, intelligent and automated workflows\n                                                                                    for triage and routing will be more effective than current business rules.\n                                                                                    Risk assessment and underwriting to be more cost effective, engaging\nDigitisation will reduce the cost of                                                Availability of digitised risk information (eg, biomarkers, activity data, building\ninsurance and turn risk assessment                                                  footprints, occupancy, vessel/vehicle data) will create risk profiles without physical\ninto an early warning tool.                                                         risk inspection or medical underwriting requirements (see Figure 20). These data\n                                                                                    sets will be especially useful for geographically-scattered assets and individuals.\n                                                                                    More complex proposals may still require traditional risk assessment, but intelligent\n                                                                                    workflows can automatically assign tasks to suitable risk engineers/doctors, and\n                                                                                    natural language processing can enable underwriters to interpret results faster.\n Figure 20\n Different levels of underwriting maturity\n                                                              Data                                AI module                              UW evidence\n                    Risk data                                                                                                                                §\n                                                                                                                           Learning                               Learning\n                                                                           Data                          Modelling insights                         UW models\n                                                              Inputs for underwriting             Effort required from customer          Decision/outcome\n                                                              Dynamic underwriting                Self optimizing AI driven              Automatically\n                                                              – Social media data                 underwriting algorithms                assign risk class\n                                                              – IoT data, fitness, home           using parametrised indexes\n                                                              – Activity/lifestyle                accessed through platforms\n                                                                data/mobility                     – Minimal customer input\n                                                              – Electronic health record\n                                                              Accelerated underwriting            The insurer may request                   ‘No’ –\n                                                              – Prescription history              either third parties or the               assign risk class\n                                                              – Credit data                       customer for additional data\n                                                              – Driving records/                  if needed\n                                                                telematics\n                                                              – Claim experience                                                            ‘Yes’ –\n                                                                                                                                                                                 Machine learning expertise\n                                                                                                                                            additional underwriting\n                                                              Traditional underwriting            Underwriting decision solely\n                     Data sources\n                                                              – Proposal                          based on customer inputs\n  Data-Input from                          Emerging\n                                                              – Tele interview\n                                                                                                                                            Inspection report,\n                                                                                                                                            lab test, physician statement\n  customer           currently available   sources of data\n  Increasing data                                            Emerging complexity\n Source: Swiss Re Institute\n                                                                                    115 Andrew G. Simpson, “InsurTechs Take Note: Chubb’s Digital Marketplace Serves 1,000 Agents a Day”,\n                                                                                        insurancejournal.com, 13 February 2019.\n34     Swiss Re sigma No 1/2020\nDynamic underwriting can provide   Standard and accelerated underwriting approaches differ in speed, but both are\nmutual benefits to insureds and    static. They evaluate the risk at the time of underwriting but do not track future\ninsurers alike.                    behaviour, either positive or negative. This results in price inefficiencies for both\n                                   policyholders and insurers. In life and health, greater availability and quality of\n                                   Electronic Health Records (EHR), and fitness and social media data can allow\n                                   dynamic underwriting and provide predictive insights on state of health. With this\n                                   information, insurers can evaluate the change in risk factors (ie, smoker status, BMI,\n                                   blood glucose, blood pressure, cholesterol) over time, and predict claims probability.\nExternal data helps digital risk   SMEs and mid-corporates have diverse business activities and many exposures,\nassessment for SME risks, even for which are sometimes too small to justify complete risk assessment by an insurer. For\nniche products.                    this reason, insurers leverage third-party data sets to auto-fill proposal forms and\n                                   offer risk scores. For example, online reviews related to small businesses can be used\n                                   as a proxy for their operational risk levels. Some insurers use social media to improve\n                                   customer experience by tracking changes in SME payrolls, office premises and\n                                   revenue to indicate growth or retrenchment, so that brokers can refine target\n                                   strategies.116 Others leverage digital footprints to deliver niche products like trade\n                                   credit insurance to SMEs by tracking live financial data. In cyber, new tools can\n                                   ingest and analyse data to generate cyber risk scores for SMEs in less than two\n                                   minutes.117\nDigital underwriting for large     Large corporate risks require bespoke solutions and critical underwriting information\ncorporate risks is less advanced.  for such risks is not yet sufficiently digitised. As such, the underwriting approach is\n                                   more case based. Digitalisation of geo-spatial information has assisted large-\n                                   corporate underwriting to some extent. For example, InsurTechs provide property\n                                   risk engineering solutions to insurers, especially to assess natural hazards.118 In\n                                   another case, cargo sensor data can be used to provide enhanced marine\n                                   coverage.119 Remote risk inspection can be enabled by allowing risk engineers to\n                                   transmit live feeds to underwriters.120 The full value of digitalisation in dynamic\n                                   pricing will only be realised when live data from industrial control systems and\n                                   facility monitoring systems is integrated with underwriting systems.\n                                   116 “A move from static data to live data in insurance”, digitalfineprint.com, 31 July 2019.\n                                   117 Cyence for Small Business, guidewire.com.\n                                   118 Insurtech Impact 25, Oxbow Partners, 2018.\n                                   119 Smart Cargo+Cyber Insurance, Covus Insurance, see info.corvusinsurance.com/.\n                                   120 “Virtual i Lets You See” on virtualitechnologies.com\n                                                                                                                Swiss Re sigma No 1/2020 35\nProduct development and underwriting\n                                       Pricing and coverage determination to accelerate, be more risk based\nPrice and coverage are being           Automation of submissions and risk assessments may trigger faster determination of\nautomated based on better              coverage and better pricing. Automated price and coverage determination are more\nassessment of risk data.               common for personal (L&H) and retail (P&C) business. Several examples are\n                                       beginning to emerge. In China, Ping An claims that of the nearly 20 million\n                                       applications for insurance received in 2018, 96% were auto-underwritten by AI.121\n                                       Continued testing, focused on scalable and replicable use cases will inform insurer\n                                       strategies in the next few years. Mid-market and commercial insurance segments\n                                       still require significant manual effort due to underwriting complexity and price\n                                       sensitivity.\nThis is valuable when there is wide    In corporate health insurance, automated pricing and coverage determination is\nvariability in costs for similar       improving. This sector struggles with high costs and wide variability in treatment\nprocedures.                            costs for similar procedures. Using Big Data and a modern tech stack,\n                                       UnitedHealthcare and Bind, on-demand health insurance start-ups, are designing\n                                       better health coverage plans for big employers. To lower costs and better match\n                                       risks to employees, they identify about 45 top-up optional procedures that have the\n                                       widest range of cost, treatment and effectiveness. Members can opt for these\n                                       options only if they need it. Customers pay a base monthly premium that can be as\n                                       much as 40% less than the other options that their employer offers.122\n                                       Fraud detection and managing moral hazard will become more critical\nUnderwriting fraud is a risk in the    In a digital world, underwriting decisions on large volumes of insurance applications\ndigital world with decisions needed in need to be made in near real-time. If insurers are not careful to filter out fraudulent\nnear real time.                        applications, they can build up a wrong customer portfolio. Some are successfully\n                                       using new sources of data to prevent high risk profiles from entering their portfolios.\n                                       For example, online insurer InShared, an Achmea initiative, has fully automated its\n                                       risk assessment, and determines risk at the point of purchase using multiple\n                                       indicators such as the person’s conduct, payment risk and claim risk.123\nInsurers will look for innovative      As digital insurance becomes more widespread, insurers will look for new\ntechnology to help identify high risk  approaches to help identify high-risk cases. About one out of five people admit to\ncases.                                 lying on insurance claims applications.124 To counter this, insurers are experimenting\n                                       with new digital tools to reduce fraud at the underwriting stage (eg, by utilising facial\n                                       biometric tools that leverage machine learning and computer vision technology). In\n                                       another example, using third-party data insurers may be able to determine – without\n                                       medical evidence – whether a customer is a smoker or not, in order to avoid\n                                       expensive medical tests in some cases.\nAdverse selection could increase if    With the spread of digital data, insurers are also more susceptible to rising risks of\ninsureds withhold new                  anti-selection (adverse selection) as consumers build more understanding of their\ninformation from their insurers.       own health with different healthy-living apps, genetic tests cheaply available direct\n                                       to consumers (DTC), and testing devices available on the market. These advances\n                                       bring equal promise and risks, including over-diagnosis and subsequent unwarranted\n                                       treatments. The major challenge for insurers is to obtain adequate and risk-relevant\n                                       information during the underwriting process, since existing regulation was mostly\n                                       enacted before the widespread adoption of DTC genetic tests.125\n                                       121 Announcement Of Audited Results For The Year Ended December 31, 2018, Ping An, 2018.\n                                       122 Susan Morse, “Insurtech entrepreneurs to lead panel at Health 2.0”, healthcarefinancenews.com,\n                                           12 September 2019.\n                                       123 How insurers are in control of risks at underwriting, celent.com, 2016.\n                                       124 Lying on an insurance application, www.finder.com, October 2018.\n                                       125 SONAR: New emeging risk insights, Swiss Re Institute, May 2019\n36 Swiss Re sigma No 1/2020\n                                     Underwriting implications, for insureds and insurers\nConstant feedback-based processes    Product development and underwriting will increasingly interact and be dependent\nwill provide better insights for new on customer facing functions. For example, marketing models will integrate data\nproducts.                            obtained from the initial underwriting experience, customer feedback and other\n                                     external data to make targeted and pre-approved offers of cover at the right time,\n                                     based on specific customer needs and behaviours. Offers will focus on increasing\n                                     existing cover to suit needs but also cross-selling into new products that match new\n                                     risks, however small, and thereby increase engagement. This can have a direct\n                                     impact on new product development, with implications for insureds and insurers.\n                                     ̤̤ Insureds: Other than for large corporate risks where placement negotiations\n                                         involve underwriters visiting and explaining technical aspects to clients,\n                                         underwriting has never been truly customer-facing. We expect modularisation of\n                                         products and dynamic pricing adjustment will bring higher understanding of\n                                         underwriting and risk assessment to retail customers. For example, life or SME\n                                         risk scoring apps will make the opaque and cumbersome underwriting process\n                                         more consumer-friendly and transparent. Underwriting bundled with proactive\n                                         risk management through value added or preventive services will change the\n                                         insurer-customer relationship from risk transfer to risk partnership.\n                                     ̤̤ Insurers: Insurers can benefit from dynamic and more accurate risk pricing, but\n                                         they will need to upgrade their systems to process terrabytes of data coming\n                                         every day from sensors. Many insurers are still ironing out legacy system\n                                         challenges.126 Also, insurers will need to become more responsive to digital\n                                         feedback on product development, tests and experiments. The digital world offers\n                                         immediate and sometimes ruthless feedback. For instance, one on-demand digital\n                                         insurer saw a massive drop in conversions and discovered it was caused by a\n                                         small change in the process flow requested by a small segment of customers.127\n                                     126 K. Harris-Ferrante, 2019 CIO Agenda: Insurance Industry Insights” Gartner, 15 October 2018.\n                                     127 A Khusid, “How we built a customer feedback loop that works”, medium.com, 15 January 2018.\n                                                                                                             Swiss Re sigma No 1/2020 37\nClaims management\nInsurers are using advanced analytics and machine learning to create early warning systems and gather practical\ninsights that prevent accidents, and simplify and speed-up claims processing. Examples include using AI to detect and\nverify accident hot spots, estimate repair costs, and identify potential fraud. Historically claims processing has been a\nform filling-in exercise. Digitalisation will help improve customer experience and efficiency of back office processes.\nA modular approach is key for                    Digitalisation can improve the claims experience both in terms of trust, lower friction\nend-to-end claim digitalisation.                 in customer experience, and complexity in back office operations. In a survey, only\n                                                 57% of adults online in the US expressed confidence that their insurer would treat\n                                                 them fairly in the event of a claim.128 A modular approach will enable end-to-end\n                                                 digitalisation and ongoing claims management processes (see Table 5). For example\n                                                 in 2018, USAA launched a 21-member innovation team to digitise components of\n                                                 claims operations and reduce the full claims cycle to days, and even hours in some\n                                                 cases.129\n  Table 5\n  Claims innovation timeline\n   Claims journey              Past                         Present                       Future                     Key enablers\n   FNOL                        Complex forms which          Customer uses the             Automatically: IoT,        Satellite imagery,\n   (First Notice of Loss)      require explanation          phone, post, online/          satellite, weather         weather stations, IoT, etc.\n                                                            mobile apps                   stations, etc.\n   Claims admin                Manually by insurer staff    Data entry by insurer         Digitally augmented        Connected systems,\n                                                            staff                         support in complex         intelligent algorithms\n                                                                                          situations; AI advice\n   Data gathering/             Rudimentary fraud            Combination of manual         Automated using            Computing power,\n   fraud detection             analytics                    and models to identify        analytics like, big data   access to varied data\n                                                            fraud                         and AI                     sources\n   Claims estimation           Manual claims                Mix of manual inspection      Automated claim            Computing power, better\n                               adjudication                 or advanced                   adjudication               algorithms\n                                                            technologies\n   Settlement                  Reimbursement via bank       Direct to vendor (garage,     Instant payout,            Linked payment systems,\n                               cheques /drafts              hospital) and                 automated validation,      intelligent algorithms\n                                                            reimbursement                 options (eg, replacement,\n                                                                                          second hand)\n  Source: Swiss Re Institute\n                                                 128 Forrester Analytics Consumer Technographics® North American Financial Services Survey, Forrester,\n                                                     Q4 2017 (US).\n                                                 129 Inside USAA’s claims innovation team, DIG IN, 15 August 2018.\n38    Swiss Re sigma No 1/2020\n                                                 End-to-end digitalisation of claims management\n                                                 Digital loss prevention\nLoss prevention is an important but              Loss prevention is an important but, arguably, undervalued aspect of the insurance\nundervalued aspect of the insurance              value chain, partly due to lack of research and understanding, and because of its\nvalue chain.                                     capital-intensive nature. Investing in loss prevention technologies requires significant\n                                                 investment with possibly little to no short-term impact on claims reduction.\nFast-growing internet and smart                  Across lines of business, it is becoming more feasible financially and technology-\nphone penetration have made loss                 wise to alert policyholders to the potential threat of damage to life and/or property.\nprevention action more feasible.                 For instance, on receiving notification from meteorological departments or sensors\n                                                 about a catastrophe, insurers can send push safety warning notifications to\n                                                 customers’ smartphones. This will help insureds take preventive measures to\n                                                 mitigate damage. Figure 21 illustrates where an insurer can use external and internal\n                                                 systems to alert insureds to potential threats.\n  Figure 21\n  Illustration of a potential digital prevention process for a natural catastrophe event\n                      System integration                                Build platform                              Real-time application\n                                                                                 Finding the\n          Data sources                                       APIs            nearest safe zone         Policy holders\n                                  Sensors                                                                                      Lower claims\n                                                                             and routes in case\n                                                                                of evacuation\n                                                      Insurers systems                               Consumer takes\n         Meteorological/                            to capture, process                              steps to prevent\n      Government agencies                               and prioritise                                loss of life and\n                                                         information                                      property\n                                                                                Notifications                                   Real-time\n                                GIS systems                                    and alerts sent                                   situation\n                                                                              to policy holders                                 monitoring\n                                                                  Cloud-based\n                                                                    network\n             External systems        Insurer systems\n  Source: Swiss Re Institute\n                                                 Digital first-loss notification\nNew insights are enabling automated-             There has been significant innovation in First Notification of Loss (FNOL) processes\nrisk management processes via Digital            given IoT sensors, high-speed internet and a wide range of automated data-capture\nFNOL.                                            technologies such as drones and satellite imagery. This data can be shared with a\n                                                 range of audiences. For instance, CSAA Insurance partnered with Owlcam on a\n                                                 Video First Notice of Loss (vFNOL) to send videos to a driver’s mobile phone when a\n                                                 car crashes or is broken into. The driver can share it with the police.130\n                                                 130 CSAA Insurance Group and Owlcam Partner to Accelerate InsuranceClaims Process for Customers by\n                                                     Utilizing Video Technology, CSAA Insurance Group, 5 June 2019.\n                                                                                                                        Swiss Re sigma No 1/2020 39\nClaims management\nInsurers can initiate contact with   Insurers now pro actively initiate contact with customers and offer help even before\npolicyholders even before a claim is a claim is reported. PZU, the largest insurer in Central and Eastern Europe, has\nreported.                            launched a Before-You-Call Service where operational coordinators actively scan\n                                     digital and analog information for news of loss events from the web, emails, calls\n                                     from witnesses, TV and radio. If they identify customers who could potentially have\n                                     been impacted by the loss event, they verify the scope of those customers’\n                                     insurance, and offer assistance services covered by their policies. A customer can\n                                     register a claim during this first contact or at a later convenient date.131\n                                     Claims administration\nAI applications can accelerate the   In this phase of the claims journey, a case is analysed based on the information\ncustomer through the next stage of   received in the FNOL (eg, checking coverage details, screening for fraudulent\nthe claims process.                  claims). With AI assistance, digital assessments can automatically verify coverage\n                                     details like sum assured, deductibles, co-payments and past claims. The reduction in\n                                     manual touchpoints can speed up the process significantly, and also lower the costs\n                                     associated with manual inspections. For example, Farmers Insurance adjudicates\n                                     claims using only photos and video-estimating technology in 95% of total-loss auto\n                                     claims. It aims to expand this capability to property as well.132\n                                     Fraud detection\nAdvanced analytics can improve fraud Insurance fraud costs billions of dollars each year. Traditional fraud-detection\ndetection…                           statistical models have drawbacks, not least that sampling methods to analyse data\n                                     can lead to undetected fraud. Also, these methods rely on experience: when a new\n                                     type of fraud occurs, insurers are vulnerable. With Big Data and AI, new tools can\n                                     combine thousands of variables from the moment a claim is filed, and identify\n                                     incoherence between data relating to the claim and the wider context. Many\n                                     insurers use social media analytics to detect fraudulent individual claims. For\n                                     example, Allstate is leveraging online information to identify fraud faster (like\n                                     claimants who say they are too injured to work, yet engage in strenuous activity such\n                                     as horseback riding).133 Analytics can help uncover large frauds by flagging certain\n                                     suspicious events like when smaller medical clinics originate large volumes of claims.\n                                     While individual insurers may lack sufficient data to detect mass fraud, law\n                                     enforcement agencies co-ordinate among carriers with analytics know-how to\n                                     uncover patterns (eg, search for comparable bills coming from the same parties).134\n                                     Digital loss assessment, claims estimation and settlement\n…and also help automate loss         Digitalisation can increase transparency in dealing with customers, with provision of\nassessment.                          a comprehensive overview of the calculation which could also prevent litigation and\n                                     recalculation costs. Moreover, automation of claims assessment can help reduce\n                                     administration costs, making insurance more affordable. During the California\n                                     wildfires of 2018, USAA used images from drones to settle many home insurance\n                                     claims on the same day. Previously, it could take weeks or even months for a team to\n                                     be safely mobilised to inspect and adjudicate the loss to a property damaged by\n                                     fire.135 Other interesting experiments are emerging. For example, Allianz ran a pilot\n                                     with Amazon to enable claims handlers to make an offer on the day a customer\n                                     makes a claim by assessing the content loss based on live Amazon prices.136\n                                     131 How PZU has successfully built a culture of innovation, EFMA, 5 February 2019.\n                                     132 “How Insurers Are Digitizing Claims with Apps, Photos, Videos, E-Payments and More”,\n                                         insurancejournal.com, 11 June 2019.\n                                     133 “Allstate Finds Fraud Faster with Innovative Data Company”, prnewswire.com, 28 November 2017.\n                                     134 sigma 4/2019 - Advanced analytics: unlocking new frontiers in P&C insurance, Swiss Re Institute,\n                                         August 2019.\n                                     135 Inside USAA’s claims innovation team, DIG IN, 15 August 2019.\n                                     136 Allianz Replacement Solution – Allianz France, EFMA, 2019.\n40 Swiss Re sigma No 1/2020\nThere has been wider adoption of     Figure 22 is a graphical representation of touchless claims settlements at work in\ntouchless loss assessment and claims parametric insurance policies. Claims for personal lines and small-business\nsettlement in personal and small     insurance can be largely automated, enabling carriers to achieve straight-through\nbusiness insurance…                  processing rates and reduce claims processing times from days to hours or minutes.\n                                     In motor insurance, use of AI-assisted assessment can significantly reduce the time\n                                     and cost of loss assessment.137 Ping An has employed AI to settle motor claims\n                                     across the spectrum of activities, including first notice, sharing digital pictures, loss\n                                     assessment and document handling. Surveys can be completed within 5–10\n                                     minutes for 95.5% of daytime accidents and, using facial recognition, payments can\n                                     be made in seconds.138\n   Figure 22\n   Touchless claims settlement                           Smart                  Smart                  Smart claims        Smart\n   in parametric insurance                               distribution           policy                 trigger             payout\n                                          Flight                                                                                         $\n                                          delay               Travel                    Buy                                            $\n                                                             platform\n                                          Earth-                                                                                         $\n                                          quake          (Digital) agent/               Buy                                            $\n                                                              broker\n                                         Tropical                                                                                        $\n                                        cyclone           Direct (digital)              Buy                                            $\n                                                              mobile\n                                           Agri-                                                                                         $\n                                         culture            Agriculture                 Buy                                            $\n                                                            ecosystem\n                                     Source: Swiss Re Institute\n…and some early experimentation in   Insurers are experimenting with new claims technology in life insurance too. MetLife\nlife lines.                          piloted an Ethereum blockchain pilot known as Lifechain to help families determine if\n                                     a deceased is protected by a policy. Once an obituary is placed in the press, the\n                                     family is automatically informed of this MetLife programme and, if they participate,\n                                     Lifechain encrypts the deceased’s national ID number and searches for a matching\n                                     policy. If a match is found, an automatic notification initiates the claims process.139\n                                     137 In case of an accident, the policyholder just needs to take few photos and streaming videos of the\n                                         vehicle and damaged area and upload it on insurer’s app together with the description of the damage\n                                         and the accident.\n                                     138 “China’s Ping An P&C Unveils Credit-Based Smart Auto Insurance Claim Solution”, insurancejournal.\n                                         com, 30 January 2019.\n                                     139 “How does it make faster life insurance with blockchain?”, medium.com, 13 August 2019.\n                                                                                                                 Swiss Re sigma No 1/2020 41\nClaims management\n                                   Claims implications, for insureds and insurers\nProcesses will become flexible and In the future, greater use will be made of characteristics of claims submissions to\nopen to innovation                 better inform product pricing and underwriting decisions. Insurers can generate data\n                                   that leads to better risk assessments for policy underwriting. Using real time claims\n                                   data, insurers will also be able to better monitor which partners and customer\n                                   segments offer most growth potential, and adapt marketing and distribution\n                                   initiatives in real time. To iterate quickly, processes will need to be flexible and open\n                                   to innovation.\n                                   ̤̤ Insureds: The quality of the claims experience has a strong impact on loyalty and\n                                       renewal behaviour. In a property claims satisfaction study by JD Power, 90% of\n                                       “highly satisfied” claimants said they would definitely renew their policies. On the\n                                       other hand, 60% of claimants who were “indifferent” or “displeased” said they\n                                       would shop for a new carrier within the year.140 Insurers have built customer\n                                       experience and digital channels development units to introduce design skills and\n                                       thinking methodologies into key areas. They have redesigned services and\n                                       processes by putting themselves in customers’ shoes.141\n                                   ̤̤ Insurers: Loss prevention will become a bigger theme in claims management.\n                                       Insurers will combine claims and behavioural data to action loss prevention\n                                       measures. As different components of the value chain digitally share information\n                                       and feedback, this could compress the value chain as digital distribution platforms\n                                       connect directly with capital providers without depending on traditional\n                                       capabilities of primary insurers. For example, providers of parametric insurance\n                                       may dispense with large claims departments and assessors, except in cases of\n                                       suspected fraud. In another example, a global insurer used an InsurTech program\n                                       platform to transfer part of a >USD 100 million layer of parametric risk to multiple\n                                       risk markets.142\n                                   140 Satisfaction with Property Insurance Claims Surges, Even as Number of Catastrophes Reaches\n                                       10-Year High, J.D. Power Finds, J.D. Power, 1 March 2017.\n                                   141 We innovate for our customers, Generali. op. cit.\n                                   142 “Tremor places world’s first programmatic parametric swap transaction”, Artemis, 20 May 2019.\n42  Swiss Re sigma No 1/2020\nOutlook\nDigitalisation will enable development of new data-driven business models that impact the entire insurance value\nchain. Access to data and the capability to model risks will be core to success. True leverage will be through\ncollaboration with other assets such as key supplier partners, entry points to ecosystems and the know how to\ngenerate customer insights.\n                                                Refining existing and developing new competencies\nThe insurance value chain can benefit           In recent years, as premium growth has slowed, insurers have been looking to a\nfrom new data and analytics.                    variety of approaches to differentiate products and services within the rapidly\n                                                expanding new economy sectors (see Figure 23).\n  Figure 23\n  New capabilities to be established\n                                                          Business transformation\n                               Mindset                                                                  Culture\n            Partner network         Data & insights          Customer insights             Analytics               Underwriting\n             Superior partner       Unique data-sets           Deep customer            Data analytics &             expertise\n           network (access to     and/or insights access    (behavioural) insights   machine intelligence,     Scalable risk modelling\n            risk and access to                                                            collaborative,        and pricing expertise\n               capabilities)                                                          autonomous robotics\n                                                                 Capabilities\n  Source: Swiss Re Institute\n                                                Insurers will need to build capabilities in the following areas:\n                                                ̤̤ Building a superior partner network: Identifying innovative and appropriate\n                                                   ecosystem partners and suppliers, including in the technology space, to offer\n                                                   unique value propositions to customers. Competition for suitable partners is high,\n                                                   and relationships need to be fostered, the focus being on creating mutual value.\n                                                ̤̤ Accessing unique and large data sets: Besides existing and accessible data,\n                                                   owning unique data sets for superior risk modelling is becoming an important\n                                                   differentiating factor. The key is to assemble data layers in a way that creates\n                                                   unique insights that cannot be easily copied by competitors.\n                                                ̤̤ Developing deeper customer insights: Putting the customer at the centre of the\n                                                   ecosystem and aligning all actions to customer needs. Using behavioural insights\n                                                   to personalise offerings is essential. This holds true even if an insurer does not\n                                                   own a customer relationship directly.\n                                                                                                                 Swiss Re sigma No 1/2020 43\nOutlook\n                                                    ̤̤ Analytical expertise: The more insurers understand data and related context, the\n                                                       more sophisticated risk modelling can become. AI stands to reshape the risk\n                                                       landscape, resulting in better simulations.\n                                                    ̤̤ Scalable underwriting expertise: Insurers can build superior risk models based\n                                                       on existing and unique data sets, combined with analytical methods and tools that\n                                                       can be scaled across ecosystems and value chains. This will enable insurers to\n                                                       make appropriate adjustments to value chains and respond more dynamically.\n                                                    Impact on the existing insurance value chain\nThe focus of customer insights within               There will be a close interplay between product design, claims management and\nthe value chain is changing.                        distribution touchpoints to generate customer insights. The changing location of the\n                                                    yellow box across the three panels in Figure 24 shows how the sources of customer\n                                                    insights are changing. In the past, the product factory (the insurer) was disconnected\n                                                    from the direct customer. There was instead close collaboration between the insurer\n                                                    and the physical sales channel, the intermediary. Intermediaries were in direct\n                                                    contact with the customer and, with their customer insights, could go back to the\n                                                    insurer to ask for products to be designed in a particular way.\n Figure 24\n Decreasing distance between supplier and customer via digital channels\n                Yesterday                               Today                                      Tomorrow\n                                        offline                                  offline                                     offline\n                 Customer journey                        Customer journey                           Customer journey\n                                        online                                   online                                      online\n     Customer\n                     Corporation         Retail              Corporation           Retail           Corporation     Retail        Digital\n                      customer         customer               customer           customer            customer     customer       platform\n                                                                                                          Digitally    Digitally\n                                                                                                        augmented    augmented\n                                                                                                      empathic human    digital\n  Interaction         Personal          Personal             Direct       Phone     Personal\n                                                                                                         interface    interface\n  channel\n                       direct           3rd party        online offline    chat direct 3rd party\n                                                                                                                   Smart\n                                                                                                              augmented Platform\n  Producer\n                            Product factory                          Product factory                   Digital\n  (supplier)                  (insurance)                              (insurance)                     augmentation\n       Critical insights\n Source: Swiss Re Institute\nThe product factory accesses insights               Today, the landscape has shifted towards a multi- and omni-channel approach, and\nfrom multiple channels.                             the product factory is getting closer to the end customer. This is because insurers\n                                                    have greater influence over product design. Products must be customised to be sold\n                                                    via different channels: simpler products are sold direct, while products with greater\n44     Swiss Re sigma No 1/2020\n                                                       complexity are largely sold through personal channels (eg, life insurance combined\n                                                       with investment management). The greater the complexity of products, the lower\n                                                       the willingness on the part of consumers to buy online. At the same time, the more\n                                                       insurers move to direct channels, the simpler their products need to become.\n                                                       Product portfolios will therefore begin to display a wide mix, as insurers create\n                                                       products that can be modularised and sliced and diced for different channels.\nIn the future, product factories will                  In the future, product factories will be able to directly access all the ingredients of\ndirectly access all the ingredients for                product design and will be much closer to the risk. These insights will be customised\npolicy design...                                       based on digital touchpoints, and inputs about exposure will be generated in real\n                                                       time. We think this will result in new business models being created purely around\n                                                       data access.\n                                                       New business models around data\n...including access to secure data.                    These business models will require secure access to data that can be combined in\n                                                       different ways, including insights from connected objects, platform providers,\n                                                       behavioural insights from the consumer and environmental data. Categories 1 to 10\n                                                       in Figure 25 show examples of data that will be available to an insurer. Some of these\n                                                       are newer data (eg, behavioural data based on customer context), while others are\n                                                       classical traditional data (eg, from claims and preventive services).\n   Figure 25\n   Illustration of a potential digital proposition and digital business models along the customer journey\n                                         Customer journey\n                         Behavioural &   offline\n     1)\n              contextual data insights   online\n                       Smart assistant\n     2)\n                      services insights\n                     Touch-points and\n     3)\n                  distribution insights\n                      Channel/market\n     4)\n                        places insights\n                           Aggregated\n     5)\n                          data insights\n                                                                                                                                          BM1\n                        Environmental\n     6)\n                          data insights\n                            Contextual\n     7)\n                      provider insights\n                                                                                                                                          BM2\n                  Sensor data insights\n     8)\n           (customer lifestyle cluster)\n                                          Health        Education/     Living      Free time   Mobility\n                                                         working\n                Insurance value chain\n     9)\n                   insights (ie, claims)        Product       UWR     Marketing &   Claims   Customer\n                                              management     pricing     sales    management  service\n             Preventive service/value\n     10)\n               adding service insights\n   Note: BM stands for Business Model\n   Source: Swiss Re Institute\n                                                                                                                     Swiss Re sigma No 1/2020 45\nOutlook\nData from multiple providers via open In the future, insurers will operate in an environment where they will need continuous\nApplication Programming Interfaces    access to different data sources. This is a strategic issue, taking insurers beyond their\n(API) may be combined to enable       existing value chain. No single firm or market place currently provides all these\ndifferent business models.            sources of data. While many data vendors focus on extraction and distribution of\n                                      data, few concentrate on data refinement.143 We expect that this will give rise to\n                                      specialised aggregators focused on integration and refinement. The more integrated\n                                      and refined the data, the wider the service offering to a customer. This has many\n                                      implications for the insurance industry, in particular the need for modular products,\n                                      personalisation and better distribution.\nInsurers may need to invest in        Insurers will need to decide if they are mere suppliers of coverage, or whether they\nseparate new data businesses.         are willing to collaborate or own other business models to gain control over the key\n                                      areas that impact their business. Different stakeholders will seek control over data\n                                      driven businesses. For example, if all telematics data were put onto one platform\n                                      anonymously, who would be most likely to own that market place? Will brokers stake\n                                      a claim? Players that cannot influence such data aggregators will find the going\n                                      difficult. If data brokers become omnipresent, the insurers dependent on them could,\n                                      over time, be relegated to the status of suppliers.\n                                      143 How data will shape the new urban future, Swiss Re Institute, 15 January 2019.\n46   Swiss Re sigma No 1/2020\nConclusion\nConsumers are more demanding than       The insurance landscape is changing as consumers seek more engaging and\never before. Using digital technology,  personalised purchase experiences, relevant to their lifestyle. Empowered with\ninsurers can respond.                   digitally-facilitated information and greater choice, consumers have become more\n                                        informed and self-directed than ever before. On the supply side, tools like AI enable\n                                        more effective customer interaction, allowing insurers to better understand\n                                        consumer preferences to develop customised and flexible product offering, with\n                                        dynamic pricing and servicing through (personal) virtual assistance, 24/7. New\n                                        sources of data also offer opportunities for more granular client segmentation\nBoundaries are increasingly blurred     The impact of digitalisation extends beyond the insurance value chain itself to the\nacross sectors.                         whole business ecosystem in which insurers operate. Industry boundaries are\n                                        becoming blurred as firms in several sectors build digital platforms that can connect\n                                        to different market places, supply chain hubs and financial networks. Non-insurer\n                                        participants in business ecosystems like manufacturers and telecom companies too\n                                        are gaining access to customer data, and digital analytics capabilities can enhance\n                                        their client product offering.\nLasting change will require more than   Utilised more fully and intelligently, new data and technology can reinforce and\nincremental thinking.                   secure the relevance of the insurance industry to future customers. The same is true\n                                        of leveraging cross industry ecosystems. This sigma depicts the end version of what\n                                        a digital insurer will look like. While high barriers to entry offer protection against\n                                        competitive threat to some extent, incumbents must continue to embrace both\n                                        incremental and sometimes more radical innovation to optimise the potential of\n                                        consumers touchpoints from both the insured and the insurer perspective.\nInnovation will help transform the      Full-scale disruption of existing insurers seems unlikely, at least in the near term.\neffectiveness and raison d’ étre of the Incumbents have time to adjust to the changing risk environment, shifts in customer\ninsurance industry.                     attitudes and accelerating advances in technology, but there is no room for\n                                        complacency. Successful insurers will be those that can leverage insights from their\n                                        investments, partnerships and collaborations to upgrade their business practices.\n                                        Forward thinking and innovative insurers could build on the new infrastructure being\n                                        created today to offer compelling risk protection solutions aligned with evolving\n                                        regulation and, in doing so, genuinely transform industry effectiveness.\n                                                                                                        Swiss Re sigma No 1/2020 47\nRecent sigma publications\n       2020 No 1  Data-driven insurance: ready for the next frontier?\n       2019 No 1  Emerging markets: the silver lining amid a challenging outlook\n            No 2  Natural catastrophes and man-made disasters in 2018: “secondary“ perils\n\t\t\t               on the frontline\n            No 3  World insurance: the great pivot east continues\n\t\t          No 4  Advanced analytics: unlocking new frontiers in P&C insurance\n            No 5  Indexing resilience: a primer for insurance markets and economies\n            No 6  Global economic and insurance outlook 2020/21\n       2018 No 1  Natural catastrophes and man-made disasters in 2017:\n\t\t\t               a year of record-breaking losses\n            No 2  Constructing the future: recent developments in engineering insurance\n            No 3  World insurance in 2017: solid, but mature life markets weigh on growth\n            No 4  Profitability in non-life insurance: mind the gap\n            No 5  Global economic and insurance outlook 2020\n            No 6  Mortality improvement: understanding the past and framing the future\n       2017 No 1  Cyber: getting to grips with a complex risk\n            No 2  Natural catastrophes and man-made disasters in 2016:\n\t\t\t               a year of widespread damages\n            No 3  World insurance in 2016: the China growth engine steams ahead\n            No 4  Insurance: adding value to development in emerging markets\n            No 5  Commercial insurance: expanding the scope of insurability\n            No 6  Life in-force management: improving consumer value and long-term profitability\n       2016 No 1  Natural catastrophes and man-made disasters in 2015:\n                  Asia suffers substantial losses\n            No 2  Insuring the frontier markets\n            No 3  World insurance 2015: steady growth amid regional disparities\n            No 4  Mutual insurance in the 21st century: back to the future?\n            No 5  Strategic reinsurance and insurance: the increasing trend of customised solutions\n       2015 No 1  Keeping healthy in emerging markets: insurance can help\n            No 2  Natural catastrophes and man-made disasters in 2014:\n\t\t\t               convective and winter storms generate most losses\n            No 3  M & A in insurance: start of a new wave?\n            No 4  World insurance in 2014: back to life\n            No 5  Underinsurance of property risks: closing the gap\n            No 6  Life insurance in the digital age: fundamental transformation ahead\n       2014 No 1\t\u0007Natural catastrophes and man-made disasters in 2013:\n                  large losses from floods and hail; Haiyan hits the Philippines\n            No 2 Digital distribution in insurance: a quiet revolution\n            No 3 World insurance in 2013: steering towards recovery\n            No 4 Liability claims trends: emerging risks and rebounding economic drivers\n            No 5 How will we care? Finding sustainable long-term care solutions for an ageing world\n       2013 No 1\t\u0007Partnering for food security in emerging markets\n            No 2 Natural catastrophes and man-made disasters in 2012:\n                  A year of extreme weather events in the US\n            No 3 World insurance 2012: Progressing on the long and winding road to recovery\n            No 4 Navigating recent developments in marine and airline insurance\n            No 5 Urbanisation in emerging markets: boon and bane for insurers\n            No 6 Life insurance: focusing on the consumer\nPublished by\nSwiss Re Management Ltd.\nSwiss Re Institute\nMythenquai 50/60\nP.O. Box\n8022 Zurich\nSwitzerland\nTelephone            +41 43 285 2551\nEmail                institute@swissre.com\nAuthors\nDr. Evangelos Avramakis\nJonathan Anchen\nAshish Dave\nAakash Kiran Raverkar\nBinay Biswal\nRajeev Sharan\nSophia Steinmetz\nsigma editor                               © 2020 Swiss Re. All rights reserved.\nPaul Ronke\n                                           The editorial deadline for this study was 10 November 2019.\nManaging editors                           sigma is available on Swiss Re’s website:\nDan Ryan                                   www.swissre.com/sigma\nDr Jerome Jean Haegeli                     The internet version may contain slightly updated information.\nSwiss Re Group Chief Economist\n                                           Graphic design and production:\n                                           Corporate Real Estate & Logistics / Media Production, Zurich\n                                           Printing: Multicolor Print AG, Baar\n                                           The entire content of this sigma edition is subject to copyright with all\n                                           rights reserved. The information in this edition may be used for private or\n                                           internal purposes, provided that any copyright or other proprietary\n                                           notices are not removed. Electronic reuse of the information published in\n                                           sigma is prohibited.\n                                           Reproduction in whole or in part or use for any public purpose is\n                                           permitted only with the prior written approval of Swiss Re Institute and if\n                                           the source reference “sigma 1/2020 Data-driven insurance: ready for the\n                                           next frontier?“ is indicated. Courtesy copies are appreciated.\n                                           Although all the information used in this sigma edition was taken from\n                                           reliable sources, Swiss Re does not accept any responsibility for the\n                                           accuracy or comprehensiveness of the information given or forward\n                                           looking statements made. The information provided and forward-looking\n                                           statements made are for informational purposes only and in no way\n                                           constitute or should be taken to reflect Swiss Re’s position, in particular in\n                                           relation to any ongoing or future dispute. In no event shall Swiss Re be\n                                           liable for any loss or damage arising in connection with the use of this\n                                           information and readers are cautioned not to place undue reliance on\n                                           forward-looking statements. Swiss Re undertakes no obligation to\n                                           publicly revise or update any forward-looking statements, whether as a\n                                           result of new information, future events or otherwise.\n                                           Order no: 270_0120_EN\nSwiss Re Management Ltd.\nSwiss Re Institute\nMythenquai 50 /60\nP.O. Box\n8022 Zurich\nSwitzerland\nTelephone + 41 43 285 2551\nswissre.com/institute\n","cleanText":"data driven insurance executive summary battle customer ready next frontier touchpoints digital insurer marketing distribution servicing product development underwriting claims management outlook conclusion executive summary digital technology direct change rapid spread internet enabled devices universal connectivity across insurance value chain changed consumer behaviours expectations across industries particularly among younger generations digital age also brought explosion heterogeneous data different sources platforms providers risk protection solutions use broaden reach boundaries insurability value chain becomes digitally connected insurers able better understand customer segments partners adapt near real time insurers become hyper aware short term digital insurance consumers likely young educated customer needs preferences higher levels income time innovative digital cover options consumers become increasingly available making income education less relevant factors purchasing decisions advanced analytics capabilities insurer future aware customer needs preferences provide personalised real time service flexible product offerings artificial intelligence ai used interact build understanding customer servicing personal virtual assistance meet customer expectations insurers able move away product focused sales approach one need adapt provide closely tied broader needs across life time customer greater coverage across life cycle stages focus human experience new generations systems deliver unprecedented levels proximity influence customers insurers go beyond mere channel management optimising interaction across diverse range customer touchpoints implications business models insurers interact customers nature services provide digitalisation also help create new time big data sophisticated models allow risk pricing increasingly underwriting portfolio risk granular level emergence new risks create new underwriting portfolio management techniques risk management techniques insurers create early warning systems gather practical insights prevent incidents simplify accelerate claims processing data enabled processes minimise friction streamline customer insurance journey request coverage claim digitalisation thus help improve customer experience also efficacy back office processes time using data digitalisation enable development new data driven business models multiple providers could drive new impacting entire insurance value chain access data capability business models model risks key true leverage come utilising assets key data supplier partners entry points ecosystems know generate customer insights insurers need decide whether suppliers coverage collaborate new areas business operation value propositions insurance technology could foster ongoing incremental industry change broadening scope affordability insurance alternatively potential radical transformation provision risk protection services households businesses grabs typical hurdles innovation overcome swiss sigma battle customer touchpoints digital customers expect different levels service second nature digital natives presents new world opportunity incumbent insurers still operating paper systems combining digital solutions advanced analytics deliver new insights allowing insurers provide deep holistic engagement across customer lifetimes digital interaction ' life digital technology already digital interaction become norm daily life across world transformed many industries... approximately billion people connected online use mobile devices spend several hours virtual world day new technologies digital facilities developed consumers including example location sensor based services smart homes cars factories developments cognitive systems artificial intelligence ai also creating innovation opportunities see figure including insurers figure new technologies impacting insurance impact business revolutionary * full life cycle * digital business * intelligent process api management technology platforms automation * iot platforms * digital experience platforms high * next generation * conversational * digitally engineered personal health platforms underwriting records phr * behavioural analytics * autonomous vehicles * electronic health record data extraction * genomics epigenetics medium * insurance wallets * digital advisors * administration * reward loyalty * advanced analytics management saas platforms solutions years years years years time impact * p c related technologies * sensing analytics related technologies * productivity related technologies * l h related technologies * customer interaction related technologies * customer loyalty wallet related technologies source swiss institute adapted gartner' hype cycle digital p c insurance connected commerce connectivity enabling lifestyle evolution nielsen november swiss sigma new technologies impacting engagement insight generation ... change insurers rapid spread internet enabled wearable devices ubiquitous connectivity acquire service customers enabling new ways communication information sharing led exponential growth volume digital data generated automatically cheaply non intrusively international data corp estimates billion iot devices around world generating data see figure new tools analyse data extract useful insights also proliferating change way insurers interact consumers figure forecast growth digital data cagr - real time data non real time data real time zettabytes non real time zettabytes source idc swiss institute task facing insurers upgrade consumer preferences buying behaviours also changing rapidly many online experience industries adopted customer centric business models insurers customers work hard keep customers loyal recent survey found insurers remain trusted source new risk coverage solutions consumers switching carriers often used open new entrants including insurtech big tech growth connected iot devices expected generate 4zb data international data corporation june insurers lead new era connectivity bain company september swiss sigma battle customer touchpoints digital insights life events new insurance opportunities insights data analytics boost figure shows cycles customer acquisition also involved customer retention customer retention digitalisation support cycles capturing insights end end consumer experience figure customer acquisition prospect customer experience existing customer experience loyalty lifecycle finding \"good\" customers maximising customer life time value advise advise purchase purchase evaluation use evaluation decision decision buy buy ie upselling service use etc n ive ex ega sit ce conside trigger pe tiv rie e po rien trigger conside nc e pe ration ie life event e ex ienc ie service use ration ba expde r exit research research customer acquisition cycle customer servicing cycle evaluating offers - continuous engagement - competitors put customer pays little attention little effort gain attention competitors offerings improves customer life time value source swiss institute triggering actions enable sources digital data also provide information changed new sales servicing opportunities customer' life eg family location job change see figure helps insurers better understand life events knowledge use develop personalised marketing strategies guidance next best actions predictive prescriptive individual customers entail cross selling risk mitigation value added services addition traditional insurance cover swiss sigma figure access life events enabled digital interactions data predictive aim sources predictive analytics detect problems occur prescriptive prescriptive analytics takes predictive analytics one step offering specific actionable next steps identify communicate solve issues brought next best action predictive data analysis customer online interaction social media interaction insurance event trigger insurance prevention value add life event trigger love kid pregnancy marriage kids leaving family pregnancy separation related events kid death first apartment emigration apartment house purchase life relocation house sale related events education self employment skilling workload post graduate eduction education job job change retirement related events hobbies interest food nutrition travel culture media commerce free time entertainment related events demand car bike mobility purchase sale daily commute business travel mobility ride hailing demand related events micromobility source https www proponent com predictive analytics vs prescriptive analytics source swiss institute swiss sigma battle customer touchpoints new actors insurers many however digital signals embedded environments insurers touchpoints naturally access least insurance low touch business example estimated buyers insurance us average information interactions provider per year data consumer actions readily available agencies instance data mobility behaviour owned mobility companies insurers operators smart home digital platforms know insurers' customers properties insurers urban area operation greater likelihood insights generated non insurers insurers need embed providers risk protection within broader ecosystems digital touch points consumers optimising interaction across multiple touchpoints insurers need access owned achieve insurers need optimise interaction customers across owned touchpoints multiple providers lifestyle areas like health education mobility leisure single journey customers switch touchpoints fulfil various information needs insurers need access insights across different types touchpoints owned owned paid earned social see arrows figure top performing insurers operate across touchpoints unified customer facing brand consistent messaging experience provoke recall figure dynamic personalised protection enabled digital touchpoint insights digitally augmented assistants sensors data sources customer journey context relevance digital augmented customer lifestyle interface health lifestyle clusters digital augmented education working assistant living dynamic personalised augmented protection free time consumers touchpoints owned mobility consumers touchpoints risk asset liability owned paid management earned social source swiss institute customers know want insurers listening bain company october swiss sigma owned touchpoints insurer invests controls consumer touchpoints include agents websites apps goal long term relationship building benefits cost efficiency control paid touchpoints insurer pays digital touchpoint reach customer paid searches sponsorships paid touchpoints include various channels like bancassurance agents earned touchpoints customer becomes touchpoint satisfied customers become social media influencers spread awareness social touchpoints insurer interacts via third party channels uses profile facebook twitter social media forecast fastest growing digital advertising channel globally next five years changing nature insurance touchpoints growth multiple touchpoints industry susceptibility disruption rising see figure expect insurer interactions - necessarily - increasingly begin cross conventional boundaries big tech digital ecosystems eg grab gojek ecommerce marketplaces changing touchpoints insurance context example uk four leading insurance providers retailers selling \"white label\" insurance insurers orchestrate digital physical ecosystems combining one offer services normally delivered different providers figure speed disruption across sectors insurance disrupted volatile current level disruption vs susceptibility future disruption current level disruption industry sectors - scale viability volatility disrupted comms media infrastructure transposrtation retail services high tech cg insurance software platform banking life sciences travel automotive energy utilities capital markets health chemicals natural resources industrial equipment machinery less disrupted durability vulnerability less susceptible susceptibility future disruption susceptible - scale less susceptible yet disrupted less susceptible less disrupted source v savic b moussavi good things come ' wait accenture swiss institute social fastest growing digital advertising channel globally forrester august bain company october op cit swiss sigma battle customer touchpoints front runners set outperform early mover strategy new technology could prove expensive medium term deliver strong returns longer run bcg mit survey found executives across industries yet see value ai investments made recent years activities like collecting curating data building knowledge base specific company training systems getting employees augment developing strong governance takes years rarely box solutions rapidly implemented figure demonstrates one future pay back scenario companies invest early burn cash years begin rapidly see benefits accumulated learning investments players adopting \"fast follower\" strategy worked technologies could find long term laggards figure relative changes cash flow ai adoption cohort change per cohort cumulative front runners absorb within - years followers absorb - laggards - absorb source jacques bughin et al notes ai frontier modeling impact ai world economy mckinsey global institute swiss institute mit sloan management review boston consulting group surveyed executives see winning ai pioneers combine strategy organizational behavior technology mit sloan management review october v mahidhar h davenport companies wait adopt ai may never catch harvard business review december swiss sigma digital consumer - today future millennials typical digital based sample three key markets digital insurance making insurance consumers today headway- us sweden china - today' digital consumers typically years old affluent educated digital generation generation born early 1980s late 1990s subsequent ones expect rapid access information clarity around value proposition proactive servicing key maintaining loyalty maximise customer lifetime value among clients average incomes higher income consumers afford insurance room growth lower education levels income segments value propositions need adapted sample markets younger consumers higher incomes education tend make online purchases low mid level incomes least china us china consumers incomes excess rmb use online tools buy health insurance including medical expenditure critical illness low income consumers rmb see figure along similar lines consumers doctorate degrees buy health insurance digitally compared technical vocational training seen us example median income consumers buying insurance life usd least college degree figure digital purchases health insurance products across income education levels china income level education level consumers using digital purchase methods consumers using digital purchase methods low income mid income high income technical bachelor master doctorate rmb rmb - rmb vocational associate degree professional training degree degree source swiss institute avanza swedish digital life insurer customers years old see annual report - sweden' satisfied savings customers years row\" avanza china majority signed alipay' digital critical illness product born post see online insurance china general reinsurance ag majority life customers millennials see happy birthday us life may customers us insurer lemonade see wininger \"lemonade' first quarter market\" lemonade com january majority zhongan digital p c insurer china aged insurers learn insurtech giant zhong accenture january chinese non life personal lines consumer perspectives swiss institute life may op cit swiss sigma battle customer touchpoints differences online buying behaviour exist across markets many use internet research internet used medium research l h insurance china insurance penetration online sweden however penetration online insurance purchases still purchases low uneven countries example online l h insurance distribution accounts less us18 china20 already sweden findings similar p c insurance sweden china around p c premiums written distributed online markets levels digital insurance purchases much lower despite internet used many research tool example spain use internet research l h insurance online penetration life insurance around p c premiums came online channels see figure commoditised lines motor online purchases higher consumers us consumers china28 consumers spain29 purchased motor insurance online figure online insurance purchases online distribution life insurance p c online distribution online distribution annual life insurance premiums written online distribution premiums written sweden us china spain sweden china spain source verdict china statistic press genre axco statista swiss institute chinese non life personal lines consumer perspectives swiss institute european insurance report swiss online distribution includes life health accident insurance united states special insurtech online distribution statista insurance china forty years reform opening china statistics press sweden distribution channels life axco co uk sweden distribution channels non life axco co uk \" tencent ant financial rushing china' insurance industry\" cbinsights com october european insurance report swiss spain distribution channels life axco co uk spain distribution channels non life axco co uk us insurance shopping study j power sigma - world insurance china growth engine steams ahead swiss institute ibid swiss sigma one reason availability online one reason low penetration online life cover could many insurers purchase tools limited offer tools buy online looking provider websites found spain neither biggest insurers insurtechs sell life insurance online directly consumers similarly china us large insurers sell life insurance policies digitally however sweden six biggest financial institutions could explain higher purchase penetration online purchases higher markets nevertheless equal access online purchasing facilities may necessarily lead low power concentration similar penetration across markets cultural characteristics based hofstede high individualism low uncertainty cross cultural index33 preferred leisure activities may also impact avoidance... comes comparing online insurance penetration general e commerce penetration use e commerce buyer penetration proxy online insurance purchases countries higher e shopping penetration also demonstrate higher online distribution life insurance one reason might consumers use internet often different purposes including online shopping feel comfortable going online buy insurance country specific cultural characteristics low \"power distance\" defined degree individuals expect accept power distributed unequally high individualism low uncertainty avoidance could influence adoption technology countries low power concentration flatter hierarchies people make decisions contrary hierarchical societies unacceptable disagree decisions superior individualistic cultures people prefer collect information direct sources internet collectivist cultures people rely subjective advice family friends finally countries lower uncertainty avoidance people feel comfortable trying new things including new methods buying insurers china anbong life pingan insurtechs bihubao hetai xiaoyusan us insurers include massachusetts mutual insurtechs ladder life life spot life ethos insurers sweden folksam scandia avanza nordea liv nordnet idun liv hofstede cross cultural index analyses national culture country based six cultural dimensions relative countries bellman et al \"predictors online buying behavior\" communications acm lee h et al \"consumer lifestyles adoption high technology products case south korea\" journal international consumer marketing vol see hofstede cross cultural index https www hofstede insights com models national culture matusitz et al \"power distance uncertainty avoidance technology analyzing hofstede' dimensions human development indicators\" journal technology human services ozbilen p \" impact natural culture new technology adoption firms country level analysis\" international journal innovation technology vol peterson et al \" influence national culture information technology product adoption\" amcis proceedings paper lee \" impact cultural differences technology adoption\" journal world business zakour cultural differences information technology acceptance proceedings 7th annual conference southern association information systems lee impact cultural differences technology adoption journal world business vol laukkanen \" uncertainty avoidance affects innovation resistance mobile banking \" 48th hawaii international conference system science garbarino e et al \" cultural differences uncertainty avoidance affect product perceptions\" international marketing review vol swiss sigma battle customer touchpoints every country unique combination cultural characteristics china' society highly collectivist high power distance indicating inequalities accepted yet chinese also appear comfortable uncertainty much like swedes could explain china high e commerce buyer penetration could count popular leisure activities consumers engage technology frequently daily lives free time also shop online noteworthy web surfing popular free time activity china ranked first sweden ranked third spain ... surfing web told believe insurance bought online consumers popular leisure activity easier access degree online insurance purchases grow faster markets certain cultural characteristics low power distance high individualism low uncertainty avoidance use internet preferred leisure activity end see opportunities online insurance purchases markets like sweden us uk germany china see figure figure culture leisure across markets factor sweden us uk germany china spain italy turkey high individualism culture low power concentration low uncertainty avoidance leisure activities e commerce consumption note digital e commerce buyers defined consumers use internet made least one purchase online preferred leisure activities includes web surfing source swiss institute li h et al \" impact perceived channel utilities shopping orientations demographics consumers online buying behavior\" journal computer mediated communication vol bellman et al op cit chen et al \"leisure activities leisure motivations chinese residents\" plos one vol en undersokning om svenska folkets tidsanvandning ar statistiska centralbyran time use survey instituto national de estadistica swiss sigma drives online purchasing behaviour access provided theoretical framework helps models based consumer value need theories help answer understanding affects question figure depicts theoretical model tailored online life insurance decision buy cover online buying behaviour consists three value drivers functionality emotions personal growth date insurance product design mostly focussed functionality little consideration emotional personal growth values figure schematic showing drivers insurance purchasing behaviour model functionality emotions personal growth ease purchase process low pressure empowerment convenience enjoyment epistemic value immediate fast purchase process painless process better price feeling relief feeling comfortability consumption behaviour source swiss institute consumers value simplicity want analysing consumer reviews two us digital life insurers ladder life emotionally fulfilled life find consumers buy cover online value functional also emotional personal growth elements eg empowerment commonly mentioned emotions \" pain\" feeling \"happiness\" \"delight\" said find buying online \"painless\" said feel \"great\" buy online another consumers said \"enjoy\" completing application purchase online nearly said feel lower pressure contact insurance agent enjoy products address personal growth values perceived consumers mostly relate empowerment personal growth values consumers value ability acquire information product prices terms able decide policy best fits needs one consumer stated instance \" great ... get information make decision \" empowered consumers longer passive shoppers dependent agent' advice active participants want information make decisions holbrook customer value framework analysis research routledge j sheth et al \" buy buy theory consumption value\" journal business research vol maslow \" theory human motivation\" psychological review vol \"reviews\" trustpilot com consumeraffairs com last accessed august consumer review quoted consumeraffairs com february p buhler p maas \"consumer empowerment insurance\" international journal bank marketing vol swiss sigma battle customer touchpoints insurers need better relate value drivers play important role understanding consumers' choice consumers higher level emotional behaviour future purchasing intentions example alibaba' critical illness attributes insurance58 lemonade' home insurance59 offer simple immediate purchase process also address emotional personal growth values see figure case alibaba consumers emotionally experience insurance community product since contribute pay someone ill personal growth value empowerment fulfilled allowing large group pre approved consumers decide collective voting system whether pay approved figure examples insurance products based three value drivers cases designing insurance based functional emotional personal growth values case case alibaba critical illness insurance lemonade home insurance functional value purchase process simple immediate via purchase process simple immediate via transaction app alipay lemonade app laptop tablet emotional value consumers experience insurance community customers choose non profit organisation product since collectively contribute leftover money monthly flat fee join payout someone ill community believes cause personal growth value empowerment preapproved consumers decide empowerment consumers empowered empowerment collective voting system deciding social cause want choose whether payout claimants leftover money approved source swiss institute based information company websites future ages attracted next three five years digital insurance consumer likely remain digital insurance millennials higher levels income education longer term millennials become seniors age groups attracted digital insurance income education less relevant factors purchase decisions new innovative digital cover options consumers lower income already available instance micro l h insurance products offered china life63 prosur64 jubilee also alibaba' critical illness product \"xiang hu bao\" attract consumers backgrounds irrespective income education levels j sheth et al \" buy buy theory consumption value\" journal business research vol holbrook customer value framework analysis research routledge \"alipay health plan aiming 300m users\" alizila com april \"instant everything\" lemonade com \" lemonade giveback\" lemonade com ant financial aims disrupt health insurance new chinese health 'collective' insurancejournal com april \"jack selling cancer coverage pennies month china\" bloomberg com may microinsurance rural development china q associate professor yi yao kitty peking university microinsurance center milliman march \"posur micro insurance\" prosur com kh \"micro insurance enabling people overcome uncertainty\" jubilee com august insurancejournal com april op cit swiss sigma digital insurer future insurer hyper aware customers needs preferences offer personalised flexible products services real time data enabled processes minimise friction streamline customer insurance journey request coverage claim artificial intelligence used interact build understanding customer servicing personal virtual assistance offering personalised social sustainable engaging digitisation provide timely digitalisation provide far timely feedback insurers leveraging range feedback loops daily interactions example across mobility health recreational behaviour feedback loops always existed insurance influencing purchasing patterns insureds' risk behaviours make claims feedback traditionally ex post sometimes meant long time lag balance autonomous insights empathetic care constant feedback based digitalisation goes beyond data collection also gives insurers power understand processes provide better insights risks dynamically insurers may discover uncovered exposures risk management... seek close see figure example combining traffic weather patterns factors insurers may determine driver spends majority time higher risk roads insurers taking first steps engaging customers empathetic manner based observed behaviours example generali real time coaching tool motor insurance clients helping drive better work accident prevention focus currently mobility services plans expand areas access risk insights across new touchpoints channel formats ... allow insurers propose digitalisation insurers obtain risk related data variety touchpoints innovative coverages quicker identify monitor existing new risk exposures new data sources analytical capabilities provide instant depth view risks across touchpoints facilitates quicker risk assessment allows insurers propose innovative relevant coverage pre digital insurers wait till loss data available assess customer adequately covered example someone stops working due disability insurers use data suggest ways help customer recover quickly see \"metlife hooks external data feeds help curb customer risk\" cio com october \" innovate customers\" generali com swiss sigma digital insurer figure insights improve existing value chain smart interactions smart mobility living workin smart g smart health active break start work park ride mobility end work breakfast outdoor family activity sleep shopping family risk related interactions digital risk related interactions identification monitoring existing identification monitoring new clients risk exposure future risk exposure reducing adapting existing protection gap generating new protection related products growth profitability adoption existing services value propositions new customers product uwr marketing claims customer management pricing sales management service retail business corporate business risk exposure insurance l h p c insurance l h p c protection gap source swiss institute swiss sigma shift product service orientation insurance thus become digital interactions enable insurers create personalised insurance new proactive protection related services traditional risk selection remain core activity faster simpler based real time data greater volumes digital data help quantify different kinds exposure eg behavioural external internal see figure behavioural exposure risk aberrant actions driver speeding failing maintain safe distance choosing dangerous routes internal risk exposure covers inherent risk insured asset eg quality construction property external risk exposure covers environmental factors like catastrophe risks property bad weather street conditions drivers figure representation ideal state risk quantification life health insurance property insurance automotive insurance classical lines business offline customer journey online data collection risk exposure external internal sensor behavioural identification risk exposure contextual risk exposure evidence based analytics decisions - timing - relevance relevant product service offering contextual content products services risk related offerings risk transfer risk mitigation preventive services value adding services partner community integration intervention access sharing steering support community partner network life area clusters health education working living free time mobility source swiss institute swiss sigma digital insurer ideal state still way key task insurers develop digital capabilities access relevant information changes risk exposures convert actionable insights package communicate customers user friendly manner insurers need upgrade digital capabilities across three areas identify true risk exposures data used risk exposure identification insurers need understand risk exposure evidence based analytics decisions based timing relevance true signalling power c reate relevant product service offerings insurance cover adapt changing needs ensuring cover appropriate across different life stages end insurers need able contextualise products services risk mitigation preventive services value adding services partner community integration intervention insurers steer support community partner network advanced analytics able identify customers likely buy based risk sub segments higher quality service lead insurers refine capabilities able provide higher quality greater loyalty service interactions lead greater loyalty example property insurer access analyse data changes risk exposure whether owned third party data enable safety security services prevent damage losses could also advise real estate purchases across hundreds relevant risk variables example one insurer exploring engage customers predictive indicators property public transit corridors number emergency calls made property hygiene nearby restaurants infrastructure quality area insurers emerging markets technology accelerating development insurance emerging markets leapfrogging peers advanced took many years development advanced markets compressed markets innovative areas years emerging territories \"leapfrogging\" technology steps skipped disruptive technologies facilitating convenient access insurance products services part local requirements working digital insurers emerging markets taking lead leveraging new digital ecosystems extend reach ecosystems offer multiple touchpoints capture user attention often evolve original businesses move new domains case grab based asia started passenger mobility entered food goods delivery expanding healthcare insurers asia forming partnerships firms insurer brings underwriting expertise partner firm entry point ecosystems allowing insurer extend reach target specific segments mine user behaviour along lines china aviva partnered internet major tencent sell critical illness policies online another example africa insurers selling insurers ride sharing drivers mobility ecosystems \"metlife hooks external data feeds help curb customer risk\" cio magazine october \"ping good doctor grab form joint venture deliver transformative o2o healthcare solutions southeast asia\" grab com august \" nairobi based ridesharing service little competes uber africa\" skift com june swiss sigma insurers use digital platforms modern flexible flexible modern platforms insurers leverage flexible product agnostic fully integrated digital platforms help insurers enter new markets engage customers buying journey core value chain components like underwriting transformed insurance service see figure example swiss partnering solution iptiq allows insurers harness power cloud applications data analytics make buying cover easier also connect market services platforms also act intermediaries connecting market services different different parties parties eg enabling brokers manage deliver cross border programs single online platform two types platforms established first core service platforms enable part whole value chain established industry service iaas - insurance service solutions second service platforms act intermediaries supply demand rare cases combination platform types may established figure depiction opportunities platforms insurance customers distribution primary distribution reinsurance distribution retro partner insurance partner partner cessions b2c retail b2b b2b b2b b2b b2b b2b corporate core service platform market service platform product uwr marketing claims customer management pricing sales management service service interaction platform services process platform value chain industry insurance banks technology platform enablement intermediaries aggregators platform management coordination governance brokers platforms ecosystems source swiss institute swiss partnering solution iptiq allows insurers harness power cloud based applications data analytics make buying insurance easier help people become insured see iptiq technology making easier buy insurance swiss november swiss corporate solutions announces collaboration build multinational insurance program management platform brokerslink swiss corporate solutions october starling' marketplace delivers 'app store' experience financial services ns banking february swiss sigma digital insurer insurers use expose benefit platforms insurers use apis integrate external platforms influence brokers partners eg ecommerce bring significant efficiency across value chain due partner allows seamless information exchange different components need analysis policy seamless customer journey across issue customized co branded distribution partners value chain choose products want offer consumers allows seamless customer journey across value chain virtue remaining single platform platforms multi device optimised desktop tablet smart phone multi channel ie customers switch face face agent self service even work agent call centre data saved shared value chain become integrated iterate quickly processes first wave digitalisation made insurance value chain efficient become flexible open see figure however multiple digital silos remain unconnected information innovation compartmentalised future critical processes connected insurers progress mere \"digitalised insurance providers\" \"digital insurers\" potential transformation end looking ahead data quality algorithms improve next generation ai insurers value chains \"learn\" data generated consumers ecosystems governments developments cognitive technologies help insurers integrate learning adapt value propositions real time thereby providing holistic unique customer experience figure insurance products growing comprehensive risk service involved stakeholder value chain example product uwr marketing claims customer insurance icon management pricing sales management service digitalised product uwr marketing claims customer icon insurance management pricing sales management service digital product uwr marketing claims customer icon insurance management pricing sales management service data ai driven icon product service insurance source swiss institute swiss sigma better data digital triggers availability high quality underlying data also make easier create accelerate adoption parametric products digital triggers speed adoption parametric insurance insurance property level flood insurance utilises data feeds sensors homes businesses parametric triggers already available basis risk much lower trigger based data source property time virtualise value chain lead products becoming parametric nature see smart contracts smart contracts smart contracts automatically concept smart contracts aligns idea ai driven insurer recognise terms constructed interplay multiple value chain components smart insurance policy met contracts self executing software programs instance make payment triggered occurrence risk event legal contract logic put computational logic string computer code recognises terms contract insurance policy met automatically transfers funds insurance payouts agreed time registers transfers however yet existing insurance contracts need translated computational logic far international regulation specific promising cases parametric insurance lines business smart contracts telematics used motor aviation cargo agriculture offer potential however certain challenges stand way wider adoption smart contracts notably yet international regulation specific smart contracts lloyds recently pointed unclear policyholders disagree automatic decision trigger cover defend challenges digitalisation regulation monitoring yet facilitating innovation regulatory framework play important role shaping integration priority regulators new technology insurance space monetising potential digitalisation insurers could face regulatory challenges data protection privacy providing incorrect advice records retention errors bias algorithms might contribute systemic risk prompt inappropriate insurance decisions also area regulatory scrutiny likewise might difficult regulators understand complex proprietary algorithm decided deny coverage reject claim undermining ability fulfil supervisory consumer protection tasks insurers could also face country regulators also wary unsolicited use data insurers also face country specific regulatory limitations specific regulatory limitations high implementation efforts product development modifications construct robust system data governance win customer trust abide regulation may additional legal compliance challenges integrating multiple vendors one proposition jba floodflash launch sensor based parametric flood cover first artemis june triggering innovation lloyds swiss sigma digital insurer ownership data willingness share consumer reticence share data data become easier collect increasingly important consideration varies country may ownership example ownership images properties surveyed drones may resistance contexts rest clients insurers consumer reticence share data varies country country may resistance contexts example employers offer workers boots gloves vests wearables found employees refuse monitored behaviour differs line business recent survey found us consumers willing share health related data home related data focus loss another survey found consumers cautious sharing data even prevention rather personalised know lead personalised experience however accenture premiums survey found eight would share personal data eg income location lifestyle habits insurers lowers risk injury loss suggests messaging around data sharing effective focus loss prevention rather personalised premiums data availability better ask fewer focused data easily available required depth detail insurers strike questions balance relevant data data easily misinterpreted background context fully understood example non smoker occasion buys cigarettes parent could identified casual smoker could subject higher premium rates buying health insurance data help confuse cases asking fewer focused questions could lead precise answers consumer wariness automation policyholders may less consumers may less comfortable automated world example comfortable digitisation surveyed consumers said worry mistakes filling insurance forms fear claims pay outs might accurately calculated machines policyholders may divided views digitalisation vital win customer confidence reassuring machine accuracy developing checkpoints help avoid mistakes \" exciting new ways technology streamlining claims management process\" risk insurance august u consumer survey connected devices insurance roadmap aite group may \"promise personalization little impact consumer willingness share data study reveals\" marketing dive august consumers willing share data exchange adjusted car insurance premiums based safe driving willing share data exchange life insurance premiums tied healthy lifestyle see six ten consumers willing share significant personal data banks insurers exchange lower pricing accenture study finds accenture march \"future claims customers still want humans process - many\" carrier management march swiss sigma marketing distribution servicing insurers moving away product focused sales approach one closely tied broader needs customer greater focus human experience new generations systems grant unprecedented levels proximity influence customers insurers able optimise interaction across diverse range customer touchpoints marketing become granular insurers use new data build historically insurance marketing aggregating prospective buyers consumer journey based human groups likely common needs respond similarly marketing typical experience attributes include geography demographics psychographics eg values attitudes lifestyles behavioural aspects eg price sensitivity approach leads relative broad spectrum consumers segment limit effective targeting new sources digital data life events purchase histories travel behaviour customer service records data devices wearables also social networks insurers employ advanced analytics techniques yield granular classification existing prospective customers see table table evolution marketing distribution innovation timeline marketing distribution journey past present future key enablers customer experience fragmented limited multichannel somewhat seamless continuous platform economy channels integrated insights available market size sentiment time consuming tech based still takes tremendous efficiency online surveys digital analysis physical time accuracy gains behavior personalisation possible possible limited scope complete customisation shorter development expensive time data insights channel evolution agent driven door direct online contact ecosystem based chatbots machine door center bots tech virtual connect learning enabled agents fulfillment exhausting time direct home needs based \" consuming time\" source swiss institute sigma life insurance digital age fundamental transformation ahead swiss december ibid swiss sigma marketing distribution servicing smart interaction future marketing boost customer loyalty personalisation improve loyalty management impact customer lifetime value cltv historically focus insurer engagement customers sales claims new data sources advanced analytics insurers interact customers lifetime wide range relevant issues developing systems help listen engage act proactively reactively see figure figure schematic showing smart interaction increases loyalty customer equity without limited interaction management intensive interaction management cltv segment cltv segment customer journey customer journey invoice mailing claim potential historical focus listen engage act sales distribution claims cltv cltv cltv cltv owned owned proactive reactive paid paid earned earned social social value customer content collaboration content collaboration interaction communication community communication community insurance products services source swiss institute lifestyle related interactions help high quality interactions loyal customers build deeper one one relationships end insurers pro actively use digital engagement reduce policyholder churn scale example recently major p c carrier ran three month pilot sending personalised automated texts delinquent customers followed phone call creating new level proactive digital human engagement less customers opted large majority delinquent customers retained less manual work approach yielded results fewer outbound calls insurers broaden scope interactions policyholders wider sets prospects example insurers decoupled insurance telematics apps app available drivers rather insureds alone customer lifetime value cltv measure net profit attributed entire future relationship strategy meets action proactive digital engagement reactive proactive paradigm shift insurers hearsay systems august insurance innovation month aig go efma january swiss sigma distribution traditional touchpoints adapt capitalising unique moments digitalisation insurers better able identify human interactions key increasing loyalty play significant role capitalising unique moments key increase cltv customer lifetime value currently digital interactions mostly transactional post sale interactions focused billing claims handling miss opportunities insurers address full range customer needs insurers need understand empathy important component brand value offering unified customer experience automated data capture synchronisation customer relationship management crm systems offer actionable insights example agents alerted customer files digital notice loss claim use event opportunity call day better understand life events eg baby way appropriate within contextual setting cross sell intermediaries role change agents act risk consultants customers become comfortable buying insurance online role fuller view customers life events intermediaries evolve agents act risk consultants overview ' happening customer across value chain insurers agents also need rethink traditional cooperation models including intermediaries compensated example client acquired online may require advice agent compensation model rewards agents services via advisory fees consumers willing pay may help alleviate channel conflicts could arise within pure commissions based system commercial small medium new distribution technology impacted wholesale commercial insurance sized business also becoming markets much retail personal lines however initiatives open digital distribution simplify parts value chain commercial insurance lloyd' london example mandated syndicates use electronic placements less written risks end online brokers small medium businesses sme also seeing traction instance embroker coverhound use data verification technology obtain immediately bindable quotes allowing customers complete process almost seamlessly online platforms offer additional services eg help smes upload compare policies generate vendor certificates asset tracking strengthen customer loyalty insurance next gen customer experience - dreamforce case study hearsay systems october sigma op cit \"lloyd' issues electronic placement mandate\" lloyds com march see embroker coverhound swiss sigma marketing distribution servicing digitally augmented channels rise digitally augmented channels new digital interactions driven next generation empathetic advisory tools combining man machine augmented ai see figure agents brokers leverage refine become common communication adapt customer' emotional situational personal contexts power combining functional empathetic interfaces functional interface performs analytics provides intelligence next best action agents brokers provide human empathetic interface figure data ai driven augmented advisors retail customer commercial customer intermediaries provider smart augmented platform ecosytem bank platform broker 3rd party insurer reinsurer marketplace aggregator digitally augmented empathic human interface interactions digitally augmented digital interface source swiss institute big tech intermediaries insurers believe use digital advisors combine two capabilities invest different interaction drive digital interactions targeted manner future generating channels intelligence become core capability insurers depending provider could three types levels digital interaction channel could work co existence although one perceived consumers trustworthy unbiased likely hold distinctive competitive advantage \"direct\" customer advisors insurer agnostic offered technology companies example could enhanced versions today' siri alexa google duplex assistants \"intermediary advisor\" feeds intermediaries better insights back office responsibilities minimised agents focus human relations empathy augmented better risk insights data \"product advisor\" fully insurer owned example geico' mobile virtual assistant kate intuitively guides customers relevant information policy helps self service tasks customers may use three set ups example l h insurance may go intermediary home insurance direct insurer mobility travel insurance use enhanced version customer advisors like google assistant intermediaries continue play important role although human interaction may cases become less part standard process human involvement added value emphasis contextual \"empathic interactions\" new competitive landscape insurers need balance customer interactions right manner using new loyalty interaction systems swiss sigma servicing loyalty new risk management needs additional digital tools value servicing building deeper relationship customers scale added services help brokers push risk prevention value adding services vas especially critical distribute customers commercial lines specialised knowledge leveraged whichever party along value chain possesses specialised knowledge provides robust engagement control servicing significant scope insurers enable agents brokers digital tools value added services j power measures satisfaction scores among business relationships finds insurance agents currently report low satisfaction service receive personal lines commercial lines insurers advent digital engagement servicing several implications insurers boundaries traditional insurance becoming blurred distribution moving towards integrated ecosystem focusing end end buying experience survey accenture found three five insurers forming relationships non traditional partners reach customers new ways create new value impact aspects value chain fully realise benefits partnerships insurers need upgrade predictive analytics capabilities obtaining risk related data variety touchpoints providing tailored content customers want positive experience across multiple channels touchpoints gartner believes coming five years event triggered real time marketing make biggest impact across industries however loyalty hard come digital customers still give insurers lower loyalty scores multi channel customers misalignment customers expect digital channels insurers provide example insurers fail replicate certain service components contact us page web mobile sites shifts toward platform plays ecosystems appear inevitable personal commercial lines e commerce firms retailers automotive oems non traditional players developing owning offering new unique product propositions customers build platforms based plug pay principles allow third party connections insurers need cognizant different elements value chain different customer touchpoints srikanth madani hypotheses innovation direction insurance st gallen risk finance \"insurers come short independent agents despite critical role agents play driving business j power finds\" prnewswire com january accenture technology vision insurance accenture april gartner identifies four emerging trends transform marketers run technology ecosystems gartner august bain company october op cit gartner says u insurance brands underperform digital despite customers' growing willingness provide data gartner july swiss sigma product development underwriting bikes trainers customers like tailored rather shelf products become increasingly prevalent insurance time big data sophisticated models allow risk pricing increasingly granular levels emergence new risks create new underwriting portfolio risk management techniques digital enabled product development real time demand assessment digitisation enable direct increasing digitalisation present opportunities connect directly touchpoints customers customers assess product demand real time rather rely solely real time demand assessment channel partners see table example app surveys tracking search behaviour provide clues unmet customer needs based granular customer data insurers precisely segment customers develop tailored products moreover timing product searches correlated personal data may provide context product demanded constant feedback help insurers develop new products also refine adjust existing ones table impact digitisation product development journey product development journey past present future key enablers demand assessment time consuming efficient based real time based online surveys digital dependent channel limited customer data changing customer life behavior tracking partners collection stages coverage design fixed coverage longer partially modular completely modular hyper connectivity duration coverage shorter real time coverage enabled sensors 5g duration lpwan95 satellites pricing fixed pricing defined short period pricing dynamic pricing based risk categories based limited real time behavioral behavioral data data partnership strategy mainly distribution extended partnerships ecosystem partnerships apis96 cross industry based partnerships across insurance value driven cross industry ecosystems chain value chains regulatory approach finite experimentation sandbox approval wide range sandboxes due lack data experimentation data risk modelling innovation hubs data based evidence customer feedback almost non existent limited feedback based continuous feedback end end digital mainly claims multiple touchpoints delivery products experience source swiss institute k cool c angoulvant et al \"zhongan' micropremium model future insurance \" knowlege insead edu may low powered wide area network low bandwidth wireless technology covering wide areas interface communication protocol allows two software programs talk swiss sigma data driven granular insights facilitate new risk covers digitisation enabled coverage present digitalisation drives modest coverage customisation products customisation preserving base incremental innovation preserves basic product structure using short time product structure scales price short duration covers add ons provide coverage carve outs solutions either white labelled written directly insurers covers fixed shorter durations products may use sensors track asset usage movement individuals activate coverage offer minor premium credits information widely used real time underwriting pricing real time risk data facilitate sharing economy modularisation real time coverage adjustment innovative insurance solutions enable new products adapt frequently changing risk profiles modularisation means de coupling coverage services suit changing needs customer personal risk exposures tracked modelled easily insurers confidently de couple coverage elements within existing monolithic products commercial risks complex change frequently digitalisation help coverage coupling across products reduce protection gaps achieve price efficiencies dynamic environment product life cycles get shorter speed market crucial better data quality facilitating innovative products new risk pools emerging granular data construction new risk indices collection analysis enabled digitalisation example see first solutions creating new markets emerging personal cyber use cryptocurrency exchanges iot infrastructure risks better data quality also facilitates construction new risk indices parametric products covers protect economic impacts infectious disease outbreaks use pathogen sentiment indices digital sources gauge public fear behavioural changes measure cost epidemics future think algorithmic risk products become mainstream assets processes becoming autonomous intelligent toffee insurance trov element examples agencies providing pay go insurance via online portals metromile lemonade licensed insurers usage based products traditional asset insurance mandated clear commercial personal insurable interest however classification blurring sharing economy allowing individuals put personal assets commercial use via online platforms see mobility ecosystems striving towards seamless interface customers swiss current average speed market eight months new life annuity products four months modifications property casualty insurers seven months new products three months modifications see speed market life annuity insurers novarica march speed market property casualty insurers novarica march bitflyer mitsui sumitomo selling insurance users cryptocurrency munich relayr developed customized insurance products facilitate iot infrastructure investments andrew w singer \" evolution parametric insurance\" rmmagazine com april swiss sigma product development underwriting dynamic risk pricing drive behavioural change pricing based granular data insurers able micro segment risk pools thus accurately reflecting risk generate premium efficiencies unit within insured pool example zurich insurance spain partners mould consumer behaviour klinc offer coverage turned personal devices items catalogue products plans expand categories like auto home enabled availability granular behavioural data advanced modelling pricing may change dynamically based changing risk profiles going forward cross industry platforms seamless insurance portability integrate risks exposures covers insured universal insurance policy policy customised according insured adapt coverage real time basis change insured' risk profile see figure figure tech enabled insurance product service opportunities shelf coverage digitalisation enabled personalised coverage risk levels risk levels policy period policy period static pricing level exposure inefficiency insurer coverage pricing insured risk profile insured risk profile coverage pricing inefficiences insured coverage exposure efficiences insurer insured zurich' innovation efforts recognized global innovator gold award klinc takes top award products services zurich june absence granular data insurers use proxies driver age type car gender assess driving behaviour products like pay live health insurance pay drive auto insurance rely dynamic pricing short period pricing based limited behavioral data finer version static pricing swiss sigma tech enabled insurance product service opportunities cont modularisation global insurance coverage retail business travel travel cyber cyber liability liability home home mobility mobility year family adult two kids natcat life natcat life health health exposure inefficiency insurer modularisation global insurance coverage commercial insurance cyber liability cyber liability marine marine fire fire smes mid large corporates natcat natcat engineering engineering business interruption business interruption risk exposure risk coverage price insurance protection gap insurance coverage note green risk exposure line circular charts indicates insured' dynamic risk profile revealed processing granular insured data segments blocks chart indicate extent coverage modularized across exposures different risk areas eg cyber fire etc source swiss institute swiss sigma product development underwriting extending insurance reach preventive value added services insurers may need focus insurers strong distribution networks brand ability adapt meet preventing accidents improving customer needs eg simplifying tasks offering outcomes reduce pain improve quality life gain could develop new relationships based trust value add reinvent risk protection value add preventive service providers help counter decline premiums caused reduction loss incidences due various safety features enabled technology prevention could contain future claims value add services build loyalty services see table grow sophisticated insurers harness network effects within cross industry ecosystems also increase customer stickiness increasing touchpoints wide range services could commercial lines insurers could partner providers technology monitor provided part overall industrial assets improve efficiency safety early warnings example one insurance package insurer offers free online tool helps smes assess risk cyberattacks become proactive security personal lines insurers enable access technologies coping mitigating morbidity example italy axa prototyped cancer profiling service allows patients advanced metastatic tumours access personalised cancer care south african insurer discovery service provides additional pay outs insureds stricken permanent disabilities help adjust new way life table examples value added preventive services service type auto property life health preventive services safe driving alerts remote monitoring online diagnostics medical consultations maintenance rewards alerts help finding doctors screening anti theft flood monitoring scheduling counselling breakdown alerts home basements appointments behavioural geo fencing eg alert automatic shutdowns assessments driven outside safe gadgets fires neighbourhood value added services assistance buy car property security meal fitness digital health records concierge services advice vouchers fitness club alert vehicle facility maintenance retirement financial memberships moved towed hit emergency repair planning discounts parked services healthcare services source compilation value added preventive services swiss institute digital ecosystems extending boundaries value creation insurance swiss \"cyber insurance\" armourinsurance ca efma accenture reveal winners innovation insurance awards accenture june purple life plan discovery swiss sigma digitalisation underwriting digitisation enable forward digitalisation external internal data improve risk selection pricing looking underwriting automation streamline manual submissions triaging binding see table overall developments reduce underwriting costs loss ratios free time underwriters engage softer aspects business ie negotiation relationship building also developing repositories digitised risk information enable efficient underwriting decisions based limited claims experience data example helvetia insurance enables logistics companies purchase transport insurance customers straightforward online app less two minutes table impact digitisation underwriting value chain value chain past present future key enablers submissions lengthy proposal forms proposal auto filling fraud detection advanced text human interactions third party data behaviour analysis speech analysis ai data collection chatbot interaction spotting dishonest ml post submission analysis limited data collection disclosures complete efficient information data collection storage systems triaging delays due inefficient better quote market submission ranking intelligent business workflows time speed lobs due based conversion workflows ai nlp consuming decisions auto referrals probability auto better readability referrals appropriate authority suggestions capacity digitized information management ocr risk assessment manual time consuming usage third party data evolution risk digitisation asset limited point external risk features foresight activity biological data underwriting automated report frequent touch points text image analysis interpretation real time sensor data processing sensor data coverage pricing coverage governed flexible cover real time coverage real time risk modelling strictly insurability pricing lobs pricing adjustments capacity sharing p2p pricing based driven data asset seamless cover digital risk consortiums static risk matrix usage individual portability across connected sensors movements insurers binding delayed binding due reduction binding instant binding submission data entry paper time errors due basis end end triaging based information higher flow digitised digitised submission multiple systems submission data risk assessment data binding process source swiss institute submissions triage routing become automated efficient submission level accuracy increasing digitalisation geographical personal asset information much improved availability leveraged insurers auto fill proposals offer risk scores even identify granular customer data honest brokers already pin code license plate numbers reveal location specific hazards vehicle details business registration numbers linked municipal data used provide occupancy previous loss history commercial clients example submissions chubb' business helvetia transforms logistics freight companies insurance professionals helvetia october berkshire hathway guard insurance obtains needed information based data available online offline reduced time submission quote see \"berkshire hathaway guard insurance companies partners planck create full digital underwriting commercial lines\" prnewswire com march swiss sigma product development underwriting owners policy smes flow straight without human intervention chubb' part future intelligent automated workflows triage routing effective current business rules risk assessment underwriting cost effective engaging digitisation reduce cost availability digitised risk information eg biomarkers activity data building insurance turn risk assessment footprints occupancy vessel vehicle data create risk profiles without physical early warning tool risk inspection medical underwriting requirements see figure data sets especially useful geographically scattered assets individuals complex proposals may still require traditional risk assessment intelligent workflows automatically assign tasks suitable risk engineers doctors natural language processing enable underwriters interpret results faster figure different levels underwriting maturity data ai module uw evidence risk data SS learning learning data modelling insights uw models inputs underwriting effort required customer decision outcome dynamic underwriting self optimizing ai driven automatically - social media data underwriting algorithms assign risk class - iot data fitness home using parametrised indexes - activity lifestyle accessed platforms data mobility - minimal customer input - electronic health record accelerated underwriting insurer may request '' - - prescription history either third parties assign risk class - credit data customer additional data - driving records needed telematics - claim experience 'yes' - machine learning expertise additional underwriting traditional underwriting underwriting decision solely data sources - proposal based customer inputs data input emerging - tele interview inspection report lab test physician statement customer currently available sources data increasing data emerging complexity source swiss institute andrew g simpson \"insurtechs take note chubb' digital marketplace serves agents day\" insurancejournal com february swiss sigma dynamic underwriting provide standard accelerated underwriting approaches differ speed mutual benefits insureds static evaluate risk time underwriting track future insurers alike behaviour either positive negative results price inefficiencies policyholders insurers life health greater availability quality electronic health records ehr fitness social media data allow dynamic underwriting provide predictive insights state health information insurers evaluate change risk factors ie smoker status bmi blood glucose blood pressure cholesterol time predict claims probability external data helps digital risk smes mid corporates diverse business activities many exposures assessment sme risks even sometimes small justify complete risk assessment insurer niche products reason insurers leverage third party data sets auto fill proposal forms offer risk scores example online reviews related small businesses used proxy operational risk levels insurers use social media improve customer experience tracking changes sme payrolls office premises revenue indicate growth retrenchment brokers refine target strategies others leverage digital footprints deliver niche products like trade credit insurance smes tracking live financial data cyber new tools ingest analyse data generate cyber risk scores smes less two minutes digital underwriting large large corporate risks require bespoke solutions critical underwriting information corporate risks less advanced risks yet sufficiently digitised underwriting approach case based digitalisation geo spatial information assisted large corporate underwriting extent example insurtechs provide property risk engineering solutions insurers especially assess natural hazards another case cargo sensor data used provide enhanced marine coverage remote risk inspection enabled allowing risk engineers transmit live feeds underwriters full value digitalisation dynamic pricing realised live data industrial control systems facility monitoring systems integrated underwriting systems \" move static data live data insurance\" digitalfineprint com july cyence small business guidewire com insurtech impact oxbow partners smart cargo cyber insurance covus insurance see info corvusinsurance com \"virtual lets see\" virtualitechnologies com swiss sigma product development underwriting pricing coverage determination accelerate risk based price coverage automation submissions risk assessments may trigger faster determination automated based better coverage better pricing automated price coverage determination assessment risk data common personal l h retail p c business several examples beginning emerge china ping claims nearly million applications insurance received auto underwritten ai continued testing focused scalable replicable use cases inform insurer strategies next years mid market commercial insurance segments still require significant manual effort due underwriting complexity price sensitivity valuable wide corporate health insurance automated pricing coverage determination variability costs similar improving sector struggles high costs wide variability treatment procedures costs similar procedures using big data modern tech stack unitedhealthcare bind demand health insurance start ups designing better health coverage plans big employers lower costs better match risks employees identify top optional procedures widest range cost treatment effectiveness members opt options need customers pay base monthly premium much less options employer offers fraud detection managing moral hazard become critical underwriting fraud risk digital world underwriting decisions large volumes insurance applications digital world decisions needed need made near real time insurers careful filter fraudulent near real time applications build wrong customer portfolio successfully using new sources data prevent high risk profiles entering portfolios example online insurer inshared achmea initiative fully automated risk assessment determines risk point purchase using multiple indicators person' conduct payment risk claim risk insurers look innovative digital insurance becomes widespread insurers look new technology help identify high risk approaches help identify high risk cases one five people admit cases lying insurance claims applications counter insurers experimenting new digital tools reduce fraud underwriting stage eg utilising facial biometric tools leverage machine learning computer vision technology another example using third party data insurers may able determine - without medical evidence - whether customer smoker order avoid expensive medical tests cases adverse selection could increase spread digital data insurers also susceptible rising risks insureds withhold new anti selection adverse selection consumers build understanding information insurers health different healthy living apps genetic tests cheaply available direct consumers dtc testing devices available market advances bring equal promise risks including diagnosis subsequent unwarranted treatments major challenge insurers obtain adequate risk relevant information underwriting process since existing regulation mostly enacted widespread adoption dtc genetic tests announcement audited results year ended december ping susan morse \"insurtech entrepreneurs lead panel health \" healthcarefinancenews com september insurers control risks underwriting celent com lying insurance application www finder com october sonar new emeging risk insights swiss institute may swiss sigma underwriting implications insureds insurers constant feedback based processes product development underwriting increasingly interact dependent provide better insights new customer facing functions example marketing models integrate data products obtained initial underwriting experience customer feedback external data make targeted pre approved offers cover right time based specific customer needs behaviours offers focus increasing existing cover suit needs also cross selling new products match new risks however small thereby increase engagement direct impact new product development implications insureds insurers insureds large corporate risks placement negotiations involve underwriters visiting explaining technical aspects clients underwriting never truly customer facing expect modularisation products dynamic pricing adjustment bring higher understanding underwriting risk assessment retail customers example life sme risk scoring apps make opaque cumbersome underwriting process consumer friendly transparent underwriting bundled proactive risk management value added preventive services change insurer customer relationship risk transfer risk partnership insurers insurers benefit dynamic accurate risk pricing need upgrade systems process terrabytes data coming every day sensors many insurers still ironing legacy system challenges also insurers need become responsive digital feedback product development tests experiments digital world offers immediate sometimes ruthless feedback instance one demand digital insurer saw massive drop conversions discovered caused small change process flow requested small segment customers k harris ferrante cio agenda insurance industry insights\" gartner october khusid \" built customer feedback loop works\" medium com january swiss sigma claims management insurers using advanced analytics machine learning create early warning systems gather practical insights prevent accidents simplify speed claims processing examples include using ai detect verify accident hot spots estimate repair costs identify potential fraud historically claims processing form filling exercise digitalisation help improve customer experience efficiency back office processes modular approach key digitalisation improve claims experience terms trust lower friction end end claim digitalisation customer experience complexity back office operations survey adults online us expressed confidence insurer would treat fairly event claim modular approach enable end end digitalisation ongoing claims management processes see table example usaa launched member innovation team digitise components claims operations reduce full claims cycle days even hours cases table claims innovation timeline claims journey past present future key enablers fnol complex forms customer uses automatically iot satellite imagery first notice loss require explanation phone post online satellite weather weather stations iot etc mobile apps stations etc claims admin manually insurer staff data entry insurer digitally augmented connected systems staff support complex intelligent algorithms situations ai advice data gathering rudimentary fraud combination manual automated using computing power fraud detection analytics models identify analytics like big data access varied data fraud ai sources claims estimation manual claims mix manual inspection automated claim computing power better adjudication advanced adjudication algorithms technologies settlement reimbursement via bank direct vendor garage instant payout linked payment systems cheques drafts hospital automated validation intelligent algorithms reimbursement options eg replacement second hand source swiss institute forrester analytics consumer technographics(r) north american financial services survey forrester q4 us inside usaa' claims innovation team dig august swiss sigma end end digitalisation claims management digital loss prevention loss prevention important loss prevention important arguably undervalued aspect insurance undervalued aspect insurance value chain partly due lack research understanding value chain capital intensive nature investing loss prevention technologies requires significant investment possibly little short term impact claims reduction fast growing internet smart across lines business becoming feasible financially technology phone penetration made loss wise alert policyholders potential threat damage life property prevention action feasible instance receiving notification meteorological departments sensors catastrophe insurers send push safety warning notifications customers' smartphones help insureds take preventive measures mitigate damage figure illustrates insurer use external internal systems alert insureds potential threats figure illustration potential digital prevention process natural catastrophe event system integration build platform real time application finding data sources apis nearest safe zone policy holders sensors lower claims routes case evacuation insurers systems consumer takes meteorological capture process steps prevent government agencies prioritise loss life information property notifications real time gis systems alerts sent situation policy holders monitoring cloud based network external systems insurer systems source swiss institute digital first loss notification new insights enabling automated significant innovation first notification loss fnol processes risk management processes via digital given iot sensors high speed internet wide range automated data capture fnol technologies drones satellite imagery data shared range audiences instance csaa insurance partnered owlcam video first notice loss vfnol send videos driver' mobile phone car crashes broken driver share police csaa insurance group owlcam partner accelerate insuranceclaims process customers utilizing video technology csaa insurance group june swiss sigma claims management insurers initiate contact insurers pro actively initiate contact customers offer help even policyholders even claim claim reported pzu largest insurer central eastern europe reported launched call service operational coordinators actively scan digital analog information news loss events web emails calls witnesses tv radio identify customers could potentially impacted loss event verify scope customers' insurance offer assistance services covered policies customer register claim first contact later convenient date claims administration ai applications accelerate phase claims journey case analysed based information customer next stage received fnol eg checking coverage details screening fraudulent claims process claims ai assistance digital assessments automatically verify coverage details like sum assured deductibles co payments past claims reduction manual touchpoints speed process significantly also lower costs associated manual inspections example farmers insurance adjudicates claims using photos video estimating technology total loss auto claims aims expand capability property well fraud detection advanced analytics improve fraud insurance fraud costs billions dollars year traditional fraud detection detection... statistical models drawbacks least sampling methods analyse data lead undetected fraud also methods rely experience new type fraud occurs insurers vulnerable big data ai new tools combine thousands variables moment claim filed identify incoherence data relating claim wider context many insurers use social media analytics detect fraudulent individual claims example allstate leveraging online information identify fraud faster like claimants say injured work yet engage strenuous activity horseback riding analytics help uncover large frauds flagging certain suspicious events like smaller medical clinics originate large volumes claims individual insurers may lack sufficient data detect mass fraud law enforcement agencies co ordinate among carriers analytics know uncover patterns eg search comparable bills coming parties digital loss assessment claims estimation settlement ... also help automate loss digitalisation increase transparency dealing customers provision assessment comprehensive overview calculation could also prevent litigation recalculation costs moreover automation claims assessment help reduce administration costs making insurance affordable california wildfires usaa used images drones settle many home insurance claims day previously could take weeks even months team safely mobilised inspect adjudicate loss property damaged fire interesting experiments emerging example allianz ran pilot amazon enable claims handlers make offer day customer makes claim assessing content loss based live amazon prices pzu successfully built culture innovation efma february \" insurers digitizing claims apps photos videos e payments \" insurancejournal com june \"allstate finds fraud faster innovative data company\" prnewswire com november sigma advanced analytics unlocking new frontiers p c insurance swiss institute august inside usaa' claims innovation team dig august allianz replacement solution - allianz france efma swiss sigma wider adoption figure graphical representation touchless claims settlements work touchless loss assessment claims parametric insurance policies claims personal lines small business settlement personal small insurance largely automated enabling carriers achieve straight business insurance... processing rates reduce claims processing times days hours minutes motor insurance use ai assisted assessment significantly reduce time cost loss assessment ping employed ai settle motor claims across spectrum activities including first notice sharing digital pictures loss assessment document handling surveys completed within - minutes daytime accidents using facial recognition payments made seconds figure touchless claims settlement smart smart smart claims smart parametric insurance distribution policy trigger payout flight delay travel buy platform earth quake digital agent buy broker tropical cyclone direct digital buy mobile agri culture agriculture buy ecosystem source swiss institute ... early experimentation insurers experimenting new claims technology life insurance metlife life lines piloted ethereum blockchain pilot known lifechain help families determine deceased protected policy obituary placed press family automatically informed metlife programme participate lifechain encrypts deceased' national id number searches matching policy match found automatic notification initiates claims process case accident policyholder needs take photos streaming videos vehicle damaged area upload insurer' app together description damage accident \"china' ping p c unveils credit based smart auto insurance claim solution\" insurancejournal com january \" make faster life insurance blockchain \" medium com august swiss sigma claims management claims implications insureds insurers processes become flexible future greater use made characteristics claims submissions open innovation better inform product pricing underwriting decisions insurers generate data leads better risk assessments policy underwriting using real time claims data insurers also able better monitor partners customer segments offer growth potential adapt marketing distribution initiatives real time iterate quickly processes need flexible open innovation insureds quality claims experience strong impact loyalty renewal behaviour property claims satisfaction study jd power \"highly satisfied\" claimants said would definitely renew policies hand claimants \"indifferent\" \"displeased\" said would shop new carrier within year insurers built customer experience digital channels development units introduce design skills thinking methodologies key areas redesigned services processes putting customers' shoes insurers loss prevention become bigger theme claims management insurers combine claims behavioural data action loss prevention measures different components value chain digitally share information feedback could compress value chain digital distribution platforms connect directly capital providers without depending traditional capabilities primary insurers example providers parametric insurance may dispense large claims departments assessors except cases suspected fraud another example global insurer used insurtech program platform transfer part usd million layer parametric risk multiple risk markets satisfaction property insurance claims surges even number catastrophes reaches year high j power finds j power march innovate customers generali op cit \"tremor places world' first programmatic parametric swap transaction\" artemis may swiss sigma outlook digitalisation enable development new data driven business models impact entire insurance value chain access data capability model risks core success true leverage collaboration assets key supplier partners entry points ecosystems know generate customer insights refining existing developing new competencies insurance value chain benefit recent years premium growth slowed insurers looking new data analytics variety approaches differentiate products services within rapidly expanding new economy sectors see figure figure new capabilities established business transformation mindset culture partner network data insights customer insights analytics underwriting superior partner unique data sets deep customer data analytics expertise network access insights access behavioural insights machine intelligence scalable risk modelling risk access collaborative pricing expertise capabilities autonomous robotics capabilities source swiss institute insurers need build capabilities following areas building superior partner network identifying innovative appropriate ecosystem partners suppliers including technology space offer unique value propositions customers competition suitable partners high relationships need fostered focus creating mutual value accessing unique large data sets besides existing accessible data owning unique data sets superior risk modelling becoming important differentiating factor key assemble data layers way creates unique insights cannot easily copied competitors developing deeper customer insights putting customer centre ecosystem aligning actions customer needs using behavioural insights personalise offerings essential holds true even insurer customer relationship directly swiss sigma outlook analytical expertise insurers understand data related context sophisticated risk modelling become ai stands reshape risk landscape resulting better simulations scalable underwriting expertise insurers build superior risk models based existing unique data sets combined analytical methods tools scaled across ecosystems value chains enable insurers make appropriate adjustments value chains respond dynamically impact existing insurance value chain focus customer insights within close interplay product design claims management value chain changing distribution touchpoints generate customer insights changing location yellow box across three panels figure shows sources customer insights changing past product factory insurer disconnected direct customer instead close collaboration insurer physical sales channel intermediary intermediaries direct contact customer customer insights could go back insurer ask products designed particular way figure decreasing distance supplier customer via digital channels yesterday today tomorrow offline offline offline customer journey customer journey customer journey online online online customer corporation retail corporation retail corporation retail digital customer customer customer customer customer customer platform digitally digitally augmented augmented empathic human digital interaction personal personal direct phone personal interface interface channel direct 3rd party online offline chat direct 3rd party smart augmented platform producer product factory product factory digital supplier insurance insurance augmentation critical insights source swiss institute product factory accesses insights today landscape shifted towards multi omni channel approach multiple channels product factory getting closer end customer insurers greater influence product design products must customised sold via different channels simpler products sold direct products greater swiss sigma complexity largely sold personal channels eg life insurance combined investment management greater complexity products lower willingness part consumers buy online time insurers move direct channels simpler products need become product portfolios therefore begin display wide mix insurers create products modularised sliced diced different channels future product factories future product factories able directly access ingredients directly access ingredients product design much closer risk insights customised policy design based digital touchpoints inputs exposure generated real time think result new business models created purely around data access new business models around data including access secure data business models require secure access data combined different ways including insights connected objects platform providers behavioural insights consumer environmental data categories figure show examples data available insurer newer data eg behavioural data based customer context others classical traditional data eg claims preventive services figure illustration potential digital proposition digital business models along customer journey customer journey behavioural offline contextual data insights online smart assistant services insights touch points distribution insights channel market places insights aggregated data insights bm1 environmental data insights contextual provider insights bm2 sensor data insights customer lifestyle cluster health education living free time mobility working insurance value chain insights ie claims product uwr marketing claims customer management pricing sales management service preventive service value adding service insights note bm stands business model source swiss institute swiss sigma outlook data multiple providers via open future insurers operate environment need continuous application programming interfaces access different data sources strategic issue taking insurers beyond api may combined enable existing value chain single firm market place currently provides different business models sources data many data vendors focus extraction distribution data concentrate data refinement expect give rise specialised aggregators focused integration refinement integrated refined data wider service offering customer many implications insurance industry particular need modular products personalisation better distribution insurers may need invest insurers need decide mere suppliers coverage whether separate new data businesses willing collaborate business models gain control key areas impact business different stakeholders seek control data driven businesses example telematics data put onto one platform anonymously would likely market place brokers stake claim players cannot influence data aggregators find going difficult data brokers become omnipresent insurers dependent could time relegated status suppliers data shape new urban future swiss institute january swiss sigma conclusion consumers demanding insurance landscape changing consumers seek engaging ever using digital technology personalised purchase experiences relevant lifestyle empowered insurers respond digitally facilitated information greater choice consumers become informed self directed ever supply side tools like ai enable effective customer interaction allowing insurers better understand consumer preferences develop customised flexible product offering dynamic pricing servicing personal virtual assistance new sources data also offer opportunities granular client segmentation boundaries increasingly blurred impact digitalisation extends beyond insurance value chain across sectors whole business ecosystem insurers operate industry boundaries becoming blurred firms several sectors build digital platforms connect different market places supply chain hubs financial networks non insurer participants business ecosystems like manufacturers telecom companies gaining access customer data digital analytics capabilities enhance client product offering lasting change require utilised fully intelligently new data technology reinforce incremental thinking secure relevance insurance industry future customers true leveraging cross industry ecosystems sigma depicts end version digital insurer look like high barriers entry offer protection competitive threat extent incumbents must continue embrace incremental sometimes radical innovation optimise potential consumers touchpoints insured insurer perspective innovation help transform full scale disruption existing insurers seems unlikely least near term effectiveness raison ' etre incumbents time adjust changing risk environment shifts customer insurance industry attitudes accelerating advances technology room complacency successful insurers leverage insights investments partnerships collaborations upgrade business practices forward thinking innovative insurers could build new infrastructure created today offer compelling risk protection solutions aligned evolving regulation genuinely transform industry effectiveness swiss sigma recent sigma publications data driven insurance ready next frontier emerging markets silver lining amid challenging outlook natural catastrophes man made disasters \"secondary\" perils frontline world insurance great pivot east continues advanced analytics unlocking new frontiers p c insurance indexing resilience primer insurance markets economies global economic insurance outlook natural catastrophes man made disasters year record breaking losses constructing future recent developments engineering insurance world insurance solid mature life markets weigh growth profitability non life insurance mind gap global economic insurance outlook mortality improvement understanding past framing future cyber getting grips complex risk natural catastrophes man made disasters year widespread damages world insurance china growth engine steams ahead insurance adding value development emerging markets commercial insurance expanding scope insurability life force management improving consumer value long term profitability natural catastrophes man made disasters asia suffers substantial losses insuring frontier markets world insurance steady growth amid regional disparities mutual insurance 21st century back future strategic reinsurance insurance increasing trend customised solutions keeping healthy emerging markets insurance help natural catastrophes man made disasters convective winter storms generate losses insurance start new wave world insurance back life underinsurance property risks closing gap life insurance digital age fundamental transformation ahead natural catastrophes man made disasters large losses floods hail haiyan hits philippines digital distribution insurance quiet revolution world insurance steering towards recovery liability claims trends emerging risks rebounding economic drivers care finding sustainable long term care solutions ageing world partnering food security emerging markets natural catastrophes man made disasters year extreme weather events us world insurance progressing long winding road recovery navigating recent developments marine airline insurance urbanisation emerging markets boon bane insurers life insurance focusing consumer published swiss management ltd swiss institute mythenquai p box zurich switzerland telephone email institute swissre com authors dr evangelos avramakis jonathan anchen ashish dave aakash kiran raverkar binay biswal rajeev sharan sophia steinmetz sigma editor (c) swiss rights reserved paul ronke editorial deadline study november managing editors sigma available swiss ' website dan ryan www swissre com sigma dr jerome jean haegeli internet version may contain slightly updated information swiss group chief economist graphic design production corporate real estate logistics media production zurich printing multicolor print ag baar entire content sigma edition subject copyright rights reserved information edition may used private internal purposes provided copyright proprietary notices removed electronic reuse information published sigma prohibited reproduction whole part use public purpose permitted prior written approval swiss institute source reference \"sigma data driven insurance ready next frontier \" indicated courtesy copies appreciated although information used sigma edition taken reliable sources swiss accept responsibility accuracy comprehensiveness information given forward looking statements made information provided forward looking statements made informational purposes way constitute taken reflect swiss ' position particular relation ongoing future dispute event shall swiss liable loss damage arising connection use information readers cautioned place undue reliance forward looking statements swiss undertakes obligation publicly revise update forward looking statements whether result new information future events otherwise order 270 0120 en swiss management ltd swiss institute mythenquai p box zurich switzerland telephone swissre com institute","NLP":"data driven insurance executive summary battle customer ready next frontier touchpoints digital insurer marketing distribution servicing product development underwriting claims management outlook conclusion executive summary digital technology direct change rapid spread internet enabled devices universal connectivity across insurance value chain changed consumer behaviours expectations across industries particularly among younger generations digital age also brought explosion heterogeneous data different sources platforms providers risk protection solutions use broaden reach boundaries insurability value chain becomes digitally connected insurers able better understand customer segments partners adapt near real time insurers become hyper aware short term digital insurance consumers likely young educated customer needs preferences higher levels income time innovative digital cover options consumers become increasingly available making income education less relevant factors purchasing decisions advanced analytics capabilities insurer future aware customer needs preferences provide personalised real time service flexible product offerings artificial intelligence ai used interact build understanding customer servicing personal virtual assistance meet customer expectations insurers able move away product focused sales approach one need adapt provide closely tied broader needs across life time customer greater coverage across life cycle stages focus human experience new generations systems deliver unprecedented levels proximity influence customers insurers go beyond mere channel management optimising interaction across diverse range customer touchpoints implications business models insurers interact customers nature services provide digitalisation also help create new time big data sophisticated models allow risk pricing increasingly underwriting portfolio risk granular level emergence new risks create new underwriting portfolio management techniques risk management techniques insurers create early warning systems gather practical insights prevent incidents simplify accelerate claims processing data enabled processes minimise friction streamline customer insurance journey request coverage claim digitalisation thus help improve customer experience also efficacy back office processes time using data digitalisation enable development new data driven business models multiple providers could drive new impacting entire insurance value chain access data capability business models model risks key true leverage come utilising assets key data supplier partners entry points ecosystems know generate customer insights insurers need decide whether suppliers coverage collaborate new areas business operation value propositions insurance technology could foster ongoing incremental industry change broadening scope affordability insurance alternatively potential radical transformation provision risk protection services households businesses grabs typical hurdles innovation overcome swiss sigma battle customer touchpoints digital customers expect different levels service second nature digital natives presents new world opportunity incumbent insurers still operating paper systems combining digital solutions advanced analytics deliver new insights allowing insurers provide deep holistic engagement across customer lifetimes digital interaction ' life digital technology already digital interaction become norm daily life across world transformed many industries... approximately billion people connected online use mobile devices spend several hours virtual world day new technologies digital facilities developed consumers including example location sensor based services smart homes cars factories developments cognitive systems artificial intelligence ai also creating innovation opportunities see figure including insurers figure new technologies impacting insurance impact business revolutionary * full life cycle * digital business * intelligent process api management technology platforms automation * iot platforms * digital experience platforms high * next generation * conversational * digitally engineered personal health platforms underwriting records phr * behavioural analytics * autonomous vehicles * electronic health record data extraction * genomics epigenetics medium * insurance wallets * digital advisors * administration * reward loyalty * advanced analytics management saas platforms solutions years years years years time impact * p c related technologies * sensing analytics related technologies * productivity related technologies * l h related technologies * customer interaction related technologies * customer loyalty wallet related technologies source swiss institute adapted gartner' hype cycle digital p c insurance connected commerce connectivity enabling lifestyle evolution nielsen november swiss sigma new technologies impacting engagement insight generation ... change insurers rapid spread internet enabled wearable devices ubiquitous connectivity acquire service customers enabling new ways communication information sharing led exponential growth volume digital data generated automatically cheaply non intrusively international data corp estimates billion iot devices around world generating data see figure new tools analyse data extract useful insights also proliferating change way insurers interact consumers figure forecast growth digital data cagr - real time data non real time data real time zettabytes non real time zettabytes source idc swiss institute task facing insurers upgrade consumer preferences buying behaviours also changing rapidly many online experience industries adopted customer centric business models insurers customers work hard keep customers loyal recent survey found insurers remain trusted source new risk coverage solutions consumers switching carriers often used open new entrants including insurtech big tech growth connected iot devices expected generate 4zb data international data corporation june insurers lead new era connectivity bain company september swiss sigma battle customer touchpoints digital insights life events new insurance opportunities insights data analytics boost figure shows cycles customer acquisition also involved customer retention customer retention digitalisation support cycles capturing insights end end consumer experience figure customer acquisition prospect customer experience existing customer experience loyalty lifecycle finding \"good\" customers maximising customer life time value advise advise purchase purchase evaluation use evaluation decision decision buy buy ie upselling service use etc n ive ex ega sit ce conside trigger pe tiv rie e po rien trigger conside nc e pe ration ie life event e ex ienc ie service use ration ba expde r exit research research customer acquisition cycle customer servicing cycle evaluating offers - continuous engagement - competitors put customer pays little attention little effort gain attention competitors offerings improves customer life time value source swiss institute triggering actions enable sources digital data also provide information changed new sales servicing opportunities customer' life eg family location job change see figure helps insurers better understand life events knowledge use develop personalised marketing strategies guidance next best actions predictive prescriptive individual customers entail cross selling risk mitigation value added services addition traditional insurance cover swiss sigma figure access life events enabled digital interactions data predictive aim sources predictive analytics detect problems occur prescriptive prescriptive analytics takes predictive analytics one step offering specific actionable next steps identify communicate solve issues brought next best action predictive data analysis customer online interaction social media interaction insurance event trigger insurance prevention value add life event trigger love kid pregnancy marriage kids leaving family pregnancy separation related events kid death first apartment emigration apartment house purchase life relocation house sale related events education self employment skilling workload post graduate eduction education job job change retirement related events hobbies interest food nutrition travel culture media commerce free time entertainment related events demand car bike mobility purchase sale daily commute business travel mobility ride hailing demand related events micromobility source https www proponent com predictive analytics vs prescriptive analytics source swiss institute swiss sigma battle customer touchpoints new actors insurers many however digital signals embedded environments insurers touchpoints naturally access least insurance low touch business example estimated buyers insurance us average information interactions provider per year data consumer actions readily available agencies instance data mobility behaviour owned mobility companies insurers operators smart home digital platforms know insurers' customers properties insurers urban area operation greater likelihood insights generated non insurers insurers need embed providers risk protection within broader ecosystems digital touch points consumers optimising interaction across multiple touchpoints insurers need access owned achieve insurers need optimise interaction customers across owned touchpoints multiple providers lifestyle areas like health education mobility leisure single journey customers switch touchpoints fulfil various information needs insurers need access insights across different types touchpoints owned owned paid earned social see arrows figure top performing insurers operate across touchpoints unified customer facing brand consistent messaging experience provoke recall figure dynamic personalised protection enabled digital touchpoint insights digitally augmented assistants sensors data sources customer journey context relevance digital augmented customer lifestyle interface health lifestyle clusters digital augmented education working assistant living dynamic personalised augmented protection free time consumers touchpoints owned mobility consumers touchpoints risk asset liability owned paid management earned social source swiss institute customers know want insurers listening bain company october swiss sigma owned touchpoints insurer invests controls consumer touchpoints include agents websites apps goal long term relationship building benefits cost efficiency control paid touchpoints insurer pays digital touchpoint reach customer paid searches sponsorships paid touchpoints include various channels like bancassurance agents earned touchpoints customer becomes touchpoint satisfied customers become social media influencers spread awareness social touchpoints insurer interacts via third party channels uses profile facebook twitter social media forecast fastest growing digital advertising channel globally next five years changing nature insurance touchpoints growth multiple touchpoints industry susceptibility disruption rising see figure expect insurer interactions - necessarily - increasingly begin cross conventional boundaries big tech digital ecosystems eg grab gojek ecommerce marketplaces changing touchpoints insurance context example uk four leading insurance providers retailers selling \"white label\" insurance insurers orchestrate digital physical ecosystems combining one offer services normally delivered different providers figure speed disruption across sectors insurance disrupted volatile current level disruption vs susceptibility future disruption current level disruption industry sectors - scale viability volatility disrupted comms media infrastructure transposrtation retail services high tech cg insurance software platform banking life sciences travel automotive energy utilities capital markets health chemicals natural resources industrial equipment machinery less disrupted durability vulnerability less susceptible susceptibility future disruption susceptible - scale less susceptible yet disrupted less susceptible less disrupted source v savic b moussavi good things come ' wait accenture swiss institute social fastest growing digital advertising channel globally forrester august bain company october op cit swiss sigma battle customer touchpoints front runners set outperform early mover strategy new technology could prove expensive medium term deliver strong returns longer run bcg mit survey found executives across industries yet see value ai investments made recent years activities like collecting curating data building knowledge base specific company training systems getting employees augment developing strong governance takes years rarely box solutions rapidly implemented figure demonstrates one future pay back scenario companies invest early burn cash years begin rapidly see benefits accumulated learning investments players adopting \"fast follower\" strategy worked technologies could find long term laggards figure relative changes cash flow ai adoption cohort change per cohort cumulative front runners absorb within - years followers absorb - laggards - absorb source jacques bughin et al notes ai frontier modeling impact ai world economy mckinsey global institute swiss institute mit sloan management review boston consulting group surveyed executives see winning ai pioneers combine strategy organizational behavior technology mit sloan management review october v mahidhar h davenport companies wait adopt ai may never catch harvard business review december swiss sigma digital consumer - today future millennials typical digital based sample three key markets digital insurance making insurance consumers today headway- us sweden china - today' digital consumers typically years old affluent educated digital generation generation born early 1980s late 1990s subsequent ones expect rapid access information clarity around value proposition proactive servicing key maintaining loyalty maximise customer lifetime value among clients average incomes higher income consumers afford insurance room growth lower education levels income segments value propositions need adapted sample markets younger consumers higher incomes education tend make online purchases low mid level incomes least china us china consumers incomes excess rmb use online tools buy health insurance including medical expenditure critical illness low income consumers rmb see figure along similar lines consumers doctorate degrees buy health insurance digitally compared technical vocational training seen us example median income consumers buying insurance life usd least college degree figure digital purchases health insurance products across income education levels china income level education level consumers using digital purchase methods consumers using digital purchase methods low income mid income high income technical bachelor master doctorate rmb rmb - rmb vocational associate degree professional training degree degree source swiss institute avanza swedish digital life insurer customers years old see annual report - sweden' satisfied savings customers years row\" avanza china majority signed alipay' digital critical illness product born post see online insurance china general reinsurance ag majority life customers millennials see happy birthday us life may customers us insurer lemonade see wininger \"lemonade' first quarter market\" lemonade com january majority zhongan digital p c insurer china aged insurers learn insurtech giant zhong accenture january chinese non life personal lines consumer perspectives swiss institute life may op cit swiss sigma battle customer touchpoints differences online buying behaviour exist across markets many use internet research internet used medium research l h insurance china insurance penetration online sweden however penetration online insurance purchases still purchases low uneven countries example online l h insurance distribution accounts less us18 china20 already sweden findings similar p c insurance sweden china around p c premiums written distributed online markets levels digital insurance purchases much lower despite internet used many research tool example spain use internet research l h insurance online penetration life insurance around p c premiums came online channels see figure commoditised lines motor online purchases higher consumers us consumers china28 consumers spain29 purchased motor insurance online figure online insurance purchases online distribution life insurance p c online distribution online distribution annual life insurance premiums written online distribution premiums written sweden us china spain sweden china spain source verdict china statistic press genre axco statista swiss institute chinese non life personal lines consumer perspectives swiss institute european insurance report swiss online distribution includes life health accident insurance united states special insurtech online distribution statista insurance china forty years reform opening china statistics press sweden distribution channels life axco co uk sweden distribution channels non life axco co uk \" tencent ant financial rushing china' insurance industry\" cbinsights com october european insurance report swiss spain distribution channels life axco co uk spain distribution channels non life axco co uk us insurance shopping study j power sigma - world insurance china growth engine steams ahead swiss institute ibid swiss sigma one reason availability online one reason low penetration online life cover could many insurers purchase tools limited offer tools buy online looking provider websites found spain neither biggest insurers insurtechs sell life insurance online directly consumers similarly china us large insurers sell life insurance policies digitally however sweden six biggest financial institutions could explain higher purchase penetration online purchases higher markets nevertheless equal access online purchasing facilities may necessarily lead low power concentration similar penetration across markets cultural characteristics based hofstede high individualism low uncertainty cross cultural index33 preferred leisure activities may also impact avoidance... comes comparing online insurance penetration general e commerce penetration use e commerce buyer penetration proxy online insurance purchases countries higher e shopping penetration also demonstrate higher online distribution life insurance one reason might consumers use internet often different purposes including online shopping feel comfortable going online buy insurance country specific cultural characteristics low \"power distance\" defined degree individuals expect accept power distributed unequally high individualism low uncertainty avoidance could influence adoption technology countries low power concentration flatter hierarchies people make decisions contrary hierarchical societies unacceptable disagree decisions superior individualistic cultures people prefer collect information direct sources internet collectivist cultures people rely subjective advice family friends finally countries lower uncertainty avoidance people feel comfortable trying new things including new methods buying insurers china anbong life pingan insurtechs bihubao hetai xiaoyusan us insurers include massachusetts mutual insurtechs ladder life life spot life ethos insurers sweden folksam scandia avanza nordea liv nordnet idun liv hofstede cross cultural index analyses national culture country based six cultural dimensions relative countries bellman et al \"predictors online buying behavior\" communications acm lee h et al \"consumer lifestyles adoption high technology products case south korea\" journal international consumer marketing vol see hofstede cross cultural index https www hofstede insights com models national culture matusitz et al \"power distance uncertainty avoidance technology analyzing hofstede' dimensions human development indicators\" journal technology human services ozbilen p \" impact natural culture new technology adoption firms country level analysis\" international journal innovation technology vol peterson et al \" influence national culture information technology product adoption\" amcis proceedings paper lee \" impact cultural differences technology adoption\" journal world business zakour cultural differences information technology acceptance proceedings 7th annual conference southern association information systems lee impact cultural differences technology adoption journal world business vol laukkanen \" uncertainty avoidance affects innovation resistance mobile banking \" 48th hawaii international conference system science garbarino e et al \" cultural differences uncertainty avoidance affect product perceptions\" international marketing review vol swiss sigma battle customer touchpoints every country unique combination cultural characteristics china' society highly collectivist high power distance indicating inequalities accepted yet chinese also appear comfortable uncertainty much like swedes could explain china high e commerce buyer penetration could count popular leisure activities consumers engage technology frequently daily lives free time also shop online noteworthy web surfing popular free time activity china ranked first sweden ranked third spain ... surfing web told believe insurance bought online consumers popular leisure activity easier access degree online insurance purchases grow faster markets certain cultural characteristics low power distance high individualism low uncertainty avoidance use internet preferred leisure activity end see opportunities online insurance purchases markets like sweden us uk germany china see figure figure culture leisure across markets factor sweden us uk germany china spain italy turkey high individualism culture low power concentration low uncertainty avoidance leisure activities e commerce consumption note digital e commerce buyers defined consumers use internet made least one purchase online preferred leisure activities includes web surfing source swiss institute li h et al \" impact perceived channel utilities shopping orientations demographics consumers online buying behavior\" journal computer mediated communication vol bellman et al op cit chen et al \"leisure activities leisure motivations chinese residents\" plos one vol en undersokning om svenska folkets tidsanvandning ar statistiska centralbyran time use survey instituto national de estadistica swiss sigma drives online purchasing behaviour access provided theoretical framework helps models based consumer value need theories help answer understanding affects question figure depicts theoretical model tailored online life insurance decision buy cover online buying behaviour consists three value drivers functionality emotions personal growth date insurance product design mostly focussed functionality little consideration emotional personal growth values figure schematic showing drivers insurance purchasing behaviour model functionality emotions personal growth ease purchase process low pressure empowerment convenience enjoyment epistemic value immediate fast purchase process painless process better price feeling relief feeling comfortability consumption behaviour source swiss institute consumers value simplicity want analysing consumer reviews two us digital life insurers ladder life emotionally fulfilled life find consumers buy cover online value functional also emotional personal growth elements eg empowerment commonly mentioned emotions \" pain\" feeling \"happiness\" \"delight\" said find buying online \"painless\" said feel \"great\" buy online another consumers said \"enjoy\" completing application purchase online nearly said feel lower pressure contact insurance agent enjoy products address personal growth values perceived consumers mostly relate empowerment personal growth values consumers value ability acquire information product prices terms able decide policy best fits needs one consumer stated instance \" great ... get information make decision \" empowered consumers longer passive shoppers dependent agent' advice active participants want information make decisions holbrook customer value framework analysis research routledge j sheth et al \" buy buy theory consumption value\" journal business research vol maslow \" theory human motivation\" psychological review vol \"reviews\" trustpilot com consumeraffairs com last accessed august consumer review quoted consumeraffairs com february p buhler p maas \"consumer empowerment insurance\" international journal bank marketing vol swiss sigma battle customer touchpoints insurers need better relate value drivers play important role understanding consumers' choice consumers higher level emotional behaviour future purchasing intentions example alibaba' critical illness attributes insurance58 lemonade' home insurance59 offer simple immediate purchase process also address emotional personal growth values see figure case alibaba consumers emotionally experience insurance community product since contribute pay someone ill personal growth value empowerment fulfilled allowing large group pre approved consumers decide collective voting system whether pay approved figure examples insurance products based three value drivers cases designing insurance based functional emotional personal growth values case case alibaba critical illness insurance lemonade home insurance functional value purchase process simple immediate via purchase process simple immediate via transaction app alipay lemonade app laptop tablet emotional value consumers experience insurance community customers choose non profit organisation product since collectively contribute leftover money monthly flat fee join payout someone ill community believes cause personal growth value empowerment preapproved consumers decide empowerment consumers empowered empowerment collective voting system deciding social cause want choose whether payout claimants leftover money approved source swiss institute based information company websites future ages attracted next three five years digital insurance consumer likely remain digital insurance millennials higher levels income education longer term millennials become seniors age groups attracted digital insurance income education less relevant factors purchase decisions new innovative digital cover options consumers lower income already available instance micro l h insurance products offered china life63 prosur64 jubilee also alibaba' critical illness product \"xiang hu bao\" attract consumers backgrounds irrespective income education levels j sheth et al \" buy buy theory consumption value\" journal business research vol holbrook customer value framework analysis research routledge \"alipay health plan aiming 300m users\" alizila com april \"instant everything\" lemonade com \" lemonade giveback\" lemonade com ant financial aims disrupt health insurance new chinese health 'collective' insurancejournal com april \"jack selling cancer coverage pennies month china\" bloomberg com may microinsurance rural development china q associate professor yi yao kitty peking university microinsurance center milliman march \"posur micro insurance\" prosur com kh \"micro insurance enabling people overcome uncertainty\" jubilee com august insurancejournal com april op cit swiss sigma digital insurer future insurer hyper aware customers needs preferences offer personalised flexible products services real time data enabled processes minimise friction streamline customer insurance journey request coverage claim artificial intelligence used interact build understanding customer servicing personal virtual assistance offering personalised social sustainable engaging digitisation provide timely digitalisation provide far timely feedback insurers leveraging range feedback loops daily interactions example across mobility health recreational behaviour feedback loops always existed insurance influencing purchasing patterns insureds' risk behaviours make claims feedback traditionally ex post sometimes meant long time lag balance autonomous insights empathetic care constant feedback based digitalisation goes beyond data collection also gives insurers power understand processes provide better insights risks dynamically insurers may discover uncovered exposures risk management... seek close see figure example combining traffic weather patterns factors insurers may determine driver spends majority time higher risk roads insurers taking first steps engaging customers empathetic manner based observed behaviours example generali real time coaching tool motor insurance clients helping drive better work accident prevention focus currently mobility services plans expand areas access risk insights across new touchpoints channel formats ... allow insurers propose digitalisation insurers obtain risk related data variety touchpoints innovative coverages quicker identify monitor existing new risk exposures new data sources analytical capabilities provide instant depth view risks across touchpoints facilitates quicker risk assessment allows insurers propose innovative relevant coverage pre digital insurers wait till loss data available assess customer adequately covered example someone stops working due disability insurers use data suggest ways help customer recover quickly see \"metlife hooks external data feeds help curb customer risk\" cio com october \" innovate customers\" generali com swiss sigma digital insurer figure insights improve existing value chain smart interactions smart mobility living workin smart g smart health active break start work park ride mobility end work breakfast outdoor family activity sleep shopping family risk related interactions digital risk related interactions identification monitoring existing identification monitoring new clients risk exposure future risk exposure reducing adapting existing protection gap generating new protection related products growth profitability adoption existing services value propositions new customers product uwr marketing claims customer management pricing sales management service retail business corporate business risk exposure insurance l h p c insurance l h p c protection gap source swiss institute swiss sigma shift product service orientation insurance thus become digital interactions enable insurers create personalised insurance new proactive protection related services traditional risk selection remain core activity faster simpler based real time data greater volumes digital data help quantify different kinds exposure eg behavioural external internal see figure behavioural exposure risk aberrant actions driver speeding failing maintain safe distance choosing dangerous routes internal risk exposure covers inherent risk insured asset eg quality construction property external risk exposure covers environmental factors like catastrophe risks property bad weather street conditions drivers figure representation ideal state risk quantification life health insurance property insurance automotive insurance classical lines business offline customer journey online data collection risk exposure external internal sensor behavioural identification risk exposure contextual risk exposure evidence based analytics decisions - timing - relevance relevant product service offering contextual content products services risk related offerings risk transfer risk mitigation preventive services value adding services partner community integration intervention access sharing steering support community partner network life area clusters health education working living free time mobility source swiss institute swiss sigma digital insurer ideal state still way key task insurers develop digital capabilities access relevant information changes risk exposures convert actionable insights package communicate customers user friendly manner insurers need upgrade digital capabilities across three areas identify true risk exposures data used risk exposure identification insurers need understand risk exposure evidence based analytics decisions based timing relevance true signalling power c reate relevant product service offerings insurance cover adapt changing needs ensuring cover appropriate across different life stages end insurers need able contextualise products services risk mitigation preventive services value adding services partner community integration intervention insurers steer support community partner network advanced analytics able identify customers likely buy based risk sub segments higher quality service lead insurers refine capabilities able provide higher quality greater loyalty service interactions lead greater loyalty example property insurer access analyse data changes risk exposure whether owned third party data enable safety security services prevent damage losses could also advise real estate purchases across hundreds relevant risk variables example one insurer exploring engage customers predictive indicators property public transit corridors number emergency calls made property hygiene nearby restaurants infrastructure quality area insurers emerging markets technology accelerating development insurance emerging markets leapfrogging peers advanced took many years development advanced markets compressed markets innovative areas years emerging territories \"leapfrogging\" technology steps skipped disruptive technologies facilitating convenient access insurance products services part local requirements working digital insurers emerging markets taking lead leveraging new digital ecosystems extend reach ecosystems offer multiple touchpoints capture user attention often evolve original businesses move new domains case grab based asia started passenger mobility entered food goods delivery expanding healthcare insurers asia forming partnerships firms insurer brings underwriting expertise partner firm entry point ecosystems allowing insurer extend reach target specific segments mine user behaviour along lines china aviva partnered internet major tencent sell critical illness policies online another example africa insurers selling insurers ride sharing drivers mobility ecosystems \"metlife hooks external data feeds help curb customer risk\" cio magazine october \"ping good doctor grab form joint venture deliver transformative o2o healthcare solutions southeast asia\" grab com august \" nairobi based ridesharing service little competes uber africa\" skift com june swiss sigma insurers use digital platforms modern flexible flexible modern platforms insurers leverage flexible product agnostic fully integrated digital platforms help insurers enter new markets engage customers buying journey core value chain components like underwriting transformed insurance service see figure example swiss partnering solution iptiq allows insurers harness power cloud applications data analytics make buying cover easier also connect market services platforms also act intermediaries connecting market services different different parties parties eg enabling brokers manage deliver cross border programs single online platform two types platforms established first core service platforms enable part whole value chain established industry service iaas - insurance service solutions second service platforms act intermediaries supply demand rare cases combination platform types may established figure depiction opportunities platforms insurance customers distribution primary distribution reinsurance distribution retro partner insurance partner partner cessions b2c retail b2b b2b b2b b2b b2b b2b corporate core service platform market service platform product uwr marketing claims customer management pricing sales management service service interaction platform services process platform value chain industry insurance banks technology platform enablement intermediaries aggregators platform management coordination governance brokers platforms ecosystems source swiss institute swiss partnering solution iptiq allows insurers harness power cloud based applications data analytics make buying insurance easier help people become insured see iptiq technology making easier buy insurance swiss november swiss corporate solutions announces collaboration build multinational insurance program management platform brokerslink swiss corporate solutions october starling' marketplace delivers 'app store' experience financial services ns banking february swiss sigma digital insurer insurers use expose benefit platforms insurers use apis integrate external platforms influence brokers partners eg ecommerce bring significant efficiency across value chain due partner allows seamless information exchange different components need analysis policy seamless customer journey across issue customized co branded distribution partners value chain choose products want offer consumers allows seamless customer journey across value chain virtue remaining single platform platforms multi device optimised desktop tablet smart phone multi channel ie customers switch face face agent self service even work agent call centre data saved shared value chain become integrated iterate quickly processes first wave digitalisation made insurance value chain efficient become flexible open see figure however multiple digital silos remain unconnected information innovation compartmentalised future critical processes connected insurers progress mere \"digitalised insurance providers\" \"digital insurers\" potential transformation end looking ahead data quality algorithms improve next generation ai insurers value chains \"learn\" data generated consumers ecosystems governments developments cognitive technologies help insurers integrate learning adapt value propositions real time thereby providing holistic unique customer experience figure insurance products growing comprehensive risk service involved stakeholder value chain example product uwr marketing claims customer insurance icon management pricing sales management service digitalised product uwr marketing claims customer icon insurance management pricing sales management service digital product uwr marketing claims customer icon insurance management pricing sales management service data ai driven icon product service insurance source swiss institute swiss sigma better data digital triggers availability high quality underlying data also make easier create accelerate adoption parametric products digital triggers speed adoption parametric insurance insurance property level flood insurance utilises data feeds sensors homes businesses parametric triggers already available basis risk much lower trigger based data source property time virtualise value chain lead products becoming parametric nature see smart contracts smart contracts smart contracts automatically concept smart contracts aligns idea ai driven insurer recognise terms constructed interplay multiple value chain components smart insurance policy met contracts self executing software programs instance make payment triggered occurrence risk event legal contract logic put computational logic string computer code recognises terms contract insurance policy met automatically transfers funds insurance payouts agreed time registers transfers however yet existing insurance contracts need translated computational logic far international regulation specific promising cases parametric insurance lines business smart contracts telematics used motor aviation cargo agriculture offer potential however certain challenges stand way wider adoption smart contracts notably yet international regulation specific smart contracts lloyds recently pointed unclear policyholders disagree automatic decision trigger cover defend challenges digitalisation regulation monitoring yet facilitating innovation regulatory framework play important role shaping integration priority regulators new technology insurance space monetising potential digitalisation insurers could face regulatory challenges data protection privacy providing incorrect advice records retention errors bias algorithms might contribute systemic risk prompt inappropriate insurance decisions also area regulatory scrutiny likewise might difficult regulators understand complex proprietary algorithm decided deny coverage reject claim undermining ability fulfil supervisory consumer protection tasks insurers could also face country regulators also wary unsolicited use data insurers also face country specific regulatory limitations specific regulatory limitations high implementation efforts product development modifications construct robust system data governance win customer trust abide regulation may additional legal compliance challenges integrating multiple vendors one proposition jba floodflash launch sensor based parametric flood cover first artemis june triggering innovation lloyds swiss sigma digital insurer ownership data willingness share consumer reticence share data data become easier collect increasingly important consideration varies country may ownership example ownership images properties surveyed drones may resistance contexts rest clients insurers consumer reticence share data varies country country may resistance contexts example employers offer workers boots gloves vests wearables found employees refuse monitored behaviour differs line business recent survey found us consumers willing share health related data home related data focus loss another survey found consumers cautious sharing data even prevention rather personalised know lead personalised experience however accenture premiums survey found eight would share personal data eg income location lifestyle habits insurers lowers risk injury loss suggests messaging around data sharing effective focus loss prevention rather personalised premiums data availability better ask fewer focused data easily available required depth detail insurers strike questions balance relevant data data easily misinterpreted background context fully understood example non smoker occasion buys cigarettes parent could identified casual smoker could subject higher premium rates buying health insurance data help confuse cases asking fewer focused questions could lead precise answers consumer wariness automation policyholders may less consumers may less comfortable automated world example comfortable digitisation surveyed consumers said worry mistakes filling insurance forms fear claims pay outs might accurately calculated machines policyholders may divided views digitalisation vital win customer confidence reassuring machine accuracy developing checkpoints help avoid mistakes \" exciting new ways technology streamlining claims management process\" risk insurance august u consumer survey connected devices insurance roadmap aite group may \"promise personalization little impact consumer willingness share data study reveals\" marketing dive august consumers willing share data exchange adjusted car insurance premiums based safe driving willing share data exchange life insurance premiums tied healthy lifestyle see six ten consumers willing share significant personal data banks insurers exchange lower pricing accenture study finds accenture march \"future claims customers still want humans process - many\" carrier management march swiss sigma marketing distribution servicing insurers moving away product focused sales approach one closely tied broader needs customer greater focus human experience new generations systems grant unprecedented levels proximity influence customers insurers able optimise interaction across diverse range customer touchpoints marketing become granular insurers use new data build historically insurance marketing aggregating prospective buyers consumer journey based human groups likely common needs respond similarly marketing typical experience attributes include geography demographics psychographics eg values attitudes lifestyles behavioural aspects eg price sensitivity approach leads relative broad spectrum consumers segment limit effective targeting new sources digital data life events purchase histories travel behaviour customer service records data devices wearables also social networks insurers employ advanced analytics techniques yield granular classification existing prospective customers see table table evolution marketing distribution innovation timeline marketing distribution journey past present future key enablers customer experience fragmented limited multichannel somewhat seamless continuous platform economy channels integrated insights available market size sentiment time consuming tech based still takes tremendous efficiency online surveys digital analysis physical time accuracy gains behavior personalisation possible possible limited scope complete customisation shorter development expensive time data insights channel evolution agent driven door direct online contact ecosystem based chatbots machine door center bots tech virtual connect learning enabled agents fulfillment exhausting time direct home needs based \" consuming time\" source swiss institute sigma life insurance digital age fundamental transformation ahead swiss december ibid swiss sigma marketing distribution servicing smart interaction future marketing boost customer loyalty personalisation improve loyalty management impact customer lifetime value cltv historically focus insurer engagement customers sales claims new data sources advanced analytics insurers interact customers lifetime wide range relevant issues developing systems help listen engage act proactively reactively see figure figure schematic showing smart interaction increases loyalty customer equity without limited interaction management intensive interaction management cltv segment cltv segment customer journey customer journey invoice mailing claim potential historical focus listen engage act sales distribution claims cltv cltv cltv cltv owned owned proactive reactive paid paid earned earned social social value customer content collaboration content collaboration interaction communication community communication community insurance products services source swiss institute lifestyle related interactions help high quality interactions loyal customers build deeper one one relationships end insurers pro actively use digital engagement reduce policyholder churn scale example recently major p c carrier ran three month pilot sending personalised automated texts delinquent customers followed phone call creating new level proactive digital human engagement less customers opted large majority delinquent customers retained less manual work approach yielded results fewer outbound calls insurers broaden scope interactions policyholders wider sets prospects example insurers decoupled insurance telematics apps app available drivers rather insureds alone customer lifetime value cltv measure net profit attributed entire future relationship strategy meets action proactive digital engagement reactive proactive paradigm shift insurers hearsay systems august insurance innovation month aig go efma january swiss sigma distribution traditional touchpoints adapt capitalising unique moments digitalisation insurers better able identify human interactions key increasing loyalty play significant role capitalising unique moments key increase cltv customer lifetime value currently digital interactions mostly transactional post sale interactions focused billing claims handling miss opportunities insurers address full range customer needs insurers need understand empathy important component brand value offering unified customer experience automated data capture synchronisation customer relationship management crm systems offer actionable insights example agents alerted customer files digital notice loss claim use event opportunity call day better understand life events eg baby way appropriate within contextual setting cross sell intermediaries role change agents act risk consultants customers become comfortable buying insurance online role fuller view customers life events intermediaries evolve agents act risk consultants overview ' happening customer across value chain insurers agents also need rethink traditional cooperation models including intermediaries compensated example client acquired online may require advice agent compensation model rewards agents services via advisory fees consumers willing pay may help alleviate channel conflicts could arise within pure commissions based system commercial small medium new distribution technology impacted wholesale commercial insurance sized business also becoming markets much retail personal lines however initiatives open digital distribution simplify parts value chain commercial insurance lloyd' london example mandated syndicates use electronic placements less written risks end online brokers small medium businesses sme also seeing traction instance embroker coverhound use data verification technology obtain immediately bindable quotes allowing customers complete process almost seamlessly online platforms offer additional services eg help smes upload compare policies generate vendor certificates asset tracking strengthen customer loyalty insurance next gen customer experience - dreamforce case study hearsay systems october sigma op cit \"lloyd' issues electronic placement mandate\" lloyds com march see embroker coverhound swiss sigma marketing distribution servicing digitally augmented channels rise digitally augmented channels new digital interactions driven next generation empathetic advisory tools combining man machine augmented ai see figure agents brokers leverage refine become common communication adapt customer' emotional situational personal contexts power combining functional empathetic interfaces functional interface performs analytics provides intelligence next best action agents brokers provide human empathetic interface figure data ai driven augmented advisors retail customer commercial customer intermediaries provider smart augmented platform ecosytem bank platform broker 3rd party insurer reinsurer marketplace aggregator digitally augmented empathic human interface interactions digitally augmented digital interface source swiss institute big tech intermediaries insurers believe use digital advisors combine two capabilities invest different interaction drive digital interactions targeted manner future generating channels intelligence become core capability insurers depending provider could three types levels digital interaction channel could work co existence although one perceived consumers trustworthy unbiased likely hold distinctive competitive advantage \"direct\" customer advisors insurer agnostic offered technology companies example could enhanced versions today' siri alexa google duplex assistants \"intermediary advisor\" feeds intermediaries better insights back office responsibilities minimised agents focus human relations empathy augmented better risk insights data \"product advisor\" fully insurer owned example geico' mobile virtual assistant kate intuitively guides customers relevant information policy helps self service tasks customers may use three set ups example l h insurance may go intermediary home insurance direct insurer mobility travel insurance use enhanced version customer advisors like google assistant intermediaries continue play important role although human interaction may cases become less part standard process human involvement added value emphasis contextual \"empathic interactions\" new competitive landscape insurers need balance customer interactions right manner using new loyalty interaction systems swiss sigma servicing loyalty new risk management needs additional digital tools value servicing building deeper relationship customers scale added services help brokers push risk prevention value adding services vas especially critical distribute customers commercial lines specialised knowledge leveraged whichever party along value chain possesses specialised knowledge provides robust engagement control servicing significant scope insurers enable agents brokers digital tools value added services j power measures satisfaction scores among business relationships finds insurance agents currently report low satisfaction service receive personal lines commercial lines insurers advent digital engagement servicing several implications insurers boundaries traditional insurance becoming blurred distribution moving towards integrated ecosystem focusing end end buying experience survey accenture found three five insurers forming relationships non traditional partners reach customers new ways create new value impact aspects value chain fully realise benefits partnerships insurers need upgrade predictive analytics capabilities obtaining risk related data variety touchpoints providing tailored content customers want positive experience across multiple channels touchpoints gartner believes coming five years event triggered real time marketing make biggest impact across industries however loyalty hard come digital customers still give insurers lower loyalty scores multi channel customers misalignment customers expect digital channels insurers provide example insurers fail replicate certain service components contact us page web mobile sites shifts toward platform plays ecosystems appear inevitable personal commercial lines e commerce firms retailers automotive oems non traditional players developing owning offering new unique product propositions customers build platforms based plug pay principles allow third party connections insurers need cognizant different elements value chain different customer touchpoints srikanth madani hypotheses innovation direction insurance st gallen risk finance \"insurers come short independent agents despite critical role agents play driving business j power finds\" prnewswire com january accenture technology vision insurance accenture april gartner identifies four emerging trends transform marketers run technology ecosystems gartner august bain company october op cit gartner says u insurance brands underperform digital despite customers' growing willingness provide data gartner july swiss sigma product development underwriting bikes trainers customers like tailored rather shelf products become increasingly prevalent insurance time big data sophisticated models allow risk pricing increasingly granular levels emergence new risks create new underwriting portfolio risk management techniques digital enabled product development real time demand assessment digitisation enable direct increasing digitalisation present opportunities connect directly touchpoints customers customers assess product demand real time rather rely solely real time demand assessment channel partners see table example app surveys tracking search behaviour provide clues unmet customer needs based granular customer data insurers precisely segment customers develop tailored products moreover timing product searches correlated personal data may provide context product demanded constant feedback help insurers develop new products also refine adjust existing ones table impact digitisation product development journey product development journey past present future key enablers demand assessment time consuming efficient based real time based online surveys digital dependent channel limited customer data changing customer life behavior tracking partners collection stages coverage design fixed coverage longer partially modular completely modular hyper connectivity duration coverage shorter real time coverage enabled sensors 5g duration lpwan95 satellites pricing fixed pricing defined short period pricing dynamic pricing based risk categories based limited real time behavioral behavioral data data partnership strategy mainly distribution extended partnerships ecosystem partnerships apis96 cross industry based partnerships across insurance value driven cross industry ecosystems chain value chains regulatory approach finite experimentation sandbox approval wide range sandboxes due lack data experimentation data risk modelling innovation hubs data based evidence customer feedback almost non existent limited feedback based continuous feedback end end digital mainly claims multiple touchpoints delivery products experience source swiss institute k cool c angoulvant et al \"zhongan' micropremium model future insurance \" knowlege insead edu may low powered wide area network low bandwidth wireless technology covering wide areas interface communication protocol allows two software programs talk swiss sigma data driven granular insights facilitate new risk covers digitisation enabled coverage present digitalisation drives modest coverage customisation products customisation preserving base incremental innovation preserves basic product structure using short time product structure scales price short duration covers add ons provide coverage carve outs solutions either white labelled written directly insurers covers fixed shorter durations products may use sensors track asset usage movement individuals activate coverage offer minor premium credits information widely used real time underwriting pricing real time risk data facilitate sharing economy modularisation real time coverage adjustment innovative insurance solutions enable new products adapt frequently changing risk profiles modularisation means de coupling coverage services suit changing needs customer personal risk exposures tracked modelled easily insurers confidently de couple coverage elements within existing monolithic products commercial risks complex change frequently digitalisation help coverage coupling across products reduce protection gaps achieve price efficiencies dynamic environment product life cycles get shorter speed market crucial better data quality facilitating innovative products new risk pools emerging granular data construction new risk indices collection analysis enabled digitalisation example see first solutions creating new markets emerging personal cyber use cryptocurrency exchanges iot infrastructure risks better data quality also facilitates construction new risk indices parametric products covers protect economic impacts infectious disease outbreaks use pathogen sentiment indices digital sources gauge public fear behavioural changes measure cost epidemics future think algorithmic risk products become mainstream assets processes becoming autonomous intelligent toffee insurance trov element examples agencies providing pay go insurance via online portals metromile lemonade licensed insurers usage based products traditional asset insurance mandated clear commercial personal insurable interest however classification blurring sharing economy allowing individuals put personal assets commercial use via online platforms see mobility ecosystems striving towards seamless interface customers swiss current average speed market eight months new life annuity products four months modifications property casualty insurers seven months new products three months modifications see speed market life annuity insurers novarica march speed market property casualty insurers novarica march bitflyer mitsui sumitomo selling insurance users cryptocurrency munich relayr developed customized insurance products facilitate iot infrastructure investments andrew w singer \" evolution parametric insurance\" rmmagazine com april swiss sigma product development underwriting dynamic risk pricing drive behavioural change pricing based granular data insurers able micro segment risk pools thus accurately reflecting risk generate premium efficiencies unit within insured pool example zurich insurance spain partners mould consumer behaviour klinc offer coverage turned personal devices items catalogue products plans expand categories like auto home enabled availability granular behavioural data advanced modelling pricing may change dynamically based changing risk profiles going forward cross industry platforms seamless insurance portability integrate risks exposures covers insured universal insurance policy policy customised according insured adapt coverage real time basis change insured' risk profile see figure figure tech enabled insurance product service opportunities shelf coverage digitalisation enabled personalised coverage risk levels risk levels policy period policy period static pricing level exposure inefficiency insurer coverage pricing insured risk profile insured risk profile coverage pricing inefficiences insured coverage exposure efficiences insurer insured zurich' innovation efforts recognized global innovator gold award klinc takes top award products services zurich june absence granular data insurers use proxies driver age type car gender assess driving behaviour products like pay live health insurance pay drive auto insurance rely dynamic pricing short period pricing based limited behavioral data finer version static pricing swiss sigma tech enabled insurance product service opportunities cont modularisation global insurance coverage retail business travel travel cyber cyber liability liability home home mobility mobility year family adult two kids natcat life natcat life health health exposure inefficiency insurer modularisation global insurance coverage commercial insurance cyber liability cyber liability marine marine fire fire smes mid large corporates natcat natcat engineering engineering business interruption business interruption risk exposure risk coverage price insurance protection gap insurance coverage note green risk exposure line circular charts indicates insured' dynamic risk profile revealed processing granular insured data segments blocks chart indicate extent coverage modularized across exposures different risk areas eg cyber fire etc source swiss institute swiss sigma product development underwriting extending insurance reach preventive value added services insurers may need focus insurers strong distribution networks brand ability adapt meet preventing accidents improving customer needs eg simplifying tasks offering outcomes reduce pain improve quality life gain could develop new relationships based trust value add reinvent risk protection value add preventive service providers help counter decline premiums caused reduction loss incidences due various safety features enabled technology prevention could contain future claims value add services build loyalty services see table grow sophisticated insurers harness network effects within cross industry ecosystems also increase customer stickiness increasing touchpoints wide range services could commercial lines insurers could partner providers technology monitor provided part overall industrial assets improve efficiency safety early warnings example one insurance package insurer offers free online tool helps smes assess risk cyberattacks become proactive security personal lines insurers enable access technologies coping mitigating morbidity example italy axa prototyped cancer profiling service allows patients advanced metastatic tumours access personalised cancer care south african insurer discovery service provides additional pay outs insureds stricken permanent disabilities help adjust new way life table examples value added preventive services service type auto property life health preventive services safe driving alerts remote monitoring online diagnostics medical consultations maintenance rewards alerts help finding doctors screening anti theft flood monitoring scheduling counselling breakdown alerts home basements appointments behavioural geo fencing eg alert automatic shutdowns assessments driven outside safe gadgets fires neighbourhood value added services assistance buy car property security meal fitness digital health records concierge services advice vouchers fitness club alert vehicle facility maintenance retirement financial memberships moved towed hit emergency repair planning discounts parked services healthcare services source compilation value added preventive services swiss institute digital ecosystems extending boundaries value creation insurance swiss \"cyber insurance\" armourinsurance ca efma accenture reveal winners innovation insurance awards accenture june purple life plan discovery swiss sigma digitalisation underwriting digitisation enable forward digitalisation external internal data improve risk selection pricing looking underwriting automation streamline manual submissions triaging binding see table overall developments reduce underwriting costs loss ratios free time underwriters engage softer aspects business ie negotiation relationship building also developing repositories digitised risk information enable efficient underwriting decisions based limited claims experience data example helvetia insurance enables logistics companies purchase transport insurance customers straightforward online app less two minutes table impact digitisation underwriting value chain value chain past present future key enablers submissions lengthy proposal forms proposal auto filling fraud detection advanced text human interactions third party data behaviour analysis speech analysis ai data collection chatbot interaction spotting dishonest ml post submission analysis limited data collection disclosures complete efficient information data collection storage systems triaging delays due inefficient better quote market submission ranking intelligent business workflows time speed lobs due based conversion workflows ai nlp consuming decisions auto referrals probability auto better readability referrals appropriate authority suggestions capacity digitized information management ocr risk assessment manual time consuming usage third party data evolution risk digitisation asset limited point external risk features foresight activity biological data underwriting automated report frequent touch points text image analysis interpretation real time sensor data processing sensor data coverage pricing coverage governed flexible cover real time coverage real time risk modelling strictly insurability pricing lobs pricing adjustments capacity sharing p2p pricing based driven data asset seamless cover digital risk consortiums static risk matrix usage individual portability across connected sensors movements insurers binding delayed binding due reduction binding instant binding submission data entry paper time errors due basis end end triaging based information higher flow digitised digitised submission multiple systems submission data risk assessment data binding process source swiss institute submissions triage routing become automated efficient submission level accuracy increasing digitalisation geographical personal asset information much improved availability leveraged insurers auto fill proposals offer risk scores even identify granular customer data honest brokers already pin code license plate numbers reveal location specific hazards vehicle details business registration numbers linked municipal data used provide occupancy previous loss history commercial clients example submissions chubb' business helvetia transforms logistics freight companies insurance professionals helvetia october berkshire hathway guard insurance obtains needed information based data available online offline reduced time submission quote see \"berkshire hathaway guard insurance companies partners planck create full digital underwriting commercial lines\" prnewswire com march swiss sigma product development underwriting owners policy smes flow straight without human intervention chubb' part future intelligent automated workflows triage routing effective current business rules risk assessment underwriting cost effective engaging digitisation reduce cost availability digitised risk information eg biomarkers activity data building insurance turn risk assessment footprints occupancy vessel vehicle data create risk profiles without physical early warning tool risk inspection medical underwriting requirements see figure data sets especially useful geographically scattered assets individuals complex proposals may still require traditional risk assessment intelligent workflows automatically assign tasks suitable risk engineers doctors natural language processing enable underwriters interpret results faster figure different levels underwriting maturity data ai module uw evidence risk data SS learning learning data modelling insights uw models inputs underwriting effort required customer decision outcome dynamic underwriting self optimizing ai driven automatically - social media data underwriting algorithms assign risk class - iot data fitness home using parametrised indexes - activity lifestyle accessed platforms data mobility - minimal customer input - electronic health record accelerated underwriting insurer may request '' - - prescription history either third parties assign risk class - credit data customer additional data - driving records needed telematics - claim experience 'yes' - machine learning expertise additional underwriting traditional underwriting underwriting decision solely data sources - proposal based customer inputs data input emerging - tele interview inspection report lab test physician statement customer currently available sources data increasing data emerging complexity source swiss institute andrew g simpson \"insurtechs take note chubb' digital marketplace serves agents day\" insurancejournal com february swiss sigma dynamic underwriting provide standard accelerated underwriting approaches differ speed mutual benefits insureds static evaluate risk time underwriting track future insurers alike behaviour either positive negative results price inefficiencies policyholders insurers life health greater availability quality electronic health records ehr fitness social media data allow dynamic underwriting provide predictive insights state health information insurers evaluate change risk factors ie smoker status bmi blood glucose blood pressure cholesterol time predict claims probability external data helps digital risk smes mid corporates diverse business activities many exposures assessment sme risks even sometimes small justify complete risk assessment insurer niche products reason insurers leverage third party data sets auto fill proposal forms offer risk scores example online reviews related small businesses used proxy operational risk levels insurers use social media improve customer experience tracking changes sme payrolls office premises revenue indicate growth retrenchment brokers refine target strategies others leverage digital footprints deliver niche products like trade credit insurance smes tracking live financial data cyber new tools ingest analyse data generate cyber risk scores smes less two minutes digital underwriting large large corporate risks require bespoke solutions critical underwriting information corporate risks less advanced risks yet sufficiently digitised underwriting approach case based digitalisation geo spatial information assisted large corporate underwriting extent example insurtechs provide property risk engineering solutions insurers especially assess natural hazards another case cargo sensor data used provide enhanced marine coverage remote risk inspection enabled allowing risk engineers transmit live feeds underwriters full value digitalisation dynamic pricing realised live data industrial control systems facility monitoring systems integrated underwriting systems \" move static data live data insurance\" digitalfineprint com july cyence small business guidewire com insurtech impact oxbow partners smart cargo cyber insurance covus insurance see info corvusinsurance com \"virtual lets see\" virtualitechnologies com swiss sigma product development underwriting pricing coverage determination accelerate risk based price coverage automation submissions risk assessments may trigger faster determination automated based better coverage better pricing automated price coverage determination assessment risk data common personal l h retail p c business several examples beginning emerge china ping claims nearly million applications insurance received auto underwritten ai continued testing focused scalable replicable use cases inform insurer strategies next years mid market commercial insurance segments still require significant manual effort due underwriting complexity price sensitivity valuable wide corporate health insurance automated pricing coverage determination variability costs similar improving sector struggles high costs wide variability treatment procedures costs similar procedures using big data modern tech stack unitedhealthcare bind demand health insurance start ups designing better health coverage plans big employers lower costs better match risks employees identify top optional procedures widest range cost treatment effectiveness members opt options need customers pay base monthly premium much less options employer offers fraud detection managing moral hazard become critical underwriting fraud risk digital world underwriting decisions large volumes insurance applications digital world decisions needed need made near real time insurers careful filter fraudulent near real time applications build wrong customer portfolio successfully using new sources data prevent high risk profiles entering portfolios example online insurer inshared achmea initiative fully automated risk assessment determines risk point purchase using multiple indicators person' conduct payment risk claim risk insurers look innovative digital insurance becomes widespread insurers look new technology help identify high risk approaches help identify high risk cases one five people admit cases lying insurance claims applications counter insurers experimenting new digital tools reduce fraud underwriting stage eg utilising facial biometric tools leverage machine learning computer vision technology another example using third party data insurers may able determine - without medical evidence - whether customer smoker order avoid expensive medical tests cases adverse selection could increase spread digital data insurers also susceptible rising risks insureds withhold new anti selection adverse selection consumers build understanding information insurers health different healthy living apps genetic tests cheaply available direct consumers dtc testing devices available market advances bring equal promise risks including diagnosis subsequent unwarranted treatments major challenge insurers obtain adequate risk relevant information underwriting process since existing regulation mostly enacted widespread adoption dtc genetic tests announcement audited results year ended december ping susan morse \"insurtech entrepreneurs lead panel health \" healthcarefinancenews com september insurers control risks underwriting celent com lying insurance application www finder com october sonar new emeging risk insights swiss institute may swiss sigma underwriting implications insureds insurers constant feedback based processes product development underwriting increasingly interact dependent provide better insights new customer facing functions example marketing models integrate data products obtained initial underwriting experience customer feedback external data make targeted pre approved offers cover right time based specific customer needs behaviours offers focus increasing existing cover suit needs also cross selling new products match new risks however small thereby increase engagement direct impact new product development implications insureds insurers insureds large corporate risks placement negotiations involve underwriters visiting explaining technical aspects clients underwriting never truly customer facing expect modularisation products dynamic pricing adjustment bring higher understanding underwriting risk assessment retail customers example life sme risk scoring apps make opaque cumbersome underwriting process consumer friendly transparent underwriting bundled proactive risk management value added preventive services change insurer customer relationship risk transfer risk partnership insurers insurers benefit dynamic accurate risk pricing need upgrade systems process terrabytes data coming every day sensors many insurers still ironing legacy system challenges also insurers need become responsive digital feedback product development tests experiments digital world offers immediate sometimes ruthless feedback instance one demand digital insurer saw massive drop conversions discovered caused small change process flow requested small segment customers k harris ferrante cio agenda insurance industry insights\" gartner october khusid \" built customer feedback loop works\" medium com january swiss sigma claims management insurers using advanced analytics machine learning create early warning systems gather practical insights prevent accidents simplify speed claims processing examples include using ai detect verify accident hot spots estimate repair costs identify potential fraud historically claims processing form filling exercise digitalisation help improve customer experience efficiency back office processes modular approach key digitalisation improve claims experience terms trust lower friction end end claim digitalisation customer experience complexity back office operations survey adults online us expressed confidence insurer would treat fairly event claim modular approach enable end end digitalisation ongoing claims management processes see table example usaa launched member innovation team digitise components claims operations reduce full claims cycle days even hours cases table claims innovation timeline claims journey past present future key enablers fnol complex forms customer uses automatically iot satellite imagery first notice loss require explanation phone post online satellite weather weather stations iot etc mobile apps stations etc claims admin manually insurer staff data entry insurer digitally augmented connected systems staff support complex intelligent algorithms situations ai advice data gathering rudimentary fraud combination manual automated using computing power fraud detection analytics models identify analytics like big data access varied data fraud ai sources claims estimation manual claims mix manual inspection automated claim computing power better adjudication advanced adjudication algorithms technologies settlement reimbursement via bank direct vendor garage instant payout linked payment systems cheques drafts hospital automated validation intelligent algorithms reimbursement options eg replacement second hand source swiss institute forrester analytics consumer technographics(r) north american financial services survey forrester q4 us inside usaa' claims innovation team dig august swiss sigma end end digitalisation claims management digital loss prevention loss prevention important loss prevention important arguably undervalued aspect insurance undervalued aspect insurance value chain partly due lack research understanding value chain capital intensive nature investing loss prevention technologies requires significant investment possibly little short term impact claims reduction fast growing internet smart across lines business becoming feasible financially technology phone penetration made loss wise alert policyholders potential threat damage life property prevention action feasible instance receiving notification meteorological departments sensors catastrophe insurers send push safety warning notifications customers' smartphones help insureds take preventive measures mitigate damage figure illustrates insurer use external internal systems alert insureds potential threats figure illustration potential digital prevention process natural catastrophe event system integration build platform real time application finding data sources apis nearest safe zone policy holders sensors lower claims routes case evacuation insurers systems consumer takes meteorological capture process steps prevent government agencies prioritise loss life information property notifications real time gis systems alerts sent situation policy holders monitoring cloud based network external systems insurer systems source swiss institute digital first loss notification new insights enabling automated significant innovation first notification loss fnol processes risk management processes via digital given iot sensors high speed internet wide range automated data capture fnol technologies drones satellite imagery data shared range audiences instance csaa insurance partnered owlcam video first notice loss vfnol send videos driver' mobile phone car crashes broken driver share police csaa insurance group owlcam partner accelerate insuranceclaims process customers utilizing video technology csaa insurance group june swiss sigma claims management insurers initiate contact insurers pro actively initiate contact customers offer help even policyholders even claim claim reported pzu largest insurer central eastern europe reported launched call service operational coordinators actively scan digital analog information news loss events web emails calls witnesses tv radio identify customers could potentially impacted loss event verify scope customers' insurance offer assistance services covered policies customer register claim first contact later convenient date claims administration ai applications accelerate phase claims journey case analysed based information customer next stage received fnol eg checking coverage details screening fraudulent claims process claims ai assistance digital assessments automatically verify coverage details like sum assured deductibles co payments past claims reduction manual touchpoints speed process significantly also lower costs associated manual inspections example farmers insurance adjudicates claims using photos video estimating technology total loss auto claims aims expand capability property well fraud detection advanced analytics improve fraud insurance fraud costs billions dollars year traditional fraud detection detection... statistical models drawbacks least sampling methods analyse data lead undetected fraud also methods rely experience new type fraud occurs insurers vulnerable big data ai new tools combine thousands variables moment claim filed identify incoherence data relating claim wider context many insurers use social media analytics detect fraudulent individual claims example allstate leveraging online information identify fraud faster like claimants say injured work yet engage strenuous activity horseback riding analytics help uncover large frauds flagging certain suspicious events like smaller medical clinics originate large volumes claims individual insurers may lack sufficient data detect mass fraud law enforcement agencies co ordinate among carriers analytics know uncover patterns eg search comparable bills coming parties digital loss assessment claims estimation settlement ... also help automate loss digitalisation increase transparency dealing customers provision assessment comprehensive overview calculation could also prevent litigation recalculation costs moreover automation claims assessment help reduce administration costs making insurance affordable california wildfires usaa used images drones settle many home insurance claims day previously could take weeks even months team safely mobilised inspect adjudicate loss property damaged fire interesting experiments emerging example allianz ran pilot amazon enable claims handlers make offer day customer makes claim assessing content loss based live amazon prices pzu successfully built culture innovation efma february \" insurers digitizing claims apps photos videos e payments \" insurancejournal com june \"allstate finds fraud faster innovative data company\" prnewswire com november sigma advanced analytics unlocking new frontiers p c insurance swiss institute august inside usaa' claims innovation team dig august allianz replacement solution - allianz france efma swiss sigma wider adoption figure graphical representation touchless claims settlements work touchless loss assessment claims parametric insurance policies claims personal lines small business settlement personal small insurance largely automated enabling carriers achieve straight business insurance... processing rates reduce claims processing times days hours minutes motor insurance use ai assisted assessment significantly reduce time cost loss assessment ping employed ai settle motor claims across spectrum activities including first notice sharing digital pictures loss assessment document handling surveys completed within - minutes daytime accidents using facial recognition payments made seconds figure touchless claims settlement smart smart smart claims smart parametric insurance distribution policy trigger payout flight delay travel buy platform earth quake digital agent buy broker tropical cyclone direct digital buy mobile agri culture agriculture buy ecosystem source swiss institute ... early experimentation insurers experimenting new claims technology life insurance metlife life lines piloted ethereum blockchain pilot known lifechain help families determine deceased protected policy obituary placed press family automatically informed metlife programme participate lifechain encrypts deceased' national id number searches matching policy match found automatic notification initiates claims process case accident policyholder needs take photos streaming videos vehicle damaged area upload insurer' app together description damage accident \"china' ping p c unveils credit based smart auto insurance claim solution\" insurancejournal com january \" make faster life insurance blockchain \" medium com august swiss sigma claims management claims implications insureds insurers processes become flexible future greater use made characteristics claims submissions open innovation better inform product pricing underwriting decisions insurers generate data leads better risk assessments policy underwriting using real time claims data insurers also able better monitor partners customer segments offer growth potential adapt marketing distribution initiatives real time iterate quickly processes need flexible open innovation insureds quality claims experience strong impact loyalty renewal behaviour property claims satisfaction study jd power \"highly satisfied\" claimants said would definitely renew policies hand claimants \"indifferent\" \"displeased\" said would shop new carrier within year insurers built customer experience digital channels development units introduce design skills thinking methodologies key areas redesigned services processes putting customers' shoes insurers loss prevention become bigger theme claims management insurers combine claims behavioural data action loss prevention measures different components value chain digitally share information feedback could compress value chain digital distribution platforms connect directly capital providers without depending traditional capabilities primary insurers example providers parametric insurance may dispense large claims departments assessors except cases suspected fraud another example global insurer used insurtech program platform transfer part usd million layer parametric risk multiple risk markets satisfaction property insurance claims surges even number catastrophes reaches year high j power finds j power march innovate customers generali op cit \"tremor places world' first programmatic parametric swap transaction\" artemis may swiss sigma outlook digitalisation enable development new data driven business models impact entire insurance value chain access data capability model risks core success true leverage collaboration assets key supplier partners entry points ecosystems know generate customer insights refining existing developing new competencies insurance value chain benefit recent years premium growth slowed insurers looking new data analytics variety approaches differentiate products services within rapidly expanding new economy sectors see figure figure new capabilities established business transformation mindset culture partner network data insights customer insights analytics underwriting superior partner unique data sets deep customer data analytics expertise network access insights access behavioural insights machine intelligence scalable risk modelling risk access collaborative pricing expertise capabilities autonomous robotics capabilities source swiss institute insurers need build capabilities following areas building superior partner network identifying innovative appropriate ecosystem partners suppliers including technology space offer unique value propositions customers competition suitable partners high relationships need fostered focus creating mutual value accessing unique large data sets besides existing accessible data owning unique data sets superior risk modelling becoming important differentiating factor key assemble data layers way creates unique insights cannot easily copied competitors developing deeper customer insights putting customer centre ecosystem aligning actions customer needs using behavioural insights personalise offerings essential holds true even insurer customer relationship directly swiss sigma outlook analytical expertise insurers understand data related context sophisticated risk modelling become ai stands reshape risk landscape resulting better simulations scalable underwriting expertise insurers build superior risk models based existing unique data sets combined analytical methods tools scaled across ecosystems value chains enable insurers make appropriate adjustments value chains respond dynamically impact existing insurance value chain focus customer insights within close interplay product design claims management value chain changing distribution touchpoints generate customer insights changing location yellow box across three panels figure shows sources customer insights changing past product factory insurer disconnected direct customer instead close collaboration insurer physical sales channel intermediary intermediaries direct contact customer customer insights could go back insurer ask products designed particular way figure decreasing distance supplier customer via digital channels yesterday today tomorrow offline offline offline customer journey customer journey customer journey online online online customer corporation retail corporation retail corporation retail digital customer customer customer customer customer customer platform digitally digitally augmented augmented empathic human digital interaction personal personal direct phone personal interface interface channel direct 3rd party online offline chat direct 3rd party smart augmented platform producer product factory product factory digital supplier insurance insurance augmentation critical insights source swiss institute product factory accesses insights today landscape shifted towards multi omni channel approach multiple channels product factory getting closer end customer insurers greater influence product design products must customised sold via different channels simpler products sold direct products greater swiss sigma complexity largely sold personal channels eg life insurance combined investment management greater complexity products lower willingness part consumers buy online time insurers move direct channels simpler products need become product portfolios therefore begin display wide mix insurers create products modularised sliced diced different channels future product factories future product factories able directly access ingredients directly access ingredients product design much closer risk insights customised policy design based digital touchpoints inputs exposure generated real time think result new business models created purely around data access new business models around data including access secure data business models require secure access data combined different ways including insights connected objects platform providers behavioural insights consumer environmental data categories figure show examples data available insurer newer data eg behavioural data based customer context others classical traditional data eg claims preventive services figure illustration potential digital proposition digital business models along customer journey customer journey behavioural offline contextual data insights online smart assistant services insights touch points distribution insights channel market places insights aggregated data insights bm1 environmental data insights contextual provider insights bm2 sensor data insights customer lifestyle cluster health education living free time mobility working insurance value chain insights ie claims product uwr marketing claims customer management pricing sales management service preventive service value adding service insights note bm stands business model source swiss institute swiss sigma outlook data multiple providers via open future insurers operate environment need continuous application programming interfaces access different data sources strategic issue taking insurers beyond api may combined enable existing value chain single firm market place currently provides different business models sources data many data vendors focus extraction distribution data concentrate data refinement expect give rise specialised aggregators focused integration refinement integrated refined data wider service offering customer many implications insurance industry particular need modular products personalisation better distribution insurers may need invest insurers need decide mere suppliers coverage whether separate new data businesses willing collaborate business models gain control key areas impact business different stakeholders seek control data driven businesses example telematics data put onto one platform anonymously would likely market place brokers stake claim players cannot influence data aggregators find going difficult data brokers become omnipresent insurers dependent could time relegated status suppliers data shape new urban future swiss institute january swiss sigma conclusion consumers demanding insurance landscape changing consumers seek engaging ever using digital technology personalised purchase experiences relevant lifestyle empowered insurers respond digitally facilitated information greater choice consumers become informed self directed ever supply side tools like ai enable effective customer interaction allowing insurers better understand consumer preferences develop customised flexible product offering dynamic pricing servicing personal virtual assistance new sources data also offer opportunities granular client segmentation boundaries increasingly blurred impact digitalisation extends beyond insurance value chain across sectors whole business ecosystem insurers operate industry boundaries becoming blurred firms several sectors build digital platforms connect different market places supply chain hubs financial networks non insurer participants business ecosystems like manufacturers telecom companies gaining access customer data digital analytics capabilities enhance client product offering lasting change require utilised fully intelligently new data technology reinforce incremental thinking secure relevance insurance industry future customers true leveraging cross industry ecosystems sigma depicts end version digital insurer look like high barriers entry offer protection competitive threat extent incumbents must continue embrace incremental sometimes radical innovation optimise potential consumers touchpoints insured insurer perspective innovation help transform full scale disruption existing insurers seems unlikely least near term effectiveness raison ' etre incumbents time adjust changing risk environment shifts customer insurance industry attitudes accelerating advances technology room complacency successful insurers leverage insights investments partnerships collaborations upgrade business practices forward thinking innovative insurers could build new infrastructure created today offer compelling risk protection solutions aligned evolving regulation genuinely transform industry effectiveness swiss sigma recent sigma publications data driven insurance ready next frontier emerging markets silver lining amid challenging outlook natural catastrophes man made disasters \"secondary\" perils frontline world insurance great pivot east continues advanced analytics unlocking new frontiers p c insurance indexing resilience primer insurance markets economies global economic insurance outlook natural catastrophes man made disasters year record breaking losses constructing future recent developments engineering insurance world insurance solid mature life markets weigh growth profitability non life insurance mind gap global economic insurance outlook mortality improvement understanding past framing future cyber getting grips complex risk natural catastrophes man made disasters year widespread damages world insurance china growth engine steams ahead insurance adding value development emerging markets commercial insurance expanding scope insurability life force management improving consumer value long term profitability natural catastrophes man made disasters asia suffers substantial losses insuring frontier markets world insurance steady growth amid regional disparities mutual insurance 21st century back future strategic reinsurance insurance increasing trend customised solutions keeping healthy emerging markets insurance help natural catastrophes man made disasters convective winter storms generate losses insurance start new wave world insurance back life underinsurance property risks closing gap life insurance digital age fundamental transformation ahead natural catastrophes man made disasters large losses floods hail haiyan hits philippines digital distribution insurance quiet revolution world insurance steering towards recovery liability claims trends emerging risks rebounding economic drivers care finding sustainable long term care solutions ageing world partnering food security emerging markets natural catastrophes man made disasters year extreme weather events us world insurance progressing long winding road recovery navigating recent developments marine airline insurance urbanisation emerging markets boon bane insurers life insurance focusing consumer published swiss management ltd swiss institute mythenquai p box zurich switzerland telephone email institute swissre com authors dr evangelos avramakis jonathan anchen ashish dave aakash kiran raverkar binay biswal rajeev sharan sophia steinmetz sigma editor (c) swiss rights reserved paul ronke editorial deadline study november managing editors sigma available swiss ' website dan ryan www swissre com sigma dr jerome jean haegeli internet version may contain slightly updated information swiss group chief economist graphic design production corporate real estate logistics media production zurich printing multicolor print ag baar entire content sigma edition subject copyright rights reserved information edition may used private internal purposes provided copyright proprietary notices removed electronic reuse information published sigma prohibited reproduction whole part use public purpose permitted prior written approval swiss institute source reference \"sigma data driven insurance ready next frontier \" indicated courtesy copies appreciated although information used sigma edition taken reliable sources swiss accept responsibility accuracy comprehensiveness information given forward looking statements made information provided forward looking statements made informational purposes way constitute taken reflect swiss ' position particular relation ongoing future dispute event shall swiss liable loss damage arising connection use information readers cautioned place undue reliance forward looking statements swiss undertakes obligation publicly revise update forward looking statements whether result new information future events otherwise order 270 0120 en swiss management ltd swiss institute mythenquai p box zurich switzerland telephone swissre com institute","text_word_count":18846,"text_unique_words":5317,"_deepnote_index_column":4}],"rows_bottom":null},"text/plain":"                                                Name Type  \\\n0           ./sampledocs/Module-1-Lecture-Slides.pdf  pdf   \n1  ./sampledocs/fra-2020-artificial-intelligence_...  pdf   \n2  ./sampledocs/EPAM_Streamlining_the_Auto_Claims...  pdf   \n3  ./sampledocs/Issues_Paper_on_Increasing_Digita...  pdf   \n4                    ./sampledocs/sigma1_2020_en.pdf  pdf   \n\n                                                Text  \\\n0  Application of AI, Insurtech and Real Estate\\n...   \n1  GETTING THE\\nFUTURE RIGHT\\n―\\nARTIFICIAL\\nINTE...   \n2                      WHITE PAPER\\nStreamlining ...   \n3        Issues Paper on Increasing Digitalisatio...   \n4                               No 1 /2020\\nData-...   \n\n                                           cleanText  \\\n0  application ai insurtech real estate technolog...   \n1  getting future right -- artificial intelligenc...   \n2  white paper streamlining automotive claims pro...   \n3  issues paper increasing digitalisation insuran...   \n4  data driven insurance executive summary battle...   \n\n                                                 NLP  text_word_count  \\\n0  (application, ai, insurtech, real, estate, tec...             3732   \n1  (getting, future, right, --, artificial, intel...            49776   \n2  (white, paper, streamlining, automotive, claim...             2502   \n3  (issues, paper, increasing, digitalisation, in...            15393   \n4  (data, driven, insurance, executive, summary, ...            18846   \n\n   text_unique_words  \n0               1509  \n1               8464  \n2               1006  \n3               3684  \n4               5317  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Name</th>\n      <th>Type</th>\n      <th>Text</th>\n      <th>cleanText</th>\n      <th>NLP</th>\n      <th>text_word_count</th>\n      <th>text_unique_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>./sampledocs/Module-1-Lecture-Slides.pdf</td>\n      <td>pdf</td>\n      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n      <td>application ai insurtech real estate technolog...</td>\n      <td>(application, ai, insurtech, real, estate, tec...</td>\n      <td>3732</td>\n      <td>1509</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>./sampledocs/fra-2020-artificial-intelligence_...</td>\n      <td>pdf</td>\n      <td>GETTING THE\\nFUTURE RIGHT\\n―\\nARTIFICIAL\\nINTE...</td>\n      <td>getting future right -- artificial intelligenc...</td>\n      <td>(getting, future, right, --, artificial, intel...</td>\n      <td>49776</td>\n      <td>8464</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>./sampledocs/EPAM_Streamlining_the_Auto_Claims...</td>\n      <td>pdf</td>\n      <td>WHITE PAPER\\nStreamlining ...</td>\n      <td>white paper streamlining automotive claims pro...</td>\n      <td>(white, paper, streamlining, automotive, claim...</td>\n      <td>2502</td>\n      <td>1006</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>./sampledocs/Issues_Paper_on_Increasing_Digita...</td>\n      <td>pdf</td>\n      <td>Issues Paper on Increasing Digitalisatio...</td>\n      <td>issues paper increasing digitalisation insuran...</td>\n      <td>(issues, paper, increasing, digitalisation, in...</td>\n      <td>15393</td>\n      <td>3684</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>./sampledocs/sigma1_2020_en.pdf</td>\n      <td>pdf</td>\n      <td>No 1 /2020\\nData-...</td>\n      <td>data driven insurance executive summary battle...</td>\n      <td>(data, driven, insurance, executive, summary, ...</td>\n      <td>18846</td>\n      <td>5317</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","metadata":{"cell_id":"00024-d02c0230-e529-449a-b094-5074c254f871","deepnote_to_be_reexecuted":false,"source_hash":"8a46534f","execution_start":1609409278383,"deepnote_cell_type":"code"},"source":"fullDocs.info()","execution_count":null,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 8 entries, 0 to 7\nData columns (total 7 columns):\n #   Column             Non-Null Count  Dtype \n---  ------             --------------  ----- \n 0   Name               8 non-null      object\n 1   Type               8 non-null      object\n 2   Text               8 non-null      object\n 3   cleanText          8 non-null      object\n 4   NLP                8 non-null      object\n 5   text_word_count    8 non-null      int64 \n 6   text_unique_words  8 non-null      int64 \ndtypes: int64(2), object(5)\nmemory usage: 576.0+ bytes\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00025-b66fdba4-5470-4f11-96ce-611c4950287a","deepnote_to_be_reexecuted":false,"source_hash":"631c63aa","execution_start":1609409278430,"deepnote_cell_type":"code"},"source":"fullDocs.describe()","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":32,"data":{"application/vnd.deepnote.dataframe.v2+json":{"row_count":8,"column_count":2,"columns":[{"name":"text_word_count","dtype":"float64","stats":{"unique_count":8,"nan_count":0,"min":8,"max":49776,"histogram":[{"bin_start":8,"bin_end":4984.8,"count":2},{"bin_start":4984.8,"bin_end":9961.6,"count":0},{"bin_start":9961.6,"bin_end":14938.400000000001,"count":1},{"bin_start":14938.400000000001,"bin_end":19915.2,"count":3},{"bin_start":19915.2,"bin_end":24892,"count":1},{"bin_start":24892,"bin_end":29868.800000000003,"count":0},{"bin_start":29868.800000000003,"bin_end":34845.6,"count":0},{"bin_start":34845.6,"bin_end":39822.4,"count":0},{"bin_start":39822.4,"bin_end":44799.200000000004,"count":0},{"bin_start":44799.200000000004,"bin_end":49776,"count":1}]}},{"name":"text_unique_words","dtype":"float64","stats":{"unique_count":8,"nan_count":0,"min":8,"max":8464,"histogram":[{"bin_start":8,"bin_end":853.6,"count":1},{"bin_start":853.6,"bin_end":1699.2,"count":1},{"bin_start":1699.2,"bin_end":2544.8,"count":1},{"bin_start":2544.8,"bin_end":3390.4,"count":1},{"bin_start":3390.4,"bin_end":4236,"count":0},{"bin_start":4236,"bin_end":5081.6,"count":2},{"bin_start":5081.6,"bin_end":5927.2,"count":1},{"bin_start":5927.2,"bin_end":6772.8,"count":0},{"bin_start":6772.8,"bin_end":7618.400000000001,"count":0},{"bin_start":7618.400000000001,"bin_end":8464,"count":1}]}},{"name":"_deepnote_index_column","dtype":"object"}],"rows_top":[{"text_word_count":8,"text_unique_words":8,"_deepnote_index_column":"count"},{"text_word_count":19506.375,"text_unique_words":4453.375,"_deepnote_index_column":"mean"},{"text_word_count":15720.028343644932,"text_unique_words":2531.703939896155,"_deepnote_index_column":"std"},{"text_word_count":2502,"text_unique_words":1006,"_deepnote_index_column":"min"},{"text_word_count":11814,"text_unique_words":3140.25,"_deepnote_index_column":"25%"},{"text_word_count":16078,"text_unique_words":4290,"_deepnote_index_column":"50%"},{"text_word_count":22767.25,"text_unique_words":5754.5,"_deepnote_index_column":"75%"},{"text_word_count":49776,"text_unique_words":8464,"_deepnote_index_column":"max"}],"rows_bottom":null},"text/plain":"       text_word_count  text_unique_words\ncount         8.000000            8.00000\nmean      19506.375000         4453.37500\nstd       15720.028344         2531.70394\nmin        2502.000000         1006.00000\n25%       11814.000000         3140.25000\n50%       16078.000000         4290.00000\n75%       22767.250000         5754.50000\nmax       49776.000000         8464.00000","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text_word_count</th>\n      <th>text_unique_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>8.000000</td>\n      <td>8.00000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>19506.375000</td>\n      <td>4453.37500</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>15720.028344</td>\n      <td>2531.70394</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>2502.000000</td>\n      <td>1006.00000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>11814.000000</td>\n      <td>3140.25000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>16078.000000</td>\n      <td>4290.00000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>22767.250000</td>\n      <td>5754.50000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>49776.000000</td>\n      <td>8464.00000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Word count and most common words and nouns","metadata":{"cell_id":"00026-11f4e363-3b29-46dc-bc0c-f62143cb620e","deepnote_cell_type":"markdown"}},{"cell_type":"code","metadata":{"cell_id":"00027-3a8c83b9-96b0-4071-8389-8d9b9a4f32c5","deepnote_to_be_reexecuted":false,"source_hash":"cee157d5","execution_start":1609409278431,"execution_millis":34,"deepnote_cell_type":"code"},"source":"# let us look most used words for each part of speech\nfrom collections import defaultdict, Counter\n\nposCounts = defaultdict(Counter)\n\nfor doc in fullDocs.NLP:\n    for token in doc:\n        posCounts[token.pos][token.orth] += 1\n\nfor pos_id, counts in sorted(posCounts.items()):\n    pos = doc.vocab.strings[pos_id]\n    for orth_id, count in counts.most_common(1):\n        print(pos, count, doc.vocab.strings[orth_id])","execution_count":null,"outputs":[{"name":"stdout","text":"ADJ 547 digital\nADP 134 across\nADV 421 also\nAUX 14 get\nCCONJ 33 yet\nDET 72 another\nINTJ 20 ai\nNOUN 1376 data\nNUM 208 one\nPART 471 '\nPRON 36 us\nPROPN 533 ai\nPUNCT 867 \"\nSCONJ 73 whether\nSYM 3 x\nVERB 376 based\nX 30 ml\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00028-510ac7f3-89eb-48e7-a5a1-1e6b2e345f15","deepnote_to_be_reexecuted":false,"source_hash":"1aa898b7","execution_start":1609409278465,"execution_millis":1,"deepnote_cell_type":"code"},"source":"# add some common words to stop wordds that may not add much context\nfrom spacy.lang.en.stop_words import STOP_WORDS\ncustomStopWords = [\n    'insurance','insurer','customer','technology','datum']\n\nfor w in customStopWords:\n    if w not in STOP_WORDS:\n        STOP_WORDS.add(w)","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cell_id":"00029-f55936ad-a073-4492-b812-e54d976d0d29","deepnote_to_be_reexecuted":false,"source_hash":"9cd9cab8","execution_start":1609409278466,"execution_millis":122,"deepnote_cell_type":"code"},"source":"# get most common nouns\nfrom collections import defaultdict, Counter\nfrom spacy.symbols import nsubj, VERB, dobj, NOUN, root, xcomp, PROPN, NUM,SYM\n# all tokens that arent stop words or punctuations\nwords=[]\nfor doc in fullDocs.NLP:\n    words += [token.lemma_ for token in doc if token.pos==NOUN and (not token.lemma_ in STOP_WORDS)] \n# five most common tokens\nwordFreq = Counter(words)\ncommonWords = wordFreq.most_common(20)\nprint(commonWords)","execution_count":null,"outputs":[{"name":"stdout","text":"[('risk', 503), ('right', 492), ('system', 485), ('business', 432), ('product', 399), ('example', 390), ('company', 366), ('use', 344), ('consumer', 343), ('data', 335), ('model', 332), ('service', 330), ('information', 310), ('case', 290), ('decision', 289), ('process', 281), ('protection', 269), ('value', 259), ('impact', 259), ('claim', 250)]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00030-ad708e1a-9cff-41ad-9092-ce05f825588f","deepnote_to_be_reexecuted":false,"source_hash":"c1f41ff2","execution_start":1609409278591,"execution_millis":463,"deepnote_cell_type":"code"},"source":"import seaborn as sns\n\n\nx, y= [], []\nfor word,count in commonWords[:20]:\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":36,"data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"cell_id":"00031-8dfceb14-5e09-45af-961d-a08155db6a97","deepnote_to_be_reexecuted":false,"source_hash":"a1956e67","execution_start":1609409279057,"execution_millis":92,"deepnote_cell_type":"code"},"source":"# get most common proper nouns\nfrom collections import defaultdict, Counter\nfrom spacy.symbols import nsubj, VERB, dobj, NOUN, root, xcomp, PROPN, NUM,SYM\n# all tokens that arent stop words or punctuations\nwords=[]\nfor doc in fullDocs.NLP:\n    \n    words += [token.lemma_ for token in doc if token.pos==PROPN and (not token.lemma_ in STOP_WORDS)\\\n             and (len(token)>1)] \n\n\n# five most common tokens\nwordFreq = Counter(words)\ncommonWords = wordFreq.most_common(20)\nprint(commonWords)","execution_count":null,"outputs":[{"name":"stdout","text":"[('ai', 533), ('eu', 267), ('insurtech', 173), ('european', 164), ('mi', 163), ('data', 160), ('swiss', 148), ('digital', 102), ('sigma', 98), ('institute', 91), ('council', 74), ('committee', 74), ('fra', 68), ('gdpr', 67), ('mckinsey', 67), ('protection', 66), ('commission', 62), ('iais', 61), ('com', 59), ('iot', 58)]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00032-3d9982ff-75cf-40ec-af2b-fccca70af01f","deepnote_to_be_reexecuted":false,"source_hash":"c1f41ff2","execution_start":1609409279153,"execution_millis":376,"deepnote_cell_type":"code"},"source":"import seaborn as sns\n\n\nx, y= [], []\nfor word,count in commonWords[:20]:\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":38,"data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"cell_id":"00033-ffcba3ff-531b-4a7f-9d1c-86110ada16a5","deepnote_to_be_reexecuted":false,"source_hash":"b8d79b81","execution_start":1609409279532,"execution_millis":40,"deepnote_cell_type":"code"},"source":"# most frequest ORG entities\nentities=[]\nfor doc in fullDocs.NLP:\n    # all entities\n    for ent in doc.ents:\n        #print(ent.text)\n        if ent.label_ == 'ORG':\n            if ent.text not in STOP_WORDS:\n                entities += [ent.text]\nentityFreq = Counter(entities)\ncommonEntities = entityFreq.most_common(40)\nprint(commonEntities)","execution_count":null,"outputs":[{"name":"stdout","text":"[('eu', 205), ('insurtech', 76), ('iais executive committee', 36), ('oecd', 27), ('mckinsey', 26), ('fra', 25), ('gdpr', 22), ('digital', 21), ('ec', 19), ('fca', 17), ('luxembourg publications office', 13), ('regtech', 13), ('european commission', 11), ('dlt', 11), ('allianz', 10), ('alibaba', 10), ('accenture', 9), ('q4', 9), ('rpa', 9), ('cb', 9), ('capgemini world', 8), ('adobe', 8), ('facebook', 8), ('coe', 8), ('amazon', 7), ('google', 7), ('cdo', 7), ('swiss sigma', 7), ('aviva', 7), ('bima', 6), ('european union', 6), ('echr', 6), ('european council', 6), ('mvp', 6), ('mi', 6), ('roi', 6), ('g20', 5), ('fintech', 5), ('bafin', 5), ('cltv', 5)]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00034-4ec2076a-d1b8-4498-8273-e96f4c155677","deepnote_to_be_reexecuted":false,"source_hash":"368c4cf7","execution_start":1609409279576,"execution_millis":370,"deepnote_cell_type":"code"},"source":"import seaborn as sns\n\n\nx, y= [], []\nfor word,count in commonEntities[:20]:\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":40,"data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"cell_id":"00035-31fe0cf1-c393-4717-a1f6-589e267e079a","deepnote_to_be_reexecuted":false,"source_hash":"8477c40","execution_start":1609409279953,"execution_millis":27,"deepnote_cell_type":"code"},"source":"# most frequest PERSON entities\nentities=[]\nfor doc in fullDocs.NLP:\n    # all entities\n    for ent in doc.ents:\n        #print(ent.text)\n        if ent.label_ == 'PERSON':\n            if ent.text not in STOP_WORDS:\n                entities += [ent.text]\nentityFreq = Counter(entities)\ncommonEntities = entityFreq.most_common(40)\nprint(commonEntities)","execution_count":null,"outputs":[{"name":"stdout","text":"[('fra', 10), ('leslie willcocks', 10), ('dpia', 8), ('bima', 6), ('tanguy catlin', 6), ('andrew brem', 6), ('emr ehr', 5), ('christopher geczy', 4), ('eurobarometer', 4), ('bafin', 4), ('holger wilms', 4), ('sartor giovanni', 3), ('christopher morrison', 3), ('johannes tobias lorenz', 3), ('bjorn munstermann', 3), ('tobias lorenz', 3), ('scott simony', 3), ('christopher geczy phd', 2), ('tina rosenberg', 2), ('com bima', 2), ('wessels', 2), ('hadi kharrazi', 2), (\"harold lehmann casey overby taylor '\", 2), ('gliklich leavy', 2), ('volker und markus schecke', 2), ('karntner landesregierung', 2), ('barak', 2), ('lukasz bojarski', 2), ('voor immigratie', 2), ('alessandro mantelero', 2), ('wachter mittelstatt b russel', 2), ('robo', 2), ('evia', 2), ('j sheth et al', 2), ('klinc', 2), ('sandeep bakshi', 2), ('matthew donaldson', 2), ('ido segev', 2), ('mendel', 2), ('nauto', 2)]\n","output_type":"stream"}]},{"cell_type":"code","metadata":{"cell_id":"00036-551c4a13-df97-400b-bfdd-058328b7350c","deepnote_to_be_reexecuted":false,"source_hash":"368c4cf7","execution_start":1609409279984,"execution_millis":380,"deepnote_cell_type":"code"},"source":"import seaborn as sns\n\n\nx, y= [], []\nfor word,count in commonEntities[:20]:\n        x.append(word)\n        y.append(count)\n        \nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":42,"data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"},"output_type":"display_data"}]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00037-c48a2e1f-322c-49b3-bdf8-63375edefe8a","deepnote_to_be_reexecuted":false,"source_hash":"b623e53d","execution_start":1609409280369,"execution_millis":5,"deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"tags":[],"cell_id":"00038-40b5b2b1-9d05-4d12-8f33-7f099c7529c1","deepnote_cell_type":"code"},"source":"","execution_count":null,"outputs":[]}],"nbformat":4,"nbformat_minor":4,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"deepnote_notebook_id":"e9069410-8d8d-4357-b5fe-9d658c38b0fc","deepnote_execution_queue":[]}}