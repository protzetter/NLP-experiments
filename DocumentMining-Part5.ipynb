{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-7079e1ca-a2f2-48e5-b5c0-c4daef44134e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Created on January 1st 2021 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "# Small experiment of document mining with various techniques Part 5\n",
    "\n",
    "This notebook will chech the Azure analytics API and see how this can accelerate the overall process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00001-4ee23b8c-5b4b-4341-b555-0f5cdfb7f019",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18,
    "execution_start": 1609408903481,
    "source_hash": "359195b9"
   },
   "outputs": [],
   "source": [
    "# Import require libraries\n",
    "import numpy as np\n",
    "import texthero as hero\n",
    "import pdftotext\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00005-a1901dd3-c023-404a-ad5e-54bb9e243052",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 6,
    "execution_start": 1609408913062,
    "source_hash": "528c1789"
   },
   "outputs": [],
   "source": [
    "# function to read PDF files using pdftotext\n",
    "def readPdfFile(filename):\n",
    "    text=\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pdf = pdftotext.PDF(f)\n",
    "        for page in pdf:\n",
    "            text=text+page\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00006-7ffcd334-575e-4630-9cdc-b490391638ea",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 7,
    "execution_start": 1609408913073,
    "source_hash": "c5e1e201"
   },
   "outputs": [],
   "source": [
    "# function to read PPT files\n",
    "def readPPTFile(filename):\n",
    "    text=\"\"  \n",
    "    prs = Presentation(filename)\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text=text+shape.text\n",
    "    text=remove_special_characters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00007-ec1a18d4-c491-4d83-bd0b-89d80d023486",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1609409219029,
    "output_cleared": false,
    "source_hash": "bb2fc269"
   },
   "outputs": [],
   "source": [
    "#path of first input test file\n",
    "path='./sampledocs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00021-4abcc142-8b60-4c8d-8eb7-6dfc54ede2cf",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25755,
    "execution_start": 1609409251322,
    "source_hash": "4ff71c02"
   },
   "outputs": [],
   "source": [
    "# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "docNLP=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.pdf')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    fileText=readPdfFile(file_name)\n",
    "    docName.append(file_name)\n",
    "    docType.append('pdf')\n",
    "    docText.append(fileText)\n",
    "list_of_files = glob.glob(path+'*.pptx')           # create the list of file\n",
    "for file_name in list_of_files:\n",
    "    fileText=readPPTFile(file_name)\n",
    "    docName.append(file_name)\n",
    "    docType.append('ppt')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])\n",
    "#fullDocs['NLP']=fullDocs['cleanText'].apply(processDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00022-f30704b2-24ac-4575-a46b-c496aba2c153",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1609409277078,
    "source_hash": "2775c8a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text:173026.0\n",
      "Min length of text:17987\n",
      "Max length of text:464271\n"
     ]
    }
   ],
   "source": [
    " print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00023-477dc6c1-2d27-40f1-8aa6-41969f133484",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 581,
    "execution_start": 1609409277796,
    "source_hash": "aa02f66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/ai-360-research.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>ai insights next frontier business corner offi...</td>\n",
       "      <td>5289</td>\n",
       "      <td>1752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>application ai insurtech real estate technolog...</td>\n",
       "      <td>3732</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16763</td>\n",
       "      <td>4237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>No 5 /2020\\n\\n\\n\\n\\n...</td>\n",
       "      <td>machine intelligence executive summary machine...</td>\n",
       "      <td>14512</td>\n",
       "      <td>4342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/_content_dam_Deloitte_de_Document...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>From mystery to mastery:\\nUnlocking the busine...</td>\n",
       "      <td>mystery mastery unlocking business value artif...</td>\n",
       "      <td>11032</td>\n",
       "      <td>3457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0                   ./sampledocs/ai-360-research.pdf  pdf   \n",
       "1           ./sampledocs/Module-1-Lecture-Slides.pdf  pdf   \n",
       "2  ./sampledocs/Technology-and-innovation-in-the-...  pdf   \n",
       "3                   ./sampledocs/sigma-5-2020-en.pdf  pdf   \n",
       "4  ./sampledocs/_content_dam_Deloitte_de_Document...  pdf   \n",
       "\n",
       "                                                Text  \\\n",
       "0  AI 360: insights from the\\nnext frontier of bu...   \n",
       "1  Application of AI, Insurtech and Real Estate\\n...   \n",
       "2  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "3                            No 5 /2020\\n\\n\\n\\n\\n...   \n",
       "4  From mystery to mastery:\\nUnlocking the busine...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  ai insights next frontier business corner offi...             5289   \n",
       "1  application ai insurtech real estate technolog...             3732   \n",
       "2  technology innovation insurance sector technol...            16763   \n",
       "3  machine intelligence executive summary machine...            14512   \n",
       "4  mystery mastery unlocking business value artif...            11032   \n",
       "\n",
       "   text_unique_words  \n",
       "0               1752  \n",
       "1               1509  \n",
       "2               4237  \n",
       "3               4342  \n",
       "4               3457  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00024-d02c0230-e529-449a-b094-5074c254f871",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1609409278383,
    "source_hash": "8a46534f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10 entries, 0 to 9\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Name               10 non-null     object\n",
      " 1   Type               10 non-null     object\n",
      " 2   Text               10 non-null     object\n",
      " 3   cleanText          10 non-null     object\n",
      " 4   text_word_count    10 non-null     int64 \n",
      " 5   text_unique_words  10 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 608.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cell_id": "00025-b66fdba4-5470-4f11-96ce-611c4950287a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1609409278430,
    "source_hash": "631c63aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>17238.100000</td>\n",
       "      <td>4083.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14729.007181</td>\n",
       "      <td>2398.566797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2502.000000</td>\n",
       "      <td>1006.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6724.750000</td>\n",
       "      <td>2178.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>14951.000000</td>\n",
       "      <td>3961.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>18329.000000</td>\n",
       "      <td>5075.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49779.000000</td>\n",
       "      <td>8462.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_word_count  text_unique_words\n",
       "count        10.000000          10.000000\n",
       "mean      17238.100000        4083.700000\n",
       "std       14729.007181        2398.566797\n",
       "min        2502.000000        1006.000000\n",
       "25%        6724.750000        2178.250000\n",
       "50%       14951.000000        3961.000000\n",
       "75%       18329.000000        5075.500000\n",
       "max       49779.000000        8462.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Azure analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required Azure libraries\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00038-40b5b2b1-9d05-4d12-8f33-7f099c7529c1",
    "deepnote_cell_type": "code",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall sentiment: negative\n",
      "Scores: positive=0.01; neutral=0.21; negative=0.78 \n",
      "\n",
      "Overall sentiment: positive\n",
      "Scores: positive=1.0; neutral=0.0; negative=0.0 \n",
      "\n",
      "Overall sentiment: positive\n",
      "Scores: positive=0.94; neutral=0.05; negative=0.01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#just a quick check that everything is working correctly\n",
    "\n",
    "credential = AzureKeyCredential(\"xxxxxxxxxxxxxxxxxxxxxx\") # put your own access key\n",
    "endpoint=\"https://nlexperiments.cognitiveservices.azure.com/\" # put your access keypoint\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(endpoint, credential)\n",
    "\n",
    "documents = [\n",
    "    \"I did not like the restaurant. The food was too spicy.\",\n",
    "    \"The restaurant was decorated beautifully. The atmosphere was unlike any other restaurant I've been to.\",\n",
    "    \"The food was yummy. :)\"\n",
    "]\n",
    "\n",
    "response = text_analytics_client.analyze_sentiment(documents, language=\"en\")\n",
    "result = [doc for doc in response if not doc.is_error]\n",
    "\n",
    "for doc in result:\n",
    "    print(\"Overall sentiment: {}\".format(doc.sentiment))\n",
    "    print(\"Scores: positive={}; neutral={}; negative={} \\n\".format(\n",
    "        doc.confidence_scores.positive,\n",
    "        doc.confidence_scores.neutral,\n",
    "        doc.confidence_scores.negative,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=list(fullDocs.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Args:\n",
    "        data: str\n",
    "    \n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    sentences = data.split(\"\\n\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Additional clearning (This part is already implemented)\n",
    "    # - Remove leading and trailing spaces from each sentence\n",
    "    # - Drop sentences if they are empty strings.\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split all documents in sentences to process in Azure Analytics later on\n",
    "listofdocumentsentences=[]\n",
    "for doc in documents:\n",
    "    sentences=split_to_sentences(doc)\n",
    "    listofsentences=[]\n",
    "    for sent in sentences:\n",
    "        if len(sent) > 5120:\n",
    "            print('max limit reached')\n",
    "        else:\n",
    "            listofsentences.append(sent)\n",
    "    listofdocumentsentences.append(listofsentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a generator to return list of 5 sentences to use with Azure Analytics\n",
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityList=[]\n",
    "for listofsentences in listofdocumentsentences:\n",
    "    entPerDoc=[]\n",
    "    for c in chunks(listofsentences,5):\n",
    "        try:\n",
    "            response = text_analytics_client.recognize_entities(c, language=\"en\")\n",
    "            result = [doc for doc in response if not doc.is_error]\n",
    "            for doc in result:\n",
    "                for entity in doc.entities:\n",
    "                    entPerDoc.append(entity)\n",
    "        except Exception as err:\n",
    "            print(\"Encountered exception. {}\".format(err))\n",
    "    entityList.append(entPerDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let us save the result in a file for later use\n",
    "import pickle \n",
    "filename='./results'\n",
    "filehandler = open(filename, 'wb') \n",
    "pickle.dump(entityList,filehandler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entityList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Address 41 University Drive, Suite 202 1\n",
      "DateTime may 144\n",
      "Email institute@swissre.com 2\n",
      "Event interviews 19\n",
      "Location US 71\n",
      "Organization EU 177\n",
      "Person Digitalisation 11\n",
      "PersonType customer 393\n",
      "Phone Number 41 43 285 2551 4\n",
      "Product cars 26\n",
      "Quantity 1 223\n",
      "Skill AI 1109\n",
      "URL axco.co.uk 4\n"
     ]
    }
   ],
   "source": [
    "# let us look most used words for each entity category\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "posCounts = defaultdict(Counter)\n",
    "\n",
    "for doc in entityList:\n",
    "    for entity in doc:\n",
    "        posCounts[entity.category][entity.text] += 1\n",
    "\n",
    "for pos_id, counts in sorted(posCounts.items()):\n",
    "    #print(pos_id, counts)\n",
    "    for orth_id, count in counts.most_common(1):\n",
    "        print(pos_id,orth_id, count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "e9069410-8d8d-4357-b5fe-9d658c38b0fc",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
