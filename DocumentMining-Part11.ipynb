{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-7079e1ca-a2f2-48e5-b5c0-c4daef44134e",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "Created on April 11th 2021 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "# Small experiment of document mining with various techniques Part 11\n",
    "\n",
    "We will be using the brand new spacy version 3.0 for sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00001-4ee23b8c-5b4b-4341-b555-0f5cdfb7f019",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 18,
    "execution_start": 1609408903481,
    "source_hash": "359195b9"
   },
   "outputs": [],
   "source": [
    "# Import require libraries\n",
    "import spacy\n",
    "import texthero as hero\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\u001b[38;5;2m✔ Loaded compatibility table\u001b[0m\n",
      "\u001b[1m\n",
      "====================== Installed models (spaCy v2.3.7) ======================\u001b[0m\n",
      "\u001b[38;5;4mℹ spaCy installation:\n",
      "/home/studio-lab-user/.conda/envs/default/lib/python3.9/site-packages/spacy\u001b[0m\n",
      "\n",
      "TYPE      NAME             MODEL            VERSION                            \n",
      "package   en-core-web-sm   en_core_web_sm   \u001b[38;5;2m2.3.1\u001b[0m   \u001b[38;5;2m✔\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#  validate spacy language models just in case, this command does not work on Mac ARM systems unless you have installed the brew workaround and reinstalled python\n",
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00007-ec1a18d4-c491-4d83-bd0b-89d80d023486",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1609409219029,
    "output_cleared": false,
    "source_hash": "bb2fc269"
   },
   "outputs": [],
   "source": [
    "#path of first input test file\n",
    "path='./sampledocs/txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00021-4abcc142-8b60-4c8d-8eb7-6dfc54ede2cf",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 25755,
    "execution_start": 1609409251322,
    "source_hash": "4ff71c02"
   },
   "outputs": [],
   "source": [
    "# let us scan the full directory, read the text files and clean them using texthero\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.txt')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    f = open(file_name,'r')\n",
    "    fileText=f.read()\n",
    "    docName.append(file_name)\n",
    "    docType.append('txt')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00022-f30704b2-24ac-4575-a46b-c496aba2c153",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 0,
    "execution_start": 1609409277078,
    "source_hash": "2775c8a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text:91714.61111111111\n",
      "Min length of text:9170\n",
      "Max length of text:328295\n"
     ]
    }
   ],
   "source": [
    " print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00023-477dc6c1-2d27-40f1-8aa6-41969f133484",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 581,
    "execution_start": 1609409277796,
    "source_hash": "aa02f66"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/txt/AI-bank-of-the-future-Can-ban...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Global Banking &amp; Securities\\n\\nAI-bank of the ...</td>\n",
       "      <td>global banking securities ai bank future banks...</td>\n",
       "      <td>5774</td>\n",
       "      <td>2144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/txt/Artificial Financial Intellig...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Texas A&amp;M University School of Law\\n\\nTexas A&amp;...</td>\n",
       "      <td>texas university school law texas law scholars...</td>\n",
       "      <td>22240</td>\n",
       "      <td>6349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/txt/Data machine the insurers usi...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Data machine: the insurers using AI to reshape...</td>\n",
       "      <td>data machine insurers using ai reshape industr...</td>\n",
       "      <td>1454</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/txt/Digital-disruption-in-Insuran...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Digital disruption\\nin insurance:\\nCutting thr...</td>\n",
       "      <td>digital disruption insurance cutting noise con...</td>\n",
       "      <td>34485</td>\n",
       "      <td>7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/txt/Impact-Big-Data-AI-in-the-Ins...</td>\n",
       "      <td>txt</td>\n",
       "      <td>The Impact of Big Data and\\nArtificial Intelli...</td>\n",
       "      <td>impact big data artificial intelligence ai ins...</td>\n",
       "      <td>13471</td>\n",
       "      <td>3467</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/txt/AI-bank-of-the-future-Can-ban...  txt   \n",
       "1  ./sampledocs/txt/Artificial Financial Intellig...  txt   \n",
       "2  ./sampledocs/txt/Data machine the insurers usi...  txt   \n",
       "3  ./sampledocs/txt/Digital-disruption-in-Insuran...  txt   \n",
       "4  ./sampledocs/txt/Impact-Big-Data-AI-in-the-Ins...  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Global Banking & Securities\\n\\nAI-bank of the ...   \n",
       "1  Texas A&M University School of Law\\n\\nTexas A&...   \n",
       "2  Data machine: the insurers using AI to reshape...   \n",
       "3  Digital disruption\\nin insurance:\\nCutting thr...   \n",
       "4  The Impact of Big Data and\\nArtificial Intelli...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  global banking securities ai bank future banks...             5774   \n",
       "1  texas university school law texas law scholars...            22240   \n",
       "2  data machine insurers using ai reshape industr...             1454   \n",
       "3  digital disruption insurance cutting noise con...            34485   \n",
       "4  impact big data artificial intelligence ai ins...            13471   \n",
       "\n",
       "   text_unique_words  \n",
       "0               2144  \n",
       "1               6349  \n",
       "2                684  \n",
       "3               7049  \n",
       "4               3467  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cell_id": "00024-d02c0230-e529-449a-b094-5074c254f871",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1609409278383,
    "source_hash": "8a46534f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Name               18 non-null     object\n",
      " 1   Type               18 non-null     object\n",
      " 2   Text               18 non-null     object\n",
      " 3   cleanText          18 non-null     object\n",
      " 4   text_word_count    18 non-null     int64 \n",
      " 5   text_unique_words  18 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 992.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00025-b66fdba4-5470-4f11-96ce-611c4950287a",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_start": 1609409278430,
    "source_hash": "631c63aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>13696.666667</td>\n",
       "      <td>3516.944444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12138.456947</td>\n",
       "      <td>2126.698366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1454.000000</td>\n",
       "      <td>684.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5404.250000</td>\n",
       "      <td>1872.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>10611.000000</td>\n",
       "      <td>3148.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>16399.000000</td>\n",
       "      <td>4303.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>49748.000000</td>\n",
       "      <td>8458.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       text_word_count  text_unique_words\n",
       "count        18.000000          18.000000\n",
       "mean      13696.666667        3516.944444\n",
       "std       12138.456947        2126.698366\n",
       "min        1454.000000         684.000000\n",
       "25%        5404.250000        1872.500000\n",
       "50%       10611.000000        3148.500000\n",
       "75%       16399.000000        4303.750000\n",
       "max       49748.000000        8458.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process files with spacy Sentencizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy with transformer model excluding standard \n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "#nlp = spacy.load(\"en_core_web_sm\", exclude=[\"tok2vec\", \"tagger\", \"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to process documents in an apply function and return the nlp object\n",
    "def processDoc(doc):\n",
    "    return nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test='I love Safaris. I want to go to South Africa .'\n",
    "doc=nlp(test)\n",
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['NLP']=fullDocs['cleanText'].apply(processDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Summarization Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformers installation\n",
    "! pip install transformers datasets\n",
    "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
    "# ! pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-24 14:57:37.604478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-24 14:57:37.604510: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-24 14:57:41.848979: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-24 14:57:41.849014: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-24 14:57:41.849042: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (default): /proc/driver/nvidia/version does not exist\n",
      "2022-04-24 14:57:41.850566: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-04-24 14:57:41.881535: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "All model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\",model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'artificial intelligence (AI) technologies haveadvanced even further . their transformative impact is increasingly evident across industries . machines are tailoring digital content to individual tastes and preferences '}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarizer('in 2016, AlphaGo, a machine, defeated 18-time world champion Lee Sedol at the game of Go, a complex board game requiring intuition, imagination, and strategic thinking—abilities long considered distinctly human. Since then, artificial intelligence (AI) technologies haveadvanced even further,and their transformativeimpact is increasingly evident acrossindustries. AI-powered machines are tailoringrecommendations of digital content to individualtastes and preferences, designing clothinglines for fashion retailers, and even beginning tosurpass experienced doctors in detecting signs ofcancer', max_length=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us summarize the documents using standard transformer pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to process documents in an apply function and return the nlp object\n",
    "def summarizeDoc(doc):\n",
    "    doc = ' '.join(doc.split())\n",
    "    return summarizer(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (5452 > 512). Running this sequence through the model will result in indexing errors\n",
      "2022-04-24 15:03:01.788355: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1426766592 exceeds 10% of free system memory.\n",
      "2022-04-24 15:03:03.268146: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1426766592 exceeds 10% of free system memory.\n",
      "2022-04-24 15:03:03.596629: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1426766592 exceeds 10% of free system memory.\n",
      "2022-04-24 15:03:04.278938: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1426766592 exceeds 10% of free system memory.\n",
      "2022-04-24 15:03:04.716394: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 1426766592 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "fullDocs['summary']=fullDocs['cleanText'].apply(summarizeDoc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documents are too long to fit into the maximum sequence length and hence are trunctaed. We need to find another way to summarize longer documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing Long Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "e9069410-8d8d-4357-b5fe-9d658c38b0fc",
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
