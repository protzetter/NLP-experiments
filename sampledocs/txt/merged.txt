Global Banking & Securities

AI-bank of the future: Can
banks meet the AI challenge?
Artificial intelligence technologies are increasingly integral to world we live
in, and banks need to deploy these technologies at scale to remain relevant.
Success requires a holistic transformation spanning multiple layers of the
organization.
by Suparna Biswas, Brant Carson, Violet Chung, Shwaitang Singh, and Renny Thomas

© Getty Images

September 2020

In 2016, AlphaGo, a machine, defeated 18-time
world champion Lee Sedol at the game of
Go, a complex board game requiring intuition,
imagination, and strategic thinking—abilities
long considered distinctly human. Since then,
artificial intelligence (AI) technologies have
advanced even further,¹ and their transformative
impact is increasingly evident across
industries. AI-powered machines are tailoring
recommendations of digital content to individual
tastes and preferences, designing clothing
lines for fashion retailers, and even beginning to
surpass experienced doctors in detecting signs of
cancer. For global banking, McKinsey estimates
that AI technologies could potentially deliver up to
$1 trillion of additional value each year.²
Many banks, however, have struggled to move
from experimentation around select use cases to
scaling AI technologies across the organization.
Reasons include the lack of a clear strategy for AI,
an inflexible and investment-starved technology
core, fragmented data assets, and outmoded
operating models that hamper collaboration
between business and technology teams. What
is more, several trends in digital engagement
have accelerated during the COVID-19 pandemic,
and big-tech companies are looking to enter
financial services as the next adjacency. To
compete successfully and thrive, incumbent
banks must become “AI-first” institutions,
adopting AI technologies as the foundation for
new value propositions and distinctive customer
experiences.
In this article, we propose answers to four
questions that can help leaders articulate a clear
vision and develop a road map for becoming an
AI-first bank:
1. Why must banks become AI-first?
2. What might the AI-bank of the future look like?

1

3. What obstacles prevent banks from deploying
AI capabilities at scale?
4. How can banks transform to become AI-first?

1. Why must banks become AI-first?
Over several decades, banks have continually
adapted the latest technology innovations to
redefine how customers interact with them. Banks
introduced ATMs in the 1960s and electronic,
card-based payments in the ’70s. The 2000s saw
broad adoption of 24/7 online banking, followed
by the spread of mobile-based “banking on the go”
in the 2010s.
Few would disagree that we’re now in the
AI-powered digital age, facilitated by falling costs
for data storage and processing, increasing
access and connectivity for all, and rapid
advances in AI technologies. These technologies
can lead to higher automation and, when deployed
after controlling for risks, can often improve upon
human decision making in terms of both speed
and accuracy. The potential for value creation
is one of the largest across industries, as AI can
potentially unlock $1 trillion of incremental value
for banks, annually (Exhibit 1).
Across more than 25 use cases,³ AI technologies
can help boost revenues through increased
personalization of services to customers (and
employees); lower costs through efficiencies
generated by higher automation, reduced errors
rates, and better resource utilization; and uncover
new and previously unrealized opportunities
based on an improved ability to process and
generate insights from vast troves of data.
More broadly, disruptive AI technologies can
dramatically improve banks’ ability to achieve
four key outcomes: higher profits, at-scale
personalization, distinctive omnichannel

AI can be defined as the ability of a machine to perform cognitive functions associated with human minds (e.g., perceiving, reasoning, learning, and
problem solving). It includes various capabilities, such as machine learning, facial recognition, computer vision, smart robotics, virtual agents, and
autonomous vehicles. See “Global AI Survey: AI proves its worth, but few scale impact,” November 2019, McKinsey.com.
2
“The executive’s AI playbook,” McKinsey.com.
3
For an interactive view, visit: www.mckinsey.com/business-functions/mckinsey-analytics/our-insights/the-executives-aiplaybook?page=industries/banking/
2

Exhibit 1

Potential
ofof
AIAI
and
analytics
forfor
global
banking
could
reach
as high
Potentialannual
annualvalue
value
and
analytics
global
banking
could
reach
as as
$1
trillion.​
high as $1 trillion.
Total potential annual value, $ billion
1,022.4 (15.4% of sales)
Traditional AI
and analytics

Advanced AI

361.5

660.9

% of value driven by advanced AI, by function
100
Finance and IT: 8.0
0.0
50

0.0

2.4

HR: 14.2

Marketing and sales: 624.8
0

8.0

Other operations: $2.4 B

363.8 261.1

8.6

Risk: 372.9
288.6

5.7

84.3

Source: "The executive's AI playbook," McKinsey.com. (See "Banking," under "Value & Assess.")

experiences, and rapid innovation cycles. Banks
that fail to make AI central to their core strategy
and operations—what we refer to as becoming
“AI-first”—will risk being overtaken by competition
and deserted by their customers. This risk is
further accentuated by four current trends:
—

4
5

Rising customer expectations as adoption
of digital banking increases. In the first few
months of the COVID-19 pandemic, use of
online and mobile banking channels across
countries has increased by an estimated 20
to 50 percent and is expected to continue at
this higher level once the pandemic subsides.
Across diverse global markets, between 15 and
45 percent of consumers expect to cut back
on branch visits following the end of the crisis.⁴

As consumers increase their use of digital
banking services, they grow to expect more,
particularly when compared to the standards
they are accustomed to from leading consumerinternet companies. Meanwhile, these digital
experience leaders continuously raise the bar
on personalization, to the point where they
sometimes anticipate customer needs before
the customer is aware of them, and offer highlytailored services at the right time, through the
right channel.
—

Leading financial institutions’ use of advanced
AI technologies is steadily increasing. Nearly
60 percent of financial-services sector
respondents in McKinsey’s Global AI Survey
report⁵ that their companies have embedded

John Euart, Nuno Ferreira, Jonathan Gordon, Ajay Gupta, Atakan Hilal, Olivia White, “A global view of financial life during COVID-19—an update,”
July 2020, McKinsey.com.
Arif Cam, Michael Chui, Bryce Hall, “Global AI Survey: AI proves its worth, but few scale impact,” November 2019, McKinsey.com.

AI-bank of the future: Can banks meet the AI challenge?

3

at least one AI capability. The most commonly
used AI technologies are: robotic process
automation (36 percent) for structured
operational tasks; virtual assistants or
conversational interfaces (32 percent ) for
customer service divisions; and machine
learning techniques (25 percent) to detect
fraud and support underwriting and risk
management. While for many financial services
firms, the use of AI is episodic and focused on
specific use cases, an increasing number of
banking leaders are taking a comprehensive
approach to deploying advanced AI, and
embedding it across the full lifecycle, from the
front- to the back-office (Exhibit 2).
—

Digital ecosystems are disintermediating
traditional financial services. By enabling
access to a diverse set of services through
a common access point, digital ecosystems
have transformed the way consumers discover,
evaluate, and purchase goods and services.
For example, WeChat users in China can use
the same app not only to exchange messages,

but also to book a cab, order food, schedule
a massage, play games, send money to a
contact, and access a personal line of credit.
Similarly, across countries, nonbanking
businesses and “super apps” are embedding
financial services and products in their
journeys, delivering compelling experiences
for customers, and disrupting traditional
methods for discovering banking products and
services. As a result, banks will need to rethink
how they participate in digital ecosystems,
and use AI to harness the full power of data
available from these new sources.
— Technology giants are entering financial
services as the next adjacency to their
core business models. Globally, leading
technology giants have built extraordinary
market advantages: a large and engaged
customer network; troves of data, enabling a
robust and increasingly precise understanding
of individual customers; natural strengths
in developing and scaling innovative
technologies (including AI); and access to

Web <year>
<article slug>
Exhibit
<x>
Exhibit
2 of <y>

Banks are
AIAI
technologies
to improve
customer
Banks
areexpanding
expandingtheir
theiruse
useofof
technologies
to improve
customer
experiences and
processes.​
experiences
andback-office
back-office
processes.
Front office
Smile-to-pay facial scanning
to initiate transaction

Conversational bots for
basic servicing requests

4

Back office
Micro-expression analysis
with virtual loan officers

Biometrics (voice, video,
print) to authenticate and
authorize

Humanoid robots in branches Machine vision and naturalto serve customers
language processing to scan
and process documents

AI-bank of the future: Can banks meet the AI challenge?

Machine learning to detect
fraud patterns,
cybersecurity attacks

Real-time transaction
analysis for risk monitoring

low-cost capital. In the past, tech giants have
aggressively entered into adjacent businesses
in search of new revenue streams and to
keep customers engaged with a fresh stream
of offerings. Big-tech players have already
gained a foothold in financial services in select
domains (especially in payments and, in some
cases, lending and insurance), and they may
soon look to press their advantages to deepen
their presence and build greater scale.

2. What might the AI-bank of the
future look like?
To meet customers’ rising expectations and
beat competitive threats in the AI-powered

digital era, the AI-first bank will offer propositions
and experiences that are intelligent (that
is, recommending actions, anticipating and
automating key decisions or tasks), personalized
(that is, relevant and timely, and based on a
detailed understanding of customers’ past
behavior and context), and truly omnichannel
(seamlessly spanning the physical and online
contexts across multiple devices, and delivering
a consistent experience) and that blend banking
capabilities with relevant products and services
beyond banking. Exhibit 3 illustrates how such a
bank could engage a retail customer throughout
the day. Exhibit 4 shows an example of the banking
experience of a small-business owner or the
treasurer of a medium-size enterprise.

Exhibit 3

How
banking
a retail
customer.
How AI
AI transforms
transforms banking
forfor
a retail
customer.​
Name: Anya
Age: 28 years
Occupation: Working professional

Seamless
integration with
nonbanking apps

Anya uses smileto-pay to
initiate payment

Analyticsbacked
personalized offers

App offers moneymanagement and
savings solutions,
prioritizes card
payments

Anya receives
integrated portfolio
view and a set of
actions with the
potential to
Aggregated
augment
returns
overview of daily
activities

Savings and
Bank app
Personalized
Anya receives
Facial recognition Anya gets 2% off
investment
recomrecognizes Anya's
money-management
on health
for frictionless
end-of-day
mendations
solutions
spending patterns
payment
insurance
overview of her
and suggests
premiums based
activities, with
coffee at nearby
on her gym
augmented reality,
cafes
activity and
and reminders to
sleep habits
pay bills
Intelligent

Personalized

AI-bank of the future: Can banks meet the AI challenge?

Omnichannel

Banking and beyond banking

5

Exhibit 4

How AI transforms banking for a small- or medium-size-enterprise customer. ​

How AI transforms banking for a small- or medium-size-enterprise customer.
Name: Dany
Age: 36 years
Occupation: Treasurer of a small manufacturing unit
Dany answers
short questionnaire;
app scans his facial
movements
Customized
lending solutions

Firm is credited
with funds after
application
approval

Dany is assisted
in sourcing and
selecting the
Seamless
right vendors
inventory and receivand partners
ables management

Bank is integrated Micro-expression
App suggests
with client
analysis to review loan items to reorder,
business
applications
gives visual reports
management
on receivables
systems
management
Dany receives
Dany gets loan
customized
offer based on
solutions for
company projected
invoice discounting,
cash flows
factoring, etc.
Intelligent

Personalized

Internally, the AI-first institution will be optimized
for operational efficiency through extreme
automation of manual tasks (a “zero-ops” mindset)
and the replacement or augmentation of human
decisions by advanced diagnostic engines in
diverse areas of bank operations. These gains
in operational performance will flow from broad
application of traditional and leading-edge AI
technologies, such as machine learning and
facial recognition, to analyze large and complex
reserves of customer data in (near) real time.

6

AI-bank of the future: Can banks meet the AI challenge?

SME platform to
source suppliers
and buyers

Omnichannel

An AI-powered
virtual adviser
resolves queries
Dany seeks
Beyondprofessional advice
banking support on a lending offer
services

Dany gets prefilled Serviced by an AItax documents to powered virtual
review and
adviser
approve; files with
a single click

Banking and beyond banking

The AI-first bank of the future will also enjoy
the speed and agility that today characterize
digital-native companies. It will innovate
rapidly, launching new features in days or
weeks instead of months. It will collaborate
extensively with partners to deliver new
value propositions integrated seamlessly
across journeys, technology platforms, and
data sets.

3. What obstacles prevent banks from
deploying AI capabilities at scale?
Incumbent banks face two sets of objectives,
which on first glance appear to be at odds. On
the one hand, banks need to achieve the speed,
agility, and flexibility innate to a fintech. On the
other, they must continue managing the scale,
security standards, and regulatory requirements
of a traditional financial-services enterprise.
Despite billions of dollars spent on changethe-bank technology initiatives each year, few
banks have succeeded in diffusing and scaling
AI technologies throughout the organization.
Among the obstacles hampering banks’ efforts,
the most common is the lack of a clear strategy
for AI.⁶ Two additional challenges for many
banks are, first, a weak core technology and data
backbone and, second, an outmoded operating
model and talent strategy.
Built for stability, banks’ core technology
systems have performed well, particularly in
supporting traditional payments and lending
operations. However, banks must resolve
several weaknesses inherent to legacy systems
before they can deploy AI technologies at scale
(Exhibit 5). First and foremost, these systems
often lack the capacity and flexibility required
to support the variable computing requirements,
data-processing needs, and real-time analysis
that closed-loop AI applications require.⁷ Core
systems are also difficult to change, and their
maintenance requires significant resources.
What is more, many banks’ data reserves are
fragmented across multiple silos (separate
business and technology teams), and analytics
efforts are focused narrowly on stand-alone use

6
7

cases. Without a centralized data backbone, it is
practically impossible to analyze the relevant data
and generate an intelligent recommendation or
offer at the right moment. If data constitute the
bank’s fundamental raw material, the data must be
governed and made available securely in a manner
that enables analysis of data from internal and
external sources at scale for millions of customers,
in (near) real time, at the “point of decision” across
the organization. Lastly, for various analytics and
advanced-AI models to scale, organizations need
a robust set of tools and standardized processes
to build, test, deploy, and monitor models, in a
repeatable and “industrial” way.
Banks’ traditional operating models further
impede their efforts to meet the need for
continuous innovation. Most traditional banks
are organized around distinct business lines,
with centralized technology and analytics
teams structured as cost centers. Business
owners define goals unilaterally, and alignment
with the enterprise’s technology and analytics
strategy (where it exists) is often weak or
inadequate. Siloed working teams and “waterfall”
implementation processes invariably lead
to delays, cost overruns, and suboptimal
performance. Additionally, organizations lack
a test-and-learn mindset and robust feedback
loops that promote rapid experimentation and
iterative improvement. Often unsatisfied with the
performance of past projects and experiments,
business executives tend to rely on third-party
technology providers for critical functionalities,
starving capabilities and talent that should ideally
be developed in-house to ensure competitive
differentiation.

Michael Chui, Sankalp Malhotra, “AI adoption advances, but foundational barriers remain,” November 2018, McKinsey.com.
“Closed loop” refers to the fact that the models’ intelligence is applied to incoming data in near real time, which in turn refines the content presented
to the user in near real time.

AI-bank of the future: Can banks meet the AI challenge?

7

Exhibit 5

Investmentsinincore
core
tech
critical
to meet
increasing
demands
for
Investments
tech
areare
critical
to meet
increasing
demands
for
scalability,flexibility,
flexibility,and
and
speed.
scalability,
speed.​

Cloud

Data

Challenges

How cloud computing can help

Core/legacy systems can’t scale sufficiently
(eg, 150+ transactions/second)

Enables higher scalability, resilience of services and
platforms through virtualization of infrastructure

Significant time, effort, and team sizes
required to maintain infrastructure

Reduces IT overhead, enables automation of several
infrastructure-management tasks, and allows development
teams to “self-serve”

Long time required to provision environments
for development and testing (eg, 40+ days in
some cases)

8

Enables faster time to market; dramatically reduces time by
providing managed services (e., setting up new environments
in minutes vs days)

Challenges

How best-in-class data management can help

High error rates; poor refresh rates; lack of
golden source of truth

Ensures high degree of accuracy and single source of truth
in a cost-effective manner

Hard to access in a timely fashion for various
use cases

Enables timely and role-appropriate access for various use
cases (eg, regulatory, business intelligence at scale, advanced
analytics and machine learning, exploratory)

Data trapped in silos across multiple units and
hard to integrate with external sources

1

API1

Enables a 360-degree view across the organization to enable
generation of deeper insights by decision-making algorithms
and models

Challenges

How APIs can help

Longer time to market, limited reusability of
code and software across internal teams

Promote reusability and accelerate development by enabling
access to granular services (internal and external)

Hard to partner or collaborate with external
partners; long time to integrate

Reduce complexity and enable faster collaboration with
external partners

Suboptimal user experience—hard to stitch
data and services across multiple functional
siloes for an integrated proposition

Enhance customer experience by enabling timely access to
data and services across different teams; faster time to market
due to limited coordination, cross-team testing

Application programming interface.

AI-bank of the future: Can banks meet the AI challenge?

4. How can banks transform to
become AI-first?
To overcome the challenges that limit
organization-wide deployment of AI
technologies, banks must take a holistic
approach. To become AI-first, banks must invest
in transforming capabilities across all four layers
of the integrated capability stack (Exhibit 6): the
engagement layer, the AI-powered decisioning
layer, the core technology and data layer, and the
operating model.
As we will explain, when these interdependent
layers work in unison, they enable a bank to
provide customers with distinctive omnichannel
experiences, support at-scale personalization,
and drive the rapid innovation cycles critical
to remaining competitive in today’s world.
Each layer has a unique role to play—underinvestment in a single layer creates a weak link
that can cripple the entire enterprise.
The following paragraphs explore some of the
changes banks will need to undertake in each
layer of this capability stack.
Layer 1: Reimagining the customer
engagement layer
Increasingly, customers expect their bank to be
present in their end-use journeys, know their
context and needs no matter where they interact
with the bank, and to enable a frictionless
experience. Numerous banking activities
(e.g., payments, certain types of lending) are
becoming invisible, as journeys often begin and
end on interfaces beyond the bank’s proprietary
platforms. For the bank to be ubiquitous in
customers’ lives, solving latent and emerging
needs while delivering intuitive omnichannel
experiences, banks will need to reimagine how
they engage with customers and undertake
several key shifts.

First, banks will need to move beyond highly
standardized products to create integrated
propositions that target “jobs to be done.”⁸ This
requires embedding personalization decisions
(what to offer, when to offer, which channel
to offer) in the core customer journeys and
designing value propositions that go beyond the
core banking product and include intelligence
that automates decisions and activities on
behalf of the customer. Further, banks should
strive to integrate relevant non-banking
products and services that, together with the
core banking product, comprehensively address
the customer end need. An illustration of the
“jobs-to-be-done” approach can be seen in the
way fintech Tally helps customers grapple with
the challenge of managing multiple credit cards.
The fintech’s customers can solve several pain
points—including decisions about which card to
pay first (tailored to the forecast of their monthly
income and expenses), when to pay, and how
much to pay (minimum balance versus retiring
principal)—a complex set of tasks that are often
not done well by customers themselves.
The second necessary shift is to embed
customer journeys seamlessly in partner
ecosystems and platforms, so that banks
engage customers at the point of end use and
in the process take advantage of partners’
data and channel platform to increase higher
engagement and usage. ICICI Bank in India
embedded basic banking services on WhatsApp
(a popular messaging platform in India) and
scaled up to one million users within three
months of launch.⁹ In a world where consumers
and businesses rely increasingly on digital
ecosystems, banks should decide on the
posture they would like to adopt across multiple
ecosystems—that is, to build, orchestrate, or
partner—and adapt the capabilities of their
engagement layer accordingly.

8

Clayton M. Christensen, Taddy Hall, Karen Dillon and David S. Duncan, “Know your customers ‘jobs to be done,” Harvard Business Review,
September 2016, hbr.org.
9
“ICICI Bank crosses 1 million users on WhatsApp platform,” Live Mint, July 7, 2020, livemint.com.

AI-bank of the future: Can banks meet the AI challenge?

9

Exhibit 6

To
AI-first
institution,
a bank
must
streamline
its capability
stack stack
for
Tobecome
becomeanan
AI-first
institution,
a bank
must
streamline
its capability
value
creation.​
for value creation.
AI bank of the future
Profitability

Reimagined
engagement

Intelligent products,
tools, experiences
for customers and
employees

Personalization
at scale

Omnichannel
experience

Speed and
innovation

Within-bank channels and Beyond-bank channels
journeys (eg, web, apps,
and journeys (eg,
mobile, smart devices,
ecosystems, partners,
branches, Internet of Things)
distributors)
2

1

AI-powered
decision
making

Credit
decision
making

Customer
acquisition

7
AI capabilities

Naturallanguage
processing

Voicescript
analysis

Monitoring
and
collections

Virtual
agents, Computer
vision
bots

4

3

5
Digital marketing

6
Advanced
analytics

Smart service and
operations

Retention
and crossselling,
upselling

Facial
recognition

Blockchain

Servicing
and
engagement

Robotics

Behavioral
analytics

A. Tech-forward strategy (in-house build of differential capabilities
vs buying offerings; in-house talent plan)

Core
technology
and data

8
Core technology
and data

B. Data
management for
AI world

Platform operating
model

D. Intelligent
infrastructure
(AI operations
command,
hybrid cloud
setup, etc)

B. Agile way
of working

C. Remote
collaboration

10 Value capture

10

E. Hollowing the
core (core
modernization)

F. Cybersecurity
and
control
tiers

A. Autonomous business + tech teams

9

Operating
model

C. Modern
API architecture

AI-bank of the future: Can banks meet the AI challenge?

D. Modern talent
strategy (hiring,
reskilling)

E. Culture and
capabilities

Third, banks will need to redesign overall
customer experiences and specific journeys for
omnichannel interaction. This involves allowing
customers to move across multiple modes (e.g.,
web, mobile app, branch, call center, smart
devices) seamlessly within a single journey
and retaining and continuously updating the
latest context of interaction. Leading consumer
internet companies with offline-to-online
business models have reshaped customer
expectations on this dimension. Some banks
are pushing ahead in the design of omnichannel
journeys, but most will need to catch up.
Reimagining the engagement layer of the
AI bank will require a clear strategy on how
to engage customers through channels
owned by non-bank partners. Banks will
need to adopt a design-thinking lens as they
build experiences within and beyond the
bank’s platform, engineering engagement
interfaces for flexibility to enable tailoring and
personalization for customers, reengineering
back-end processes, and ensuring that datacapture funnels (e.g., clickstream) are granularly
embedded in the bank’s engagement layer. All
of this aims to provide a granular understanding
of journeys and enable continuous
improvement.10
Layer 2: Building the AI-powered decisionmaking layer
Delivering personalized messages and
decisions to millions of users and thousands
of employees, in (near) real time across the full
spectrum of engagement channels, will require
the bank to develop an at-scale AI-powered
decision-making layer. Across domains within
the bank, AI techniques can either fully replace
or augment human judgment to produce
significantly better outcomes (e.g., higher
accuracy and speed), enhanced experience
for customers (e.g., more personalized
interaction and offerings), actionable insights
for employees (e.g., which customer to contact
first with next-best-action recommendations),

and stronger risk management (e.g., earlier
detection of likelihood of default and
fraudulent activities).
To establish a robust AI-powered decision
layer, banks will need to shift from attempting
to develop specific use cases and point
solutions to an enterprise-wide road map for
deploying advanced-analytics (AA)/machinelearning (ML) models across entire business
domains. As an illustration, in the domain of
unsecured consumer lending alone, more
than 20 decisions across the life cycle can be
automated.11 To enable at-scale development
of decision models, banks need to make the
development process repeatable and thus
capable of delivering solutions effectively and
on-time. In addition to strong collaboration
between business teams and analytics
talent, this requires robust tools for model
development, efficient processes (e.g., for
re-using code across projects), and diffusion
of knowledge (e.g., repositories) across teams.
Beyond the at-scale development of decision
models across domains, the road map should
also include plans to embed AI in businessas-usual process. Often underestimated,
this effort requires rewiring the business
processes in which these AA/AI models will be
embedded; making AI decisioning “explainable”
to end-users; and a change-management plan
that addresses employee mindset shifts and
skills gaps. To foster continuous improvement
beyond the first deployment, banks also
need to establish infrastructure (e.g., data
measurement) and processes (e.g., periodic
reviews of performance, risk management of AI
models) for feedback loops to flourish.
Additionally, banks will need to augment
homegrown AI models, with fast-evolving
capabilities (e.g., natural-language processing,
computer-vision techniques, AI agents
and bots, augmented or virtual reality) in
their core business processes. Many of
these leading-edge capabilities have the

10

Jennifer Kilian, Hugo Sarrazin, and Hyo Yeon, “Building a design-driven culture,” September 2015, McKinsey.com.
Renny Thomas, Vinayak HV, Raphael Bick, and Shwaitang Singh, “Ten lessons for building a winning retail and small-business digital lending
franchise,” November 2019, McKinsey.com.

11

AI-bank of the future: Can banks meet the AI challenge?

11

potential to bring a paradigm shift in customer
experience and/or operational efficiency. While
many banks may lack both the talent and the
requisite investment appetite to develop these
technologies themselves, they need at minimum
to be able to procure and integrate these
emerging capabilities from specialist providers
at rapid speed through an architecture enabled
by an application programming interface (API),
promote continuous experimentation with these
technologies in sandbox environments to test and
refine applications and evaluate potential risks,
and subsequently decide which technologies to
deploy at scale.
To deliver these decisions and capabilities and to
engage customers across the full life cycle, from
acquisition to upsell and cross-sell to retention
and win-back, banks will need to establish
enterprise-wide digital marketing machinery. This
machinery is critical for translating decisions and
insights generated in the decision-making layer
into a set of coordinated interventions delivered
through the bank’s engagement layer. This
machinery has several critical elements, which
include:
—

Data-ingestion pipelines that capture a range
of data from multiple sources both within the
bank (e.g., clickstream data from apps) and
beyond (e.g., third-party partnerships with
telco providers)

—

Data platforms that aggregate, develop, and
maintain a 360-degree view of customers and
enable AA/ML models to run and execute in
near real time

—

Campaign platforms that track past actions
and coordinate forward-looking interventions
across the range of channels in the
engagement layer

Layer 3: Strengthening the core technology and
data infrastructure
Deploying AI capabilities across the organization
requires a scalable, resilient, and adaptable set
of core-technology components. A weak core-

12

AI-bank of the future: Can banks meet the AI challenge?

technology backbone, starved of the investments
needed for modernization, can dramatically
reduce the effectiveness of the decision-making
and engagement layers.
The core-technology-and-data layer has six key
elements (Exhibit 7):
— Tech-forward strategy. Banks should have
a unified technology strategy that is tightly
aligned to business strategy and outlines
strategic choices on which elements, skill
sets, and talent the bank will keep in-house
and those it will source through partnerships
or vendor relationships. In addition, the
tech strategy needs to articulate how each
component of the target architecture will both
support the bank’s vision to be an AI-first
institution and interact with each layer of the
capability stack.
—

Data management for the AI-enabled world.
The bank’s data management must ensure
data liquidity—that is, the ability to access,
ingest, and manipulate the data that serve as
the foundation for all insights and decisions
generated in the decision-making layer.
Data liquidity increases with the removal of
functional silos and allows multiple divisions
to operate off the same data, with increased
coordination. The data value chain begins with
seamless sourcing of data from all relevant
internal systems and external platforms. This
includes ingesting data into a lake, cleaning
and labeling the data required for diverse use
cases (e.g., regulatory reporting, business
intelligence at scale, AA/ML diagnostics),
segregating incoming data (from both existing
and prospective customers) to be made
available for immediate analysis from data to
be cleaned and labeled for future analysis.
Furthermore, as banks design and build their
centralized data-management infrastructure,
they should develop additional controls and
monitoring tools to ensure data security,
privacy, and regulatory compliance—for
example, timely and role-appropriate access
across the organization for various use cases.

Exhibit 7

The
accommodates
use of the
The core-technology-and-data
core-technology-and-data layer layer
accommodates
increasingincreasing
use of the cloud
cloud
and reduction
legacy technology.
and reduction
of legacyof
technology.​
Capabilities

Our perspective

Tech-forward strategy

Build differentiating capabilities in-house by augmenting the internal skill base;
carefully weigh options to buy, build, or compose modular architecture through
best-of-breed solutions

Data management for AI world

Upgrade data management and underlying architecture to support machine-learning
use cases at scale by leveraging cloud, streaming data, and real-time analytics

Modern API1 architecture

Leverage modern cloud-native tooling to enable a scalable API platform supporting
complex orchestrations while creating experience-enhancing integrations across
the ecosystem

Intelligent infrastructure

Implement infrastructure as code across on-premises and cloud environments;
increase platform resiliency by adopting AIOps to support deep diagnostics, autorecoverability, and auto-scale

Hollowing the core

Distribute transaction processing across the enterprise stack; selectively identify
components that can be externalized to drive broader reuse, standardization, and
efficiency

Implement robust cybersecurity in the hybrid infrastructure; secure data and
Cybersecurity and control tiers applications through zero-trust design principles and centralized command-andcontrol centers
1

Application programming interface.

—

Modern API architecture. APIs are the
connective tissue enabling controlled access
to services, products, and data, both within
the bank and beyond. Within the bank, APIs
reduce the need for silos, increase reusability
of technology assets, and promote flexibility
in the technology architecture. Beyond the
bank, APIs accelerate the ability to partner
externally, unlock new business opportunities,
and enhance customer experiences. While
APIs can unlock significant value, it is critical to
start by defining where they are to be used and
establish centralized governance to support
their development and curation.¹2

—

Intelligent infrastructure. As companies
in diverse industries increase the share of
workload handled on public and private
cloud infrastructure, there is ample evidence
that cloud-based platforms allow for the
higher scalability and resilience crucial to an
AI-first strategy.13 Additionally, cloud-based
infrastructure reduces costs for IT maintenance
and enables self-serve models for development
teams, which enable rapid innovation cycles by
providing managed services (e.g., setting up new
environments in minutes instead of days).

¹2 Renny Thomas, Vinayak HV, Raphael Bick, and Shwaitang Singh, “Ten lessons for building a winning retail and small-business digital lending
franchise,” November 2019, McKinsey.com.

¹3 Arul Elumalai and Roger Roberts, “Unlocking business acceleration in a hybrid cloud world,” August 2019, McKinsey.com.

AI-bank of the future: Can banks meet the AI challenge?

13

Layer 4: Transitioning to the platform operating
model
The AI-first bank of the future will need a new
operating model for the organization, so it can
achieve the requisite agility and speed and
unleash value across the other layers. While
most banks are transitioning their technology
platforms and assets to become more modular
and flexible, working teams within the bank
continue to operate in functional silos under
suboptimal collaboration models and often lack
alignment of goals and priorities.
The platform operating model envisions crossfunctional business-and-technology teams
organized as a series of platforms within the bank.
Each platform team controls their own assets
(e.g., technology solutions, data, infrastructure),
budgets, key performance indicators, and
talent. In return, the team delivers a family of
products or services either to end customers of
the bank or to other platforms within the bank.
In the target state, the bank could end up with
three archetypes of platform teams. Business
platforms are customer- or partner-facing teams
dedicated to achieving business outcomes in
areas such as consumer lending, corporate
lending, and transaction banking. Enterprise
platforms deliver specialized capabilities and/
or shared services to establish standardization
throughout the organization in areas such as
collections, payment utilities, human resources,
and finance. And enabling platforms enable the
enterprise and business platforms to deliver
cross-cutting technical functionalities such as
cybersecurity and cloud architecture.
By integrating business and technology in
jointly owned platforms run by cross-functional
teams, banks can break up organizational silos,
increasing agility and speed and improving the
alignment of goals and priorities across the
enterprise.

The journey to becoming an AI-first bank entails
transforming capabilities across all four layers
of the capability stack. Ignoring challenges or
underinvesting in any layer will ripple through all,
resulting in a sub-optimal stack that is incapable
of delivering enterprise goals.
A practical way to get started is to evaluate
how the bank’s strategic goals (e.g., growth,
profitability, customer engagement, innovation)
can be materially enabled by the range of AI
technologies—and dovetailing AI goals with the
strategic goals of the bank. Once this alignment
is in place, bank leaders should conduct a
comprehensive diagnostic of the bank’s starting
position across the four layers, to identify areas
that need key shifts, additional investments
and new talent. They can then translate these
insights into a transformation roadmap that spans
business, technology, and analytics teams.
Equally important is the design of an execution
approach that is tailored to the organization. To
ensure sustainability of change, we recommend
a two-track approach that balances short-term
projects that deliver business value every quarter
with an iterative build of long-term institutional
capabilities. Furthermore, depending on their
market position, size, and aspirations, banks need
not build all capabilities themselves. They might
elect to keep differentiating core capabilities
in-house and acquire non-differentiating
capabilities from technology vendors and
partners, including AI specialists.
For many banks, ensuring adoption of AI
technologies across the enterprise is no longer
a choice, but a strategic imperative. Envisioning
and building the bank’s capabilities holistically
across the four layers will be critical to success.

Suparna Biswas is a partner, Shwaitang Singh is an associate partner, and Renny Thomas is a senior partner, all in McKinsey’s
Mumbai office. Brant Carson is a partner in the Sydney office, and Violet Chung is a partner in the Hong Kong office.
The authors would like to thank Milan Mitra, Anushi Shah, Arihant Kothari, and Yihong Wu for their contributions to this article.
Copyright © 2020 McKinsey & Company. All rights reserved.

14

AI-bank of the future: Can banks meet the AI challenge?

Texas A&M University School of Law

Texas A&M Law Scholarship
Faculty Scholarship
2020

Artificial Financial Intelligence
William Magnuson
Texas A&M University School of Law, magnuson@law.tamu.edu

Follow this and additional works at: https://scholarship.law.tamu.edu/facscholar
Part of the Civil Rights and Discrimination Commons, Computer Law Commons, Internet Law
Commons, and the Law and Society Commons

Recommended Citation
William Magnuson, Artificial Financial Intelligence, 10 Harv. Bus. L. Rev. 337 (2020).
Available at: https://scholarship.law.tamu.edu/facscholar/1435

This Article is brought to you for free and open access by Texas A&M Law Scholarship. It has been accepted for
inclusion in Faculty Scholarship by an authorized administrator of Texas A&M Law Scholarship. For more
information, please contact aretteen@law.tamu.edu.

ARTIFICIAL FINANCIAL INTELLIGENCE
WILLIAM MAGNUSON*
Recent advances in the field of artificial intelligence have revived longstanding debates about the interaction between humans and technology. These
debates have tended to center around the ability of computers to exceed the
capacities and understandings of human decisionmakers, and the resulting effects on the future of labor, inequality, and society more generally. These questions have found particularresonance in finance, where computers already play
a dominant role. High-frequency traders, quantitative (or "quant") hedge funds,
and robo-advisorsall represent, to a greateror lesser degree, real-worldinstantiations of the impact that artificial intelligence is having on the field. This Article, however, takes a somewhat contrarianposition. It argues that the primary
danger of artificial intelligence in finance is not so much that it will surpass
human intelligence, but rather that it will exacerbate human error. It will do so
in three ways. First, because current artificial intelligence techniques rely heavily on identifying patterns in historicaldata, use of these techniques will tend to
lead to results that perpetuate the status quo (a status quo that exhibits all the
features andfailings of the external market). Second, because some of the most
"accurate"artificial intelligence strategies are the least transparentor explainable ones, decisionmakers may well give more weight to the results of these
algorithms than they are due. Finally, because much of the financial industry
depends not just on predicting what will happen in the world, but also on predicting what other people will predict will happen in the world, it is likely that
small errors in applying artificialintelligence (either in data, programming, or
execution) will have outsized effects on markets. This is not to say that artificial
intelligence has no place in the financial industry, or even that it is bad for the
industry. It clearly is here to stay, and, what is more, has much to offer in terms
of efficiency, speed, and cost. But as governments and regulators begin to take
stock of the technology, it is worthwhile to considerartificial intelligence'srealworld limitations.
INTRODUCTION ..................................................

I.

ARTIFICIAL INTELLIGENCE IN THE FINANCIAL INDUSTRY

.....

A. The Technology of Artificial Intelligence ................
B. Artificial Intelligence Strategies in Finance .............
C. Existing Literature....................................
II.

ARTIFICIAL FINANCIAL INTELLIGENCE ..........................

A. Data Dependency.....................................
B. Overweighting........................................
C. Echo Effects .........................................

BH.

REGULATING ARTIFICIAL FINANCIAL INTELLIGENCE ..........

A.
B.
C.
D.

Artificial Fairness ....................................
Artificial Efficiency ...................................
A rtificial Risk ........................................
The Role of Self-Regulation ...........................

...

338
341
342
348
352
355
355
359
363
365
366
368
371
373

* Associate Professor, Texas A&M University School of Law; J.D., Harvard Law School;
M.A., UniversitA di Padova; A.B., Princeton University. The author wishes to thank Jack Goldsmith, Todd Henderson, Chris Brummer, Tom Vartanian, Bob Ledig, Sarah Jane Hughes, Angela Walch, Tom Lin, and Rory Van Loo for their suggestions and advice.

Harvard Business Law Review

338
IV.

[Vol. 10

OBJECTIONS ............................................

A. The Mirror Critique
..............................
B. The PrecautionaryCritique.. .......................
.............................
C. The Balance Critique
CONCLUSION.
...................................................

375
375
378
380
381

INTRODUCTION

In the novel Tell the Machine Goodnight, a super-powered computer
called the Apricity (named after the now-defunct English word for "the feeling of sun on one's skin in the winter") uses advanced artificial intelligence
to deliver lifestyle recommendations to users.' A simple cotton swab applied
to the mouth, and then swiped across the computer's reader, is all the computer needs to formulate a list of recommended changes for users to make in
their lives. These changes, if implemented, promise to lead to happier, more
fulfilled lives, or, as Apricity's corporate legal department advises them to
say, to improvements in one's "overall life satisfaction." The story opens
with one unfortunate user receiving his own personalized set of recommendations from the Apricity:
The machine said the man should eat tangerines. It listed two other
recommendations as well, so three in total. A modest number,
Pearl assured the man as she read out the list that had appeared on
the screen before her: one, he should eat tangerines on a regular
basis; two, he should work at a desk that received morning light;
three, he should amputate the uppermost section of his right index
finger.'
Pearl, the technician tasked with operating the device, watches the man raise
his right hand before his face and wonders if he is going to cry. She assures
him that his recommendation is "modest" compared to others she has seen
and reminds him that the system boasts a 99.97% approval rating. "The
proof is borne out in the numbers," she concludes.'
Writing about artificial intelligence requires a certain dose of imagination. This has always been the case. It is not just because artificial intelligence draws more than its fair share of attention in science fiction and
fantasy circles, although this certainly plays a part. It is also, and perhaps
more importantly, because the term artificialintelligence conjures up images
of a world that does not currently exist. When most people think of artificial
intelligence, they think of superpowered computers that act and think like
human beings, with complicated motives, wide-ranging capacities, and often
dangerous tendencies. This combination of qualities simply does not exist in
' KATE WILLIAMS, ThLL THE MACHINE GOODNIGHT (Ist ed. 2018).
2

Id. at 1.
at 2.

3Id.

2020]

Artificial FinancialIntelligence

339

the state of today's artificial intelligence research. As more than one artificial
intelligence researcher has remarked, most of the people making the doomsday prognostications of artificial intelligence do not actually work in the
field. Those that do have significantly more modest forecasts about the potential of artificial intelligence. This is not to say that they believe artificial
intelligence is an ineffective technology. It is just that they understand its
limitations.
At the same time, artificial intelligence algorithms have undeniably
made significant advances in recent years. Improved algorithms, bigger data
sets, and more powerful computers have combined to give machine learning
strategies ever-growing capabilities. In the last decade alone, we have witnessed machine learning-based computers beat the world's best Jeopardy
contestants, 4 accurately identify breast cancer from pathology slides,5 and
drive cars around the streets of Phoenix, Arizona.6 Governments around the
world are researching autonomous weapons systems that could potentially
mark a major shift in the nature of warfare and national security.'
Financial institutions, intrigued by the potential of machine learning,
have begun to explore how to incorporate artificial intelligence into their
own businesses. In recent years, large investment banks have hired away
talented experts from academia and Silicon Valley to head up machine learning divisions.' Fintech startups have created credit rating models and fraud
detection algorithms based on artificial intelligence strategies.9 And highfrequency traders, quantitative hedge funds, and robo-advisors are implementing artificial intelligence into their businesses as well.' 0 "Artificial financial intelligence" will likely take on increasing importance in years to
come.
But how should our legal structures respond to the rise of artificial financial intelligence? Should we attempt to encourage it, in the hopes of nur-

4 STEPHEN BAKER, FINAL JEOPARDY: THE STORY OF WATSON, THE COMPUTER THAT WILL
TRANSFORM OUR WORLD 251 (1st ed. 2012).

See Jessica Kent, Google Deep Learning Tool 99% Accurate at Breast Cancer Detection,
IT ANALYTICS, (Oct. 22, 2018), https://healthitanalytics.com/news/google-deep-learning-tool-99-accurate-at-breast-cancer-detection.
' See Tim Higgins & Jack Nicas, Waymo's Self-Driving Milepost: Humans Take a Backseat, WALL ST. J., (Nov. 7, 2017), https://www.wsj.com/articles/waymos-self-driving-milepost-humans-take-a-backseat-I510070401.
'See generally Rebecca Crootoff, War Torts: Accountability for Autonomous Weapons,
164 U. PA. L. REV. 1347 (2016); Rebecca Crootoff, Autonomour Weapons Systems and the
Limits of Analogy, 9 HARV. NAT'L SEC. J. 51 (2018); GREG ALLEN & TANIEL CHAN, BELFER
AFFAIRS, ARTIFICIAL INTELLIGENCE AND NATIONAL SECURITY (2017).
CTR. FOR SCI. & INTL'
8See Sarah Butcher, The Top Machine Learning Teams in Investment Banks, EFINANC[ALCAREERS, (May 23, 2018), https://news.efinancialcareers.com/us-en/315969/top-machinelearning-teams-banks.
' See Rory Van Loo, Making Innovation More Competitive: The Case of Fintech, 65
UCLA L. REV. 232, 240 (2018).
0
See Machine-Learning Promises to Shake Up Large Swathes of Finance, THE EcONOMIST (May 25, 2017), https://www.economist.com/finance-and-economics/2017/05/25/machine-learning-promises-to-shake-up-large-swathes-of-finance.
HEALTH

340

Harvard Business Law Review

[Vol. 10

turing a potentially transformative technology? Or should we try to restrict
it, out of concerns about its broader risks for society? Is it possible to do
both? In order to answer these questions, we must first identify the problems
that artificial financial intelligence poses, and the sources from which these
problems spring. Many scholars argue that artificial intelligence poses existential questions about the nature of discrimination, the future of work, and
the repercussions of super-human intelligence.'' This Article, however, will
take a somewhat contrarian position. It will argue that the primary danger of
artificial financial intelligence is not so much that it will surpass human intelligence, but rather that it will exacerbate human error.12
Artificial financial intelligence can magnify the effects of human error
in three ways. First, because artificial intelligence techniques, and in particular machine learning algorithms, rely heavily on identifying patterns in historical data, use of these techniques will tend to lead to results that
perpetuate the status quo in the data. Thus, if the data used to train artificial
intelligence algorithms is flawed, either through poor selection methods or
unavoidable problems in the external market itself, the resulting outputs of
artificial intelligence will reflect (and, indeed, strengthen) those flaws. Second, because many of the most powerful artificial intelligence strategies are
the least transparent or explainable, decisionmakers within financial institutions may give more weight to the results of the algorithms than they are
due. It is a well-known problem in machine learning, and in particular in the
sub-fields of deep learning and neural networks, that the complexity of the
methods used to reach a given result makes identifying a particular reason,
" See Tom C. W. Lin, The New Market Manipulation, 66 EMORY L. J. 1253, 1254 (2017)
(arguing that artificial intelligence risks widespread market manipulation); Rory Van Loo, Digital Market Perfection, 117 MICH. L. REv. 815 (2019) (arguing that Al-based automated assistants "could make some large markets more volatile, raising unemployment costs or financial
stability concerns as more firms fail"); Rory Van Loo, Technology Regulation by Default:
Platforms, Privacy, and the CFPB, 2 GEO. L. TECH. REV. 531, 544-45 (2018) (arguing that the
Consumer Financial Protection Bureau should exert additional authority to inspect financial
algorithms); Chris Brummer & Yesha Yadav, Fintech and the Innovation Trilemma, 107 GEo.
L. J. 235, 275 (2019) (arguing that "[g]iven the potential for [Al and machine learning] to
result in widespread, cascading costs, mapping the likely performance of sophisticated algorithms becomes especially necessary"); Gregory Scopino, PreparingFinancialRegulationfor
the Second Machine Age: The Need for Oversight of Digital Intermediaries in the Futures
Markets, 2015 COLUM. Bus. L. REv. 439, 440 (2015) ("[H]umans who are operating as futures market intermediaries . . . are likely to be displaced by digital intermediaries, that is,
artificial agents that perform critical roles related to enabling customers to access the futures
and derivatives markets."); Luca Enriques & Dirk A. Zetzsche, Corporate Technologies and
the Tech Nirvana Fallacy, Eur. Corp. Governance Inst. Working Paper No. 457, 2019) (analyzing the ways in which modern algorithms exacerbate agency problems in corporate law).
2 Other scholars have made related points in other contexts, such as discrimination. See
Solon Barocas & Andrew D. Selbst, Big Data'sDisparateImpact, 104 Cal. L. Rev. 671, 671
(2016) ("[A]n algorithm is only as good as the data it works with. Data is frequently imperfect in ways that allow these algorithms to inherit the prejudices of prior decision makers.");
More recently, financial regulation scholars have turned to this problem as well. See Tom C.W.
Lin, Artificial Intelligence, Finance, and the Law, 99 FoRDHAM L. Rev. 531, 531 (2019) (examining the "perils and pitfalls of artificial codes, data bias, virtual threats, and systemic risks
relating to financial artificial intelligence.").

Artificial FinancialIntelligence

2020]

341

or even several reasons, for the result difficult to do. Combined with cognitive decision biases related to availability and herding effects, we can expect
that artificial financial intelligence will be hard to resist in financial decisionmaking processes. Finally, because much of the financial industry depends not just on predicting what will happen in the world, but also on
predicting what other people will predict will happen in the world, it is likely
that small errors in artificial intelligence (either in data, programming, or
execution) will have outsized effects on markets. The artificial intelligence
strategies of one firm may interact unpredictably with the artificial intelligence strategies of other firms." The resulting echo effects could harm both
the efficiency of markets and the stability of financial systems.14
What does this mean for financial regulation? In a way, it is a cause for
optimism. Financial regulators are well-placed to deal with artificial financial intelligence because they have a wide array of laws and regulations covering the relevant behaviors-ensuring fairness, promoting efficiency, and
protecting stability. While accomplishing these goals will not be easy, the
general categories of problems are ones to which financial regulators are
accustomed. At the same time, regulating artificial financial intelligence may
require slightly different regulatory mechanisms, or at least targets. Understanding and monitoring artificial financial intelligence may, and indeed
likely will, require specialized expertise. It may also call for more intrusive
and hands-on inspections of sensitive business data. We may not need better
laws, but we do need better information.
I.

ARTIFICIAL INTELLIGENCE IN THE FINANCIAL INDUSTRY

The computer science field of artificial intelligence has exploded in recent years. New advances in technical approaches, computing power, and
the availability of data have led to notable successes in a variety of applications, from image recognition" to natural language processing 6 to medical
diagnoses 7 to game strategy.'" These advances have brought renewed atten" See Rory Van Loo, The Rise of the DigitalRegulator, 66 DuKE L.J. 1267, 1294 (2017)
(discussing the unpredictable ways that different firms' algorithmic pricing tools may interact).
1 See Hilary J. Allen, Driverless Finance, 10 HARv. Bus. L. Rnv. 158, 158 (2019) (arguing that financial regulators should adopt a precautionary approach to regulating financial algorithms due to their potential to create systemic risk for the broader financial system).
15 See Parmy Olson, Image-Recognition Technology May Not Be as Secure as We Think,
WALL Sr. J., (June 4, 2019), https://www.wsj.com/articles/image-recognition-technology-maynot-be-as-secure-as-we-think-i 1559700300.
16 See Tom Young, et al., Recent Trends in Deep Learning Based Natural Language
Processing, ARXIV 1708.02709 (2018), https://arxiv.org/pdf/1708.02709.pdf.
17 See A. Michael Froomkin, et al., When Als Outperform Doctors: Confronting the Challenges of a Tort-Induced Over-Reliance on Machine Learning, 61 ARiz. L. REv. 33 (2019).
" See Nick Statt, How Artificial Intelligence Will Revolutionize the Way Video GamesAre
Developed and Played, THE VERGE (Mar. 6, 2019), https://www.theverge.com/2019/3/6/
18222203/video-game-ai-future-procedural-generation-deep-leaming; George Anadiotis, The
State of AI in 2019: Breakthroughs in Machine Learning, Natural Language Processing,
Games, and Knowledge Graphs, ZDNEr (July 8, 2019), https://www.zdnet.com/article/the-

Harvard Business Law Review

342

[Vol. 10

tion to artificial intelligence in the business world and, in particular, in the
financial industry. Financial firms, who have always relied heavily on statistics and quantitative analysis in their services, are a natural fit for artificial
intelligence strategies. Artificial intelligence, after all, attempts to identify
patterns and probabilities from historical data in order to improve results,
such as predicting future statistics or identifying complicated associations.
Increasingly, financial firms, such as quant hedge funds, high-frequency
traders, and robo-advisors, are turning to artificial intelligence to improve
their own results. This new artificialfinancial intelligence ecosystem has, in
turn, generated its fair share of hand-wringing in policy and academic circles, where scholars have worried about cyber-systems that could potentially
exceed human capabilities. This Part will describe the rise of artificial intelligence, its use in the financial industry, and the existing literature on how
policymakers should respond to it.
A.

The Technology of Artificial Intelligence

What is artificial intelligence? The term has many meanings, a problem
derived from its long history in computer science and popular fiction.' 9 The
term itself is generally believed to have originated in 1956, when a
Dartmouth mathematics professor, John McCarthy, used it in connection
with a conference he was organizing on the topic of thinking machines.2 0 But
the general outlines of the idea go back much further. Edgar Allan Poe wrote
an essay in 1836 about a mechanical chess player called The Turk that was
purportedly better than any human. 2 1 Alan Turing, at least as early as 1950,
was directly tackling the question of whether computers could think. 22 Turing famously concluded that the question itself was ill-formed and instead
needed to be reconceptualized as whether machines could act in such a way
that they would be indistinguishable from humans. 23 In order to determine
this, he developed a test, now called the Turing test, in which a human,
corresponding through text with two players, one of whom was a human and
one of whom was a computer, attempted to determine which one was
human. 24 If he could not, then, under the Turing test, the computer could be
said to be intelligent.

state-of-ai-in-2019-breakthroughs-in-machine-learning-natural-language-processing-gamesand-knowledge-graphs/.
" See NAT'L SC. & TECH. COUNCIL, PREPARING FOR THE FUTURE OF ARTIFICIAL INTELLIGENCE
20 5 (2016).
See NILs J. NILSSON, THE QUEST FOR ARTIFICIAL INTELLIGENCE 52-56 (2009).
21 See EDGAR ALLAN POE, Maelzel's Chess-Player, in Edgar Allan Poe: Complete
Tales
and Poems 373 (2003).
22 Alan M. Turing, Computing Machinery and Intelligence, 59 MIND 433, 433 (1950).
2 Id. at 433-34.
24 It turns out that the Turing test has generated its fair share of controversy, both with
respect to whether individual computers have in fact passed it and with respect to whether the
test itself is a good indicator of true intelligence on the part of machines.

2020]

Artificial FinancialIntelligence

343

More modem definitions of artificial intelligence tend to be somewhat
more inclusive. Berkeley computer scientist Stuart Russell defines artificial
intelligence as:
the study of methods for making computers behave intelligently.
Roughly speaking, a computer is intelligent to the extent that it
does the right thing rather than the wrong thing. The right thing is
whatever action is most likely to achieve the goal, or, in more
technical terms, the action that maximizes expected utility. 25
As an even cursory perusal of this selection will reveal, this definition runs
into its own set of problems. For one, we might say that a calculator does the
right thing when it says that 1 + 1 is 2, but most observers would not say
that it is behaving intelligently in any ordinary sense of the term. For another, once we define intelligence to include an element of morality (does a
computer's action maximize expected utility?), we open the door to contentious, unanswerable questions that distract from the technology itself. To
borrow from Socrates, if a user asks a computer for a sword, and the computer receives the question, finds a sword for him in the real world,
purchases it, and then delivers it to him, most observers would conclude that
the computer is behaving intelligently. 26 But if that same user requesting the
sword happens to be insane and plans to use it to harm himself or others,
then it would (normally) be wrong to give him the sword. Does this mean
that the computer is now not displaying artificial intelligence, because it is
doing the wrong thing, ethically speaking? These types of questions could be
raised for any conceivable use of a computer, and they are not closely connected to the problems unique to artificial intelligence itself.
On the other side of the spectrum, narrower definitions of artificial intelligence tend to circumscribe the range of the field unnecessarily. Many
artificial intelligence researchers use the term artificial intelligence to refer
to a particular set of techniques or algorithms that computers can adopt to
achieve better results. So, in the 1980s, artificial intelligence became closely
associated with an approach to problem-solving known as "expert system"
design.2 7 Under expert systems, computers would be given a long set of ifthen rules that they could then apply to problems in a given field?" The logic
Q&A: THE FUTURE OF ARTIFICIAL INTELLIGENCE, UNIV. OF CAL.
http://people.eecs.berkeley.edu/-russell/temp/q-and-a.html.
26 The discussion of the madman and the sword takes place in The Republic, Book I.
Appropriately for this article, it involves the question of whether justice requires you to pay
your debts. See PLATO, REPUBLIC 6 (C. D. C. Reeve ed., G.M.A. Grube trans., Hackett Pub.
Company, Inc. 2d ed. 1992) (c. 375 B.C.E.) ("Everyone would surely agree that if a sane man
lends weapons to a friend and then asks for them back when he is out of his mind, the friend
shouldn't return them, and wouldn't be acting justly if he did.").
27 See generally PAUL HARMON & DAVID KING, EXPERT SYSTEMS: ARTIFICIAL INTELLIGENCE IN BUSINESS (1985); Bruce G. Buchanan & Reid G. Smith, Fundamentals of Expert
Systems, in 4 HANDBOOK OF ARTIFICIAL INTELLIGENCE 149 (Avron Barr, et al. eds. 1981-89).
28
See ROBERT A. EDMUNos, THE PRENTICE HALL GUIDE TO EXPERT SYSTEMS 28-31
(1988).
25 STUART RUSSELL,

BERKELEY,

Harvard Business Law Review

344

[Vol. 10

of expert systems was that humans who are specialists in a field tend to
behave according to a number of heuristics, and if those heuristics could
simply be identified and then hard-coded into a set of simple rules for computers, then computers could achieve similar or even superior results to specialists.

29

It turned out that expert systems did not live up to the high

expectations of researchers, and their limitations eventually led to a decline
in interest in artificial intelligence generally.3 0
More recently, however, artificial intelligence has had something of a
renaissance, becoming associated with a new and promising technique
known as deep learning."1 Deep learning is a sophisticated, data-driven
model from a field of computer science known as machine learning. Machine learning, rather than relying on a set of rules established by a programmer, as in expert systems, instead starts with a data set and then attempts to
derive rules on its own.3 2 For example, a computer might be given a set of
images with labels attached-this is a person, this is a tree, this is a dogand be asked to use the images (the "training set") to establish rules for
identifying when an image contains those objects.33 The machine learning
algorithm might then check the accuracy of these rules against another set of
images (the "test set") to see if the rules work well outside of the original
context. 4 Many different variants of machine learning exist, and one particular variant, known as deep learning, has had notable success in recent
years.35 In deep learning algorithms, neural networks or units identify patterns in data, transform those patterns into outputs, and then pass those outputs along to additional units. 36 The first "layer" of units might identify
patterns in the data, and then the next layer might identify patterns of pattems in the data, and so on and so forth. 3 The algorithms associated with
deep learning techniques have proven remarkably accurate at improving accuracy and predictive power in machines, and they have become synonymous with artificial intelligence in many circles.

29

See Andrea Roth, Machine Testimony, 126 YALE L. J. 1972, 1998-99 (2017).
See DANIEL CREVIER, Al: THE TUMUITUOus SEARCH FOR ARTIFICIAL INTELLIGENCE
204-8 (1993).
3 See Ryan Calo, Artificial Intelligence Policy: A Primerand Roadmap, 51 U.C. DAVIS L.
REv. 399, 402 (2017); Jack M. Balkin, The Three Laws of Robotics, 78 OHIO ST. L. J. 1217,
1219-20 (2016); Emily Berman, A Government of Laws and not of Machines, 98 B.U. L. REv.
1277,32 1284-90 (2018).
See KEvIN P. MURPHY, MACHINE LEARNING: A PROBABILISTIC PERSPECIlVE 1 (2012)
("[W]e define machine learning as a set of methods that can automatically detect patterns in
data, and then use the uncovered patterns to predict future data, or to perform other kinds of
decision making under uncertainty . . . .").
3 See David Lehr & Paul Ohm, Playing with the Data: What Legal Scholars Should
Learn About Machine Learning, 51 U.C. DAVIS L. REV. 653, 684-88 (2017).
34 Id.

Id. at 669-70.
' See Curtis E.A. Karnow, The Opinion of Machines, 19 COLUM. Sa. & TECH. L.
136, 141-50 (2017).
3 Id.
1

REV.

2020]

Artificial FinancialIntelligence

345

But defining artificial intelligence in such a narrow way, to encompass
a small set of algorithmic techniques and to exclude all others, has its
problems as well. The explosion of interest in deep learning has only occurred in the last few years, and it may well be that other, newer strategies
will emerge that tackle the same issues but use different methods. These
newer strategies may well be more effective than older ones. There would
not appear to be any categorical reasons for excluding new algorithmic strategies from the definition of artificial intelligence simply because they are
unorthodox or new.
It is hard to regulate what we cannot define." Hopefully this discussion
clarifies some of the confusion in the field about what precisely we are talking about when we talk about artificial intelligence. Some people use it to
refer generally to any instance when a computer performs an action correctly, while others use it to refer to a specific set of algorithmic strategies
that are currently en vogue in the industry. When computer scientists talk
about the AI revolution of recent years, they tend to be talking about the
machine learning and deep learning strategies that researchers have deployed
to such great success in certain fields. But policymakers, news media and
commentators often use artificial intelligence to refer more generally to
thinking machines, regardless of the actual techniques that the machines are
using to accomplish their feats. It may be most useful to conceive of artificial intelligence as a spectrum, with the capacity to perform simple tasks
(such as arithmetic and other basic mathematical calculations) at the low
end, the capacity to match human performance somewhere in the middle,
and the capacity to outperform humans at the high end. This conception is
itself simplistic (computers are already better than humans at many tasks,
while simultaneously being worse than humans at many others), but it helps
clarify the range of potential artificial intelligence applications in the world.
And while this Article will focus in particular on machine learning, it is
worthwhile to keep in mind that some other technique may come along that
replaces it.
Now that we have settled on a workable definition of artificial intelligence, we can now turn to its real-world uses. In particular, it is useful for
our purposes (that is, understanding how artificial intelligence is affecting
the financial industry, and how it might do so in the future) to know the
types of things that current instantiations of artificial intelligence are good at
doing. It is also important to know why they are good at doing these things.
A list of the accomplishments of artificial intelligence in the last decade
is breathtaking. Here is just a short list of some of the most noteworthy
achievements:
"s For an excellent analysis of the regulatory issues raised by the uncertain scope of "artificial intelligence" as a concept, see Mark A. Lemley & Bryan Casey, You Might be a Robot,
105 CORNELL L. REV. (forthcoming 2019). Lemly and Casey conclude that, in fact, there is no
correct definition of artificial intelligence for regulators, and that regulators should instead
focus on behaviors and actions.

Harvard Business Law Review

346

[Vol. 10

* In 2011, IBM's computing system, Watson, won a game of Jeopardy
against two of the game show's most successful contestants, Ken Jennings and Brad Rutter, with a final tally of $77,147 to Jennings'
$24,000 and Rutter's $21,600;3"
* Between 2010 and 2017, in a popular image recognition contest,
deep learning algorithms lowered their error rate from 28% to 2.3%
(surpassing humans, who, on average have a 5% error rate);"
* In 2016, by reviewing and analyzing all published literature on ALS,
or Lou Gehrig's disease, IBM's Watson system was able to identify
five previously unknown genes related to the disease; 41
* In 2017, a computer program known as AlphaGo defeated the
world's best player of the board game Go with a score of three
matches to zero; 42
* In 2017, countries deployed forty-nine different automated weapons
systems that could detect and attack targets without human
intervention ;43
* In 2018, Waymo launched the world's first commercial self-driving
car service, a technology based heavily on machine learning
algorithms."
These remarkable advances, displayed across a wide and diverse array
of fields, have been made possible by three related developments: better algorithms, deployed by more powerful computers, applied to ever greater
amounts of data. 45 On the algorithm side, the major advances in recent years
have been driven by machine learning, which, as described above, refer to a
computer's ability to analyze data and develop models for recognizing patterns and predicting future data-in other words, its ability to learn and imTHE STORY OF WATSON, THE COMPUTER THAT
251 (2012).
'See Imagenet, Large Scale Visual Recognition Challenge, http://image-net.org/challenges/LSVRC/2017/.
41 Emma Hinchliffe, IBM's Watson SupercomputerDiscovers 5 New Genes Linked to ALS,
MASHABLE, (Dec. 14, 2016), http://mashable.com/2016/12/14/ibm-watson-als-research/#oKfR
VPG3C8ql.
42 A later version of the AlphaGo algorithm, known as AlphaGo Zero, defeated the prior
version of AlphaGo 100 games to zero. See Larry Greenemeier, Al versus AI: Self-Taught
AlphaGo Zero Vanquishes Its Predecessor, ScmwrlFc AMERICAN, (Oct. 18, 2017).
41 See VINCENT BOULANIN & MAAIKE VERBRUGGEN, MAPPING THE DEVELOPMENf OF AuTONOMY IN WEAPON SYSTEMS 26 (2017).
" See Shuyang Cheng & Gabriel Bender, Autonomous Driving, MEDIUM, (Jan. 15, 2019),
https://medium.com/waymo/automl-automating-the-design-of-machine-leaming-models-forautonomous-driving-141a5583ec2a ("At Waymo, machine learning plays a key role in nearly
every part of our self-driving system. It helps our cars see their surroundings, make sense of
the world, predict how others will behave, and decide their next best move.").
4 See NAT'L Sci. & ThCH. COUNCIL, supra note 19, at 6; STANFORD UNIVERSITY ONE
HUNDRED YEAR STUDY, ARTIFICIAL INTELLIGENCE AND LIFE IN 2030 51 (2016).
3

See

STEPIEN BAKER, FINAL JEOPARDY:

WILL TRANSFORM OUR WORLD

20201

Artificial FinancialIntelligence

347

prove its performance over time. 46 Advancements in the sub-field of deep
learning have played particularly important roles in this success. 47 These advancements have, in turn, been supported by enormous leaps in computing
power. 48 Moore's Law-the famous proposition that the number of components per integrated circuit, and thus processing power, doubles roughly
every eighteen to twenty-four months-demonstrates the magnitude of performance improvement over time. 49 If human populations followed the same
trend, a village of 100 people in 1970 would have grown to 1.7 billion people by 2018. Even more relevant for artificial intelligence is the vast improvement in the speed of graphical processing units (GPUs) used in the
field. 0 In 2000, Nvidia's top-of-the-line desktop GPU, the GeForce 256
DDR, had a processing power of 480 million operations per second. By
2018, Nvidia's top desktop GPU, the Titan RTX, had reached a processing
power of 16,312 gigaflops, a speed more than 30,000 times faster." The
vastly greater computational capacity of modem-day computers, combined
with effective machine learning algorithms, have been unleashed on the
massive amounts of data made available by the internet age. 52 With the rise
of the internet and mobile computing, people are generating more data, and
more sensitive and personalized data, than they have ever done before. This
information has proven a fertile source of training data for artificial intelligence. 53 As the computer scientist Andy Hertzfeld has stated, the artificial
intelligence revolution of recent years
couldn't have happened no matter how brilliant your algorithm
was twenty years ago, because billions were just out of reach. But
what they found was that by just increasing the scale, suddenly
things started working incredibly well. It kind of shocked the com54
puter science world, that they started working so good.
So artificial intelligence has made enormous leaps in ability in recent
years, and these leaps have made it both practical and efficient for firms and
' See Calo, supra note 31, at 402.
47 Id. at 401-02.

48 See Alan Stevens, GPU Computing: Accelerating the Deep Learning Curve, ZDNEr,
(July 2, 2019), https://www.zdnet.com/article/gpu-computing-accelerating-the-deep-learningcurve/.
" For a broader discussion of the effects of Moore's law on artificial intelligence, see John
0. McGinnis, Accelerating AI, 104 Nw. U. L. REv. 1253 (2010).
' See Michael Byrne, Why Machine Learning Needs GPUs, MOTHERBOARD, (Jan. 9,
2018), https://motherboard.vice.com/en us/article/kznnnn/why-machine-learning-needs-gpus.
5' See CPUREvmw, nVidia GeForce 256 DDR, http://www.gpureview.com/GeForce256DDR-card-114.html; SKUEZTECH, Nvidia Titan RIX, http://www.skueztech.com/db/gpu/
nvidia-titan-rtx.
52 See Solon Barocas & Andrew D. Selbst, Big Data's Disparate Impact, 104 CALIF. L.
REV. 671 (2016); Neil M. Richards & Jonathan H. King, Big Data Ethics, 49 WAKE FOREST L.
REV. 393 (2014).
* See Calo, supra note 31, at 420-25.
54
ADAM FISHER, VALLEY OF GENIUS: THE UNCENSORED HISTORY OF SIcoN VALLEY 423
(2018).

HarvardBusiness Law Review

348

[Vol. 10

governments to use artificial intelligence to improve their day-to-day operations. And at least for now, there are no signs of this progress slowing down.
B.

Artificial Intelligence Strategies in Finance

The remarkable power of artificial intelligence to identify patterns in
complicated data and use these patterns to predict future behavior has made
the field a major area of interest for financial firms. This is true both for the
large Wall Street banks that have traditionally dominated finance and for the
wave of smaller, nimbler fintech firms that have sprung up to challenge them
in recent years. On the one hand, large financial institutions like JP Morgan,
Goldman Sachs, and Morgan Stanley are increasingly hiring machine learning specialists and creating entire divisions devoted to artificial intelligence. 5 On the other, a handful of smaller, more focused financial firms
have turned to artificial intelligence as a primary strategy for their businesses.56 These trends suggest that artificial intelligence will likely play a
large role in determining the future of the financial sector.5 7
The potential uses of artificial intelligence in finance are nearly limitless. As Charles Elkan, the head of machine learning at Goldman Sachs, has
said, "within every business area there are opportunities to apply machine
learning."" That said, most financial firms to date have focused on a few
key areas where machine learning has obvious applications. First, they are
using artificial intelligence to improve credit risk assessments for
counterparties. Second, they are using it to protect themselves from fraud
and wrongdoing, either external or internal to the firm. And third, they are
using it to devise better trading strategies. It may be helpful to look at each
of these areas in turn to get a better sense of how artificial intelligence can
affect the financial world. One should keep in mind that most financial firms
are relatively tight-lipped about how, precisely, they are using artificial intelligence in their businesses. It is, after all, competitively valuable information. At the same time, broad categories may be identified."9

" See Sarah Butcher, The Top Machine Learning Teams in Investment Banks,
May 23, 2018.

EFINANCIAL-

CAREERS,

See Sameer Maskey, How Artificial Intelligence is Helping Financial Institutions,
(Dec. 5, 2018), https://www.forbes.com/sites/forbestechcouncil/2018/12/05/how-artificial-intelligence-is-helping-financial-institutions/#441cd632460a.
" See Thomas Vartanian, Nefarious Nations Will Take the Lead on AI If the US Doesn't,
THE HILL, (Dec. 12, 2018) ("As part of the country's critical infrastructure, the highest priority
should be given to the development of a financial services and capital markets strategy to
foster Al innovations while at the same time protecting against its unprecedented threats.").
" Charles Elkan, Will Machine Learning Transform Finance, YALE INSIGHTs, (Jan. 3,
2019), https://insights.som.yale.edulinsights/will-machine-leaming-transform-finance.
" For a comprehensive industry survey of bankers' opinion on artificial intelligence in
finance, see Laura Noonan, AI in Banking: The Reality Behind the Hype, FINANCIAL TIMES,
(Apr. 11, 2018), https://www.ft.com/content/b497al34-2d21-l1e8-a34a-7e7563b0b0f.
5

FORBES,

2020]

Artificial FinancialIntelligence

349

'

Perhaps no area of artificial financial intelligence has received as much
attention (or criticism) than its use in credit ratings.60 Banks are in the business of loaning money to people-bank loan volume is a carefully watched
sign, not just of bank profits, but also of economic health more generally.6
One of the primary concerns that banks have when they extend a loan, -of
course, is that the borrower may not pay the money back. This means that
the ability to accurately predict the credit risk of borrowers is tremendously
important (and valuable) to financial institutions as a whole. They often outsource this process to credit scoring agencies, such as Equifax, Experian,
and TransUnion. But as many observers have noted, the credit rating agencies' process itself is far from perfect.6 2 Joe Nocera of the New York Times
has written that a "credit score is derived after an information-gathering process that is anything but rigorous" but, nevertheless, "essentially . . . has
become the only thing that matters anymore to the banks and other institutions that underwrite mortgages."63 Many financial firms believe that machine learning algorithms could do better. The theoretical benefits are clear.
By sifting through massive amounts of prior data about both defaulting and
non-defaulting borrowers, machine learning algorithms might be able to
identify hidden or unexpected variables that affect a borrower's likelihood of
repaying a loan. Rather than relying on simple and obvious relations, such as
whether a borrower has defaulted on past loans or has large amounts of
credit card debt, machine learning algorithms might find that other, less noticeable information, such as a borrower's purchase history, friend group, or
Twitter posts, provides meaningful information about his ability to repay a
loan. And indeed, financial firms are increasingly using artificial intelligence
to make these difficult assessments. For example, Zest Finance, a fintech
startup based in California, offers a machine learning-based credit modeling
product for mortgages, auto loans, business loans, and consumer loans. 4 It
claims that its algorithm, on average, leads to a 15% increase in loan approval rates and a 30% decrease in charge-off rates, and that it only requires

' See Danielle Keats Citron & Frank Pasquale, The Scored Society: Due Processfor Automated Predictions, 89 WASH. L. REV. 1, 34 (2014); Christopher K. Odinet, Consumer BitCredit and Fintech Lending, 69 ALA. L. REV. 781, 858 (2018); Matthew A. Bruckner,
Regulating Fintech Lending, 37 BANKING & FIN. SERVICES POL'Y REP. 1, 7 (2018); Matthew
Adam Bruckner, The Promise and Perils of Algorithmic Lenders' Use of Big Data, 93 Cm.KENT L. REV. 3, 60 (2018).
" See Peter Eavis, Is a Slowdown in Bank Lending a Bad Sign for the Economy, N.Y.
TmIEs, (Oct. 12, 2018), https://www.nytimes.com/2018/10/12/business/dealbook/bank-lending-slowdown-economy.html; Rachel Louise Ensign, Business-Loan Drought Ends for Banks,
WALL S-r. J., (July 8, 2018), https://www.wsj.com/articles/business-loan-drought-ends-forbanks- 1531058400.
62 See Citron & Pasquale, supra note 60, at 10-16.
63 Joe Nocera, CreditScore Is the Tyrant in Lending, N.Y. TInMES, (July 23, 2010), https://
www.nytimes.com/2010/07/24/business/24nocera.html.
' See Steve Lohr, Banking Start-UpsAdopt New Tools for Lending, N.Y. TIMEs, (Jan. 18,
2015), https://www.nytimes.com/2015/01/19/technology/banking-start-ups-adopt-new-toolsfor-lending.html.

Harvard Business Law Review

350

[Vol. 10

three months to implement.15 The German startup Kreditech uses big data
and machine learning to assess borrowers' creditworthiness and even extends
loans, both domestically and internationally, in real time using its automated
system. 6 Their model is so attractive and effective that some of the credit
scoring agencies are themselves getting into the game, with Equifax recently
adopting machine learning in its models and Experian similarly offering a
machine learning product to their clients. 67 And, to be clear, the effects of
Al-enhanced credit rating are not simply negative, providing a rationale for
declining a loan to someone who might otherwise have received one. Al
may also show that certain groups have been systematically underrated by
current systems, and thus more deserving of loans than traditionally
believed.
Artificial intelligence also plays an important role in financial institutions' identification and prevention of fraud and wrongdoing. Finding effective ways to prevent fraudulent payments or claims has been a constant thorn
in the side of the financial industry. And despite considerable efforts, fraud
is rising-in 2016, 15.4 million U.S. consumers were victims of identity
fraud, an all-time high, leading to $16 billion in losses.68 Most of these losses
stem from credit card fraud.6 9 In order to better identify these fraudulent
behaviors and, ideally, prevent them before they occur, financial institutions
are increasingly turning to machine learning algorithms.70 Machine learning
algorithms, after all, can analyze and assess the tremendous amounts of
transaction data constantly flowing across the sector much more quickly
than a human ever could. Bank of America is exploring how to incorporate
artificial intelligence into its fraud detection systems.71 The startup Feedzai
specializes in using machine learning to prevent fraudulent payments in the
financial industry, and its success has led Citigroup to incorporate its product into its own fraud prevention systems. 72 Monzo, the popular British
credit card company, deployed a machine learning model for preventing
fraud in its prepaid debit cards, allowing it to reduce fraud rates from 0.84%

65
66

See ZEsTAI, What is ZAML?, https://www.zest.ai/zaml.
See WALL ST. J., Germany's Kreditech Scores $40M for Automated Lending, (June 25,

2014).
67 See Alex Hickey, Equifax Debuts Machine Leaning-Based Credit Scoring System, CIO
DIVE (Mar. 28, 2018), https://www.ciodive.com/news/equifax-debuts-machine-leaming-basedcredit-scoring-system/520095/.
' See AnnaMaria Andriotis & Peter Rudegeair, Credit-CardFraud Keeps Rising, Despite
New Security Chips-Study, WALL ST. J., (Feb. 1, 2017), https://www.wsj.com/articles/creditcard-fraud-keeps-rising-despite-new-security-chipsstudy-1485954000.
69 Id.
70 See Sara Castellanos & Kim S. Nash, Bank ofAmerica confronts AI's "Black Box" With
FraudDetection Effort, WALL ST. J., (May 11, 2018), https://blogs.wsj.com/cio/2018/05/1 1/
bank-of-america-confronts-ais-black-box-with-fraud-detection-effort/.
7' Id.
72 See Bus. WIRE, Citi Partnerswith Feedzai to Provide Machine Learning Payment Solutions, (Dec. 19, 2018), https://www.businesswire.com/news/home/20181219005388/en/CitiPartners-Feedzai-Provide-Machine-Learning-Payment.

2020]

Artificial FinancialIntelligence

351

to less than 0.0 1% in just six months." Other financial institutions are experimenting with using machine learning more broadly to prevent bad behavior
among their own employees. In 2018, ING Bank partnered with a machine
learning startup specializing in speech recognition to spot and stop insider
trading and collusion within the firm.7 4
Finally, financial firms are also using artificial intelligence to improve
investment returns. Again, the change here is more a matter of degree than
of kind. Investment firms, both large and small, have long used computers to
analyze stock information, identify price discrepancies, and make investments." The artificial intelligence revolution of recent years, however, has
opened up new avenues for incorporating algorithms more deeply into investment strategies. One clear use case is for quant hedge funds, which use
algorithms to determine investment decisions.76 Already familiar (and comfortable) with computerized, automated investment decisions, quant hedge
funds might be expected to be early adopters of machine learning strategies.
And indeed, it appears that they are. In 2017, one study found that 20% of
hedge funds make use of machine learning or artificial intelligence in their
investment strategies. 7 Just a year later, that number had risen to 56%.11
Large investment firms are investing heavily in machine learning as well.
BlackRock, the asset management company, uses artificial intelligence to
spot trends in financial data, including finding relationships between securities or market indicators, scanning social media for insights into employee
sentiment, and analyzing search engines for popular search terms.79 Regular
investors can now take advantage of machine learning-based investment
strategies as well. In 2017, EquBot launched an exchange traded fund, called
AT Powered Equity, that uses machine learning algorithms to decide on its
investment components. 0 EquBot states that its mission is to "give everyone
7 See MONzo, FIGHTING FRAUD WITH MACHINE LEARNING, (Feb. 3, 2017), https://monzo
.com/blog/2017/02/03/fighting-fraud-with-machine-leaming/; see also THE EcONOMIST, Machine-Learning Promises to Shake Up Large Swathes of Finance, (May 25, 2017), https://
www.economist.com/finance-and-economics/2017/05/25/machine-learning-promises-toshake-up-large-swathes-of-finance.
7 See Intelligent Voice, Compliance Monitoring Enters Al Age at ING with Intelligence
Voice and Relativity Trace, (Dec. 3, 2018), https://www.intelligentvoice.com/2018/12/03/compliance-monitoring-enters-ai-age-at-ing-with-intelligent-voice-and-relativity-trace.
" See, e.g., Edward L. Pittman, Quantitative Investment Models, Errors, and the Federal
Securities Laws, 13 N.Y.U. J. L. & Bus. 633, 643-63 (2017).
76 See generally Thomas J. Brennan & Andrew W. Lo, Dynamic Loss Probabilitiesand
Implications for FinancialRegulation, 31 YALE J. ON REG. 667, 692-95 (2014); Scorr PATTERSON, THE QUANTS: HoW A NEw BREED OF MATH WHIZZES CONQUERED WALL STREET AND
NEARLY DESTROYED IT (2011).
" See Amy Whyte, More Hedge Funds Using AI, Machine Learning, INSTITUTIONAL INVESTOR (July 19, 2018), https://www.institutionalinvestor.com/article/bl94hmlkjbvd37/MoreHedge-Funds-Using-Al-Machine-Learning.
78 Id.
" See Conrad De Aenlle, A.I. Has Arrived in Investing. Humans Are Still Dominating.,
N.Y. TIMEs, (Jan. 12, 2018).
" See Bailey McCann, The Artificial-IntelligentInvestor: Al Funds Beckon, WALL ST. J.,
(Nov. 5, 2017).

352

HarvardBusiness Law Review

[Vol. 10

access to investment opportunities that artificial intelligence can uncover."s"
Another potential use for artificial financial intelligence is in investment
banking, where firms might use artificial intelligence algorithms to identify
opportunities for acquisitions or divestments that might not be readily apparent to insiders.8 2
All of these developments suggest that artificial financial intelligence is
poised to become an integral part of the financial industry. Financial firms
are actively researching, and in many cases adopting, artificial intelligence
strategies. These strategies have been particularly useful in assessing risk,
preventing fraud, and investing capital. And the trend is clear-artificial financial intelligence is on the rise. As further proof, one need only look as far
as the chartered financial analyst (CFA) exam, the prized certification for
future investment professionals. In 2019, the exam added computer science,
with a focus on artificial intelligence and data mining, as a new subject that
all aspiring CFAs must have familiarity with by 2020.83 Future bankers will
know much more about artificial intelligence than they do today.
C.

Existing Literature

The stunning success of artificial intelligence in recent years has
sparked renewed interest in the field from academia. Scholars have begun to
ask pointed questions about the economic, social, and moral consequences
of turning over more and more of our lives to the control of algorithms.
Legal scholars, as they are wont to do, have proposed new laws to constrain
and guide the use of artificial intelligence. And while the literature is broad
and continually growing, much like artificial intelligence itself, a few common threads can be identified in the scholarship. First, a group of scholars
has argued that the progress of artificial intelligence risks widespread negative effects on labor markets. Second, a number of scholars have argued that
the results of artificial intelligence are so difficult to explain, and, thus, to
scrutinize and monitor, that artificial intelligence will end up undermining
the regulatory state. Finally, a group of scholars has argued that artificial
intelligence methods might perpetuate racial or other discrimination in
broader society.
On the question of employment, a number of scholars have raised concerns about artificial intelligence's capacity to supplant and replace human
workers." Indeed, President Barack Obama issued a report on the topic, con-

" EquBot Website, https://equbot.com. The ETF has, however, underperformed-from its
inception in October 2017 to the end of 2018, it rose 3.1%, while the S&P 500 rose 5.1%. See
De Aenlle, supra note 79.
82 See Elkan, supra note
58.
8 See Trevor Hunnicutt, CFA Exam to Add Artificial Intelligence, "Big Data" Questions,
REUTERS, (May 23, 2017).
" See MARTIN FORD, THE RISE OF THE ROBOTS: ThCHNOLOGY AND THE THREAT OF A
JOBLESS FUTURE (2016); Erik Brynjolfsson & Andrew McAfee, Human Work in the Robotic

Artificial FinancialIntelligence

2020]

353

cluding that we need new policies and laws to "educate and prepare new
workers to enter the workforce, cushion workers who lose jobs, keep them
attached to the labor force, and combat inequality."* 5 The idea of machines
replacing humans in the labor force is not new-they have been doing so
since the Industrial Revolution. But artificial intelligence's vastly greater
power, combined with its speed of improvement and breadth of application,
present these concerns on a grander scale. Some scholars argue that artificial
intelligence will lead to job losses as artificial intelligence simply eliminates
the need for human workers in many fields. 6 Others argue that even if artificial intelligence does not lead to job losses, it will still lead to increased
wealth disparities (or "income polarization") as the techno-savvy reap the
benefits of cheaper labor and the techno-ignorant are relegated to low wages
and long hours." In either case, the artificial intelligence revolution could
cause major labor force disruptions.
Another group of scholars has argued that the problem with artificial
intelligence is explainability."8 One of the issues with machine learning-produced results is that they are often difficult to interpret in easily understood
language. Imagine, for example, if you asked a bank why they turned you
down for a loan, and, in reply, they handed you the source code of their
machine learning algorithm and the values of millions of automatically
tuned algorithm parameters-this might be an accurate description of what
they had in fact done to reach their result, but it would not be very helpful
for understanding the motivation for the decision, or what you might do to
change it. This is less of a problem in some fields than in others. For example, if a machine learning algorithm is better at spotting cancer than human
experts, we probably do not care much about how it is doing it. If it turns out
that the machine learning algorithm spotted a complex array of patterns that
have no intuitive counterpart in human reason, we probably would not care,
as long as it works. But when we turn from the world of facts (do I have
cancer?) to a world of judgment (should I loan money to this person?), then
the process and rationale of the decisionmaker suddenly takes on greater
importance. Regulators need to understand algorithms in order to ensure

Future: Policy for the Age of Automation,

FOREIGN AFFAIRS,

(July/Aug., 2016); Calo, supra

note 31, at 425-27.
"EXEC.

OFFICE OF THE PRESIDENT, ARTIFICIAL INTELLIGENCE, AUTOMATION,

AND THE

(2016), https://obamawhitehouse.archives.gov/sites/whitehouse.gov/files/documents/Artificial-Intelligence-Automation-Economy.pdf.
8 See Michael Guihot, et al., Nudging Robots: Innovative Solutions to Regulate Artificial
ECONOMY

Intelligence, 20 VAND. J. ENT. & 'IbCH. L. 385 (2017).
8 See Cynthia Estlund, What Should We Do After Work? Automation and Employment
Law, 128 YALE L. J. 254 (2018).
" See Cary Coglianese & David Lehr, Transparency and Algorithmic Governance, 71
ADMIN. L. REV. 1 (2019); Andrew D. Selbst & Solon Barocas, The Intuitive Appeal of Explainable Machines, 87 FORDHAM L. REV. 1085 (2018); David Lehr & Paul Ohm, Playing with the
Data: What Legal Scholars Should Learn About Machine Learning, 51 U.C. DAVIS L. REV.
653 (2017); Joshua A. Kroll, et al., Accountable Algorithms, 165 U. PA. L. REV. 633 (2017).

354

HarvardBusiness Law Review

[Vol. 10

they comply with the law.89 Consumers need to understand the algorithms to
trust that they have been dealt with fairly, and to be able to change their
behaviors. 0 Indeed, the European Union has gone so far as to require companies that employ automated decisionmaking to provide "meaningful information about the logic involved." 91 As one group of scholars has put it,
"[b]ecause automated decision systems can return potentially incorrect, unjustified, or unfair results, additional approaches are needed to make such
systems accountable and governable.""
In a similar vein, some scholars worry that the deployment of artificial
intelligence could lead to disguised discrimination.93 The logic is as follows.
Machine learning algorithms attempt to identify patterns in large data sets,
and then draw conclusions from those patterns. In order to do their work,
then, machine learning algorithms must be fed data, and large amounts of it.
But that data itself is not necessarily neutral. It may (in fact, it likely will)
incorporate some form of bias or discrimination from the outside world. And
if the data is biased or discriminatory, then the machine learning algorithm
itself may become biased or discriminatory, too. This could happen in one of
two ways: first, a nefarious actor might intentionally feed the machine learning algorithm bad data;9 4 and second, a perfectly innocent, but naive, actor
might unintentionally use data that has unrecognized bias in it." As an example of the first problem, intentional discrimination, if a firm (or a rogue
employee at a firm) wanted to refrain from doing business with individuals
of certain races or religions, it might construct a machine learning algorithm
that looked unbiased but in fact encoded certain nefarious factors. After all,
software engineers are the ones who have to make the hard decisions about
what data to use, how to structure the data, and how to interpret it. Bias
could creep in at any of these levels, and it might be difficult for outside
observers to detect. But even if there is no intentional discrimination in the
machine learning algorithm or the data set, it may still reflect the effects of
past discrimination in the world. For example, if minorities received poor
credit scores in the past due to discrimination, then machine learning algorithms might learn that these minorities should be scored lower than their

" See Selbst & Barocas, supra note 88, at 1089-90.
9 Id. at 1119-22.
' Commission Regulation 2016/679, 2016 O.J. (L 119) 1.
* Kroll et al., supra note 88, at 633.
1 See id. at 678-82; Citron & Pasquale, supra note 60, at 10-16; Margaret Hu, Algorithmic Jim Crow, 86 FORDHAM L. REV. 633 (2017); Barocas & Selbst, supra note 88;
Anupam Chander, The Racist Algorithm?, 115 MICH. L. REv. 1023 (2017); Sonia K. Katyal,
Private Accountability in the Age of Artificial Intelligence, 66 UCLA L. Rnv. 54, 54 (2019)
(arguing that "[tihe issue of algorithmic bias represents a crucial new world of civil rights
concerns, one that is distinct in nature from the ones that preceded it"); Kate Crawford, Think
Again: Big Data, FOR. POL'Y, (May 10, 2013); Talia B. Gillis & Jann Spiess, Big Data and
Discrimination, 86 U. CHI. L. Rav. 458 (2019) (arguing that machine learning algorithms
hinder the application of discrimination laws).
' See Lehr & Ohm, supra note 33, at 703-05.
9 Id.

2020]

Artificial FinancialIntelligence

357

the problem of "non-stationary" behavior. 103 As noted before, scholars have
pointed out that if a data set reflects a biased world (such as low credit
scores for racial minorities), then machine learning algorithms based on
those data sets may well reinforce those biases. This is a major problem, and
has slightly different valences in the financial sector, which has a major
influence on the direction of economic development. Imagine, for example,
a machine learning algorithm that was trained on a data set from the period
of 1995 to 2000. Basing its recommendations on the relative success of various industries during the dotcom boom, it might have concluded that, as a
general rule, technology startups vastly overperformed other sectors. It
might well have recommended that internet-focused companies such as
Pets.com (which raised $82 million in an IPO in 2000 before going bankrupt
just nine months later)' 1 or GeoCities (which was bought by Yahoo for $3.6
billion in 1999 but quickly collapsed)105 were excellent investments. The machine learning algorithm, by basing its recommendations on prior data from
a market that was fundamentally different from the market post-2000, was
simply ineffective. The data was incurably biased. Similarly, the use of machine learning might lead to the prolongation or even perpetuation of shortterm trends in markets, to the detriment of economic efficiency. Without
data from the period after 2000, when the dotcom bubble burst, the machine
learning algorithm might well have recommended investments in the tech
sector and, by doing so, reinforced preexisting patterns of industry focus.
One potential consequence of this is that bubbles could become more dramatic-machine learning algorithms could magnify momentum in particular
sectors or trends, leading to eventual and catastrophic collapse. Another potential consequence, and one that is perhaps even more troubling, is that
artificial financial intelligence might prevent bubbles from bursting. In other
words, if a large section of financial institutions uses machine learning algorithms to decide on their investment strategies, and those machine learning
algorithms conclude that, say, the technology sector outperforms other sectors, then it may be difficult for the market to self-correct. Other industries
might simply wither away from lack of investment or access to capital.
Third, the dependence of machine learning algorithms on data creates
strong incentives for financial institutions not only to put the data they cur1
rently have to better use, but also to gather more of it.1 6 If the quality of

artificial intelligence as a financial tool grows as the size of its data set increases, then financial firms may search for new ways to get increasingly
"oSee MASASHI SUGIYAMA & MOTOAKi KAWANABE, MACHINE LEARNING IN NON-STATIONARY ENVIRONMENTS: INTRODUCTION TO COVARIATE SHIFT ADAPFATION (2012).
'" See Pui-Wing Tam & Mylene Mangalindan, Pets.com Will Shut Down, Citing Insufficient Funding, WALL ST. J. (Nov. 8, 2000), https://www.wsj.com/articles/SB97361747513691
7228.
105 See Robert Cyran, Yahoo Is a Case Study in Poor Timing, N.Y. TIAs, (July 26, 2016),
https://www.nytimes.com/2016/07/27/business/dealbook/yahoo-is-a-case-study-in-poor-timing
.html.
" See Calo, supra note 31, at 420-23.

358

Harvard Business Law Review

[Vol. 10

intrusive and sensitive data on citizens. Their practices could quickly raise
privacy concerns. Part of the appeal of artificial intelligence is that it aims to
find patterns and relations that humans would never consider. But it cannot
find these patterns if it does not have data to work on. Engineers or investment advisers that decide what information to include in data sets therefore
have a potentially unlimited world of information to scour. Perhaps your
Facebook friend list contains useful information about your credit score. Perhaps the prior personal relationships of a CEO have correlations with company performance. Perhaps the email patterns, mobile phone conversations,
or locations of consumers contain relevant information for mortgage default
rates. We have already seen a backlash against the tech giants for their gathering, storage, and use of user data.1 07 But as the stories of privacy violations
proliferate, the value of data to firms is becoming increasingly clear. Artificial intelligence may accelerate that trend.
Finally, and on a related note, the data dependency of artificial intelligence gives significant advantages to firms that have access to large data
sets.108 Large Wall Street firms are sprawling, multi-faceted companies that
operate in hundreds of different markets and sectors. This means that they
can see and track more information than anyone else. And at the risk of
repetition, access to data in a machine learning environment is a competitive
advantage. At the extreme, this might lead to large financial institutions taking an insurmountable dominant position that no smaller startups could challenge. This worry about artificial intelligence's benefits to the powerful has
been raised in the national security world. A recent study by the Belfer
Center at Harvard University concluded that, at least in the near term,
"bringing Al technology applications into the cyber domain will benefit
powerful nation-state actors."'" The most powerful nation states are the actors that have the scientific knowledge, research budgets, and access to information that are necessary to develop artificial intelligence systems. The
same might be said of financial institutions: large financial institutions can
hire away artificial intelligence experts from academia, industry, and their
competitors, and they have the capital to fund long-term research into artificial intelligence's applications for finance. Small startups simply do not have
this capacity. Over time, this trend might exacerbate the already sizeable
amount of concentration in the financial sector. To be sure, the long-term
consequences of artificial intelligence might be to level the playing field, not
tilt it. Machine learning algorithms are, after all, algorithms, which can be
copied and disseminated much more easily than hard assets. To the extent
that prominent artificial intelligence strategies become widely accessible and
usable by new financial firms, they may serve as a powerful equalizing fac-

107 See Anita L. Allen, Protecting One's Privacy in a Big Data Economy, 130 HARV. L.
REv. F. 71, 71-72 (2016).
1" See Calo, supra note 31, at 424.
'" See ALLEN & CHAN, supra note 7, at 20.

2020]

Artificial FinancialIntelligence

359

tor in financial markets. But until then, the concern about concentration in
the financial sector will persist, and may, at the extreme, create antitrust
problems. 10
The data dependency problem that artificial intelligence struggles with,
and that raises issues for the financial sector, highlights a broader feature of
our financial system and the potential limitations of artificial financial intelligence. Capital markets simply are not like the kinds of problems that artificial intelligence has, to date, excelled at. In cancer screening, image
recognition, and language cognition, there is a fact that the artificial intelligence algorithm is attempting to discern-whether someone has cancer,
what that image depicts, what that person said. But in capital markets, there
is no similar factual counterpart. To be sure, there are factual elements of
markets, from EBITDA to market capitalization to employee numbers and
product sales (although even these can be fiddled with). But the fundamental
feature of capital markets-the allocation of value to companies-is not so
much a fact as a belief. In order to outperform the market, one needs to
understand not only the facts about a company, but also how other people
will view those facts. This is important because the very fact that a trend has
been discovered often means that the trend disappears. Momentum, small
capitalization, low volatility: all of these are market trends that have, at one
time or another, delivered market superior returns."' But once others learn
of the trend, they can quickly adopt strategies that take advantage of it, and
the very fact of their adopting those strategies eliminates the market superior
returns. This is a fundamental feature of capital markets. Indeed, it is the
underlying basis of efficient capital markets theory. Artificial intelligence is
not well-equipped to deal with these sorts of problems.
B.

Overweighting

Another issue with the use of artificial financial intelligence is the potential for human decisionmakers to rely too heavily on its recommendations. Given the difficulty of explaining and understanding machine learning
algorithms and the outputs they generate, financial decisionmakers, including consumers, might simply default, without deliberation or debate, to accepting the conclusions or recommendations that the machine learning
algorithms make. Even if decisionmakers are aware of the limitations of machine learning, in the absence of clear methods for refuting or disproving the
artificial intelligence outcome, artificial intelligence may take on undue
weight in the structure of the financial industry." 2
"o See Van Loo, supra note 9, at 251.
.i See Is Efficient-Market Theory Becoming More Efficient?, THE EcONOMIST, May 27,
2017, https://www.economist.com/finance-and-economics/2017/05/27/is-efficient-market-theory-becoming-more-efficient.
112 Of course, this may be desirable if we believe that financial decisionmakers are already
biased in problematic ways. Leaning on less-biased, or less harmfully biased, machines may

360

Harvard Business Law Review

[Vol. 10

We have already discussed one way in which machine learning algorithms fail: they often struggle with problems that possess non-stationary
qualities.' 3 When the fundamental nature of the market studied (for example, the credit risk of borrowers, the stock prices of companies, or the methods of fraudsters) changes from one period to the next, the patterns and
trends that a machine learning algorithm can identify from one period's data
will not apply to the next. The non-stationary problem is particularly pronounced where the mere knowledge of the trend may lead to the trend itself
disappearing, as might be the case if everyone knew that, say, the stock
market always rose 10% every year. Arbitrageurs could simply immediately
push the price of the stock up at the beginning of the year or even earlier.
The trend would then disappear.
But machine learning algorithms also struggle with other known
problems. One is "overfitting."ll 4 The goal of machine learning algorithms
is to find statistical trends in data and then generalize them so that they work
for new data. But if the algorithm generalizes in a way that incorporates
trends that are based on random factors that do not apply outside the particular data set, then it is said to overfit the data, that is, it finds trends that match
the training data but do not match the outside world. To take a simplified
version of this, imagine if a machine learning algorithm only had data on
two companies, Apple and Kodak. It might conclude that all companies that
start with "A" outperform the market, and all companies that start with "K"
go bankrupt. This would obviously fit the training data perfectly, but it likely
would not generalize to the wider world of non-Apple and non-Kodak companies. The process of finding trends that do not just fit the available data,
but will also generalize well to future or outside data, is a difficult one, and
one that machine learning algorithms have a hard time doing.
Another known problem for artificial intelligence algorithms is lowprevalence data."' The idea here is that machine learning algorithms need a
certain threshold amount of data on a given topic before being able to make
reliable predictions about it. But where certain subpopulations within a data
set are rare and different from the broader data set, machine learning algorithms may simply fail to draw accurate conclusions about them. This has
been a serious problem in facial recognition algorithms: one study found that
they incorrectly classified only 1% of light-skinned men, but more than one

then be preferable. See Holger Spamann & Lars Kldhn, Justice is Less Blind, and Less Legalistic, Than We Thought: Evidence from an Experiment with Real Judges, 45 J. LEG. STUD. 255,
255 (2016) (finding that judges' decisions were affected by irrelevant characteristics of defendants but failed to mention these characteristics in their written reasoning).
113 See SUGIYAMA & KAWANABE, supra note 103, at xi.
114 See STUART J. RUSSELL & PETER NORVIG, ARTIFICIAL INTELLIGENCE: A MODERN APPROACH 705 (3rd ed. 2010); RICHARD BERK, STATISTICAL LEARNING FROM A REGRESSION PERSPECTIVE 142 (2008); Lehr & Ohm, supra note 33, at 684.
11 See Lehr & Ohm, supra note 33, at 678-79.

Artificial FinancialIntelligence

2020]

355

peers. Even if the algorithm explicitly could not take race into account, it
might find that other factors that correlate with race (such as names, geography, or other information) are just as effective.9 While scholars do not have
easy solutions to these problems, they have pointed out their deeply harmful
effects.
II.

ARTIFICIAL

FINANCIAL

INTELLIGENCE

Artificial intelligence has grown exponentially in recent years as more
powerful computers apply more advanced algorithms to analyze more expansive data sets. Financial institutions have increasingly deployed these artificial intelligence strategies to improve their own results. Scholars have, in
turn, begun to grapple with the thorny legal and ethical issues raised by the
incorporation of artificial intelligence into the financial world. They have
raised questions about its effects on labor, its inscrutable outputs, and its
capacity for harmful discrimination. All of these are important problems,
and ones that apply to the financial world. But at the heart of these critiques
is the assumption that artificial intelligence's dangers are rooted in its effectiveness. In other words, artificial intelligence may lead to widespread job
losses, opaque financial decisions, or even discriminatory results, because it
is so capable at what it does. It is simply better at reading data and finding
patterns than humans are, and in doing so, it may create risks. But this Part
will argue that artificial financial intelligence's primary risks are not the
product of its potential to surpass human intelligence. To date, prognostications about artificial intelligence's "Cambrian explosion" are exaggerated.
Instead, the real risk from artificial financial intelligence is its capacity to
magnify human failings. If artificial intelligence ends up being incorporated
widely in the financial world, its harms will likely stem from inadequate data
input, improper usage, and feedback effects between algorithms. These
problems are dangerous and must be contained, but they are also, in a sense,
less radical than some of the concerns popular in the media today.
A.

Data Dependency

A common critique of artificial intelligence is that it relies heavily on
prior data sets." As mentioned earlier, the machine learning algorithms that
have become the focus of current artificial intelligence research depend on
receiving and analyzing large amounts of data. This leads to several related
problems, some of which are specific to the financial sector, and others of

&

' See Citron & Pasquale, supra note 60, at 14; Rory Van Loo, The Corporationas Courthouse, 33 YALE J. ON REG. 547, 579-80 (2016) (observing that businesses' algorithms may
lead to racial discrimination by using socioeconomic factors as a proxy); Anya Prince
Daniel Schwarcz, Proxy Discriminationin the Age of Artificial Intelligence and Big Data, 105
IOWA L. REv. 1257 (2020).
1 See Brummer & Yadav, supra note 11, at 274-75.

356

Harvard Business Law Review

[Vol. 10

which are more general. As The Economist has written, "the world's most
valuable resource is no longer oil, but data."98
First, the use of artificial financial intelligence will cause important financial decisions to increasingly turn not on human judgments about the
wisdom or soundness of the decision, but rather on the quality and nature of
the data that are fed into machine learning algorithms. This information is,
necessarily, determined by humans. Its effectiveness as a training tool will
thus depend heavily on the knowledge and sophistication of the employees
who decide which data to use and how to appropriately code it for machine
learning algorithms. 99 iBM's Watson supercomputer, which uses machine
learning techniques and was widely billed as a potentially lifesaving cancer
detecting tool, was recently found to be issuing "unsafe and incorrect" treatment recommendations. While the causes are unclear, some observers believe it is due to engineers training the software on small numbers of
hypothetical cancer cases, rather than real patient data.'" Others believe that
the machine learning algorithm simply does not do a good job at handling
rare or recurring cancers because it simply lacks data on them.' 0' And the
financial world has seen similar mistakes. McKinsey reported that one Asia
Pacific bank lost $4 billion when it applied interest rate models based on
improperly entered data. 102 Thus, the use of machine learning in financial
decisions raises concerns about potentially harmful inaccuracies contained in
data sets, as well as the possibility that these data sets will tend to miss out
on rare but important financial occurrences (the black swan types of events
that many investors focus on).
Second, even if the data that is used in financial applications of machine learning is not flawed, its use may create a series of related problems
in market efficiency. For one, the data may poorly reflect the nature of financial markets. Machine learning algorithms, by their very nature, aim to find
statistical trends in data and then generalize them so they work for new data.
This can be quite useful in a number of contexts, but it tends to run into
problems when the nature of the field changes over time. This is known as

98 See The World's Most Valuable Resource Is No Longer Oil, But Data, THE EcONOMIST
(May 6, 2017), https://www.economist.com/leaders/2017/05/06/the-worlds-most-valuable-resource-is-no-longer-oil-but-data.
9 See Brummer & Yadav, supra note 11, at 274-75.
"n See Julie Spitzer, IBM's Watson Recommended "Unsafe and Incorrect" Cancer Treatments, STAT Report Finds, BECKER'S HEALTH IT & CIO REPORT (July 25, 2018), https://www
.beckershospitalreview.com/artificial-intelligenceibm-s-watson-recommended-unsafe-and-incorrect-cancer-treatments-stat-report-finds.html.
'0 See Daniela Hernandez & Ted Greenwald, IBM Has a Watson Dilemma, WALL ST. J.,
(Aug. 11, 2018), https://www.wsj.com/articles/ibm-bet-billions-that-watson-could-improvecancer-treatment-it-hasnt-worked-1533961147.
102 See Philipp Harle, et al., The Future of Bank Risk Management, McKLNSEY & COMPANY, (July 2016), https://www.mckinsey.com/business-functions/risk/our-insights/the-futureof-bank-risk-management.

2020]

Artificial FinancialIntelligence

361

third of dark-skinned women. 116 In the financial world, this might lead to
similar problems, such as incorrectly risk-scoring minorities, but it might
also lead to quite different ones, such as failing to identify uncommon but
significant subpopulations of a market, such as unicorn startups that have
provided enormous returns to venture capitalists."'
To be sure, machine learning experts are well aware of these problems.
They have devised ways to deal with nonstationary markets, overfitting algorithms, and low-prevalence data. But these methods are imperfect and, at
most, can limit the problems, not resolve them. One researcher I spoke with
has a rule of thumb for assessing the claims of new artificial intelligence
products, something he calls the "87% rule." If an engineer or entrepreneur
claims that his artificial intelligence software can achieve accuracy rates of
higher than 87% purely by fitting data, then the researcher assumes that the
software is either overfitting or unrealistically constraining the prediction
problem in a way that does not accurately reflect real-world conditions. If,
instead, data-driven predictions provide accuracy around 87% and domain
expertise or insight are layered in to boost accuracy above that number, then
he deems it consistent with systems that have been deployed successfully in
practice. In other words, even with cutting-edge machine learning algorithms
deployed by experts in the field, the best-case result for most artificial intelligence strategies still fails a significant portion of the time.
However, there are a number of reasons to believe that, once financial
firms have incorporated artificial intelligence into their systems, it will be
hard to resist its persuasiveness."' The ready availability of a sophisticated
recommendation based on millions of data points, and the relative difficulty
of scrutinizing the reasoning or rationale of the recommendation, may lead
banks and firms to overweight artificial intelligence relative to other decisionmaking processes. As one doctor has said about the use of artificial intelligence in hospitals, "In my practice, I've often seen how any tool can
quickly become a crutch-an excuse to outsource decision making to someone or something else."" 9 This anecdotal evidence is only bolstered by our
growing knowledge of common cognitive biases in decisionmaking.
One well-known bias in human decisionmaking is the availability
bias.1 20 Availability bias refers to the tendency of decisionmakers, in the face

1"6 Joy Buolamwini & Timnit Gebru, Gender Shades: IntersectionalAccuracy Disparities
in Commercial Gender Classification, 81 PROC. Machine LEARNING REs. 1 (2018), http://proceedings.mlr.press/v81/buolamwinil8a/buolamwinil8a.pdf.
"'See Jennifer S. Fan, Regulating Unicorns: Disclosure and the New Private Economy,
57 B.C. L. Rev. 583 (2016).
" See Nizan Geslevich Packin, Consumer Finance and AL: The Death of Second Opinions?, N.Y.U. J. LEGIs. & LEG. PUB. POL'Y (forthcoming 2020).
"'Dhruv Khullar, A.I Could Worsen Health Disparities, N.Y. TIMES, (Jan. 31, 2019),
https://www.nytimes.com/2019/01/31/opinion/ai-bias-healthcare.html.
2 See Christine Jolls et al., A BehavioralApproach to Law and Economics, 50 STAN. L.
REv. 1471, 1477 (1998).

Harvard Business Law Review

362

[Vol. 10

of uncertainty, to use readily available facts to inform their beliefs.'2 1 This
seems like an obvious, and even rational, response to uncertainty-when we
do not know something, we use the facts we have at hand. But the result of
this natural instinct is that, where the availability bias is present, decisionmakers tend to overweight the representativeness of easily recallable information, and underweight more complex or distant information.12 2 This
bias is amplified when individuals lack sufficient information to form independent conclusions about an issue.' 23 In the financial world, where uncertainty is an unavoidable feature of the industry, the availability of an easily
accessible recommendation, provided by an algorithm and not susceptible to
ready refutation, might well cause people to rely excessively on the algorithm over independently reasoned deliberation. They might simply default to the algorithmically generated output.
Another well-studied cognitive bias is the anchoring effect.1 24 The
anchoring effect refers to people's tendency to be swayed by initial reference
points, or anchors.125 The classic example is in housing prices: when people
are asked how much they believe a house is worth, their answer will depend
heavily on how much they are told the seller is asking for it.1 26 The result of
setting an anchor is that decisionmakers will tend to use it as a reference
point, and then make adjustments from that point, typically departing from it
in only incremental and minimal ways.' 27 The application to artificial financial intelligence is clear. If an algorithm provides a recommended value for
an asset, or a risk score for a potential borrower, it may serve as a strong
anchor, informing and biasing subsequent discussion. Even if the ultimate
decisionmaker is aware of the flaws and limitations of machine learning, the
very process of providing a number or calculation will affect future
decisions.
Finally, and perhaps most powerfully, artificial financial intelligence
may take on undue weight in financial decisions because of a phenomenon
known as "herd behavior." 28 Herd behavior is a relatively intuitive concept:
121 See Amos Tversky & Daniel Kahneman, Extensional Versus Intuitive Reasoning:
The
Conjunction Fallacy in ProbabilityJudgment, 90 PSYCHOL. REV. 293, 295 (1983); Jolls et al.,
supra note 120, at 1477.
122 Jolls et al., supra note 120, at 1477.
123 See Timur Kuran & Cass R. Sunstein, Availability Cascades and Risk Regulation,
51
STAN. L. Rev. 683, 685-87 (1999).
124 See Marcel Kahan & Michael Klausner, Path Dependence in Corporate Contracting:
Increasing Returns, Herd Behavior and Cognitive Biases, 74 WASH. U. L. Q. 347, 362 (1996);
Gregory B. Northcraft & Margaret A. Neale, Experts, Amateurs, and Real Estate: An Anchoring-and-Adjustment Perspective on Property Pricing Decisions, 39 ORGANIZATIONAL BEHAV.
& Hum. DECISION PROCESSES 84 (1987); Eric A. Zacks, Contract Review: Cognitive Bias,
Moral Hazard, and Situational Pressure, 9 OHIo ST. ENTREPRENEURIAL Bus. L.J. 379, 394
(2015).
125 See Kahan & Klausner, supra note 124, at 362.
126 See Northcraft & Neale, supra note 124,
at 87-90.

Id.
'" See David S. Scharfstein & Jeremy C. Stein, Herd Behavior and Investment, 80 AM.
127

EcON. REV. 465, 465 (1990); Kahan & Klausner, supra note 124, at 356.

2020]1

Artificial FinancialIntelligence

363

when people see other people taking an action, or refraining from doing so,
they often imitate it, particularly if they have incentives to avoid diverging
too much from the status quo.129 It is not, strictly speaking, irrational to do
so. Individuals in management positions are often more likely to be punished
for taking an unusual risk or action than for a taking a common one, even if
they both turn out equally poorly. 13 0 After all, if they are simply doing what
everyone else is doing, it is hard to argue that they misbehaved. But if they
are underperforming their peers in material ways, then their reputation will
likely suffer. In the world of artificial financial intelligence, it is easy to
imagine that following the recommendation of a machine learning algorithm
would quickly become the low-risk, reputation-protecting action. If a decisionmaker follows an artificial intelligence-based recommendation, they can
deflect blame to the algorithm itself. On the other hand, if they intentionally
refuse to accept the artificial intelligence-based recommendation, then they
will have some explaining to do if their divergent opinion turns out to be
incorrect.
To be sure, all of these biases are avoidable with sufficient care, diligence, and monitoring. Much of cognitive psychology is concerned with
finding ways to debias individuals and correct for these errors. But the biases
highlight another important feature of artificial financial intelligence: however much we may want to eliminate human interaction or interference with
artificial intelligence, it ultimately creeps back in, often in unexpected
places. Financial decisionmakers need to review artificial intelligence recommendations based on their own knowledge about fundamental changes in
markets, changes that might not be well reflected in prior data sets. Financial
engineers must review artificial intelligence for overfitting problems. But in
the absence of easily understandable results, decisionmakers may simply defer to the artificial intelligence algorithm, either out of ignorance, risk aversion, or simply inertia. The problem here is not so much the algorithms
themselves, but our trust in them.
C.

Echo Effects

Finally, artificial financial intelligence may lead to unexpected feedback effects between competing artificial intelligence systems. This is a
well-known problem in the world of high-frequency trading.' 3 ' After the socalled "Flash Crash" of 2010, in which the Dow Jones plummeted 1000
points in a matter of minutes, many observers blamed high-frequency traders

See Scharfstein & Stein, supra note 128, at 465.
See Kahan & Klausner, supra note 124, at 356.
131 See Yesha Yadav, How Algorithmic Trading UnderminesEfficiency in CapitalMarkets,
68 VADo. L. REV. 1607, 1617-31 (2015); Pankaj K. Jain et al., Does High-FrequencyTrading
Increase Systemic Risk?, 31 J. FrN. Micrs. 1, 1 (2016); Frank J. Fabozzi et al., High-Frequency
Trading: Methodologies and Market Impact, 19 REV. FuTuREs MKTS. 7, 9-10 (2011).
129

130

Harvard Business Law Review

364

[Vol. 10

for increasing the speed and volatility of markets.1 32 And while the likelihood of echo effects between artificial intelligence systems may be strongest
in capital markets, where stock prices depend heavily on the decisions of
other players within the market, they may also appear in other areas of finance, such as credit scoring, fraud prevention, and strategic advising.
One concern is that decisions made by artificial financial intelligence
systems may not truly be independent of one another. In other words, if
financial institutions are all deploying similar machine learning algorithms
on similar data, they may reach similar results. When humans make decisions, they at least nominally are doing so on their own (even if their decisions may be affected by the decisions of their peers). Not so for artificial
intelligence. It is much simpler to copy an algorithm than it is to copy a
human brain. As one scholar has written in the national security context:
When China stole the blueprints and R&D data for America's F-35
fighter aircraft . .. it likely shaved years off the development timeline for a Chinese F-35 competitor-but China didn't actually acquire a modem jet fighter or the immediate capability to make one
....

But when a country steals the code for a cyberweapons [sic],

it has stolen not only the blueprints, but also the tool itself-and it
can reproduce that tool at near zero-marginal cost.133
If two competitors base their artificial intelligence strategies on the same
algorithms, they will likely reach the same conclusions about relevant
problems. This may well amplify both the speed and size of market swings,
as financial institutions increasingly adopt broadly consistent views of the
market.
Another concern is that, even if financial institutions do not adopt the
same artificial intelligence strategies, their separate strategies may still lead
to worrying feedback effects. One way this might occur is by machine learning algorithms incorporating the results of other machine learning algorithms
into their data sets. For example, if one financial institution's artificial intelligence system concludes that a borrower should not be given a loan, or
should only be given a loan at a certain interest rate, then that result may
filter down into the data sets that are used to train other financial institutions'
artificial intelligence systems. A loan rejection might, then, lead to more
loan rejections. This could happen even more quickly in liquid markets like
stock exchanges, where artificial intelligence strategies depend on quickly
identifying and acting on changes in the market. So even if investment com132 See Yadav, supra note 131, at 1628-29; Andrei Kirilenko et al., The Flash Crash: The
Impact of High Frequency Trading on an Electronic Market (May 5, 2014), https://www.cftc
.gov/sites/default/files/idc/groups/public/@economicanalysis/documents/file/oce-flashcrash03
14.pdf.
133 See Greg Allen, America's Planfor Stopping CyberattacksIs Dangerously Weak, Vox
(Mar. 27, 2017), https://www.vox.com/the-big-idea/2017/3/27/15052422/cyber-war-diplomacy-russia-us-wikileaks.

Artificial FinancialIntelligence

2020]

365

panies are using different artificial intelligence systems, the results of each
artificial intelligence system will affect the others. The overall outcome of
these interactions is uncertain, but, as before, might lead to more sudden or
volatile market movements.'3
Finally, another concern, and one that is written about widely in the
artificial intelligence industry, is the possibility of adversarialartificial intelligence.13 5 Adversarial artificial intelligence (or machine learning) refers
to the intentional manipulation of input data in order to fool artificial intelligence systems or lead them to unintended results.' 36 If an adversary or competitor knows how a given artificial intelligence system works, it can often
devise strategies that lead the system to malfunction. One way to do this is
by inserting poisoned data into the system. A good example of this is image
recognition in self-driving cars. Researchers have shown that by simply putting a few black and white stickers on a stop sign, they can trick machine
learning algorithms into misclassifying it 100% of the time, despite the fact
that the stop sign would still be readily identifiable to any human. 137 As more
of the financial world adopts artificial intelligence algorithms into their business strategies, the potential for adversarial strategies rises exponentially.
For example, if a criminal learns that a bank's algorithm identifies transactions as fraudulent only when certain patterns of behavior occur, he can simply avoid the behaviors that trigger a fraud alert. Or if borrowers know the
algorithm by which lenders rate their credit risk, they can simply manipulate
the relevant variables to ensure they are viewed as creditworthy. Perhaps
even more worrisome is the possibility that a competitor might manipulate
stock markets based on knowledge of how other financial institutions' investment algorithms work. Cybersecurity is already a major worry for financial institutions. This worry only increases as algorithms perform more and
more of the significant actions present in financial markets.
HI.

REGULATING ARTIFICIAL FINANCIAL INTELLIGENCE

Artificial financial intelligence raises a number of potential concerns,
from its perpetuation of historical trends, to its effect on human decisionmaking, to its capacity to magnify and exacerbate volatility. But the mere
possibility of harm, without more, does not suggest that we need new laws,
or even new regulatory approaches. To justify new laws, we need to know
that current laws are not working. To date, this threshold has not been met.
The financial industry is one of the most heavily regulated industries in the
See Tom C. W. Lin, The New FinancialIndustry, 65 ALA. L. REV. 567 (2014).
See Calo, supra note 31, at 419-20; Tom C. W. Lin, FinancialWeapons of War, 100
MINN. L. REV. 1377 (2016).
36
' See J. D. Tygar, Adversarial Machine Learning, 15 IEEE INTEfRNET COMPUTING 4, 4
(2011).
137 See Kevin Eykholt et al., Robust Physical-World Attacks on Deep Learning Visual
Classification, arXiv 1707.08945, Apr. 2018, https://arxiv.org/abs/1707.08945.
13

13

366

Harvard Business Law Review

[Vol. 10

United States, and the kaleidoscope of regulators and regulations cover
nearly every activity that a financial institution could conceivably think of
doing. The Consumer Financial Protection Bureau has authority to protect
consumers from unfair or abusive practices in the financial markets, and it
has been proactive in monitoring new financial technologies.' 38 The Securities and Exchange Commission (SEC) is tasked with protecting investors
and promoting fair, efficient capital markets, and it has itself adopted machine learning methods to monitor markets."' The Financial Stability Oversight Council monitors the stability of the nation's financial system and
responds to emerging risks, and it regularly reports on the effects of automated trading systems and financial technologies.'4 Artificial financial intelligence has yet to be shown to fall into gaps in this framework.
That said, this does not mean that current laws are perfectly attuned to
the risks of artificial intelligence. It also does not mean that new uses of
artificial intelligence will not render current financial regulation ineffective.
If anything, it is hoped that this Article serves to highlight to bankers,
policymakers, judges, and academics the importance of artificial financial
intelligence as a field, and to highlight the need for thoughtful discussions
both of how it is being used and how it should be used.
In particular, regulators should keep in mind three broad goals in adopting policies for addressing artificial financial intelligence: ensuring fairness,
facilitating efficiency, and promoting stability. These goals are not particularly novel. They are the hallmarks of financial regulation, and they form the
backbone of most statutory frameworks in the industry.141 They do, however,
have different valences when applied to the world of artificial financial intelligence. This Part will discuss the ways in which financial regulators should
use their existing authority to ensure that artificial financial intelligence does
not undermine the free and fair functioning of financial markets.
A.

Artificial Fairness

First, regulators should review artificial financial intelligence to ensure
that it is being implemented in a way that treats consumers and investors
fairly. Fairnesshere is meant to be an inclusive term. Fair markets are ones
that are free of discrimination,1 42 abusive practices,1 43 and fraud.' These are
broad and open-ended concepts, but they are essential underpinnings of our
See Van Loo, supra note 11, at 532-33.
See Scott W. Bauguess, Acting Dir. and Acting Chief Economist, DERA, Champagne
Keynote Address at OpRisk North America 2017: The Role of Big Data, Machine Learning,
and AI in Assessing Risks: a Regulatory Perspective (June 21, 2017), https://www.sec.gov/
news/speechlbauguess-big-data-ai.
140 See Financial Stability Oversight Council, 2018 ANNUAL REPORT (2018), https://
home.treasury.gov/system/files/261/FSOC2018AnnualReport.pdf.
"' See William Magnuson, Financial Regulation in the Bitcoin Era, 23 STAN. J. L. Bus.
& FIN. 159 (2018).
142 See Citron & Pasquale, supra note 60, at 16-18.
138
1'

2020]

Artificial FinancialIntelligence

367

financial system. More importantly, they have been given tangible and welldefined meaning by regulators tasked with overseeing the industry.
In terms of discrimination, regulating artificial financial intelligence
will require measures to prevent algorithms from discriminating against particular disfavored or minority groups. There is already ample legislative authority to act on this front. The Equal Credit Opportunity Act prohibits
lenders from discriminating against potential borrowers on the basis of race,
color, religion, national origin, sex, marital status, or age. 14 The Fair Housing Act prohibits banks from considering similar characteristics when making mortgage decisions.1 46 Regulation B prohibits discrimination in credit
scoring systems. 147 The key here will be identifying when discrimination is
occurring on prohibited grounds. Given the complex and, in many cases,
inscrutable operations of artificial intelligence algorithms, regulators will
need to develop methods for auditing financial algorithms and identifying
root causes.'4 After all, there are many other variables that are correlated
with race, sex, religion, and age, and artificial intelligence algorithms might
well identify those variables and use them in impermissible ways, even if
they are prevented from directly using the prohibited characteristics in
decisions.
Promoting fairness in artificial financial intelligence will also require
greater efforts to ensure that financial institutions properly disclose the nature of artificial intelligence risks to investors. These disclosures must be
both accurate and understandable. Again, there is ample legislative authority
for regulators to act here. The Securities Exchange Act of 1934 prohibits
manipulation, deception and fraud in connection with the sale of securities.1 49 Rule lOb-5 prohibits individuals from making untrue statements of
material fact or omitting material facts necessary to make statements not
misleading in connection with the sale of securities.1 50 The Investment Company Act requires financial institutions to disclose ample information about
investment policies.'"' The Investment Advisers Act requires financial advisors to give suitable investment advice to consumers, taking into account the
client's financial situation, investment experience, and investment objectives.'52 Regulators should use this broad legislative authority to set forth
143

See Stephen Choi, Regulating Investors Not Issuers: A Market-Based Proposal, 88

CAL. L. REV. 279 (2000); Jerry W. Markham, Protecting the Institutional Investor - Jungle
Predatoror Shorn Lamb?, 12 YALE J. REG. 345 (1995).

'" See John C. Coffee, Jr. & Hillary A. Sale, Redesigning the SEC: Does the Treasury
Have a Better Idea?, 95 VA. L. REV. 707, 761-62 (2009).
145 15 U.S.C. § 1691 (2012).
'"42 U.S.C. § 3605 (2012).
14' Equal Credit Opportunity Act (Regulation B), 12 C.F.R. § 202.5 (2011).
14 See Citron & Pasquale, supra note 60, at 28.
149 Securities Exchange Act of 1934 §10(b), 15 U.S.C. §78j(b).
15o 17 C.F.R. § 240.10b-5(b) (2008).
"' See Securities and Exchange Commission, Registration Form Used by Open-End Management Investment Companies, 63 FED. REG. 13,916, 13,917 (Mar. 23, 1998).
152 See Rules and Regulations, Investment Advisers Act of 1940 17 C.F.R. § 275 (2012).

368

Harvard Business Law Review

[Vol. 10

clear guidelines on what sorts of artificial financial intelligence products are
appropriate for investors, how those products can be marketed, and what
disclosures must be made.
Regulators will also need to conduct regular reviews of artificial financial intelligence to monitor for potentially harmful effects on consumers,
such as excessive interest rates or improper financial advice.'53 Currently,
too little information is available about how financial institutions are using
artificial intelligence, and how artificial intelligence is affecting financial
markets. In 2019, Senator Elizabeth Warren wrote an open letter to financial
regulators asking them how they were responding to algorithmic finance,
including what they were doing to combat discrimination, how they were
overseeing fair lending laws, and whether they are conducting analyses of
the impact of algorithms on borrowers.1 5 4 These are basic questions which
should be easily answerable. The fact that they are not suggests that regulators either are not acting to the full extent of their statutory authority, or that
they are not being forthcoming about their priorities and procedures.
Ensuring the fairness of artificial financial intelligence will thus require
a searching analysis of the machine learning process, from algorithm design,
to data gathering, to implementation procedures. Because artificial intelligence is by its very nature difficult to understand and explain, the burden on
regulators will be high."' It may require regulators to hire experts on machine learning, or deploy machine learning themselves. 5 6 But the benefits
are also high. If private and public sector actors can do it right, they could
greatly improve the fairness of financial markets.
B.

Artificial Efficiency

Second, financial regulators must be mindful of artificial intelligence's
effects on the efficiency of financial markets. If artificial financial intelligence leads to the mispricing of stocks, or asset bubbles, or a reduction in
competition, or higher fees, then regulators must be ready to respond. Again,
there is ample authority for them to do so. Indeed, promoting efficient markets is at the core of financial regulators' mission. The SEC's stated mission
is to "maintain fair, orderly and efficient markets." 5 7 The Commodity Futures Trading Commission's ("CFTC") mission is to "foster open, transpar-

153 See Robert Bartlett et al., Consumer-Lending Discrimination in the Fintech Era
(NBER, Working Paper No. 25942, 2019); Rory Van Loo, The Missing Regulatory State: Monitoring Businesses in an Age of Surveillance, 72 VAND. L. REv. 1563, 1622 (2019).
154 Letter from Senator Elizabeth Warren to Federal Reserve, Federal Deposit Insurance
Corporation, Office of the Comptroller of the Currency, and Consumer Financial Protection
Bureau (June 10, 2019), https://www.warren.senate.gov/imo/mediadoc/2019.6.10%20Letter
%20to%20Regulators%20on%20Fintech%20FINAL.pdf.
"' But not insurmountably so. See Kroll et al., supra note 88.
156 See Bauguess, supra note 139.
"' See 15 U.S.C. § 78c(f).

2020]

Artificial FinancialIntelligence

369

ent, competitive and financially sound markets.""' The Financial Industry
Regulatory Authority's ("FINRA") mission is to "promote market
integrity."'
In many ways, artificial intelligence might be expected to improve efficiency in financial markets. After all, machine learning algorithms are being
deployed for the very purpose of improving the speed and accuracy of fraud
detection, risk management, and investment decisions.'6 The great advantage of machine learning strategies is that they can find unnoticed, but material, patterns within data and then make predictions based on those
patterns.' Financial institutions have strong incentives to make sure that
their algorithms work accurately and reliably.
But the mere fact that artificial intelligence algorithms are intended to
improve efficiency, or even that financial institutions have incentives to ensure that they do so, does not mean that these algorithms will achieve their
intentions, or that in achieving them, they will not create unexpected consequences elsewhere. History is littered with examples of algorithms acting
unexpectedly, or causing unexpected harm.1 62 Regulators must therefore be
attuned to the mechanisms by which artificial intelligence algorithms might
create inefficiency in financial markets.
Although a thorough assessment of the causes of inefficiency within
artificial financial intelligence would require a close analysis of the source
code at issue, a few plausible risks are readily apparent. The first has to do
with macro trends in the economy. As discussed earlier, artificial financial
intelligence relies for its efficacy on data sets that, necessarily, involve historical information.163 But if the historical trend that the information reflects
is itself undesirable or unsustainable, then artificial intelligence algorithms
based on it might reinforce or strengthen the trend.Y" They might, for example, pile into an industry that had experienced rapid growth in recent months,
or rapidly exit an industry that had experienced a slump. Indeed, so-called
65
momentum trading is a popular strategy among quantitative funds. Again,
this might be a good thing if it leads to assets more quickly reflecting their
objective value. But it might equally be the result of a poorly designed
algorithm.
" See COMMODITY FUTURES TRADING COIMISSION, FY 2018 AGENCY FINANCIAL REPORT 5 (Nov. 2018), https://www.cftc.gov/system/files/2019/04/24/2018afr.pdf.
'" Our Mission, FINRA, https://www.finra.org/about/our-mission. (last visited May 14,
2020).
* See supra Part LB.
Lehr & Ohm, supra note 33, at 684-88.
161
62 See

'

See VIRGINIA EUBANKS, AUTOMATING INEQUALITY: How HIGH-TECH TOOLS PROFILE,

POLICE, AND PUNISH THE POOR (2018).

See supra Part I.B.
See Robin Wigglesworth, Volatility: How "Algos" Changed the Rhythm of the Market,
FINANCIAL TIMEs, Jan. 8, 2019, https://www.ft.com/content/fdclc064-1142-11e9-a581-4ff784
04524e.
6' See Matt Prewitt, High-Frequency Trading: Should Regulators Do More, 19 MICH.
ThLECOMM. & TECH. L. REv. 131, 135 (2012).
63

16

HarvardBusiness Law Review

370

[Vol. 10

Another form of inefficiency might arise not so much from unintended
actions as intentional cooperation. Artificial intelligence algorithms increase
the risk that firms will collude to raise prices, even in the absence of a formal agreement to do so.' 66 This presents a serious antitrust risk-indeed, the
Federal Trade Commission is so worried about this problem that it has held
hearings on the topic.' 7 Imagine, for example, that an artificial intelligence
algorithm deployed by banks to set mortgage rates learns that every time it
lowers its rate, its competitor does as well, or, potentially worse, every time
it raises its rate, its competitor does as well. The interaction of these algorithms might well lead to higher prices for all. This is not merely hypothetical. As early as 1994, the Department of Justice (DOJ) found that airlines
used a joint computerized booking system to set collusive ticket prices.'68
More recently, the DOJ has charged Amazon sellers with using algorithmbased software to engage in unlawful price fixing.16 9
Other forms of inefficiency might be even more pernicious. Opportunistic market actors could potentially exploit knowledge of the workings of
other firms' artificial intelligence algorithms to trigger unexpected behavior. 7 0 As mentioned before, researchers have worried for some time about

the potential of adversarialAI to insert poisoned data into artificial intelligence systems in order to defeat their intended functioning.17' This type of
strategy has obvious applications in the world of finance. Hedge funds with
short positions on a stock might try to cause other firms' algorithms to trigger sell orders.'72 Fraudsters might intentionally change their patterns of behavior in order to avoid fraud detection algorithms."' Hackers might attempt
to create financial panic by spreading false information on social media platforms or news sites. 14 All of these sorts of behavior could introduce new and
dangerous forms of inefficiency into markets, and regulators must be ready

'

See Kuhn & Tadelis, Algorithmic Collusion (2017).

6See

FEDERAL TRADE COMMISSION, THE COMPETITION AND CONSUMER PROTECTION IS-

SUES OF ALGORITHMS, ARTIFICIAL INTELLIGENCE, AND PREDICTIVE ANALYTICS (Federal Trade

Commission 2018), https://www.ftc.gov/news-events/audio-video/audio/algorithmic-collusion.
'" See Antonio Capobianco, Algorithms and Collusion, ORGANISATION FOR ECONOMIC
CO-OPERATION AND DEVELOPMENT 5 (2017).

"' See Azriel Ezrachi & Maurice E. Stucke, Artificial Intelligence & Collusion: When
Computers Inhibit Competition, 2017 U. ILL. L. REV. 1775, 1777 (2017).
"o See, e.g., Adam Janofsky, AI Could Make Cyberattacks More Dangerous, Harder to
Detect, WALL ST. J., Nov. 13, 2019, https://www.wsj.com/articles/ai-could-make-cyberattacksmore-dangerous-harder-to-detect-1542128667.
17 See supra Part I.C.
' For empirical evidence of hedge fund manipulation of stock prices, see Itzhak BenDavid et al., Do Hedge Funds Manipulate Stock Prices?, 68 J. FIN. 2383 (2013).
"' See Mary Frances Zeager et al., Adversarial Learning in Credit Card FraudDetection,
2017 Sys. AND INFo. ENGINEERING DESIGN SYMP. 112, https://ieeexplore.iee.org/abstract/document/7937699.
74 See Michelle Cantos, Breaking the Bank: Weakness in Financial AI Applications,
FIREEYE (Mar. 13, 2019), https://www.fireeye.com/blog/threat-research/2019/03/breaking-thebank-weakness-in-financial-ai-applications.html.

2020]

Artificial FinancialIntelligence

371

to identify and respond to them before they mature into more general market
failures.
C.

Artificial Risk

Finally, regulators must monitor artificial intelligence's effects on systemic risk. Observers in a wide array of areas, from national security to labor
to elections, have discussed the ways in which artificial intelligence might
lead to major shifts in systemic stability. Financial regulators must be similarly wary of artificial financial intelligence's effects on the stability of markets. Doing so will turn primarily on identifying the pathways through which
artificial financial intelligence could create contagion among financial actors
and then acting to increase the resilience of participants and systems to market shocks.
Assessing the systemic risk of artificial financial intelligence requires a
careful understanding of the uses of artificial intelligence and the ways in
which it is changing the nature of industry risks."' We have already seen that
artificial financial intelligence might lead to more volatile markets by, for
example, leading to faster and larger transaction volumes.' 6 It might also
lead to more financial bubbles, or bigger ones, if it turns out that artificial
77
intelligence algorithms tend to pile into "fashionable" market trends.
Other risks turn on the artificial intelligence industry itself. For example, if
financial institutions rely heavily on third-party providers for artificial intelligence algorithms or the data used to drive those algorithms, then those
third-party providers might well become systemically important themselves,
despite their non-financial focus.1 7 8 Additionally, if artificial financial intelligence leads financial institutions to focus on new variables or sectors (such
as social media posts or browsing activity), then those new variables could
79
suddenly become the source of interconnectedness, and thus risk.1
It just so happens that debt, one of the centerpieces for artificial financial intelligence today, is also a crucial factor in assessing the degree of
systemic risk within the market.'I" Debt, after all, was at the heart of the last
financial crisis and is one of the primary foci of many systemic risk regulators today.' 8 ' If financial institutions turn increasingly to artificial intelliSee FINANCIAL STABILITY BOARD, ARTIFICIAL INTELLIGENCE AND MACHINE LEARNING
IN FINANCIAL SERVICES: MARKET DEVELOPMENTS AND FINANCIAL STABILITY IMPLICATIONS, at

29-31 (2017).
"6 See supra Part II.C.
" Id.
" See FINANCIAL STABILITY BOARD, supra note 175, at 29.
179 Cf id. at 31 (discussing how widespread adoption of algorithms may uncover interconnections among variables that were previously thought to be unrelated).
1so Hal S. Scott, The Reduction of Systemic Risk in the United States FinancialSystem, 33
HARV. J. L. & PUB. Poit'Y 671, 677 (2010). See generally Steven L. Schwarcz, Systemic Risk,
97 GEO. L. J. 193 (2008) (discussing the definition of systemic risk).
'"' See Prasad Krishnamurthy, Regulating Capital, 4 HARV. Bus. L. REv. 1, 1 (2014)
("Most observers agree that the excessive debt or leverage of systemically important financial

372

Harvard Business Law Review

[Vol. 10

gence to make decisions related to creditworthiness and loan activity, then
the risk profile of this sector will turn heavily on the inner workings of the
algorithms. As a result, systemic stability regulators will need to better understand the ways in which artificial financial intelligence affects debt markets and risk assessments.
Again, regulators have ample authority to act here. The Dodd-Frank Act
created an entire department, the Financial Stability Oversight Council, to
monitor systemic risk. 182 Its mission is three-fold: first, to identify risks to
the financial stability of the United States; second, to promote market discipline by eliminating expectations that the government will shield financial
institutions from losses in the event of failure; and third, to respond to
emerging threats to the stability of the financial system."' Artificial financial
intelligence seems to fit squarely within the third category-it is an emerging risk, not a current one, to systemic stability. But in the future, if it becomes more widely adopted, it might well shift to the first category.
Given the regulatory ambit of the Financial Stability Oversight Council,
and its broad-ranging mission to police systemic stability, statutory authority
is unlikely to be an issue in regulating artificial financial intelligence's systemic risk. Competence and capacity, however, will be. Even within the private sector, there is a scarcity of expertise in machine learning fields.1 84 The
public sector suffers even more from this lack of experts and resources.1'
And without experts capable of understanding the workings of artificial intelligence algorithms, government regulators will struggle to monitor systemic risk within the sector.1 8 6 One important step here would be to give
regulators the ability to stress-test artificial financial intelligence systems,
allowing them to provide synthetic inputs to a system and analyze the
properties of the outputs. But doing so will first require regulators to gain a
deep knowledge of machine learning and artificial intelligence systems,
something they do not currently possess.1 7
institutions (SIFIs) was a central reason why the housing crash of 2007-2009 led to a
recession.").
182 See Hilary J. Allen, Putting the "FinancialStability" in FinancialStability
Oversight
Council, 76 OH. ST. L. J. 1087, 1113-15 (2015).
183 Dodd-Frank Wall Street Reform and Consumer Protection Act, 12 U.S.C. § 5322
(2012).
" See Bernard Marr, The AI Skills Crisis and How to Close the Gap, FORBEs, June 25,
2018, https://www.forbes.com/sites/bernardmarr/2018/06/25/the-ai-skills-crisis-and-how-toclose-the-gap/#57fa30be31f3.
185 Cf Michael Kratsios, Why the US Needs a Strategy for Al, WIRED (Feb. 11, 2019)
https://www.wired.com/story/a-national-strategy-for-ai (discussing the need for increased government focus on artificial intelligence to maintain a national competitive edge).
186 See FINANCIAL STABILITY BOARD, supra note 175, at 34.
18 Indeed, the need for greater machine learning expertise within government has been a
mainstay of many recent proposals to improve governance of the AI industry. See Cary Coglianese & David Lehr, Regulating by Robot: Administrative Decision Making in the MachineLearning Era, 105 GEO. L. J. 1147, 1216 (2017); Cary Coglianese & David Lehr, Transparency and Algorithmic Governance, 71 ADMIN. L. REv. 1, 30 (2019); Andrew Tutt, An FDA
for Algorithms, 69 ADMIN. L. REV. 83, 114 (2017); Andrew D. Selbst & Solon Barocas, The
Intuitive Appeal of Explainable Machines, 87 FORDHAM L. REV. 1085, 1093 (2018).

2020]

Artificial FinancialIntelligence
D.

373

The Role of Self-Regulation

This Part has focused primarily on what regulators can do to minimize
the risks and harms of artificial financial intelligence, but there is also much
that financial institutions themselves can do. In fact, it is likely that selfregulation will be significantly more effective at cabining artificial intelligence's risks than regulatory enforcement actions could ever be.'" Regulators are, by their very nature, outsiders. They do not know the inner
workings of financial institutions nearly as well as insiders do, and they do
not have the levels of expertise in machine learning that are available to the
private sector. This means that, for artificial financial intelligence to fulfill
its goal of making intelligent decisions, we will need bankers, engineers, and
computer scientists to be cognizant of the broader social goals of finance.
We cannot simply rely on regulators to step in to prevent financial institutions from taking risky actions-financial institutions themselves will need
to be the primary gatekeepers of artificial intelligence.
What might self-regulation look like in artificial financial intelligence?
Some general principles are clear. Bankers will need to consider the fairness
of the artificial financial intelligence systems they develop, whether they
may harm the efficiency of markets, and whether they might jeopardize systemic stability. They will need to be mindful of the data they use, the outputs
they receive, the actions they take, and the monitoring systems they have in
place. They may well need to go further and cooperate with other actors in
the industry to ensure that their systems do not interact in dangerous ways.
But self-regulation must be more than just platitudes about corporate
social responsibility.189 It should also include efforts that look more like hard
law. An illustrative example is FINRA.'9 FINRA is a self-regulatory body
for brokerage firms formed in 2007.191 But despite being an industry-run
8s Google itself has recognized the need for guiding principles for artificial intelligence in
the private sector. In April 2018, Sergey Brin, Google's founder, wrote in a letter to shareholders that "[t]he new spring in artificial intelligence is the most significant development in
computing in my lifetime" but that "such powerful tools also bring with them new questions
and responsibilities," such as how they will affect employment, fairness, and safety. SERGEY
BRIN, 2017 FOUNDERS' LETTER (2018), https://abc.xyz/investor/founders-letters/2017/index
.html. In June 2018, Google CEO Sundar Pichai published an essay on Google's principles for
Al, setting forth the objectives that guide Google's artificial intelligence strategies. See Sundar
Pichai, Al at Google: Our Principles(June. 7, 2018), https:/Iblog.google/technology/ai/ai-principles/. In February 2019, the company went further, issuing a white paper arguing that governments need to take action to govern artificial intelligence. GOOGLE, PERSPECTVES ON
ISSUES tIN Al GOVERNANCE (2018), https://ai.google/static/documents/perspectives-on-issuesin-ai-governance.pdf. But see Benjamin P. Edwards, The Dark Side of Self-Regulation, 85 U.
CIN. L. Rev. 573 (2018).
189 See generally Jonathan R. Macey, Corporate Social Responsibility: A Law & Economics Perspective, 17 CHAPMAN L. REv. 331 (2014); John M. Conley & Cynthia A. Williams,
Engage, Embed, and Embellish: Theory Versus Practicein the Corporate Social Responsibility
Movement, 31 J. CORP. L. 1 (2005).
19 See generally Andrew F. Tuch, The Self-Regulation of Investment Bankers, 83 GEO.
WASH. L. REv. 101 (2014).
- Id. at 104.

HarvardBusiness Law Review

374

[Vol. 10

organization, it does much that has the look and feel of law. Broker-dealers
are obligated to join the body.'92 It writes standards of conduct that bind its
members.' 93 It has the power to discipline members for violating its rules,
including the ability to exclude brokers from practicing in the industry.1 94
This sort of organization-a self-regulatory organization with teeth-would
be helpful in establishing the rules of the road for artificial financial intelligence. Industry groups might, for example, develop a set of best practices on
cybersecurity, machine learning algorithms, and data set development. In the
end, these sorts of private sector endeavors are essential to ensure that artificial financial intelligence leads to fair, efficient, and stable outcomes.
Some of this work is already taking place within the technology industry.'9 5 Computer science engineers are increasingly incorporating social
goals into their machine learning guides, and they are actively looking for
ways to improve artificial intelligence algorithms' ability to take into account
other external values. 9 6 One prominent example is TensorFlow.'19 TensorFlow is a machine learning system originally developed by Google for internal use but which was eventually released to the public on an open-source
basis.19 8 Its training modules now include the following note:
As a modeler and developer, think about how this data is used and
the potential benefits and harm a model's predictions can cause. A
model like this could reinforce societal biases and disparities. Is a
feature relevant to the problem you want to solve or will it introduce bias?'"
It then includes a link to a longer discussion of "machine learning fairness,"
with a set of recommended readings. While the efficacy of these warnings in
a technical training setting might be questionable, it provides a good example of the ways in which computer scientists are trying to incorporate
broader goals into their algorithms. Financial institutions might similarly
work on introducing broader situational awareness into the engineers and
bankers that create artificial financial intelligence systems.
'"9Id. at 104-05.
9

1 1 Id.

at 105.

See Barbara Black, Punishing Bad Brokers: Self-Regulation and FINRA Sanctions, 8
BROOK. J. CORP. FIN. & CoM. L. 23 (2013).
"' See Gabriel Goh et al., Satisfying Real-World Goals With DatasetConstraints, 29 ADV.
NEURAL INF. PROC. Sys. 1 (2016); Lucas Dixon et al., Measuring and Mitigating Unintended
Bias in Text Classification,2018 Al, ETHICS, AND Soc'Y CONF. 67 (2018).
196 See, e.g., Sanders Kleinfeld, A New Course to Teach People About Fairnessin Machine
Learning, GOOGLE Al (Oct. 18, 2018), https://www.blog.google/technology/ai/new-courseteach-people-about-faimess-machine-teaming/.
"' For a discussion of the history of TensorFlow's creation and its eventual release to the
public, see Daniela Hernandez, Google'sArtificial-Intelligence Tool is Offered for Free, WALL
STr. J., May 12, 2016, https://www.wsj.com/articles/googles-open-source-parsey-mcparsefacehelps-machines-understand-english-1463088180.
19

19 Id.
199 TensorFlow Data Validation Tutorial, TENSORFLOw, https://www.tensorflow.org/tfx/

tutorials/datavalidation/chicago_taxi. (last visited May 14, 2020).

2020]

Artificial FinancialIntelligence
IV.

375

OBJECTIONS

This Article has argued that artificial financial intelligence poses a
threat to free and fair financial markets, not because of its capacity to exceed
human intelligence, but because of its capacity to exacerbate human error.
Because of its reliance on potentially inaccurate datasets, its biasing effect
on human decisionmakers, and its unpredictable interactions with other algorithms, artificial financial intelligence could lead to harmful changes in how
the financial industry works. Regulators, thus, must be vigilant in ensuring
that financial institutions maintain efficient, fair, and stable markets.
These arguments, however, are not uncontroversial. In fact, they run
counter to three powerful theoretical strands in artificial intelligence scholarship. I will refer to these strands as the mirror critique, the balance critique,
and the precautionary critique, but I might just as accurately have referred to
them as the cynical view, the optimist's view, and the pessimist's view. This
Part will briefly explain what these critiques are, identify how they differ
from the views asserted in this Article, and offer a few thoughts on why I
find them unpersuasive.
A.

The Mirror Critique

One standard response to artificial intelligence today is to claim that
there is nothing particularly new about it.200 Artificial intelligence may use
different algorithms from other programs, or have greater processing capacities, but at heart, it is simply another form of computer code. And like all
computer code, it has flaws. More specifically, the flaws of computer code
mirror those of its creators.2 01 If the programmer who writes the code is riskprone, unthoughtful, or irrational, then the algorithms will reflect that. We
should not be surprised, then, that machine leaming algorithms are biased,
overweight certain data, or respond unpredictably to changes. So, too, do
humans. 202
Proponents of the mirror critique might well agree with the premise of
this Article that artificial financial intelligence has flaws in relation to its
problematic reliance on faulty data. But they would disagree that these flaws
are unique or new, or that they require special, tailored responses from regu2" See Dan Patterson, Why AI is Nothing New, TECHREPUBLIC (Jan. 22, 2019), https://
www.techrepublic.com/article/why-ai-is-nothing-new/; Richard Socher, AI Isn't Dangerous,
But Human Bias Is, WoRLD EcoN. F. (Jan. 17, 2019), https://www.weforum.org/agenda/2019/
01/ai-isn-t-dangerous-but-human-bias-is/.
201 See Jayne Williamson-Lee, How Machines Inherit Their Creators' Biases, MEDIUM
(July 9, 2018), https://medium.com/coinmonks/ai-doesnt-have-to-be-conscious-to-be-harmful385dl43bd311; Jeff Cockrell, A.I. is Only Human, CHI. BooTH REv., Summer 2019, https://
review.chicagobooth.eduleconomics/2019/article/ai-only-human; Kate Crawford, Artificial Intelligence's White Guy Problem, N.Y. TIMEs (Jun. 25, 2016).
20 For a recent summary of the many ways in which humans exhibit behavioral biases as
a matter of psychology, see DANIEL KAHNEMAN, THINKING, FAST AND SLOW (2013).

Harvard Business Law Review

376

[Vol. 10

lators. If the flaws are precisely the ones that we worry about with human
decisionmakers, then there is nothing special about artificial intelligence,
and regulators should simply maintain their current strategies of monitoring
and sanctioning bad actions when they take place.
Before responding to this critique, it is perhaps worthwhile to note
some areas of agreement. This Article agrees that the "revolutionary" nature
of artificial intelligence is often exaggerated. 203 This Article agrees that artificial intelligence's flaws are related to the flaws in human psychology.2 04
This Article agrees that we should not expect artificial intelligence to eliminate human error.2 05 All of these are important observations about the nature
of artificial intelligence today, and they form part of this Article's core argument. At the same time, the mirror critique is misguided in several ways.
First, while it may be true on a trivial level that the failures of artificial
intelligence can be categorized in similar ways as the failures of human intelligence, there are also important ways in which the failures are different.
Most legal problems today, if described at a sufficiently high level of generality, can be shown to be the product of similar problems. The concept of
negligence in tort law bears striking similarities to the concept of fiduciary
duties in corporate law, but this does not mean that they are the same thing
or, more importantly, that we should treat tort law and corporate law the
same. Similarly, while the general categories of problem are the same between human decisionmakers and artificial intelligence algorithms, the nature and effects of these problems differ. Some of the differences are quite
simple: artificial intelligence algorithms can process information much more
quickly than humans can, and thus we might expect that artificial financial
intelligence might lead to errors propagating and spreading at higher rates.
Other differences relate to the nature of decisionmaking in these systems: as
described before, adversarial artificial intelligence demonstrates just how
susceptible these algorithms are to intentional manipulation, and in ways that
that most humans would spot.2 06 Certainly, there are also ways to fool
humans. But the methods are different. Thus, even if we do believe that
artificial intelligence's flaws are also human flaws, we must also recognize
that these flaws look very different when an algorithm is behind them.
A second way in which the mirror critique is unpersuasive is that there
are some risks from artificial financial intelligence that seem to exist in an
entirely new category. One of the risks that this Article has identified is the
interaction between human decisionmakers and artificial financial intelligence. In other words, one of the problems with financial institutions implementing machine learning strategies is that human decisionmakers may rely
too heavily on the recommendations these algorithms give. 207 Studies have
2034 See
20

supra Part H.B.

Id.

205

Id.

206 See supra Part II.C.
207

See supra Part II.B.

2020]

Artificial FinancialIntelligence

377

shown, for example, that people tend to believe that algorithms are superior
to humans in making financial decisions. 208 Thus, if artificial financial intelligence proliferates, it may create problems related to overweighting by
human bankers. This suggests that artificial intelligence will introduce a new
risk that simply does not exist in traditional finance.
Third, setting aside objections relating to the extent of the harm and the
nature of the harm, even if we concede that the categories of problem within
artificial financial intelligence and traditional finance are the same, this does
not necessarily lead to the conclusion that they should both be regulated in
similar ways. The mere fact that two phenomena share similar risks does not
suggest that they will respond to similar regulatory interventions. Even
within human communities, we know that groups can respond quite differently to government action. 209 The imposition of fines for undesirable behavior might cause some groups to reduce the incidence of that behavior but
cause other groups to increase the incidence of that behavior. 2 10 Similarly,
crafting tailored regulation to reduce the risks of artificial financial intelligence likely requires different approaches than those traditionally used. As
an initial matter, simply understanding the risks of particular artificial intelligence algorithms requires different kinds of knowledge-regulators need
expertise in computer science, neural networks, and data science, things that
they have not traditionally focused on. 2 1 1 Similarly, the interventions needed
to reduce the risks of artificial intelligence are likely quite different than
traditional interventions in finance. Simple disclosure obligations may not be
sufficient for machine learning algorithms-source code is notoriously complex and inscrutable. 212 Backtesting, dynamic testing, and zero knowledge
proofs (some of the proposals for analyzing machine learning algorithms)
are more promising, and they require different sorts of action from regulators. 2 13 All of this is to suggest that even if artificial financial intelligence
presents similar categories of risk, the regulatory actions necessary to reduce
these risks are new and require specialized knowledge.

See Packin, supra note 118.
See, e.g., Richard H. McAdams, The Origin, Development, and Regulation of Norms,
96 MICH. L. REV. 338 (1997); Janice Nadler, Expressive Law, Social Norms, and Social
Groups, 42 L. & Soc. Inquiry 60 (2017).
210 See, e.g., Uri Gneezy & Aldo Rustichini, A Fine is a Price, 29 J. LEGAL STUo. 1
(2000).
211 See supra Part III.C.
212 See Kroll et al., supra note 88, at 638 ("Perhaps the most obvious approach is to
disclose a system's source code, but this is at best a partial solution to the problem of accountability for automated decisions. The source code of computer systems is illegible to nonexperts.
In fact, even experts often struggle to understand what software code will do, as inspecting
source code is a very limited way of predicting how a computer program will behave.").
213 Id. at 668.
208
20

378

HarvardBusiness Law Review
B.

[Vol. 10

The PrecautionaryCritique

Proponents of the precautionary critique take a different approach. 2 14
They assert that the risks of artificial intelligence are not just new, they are
existential. They note the dramatic improvements in artificial intelligence's
capacities in just the last few years and argue that we should assume that
these improvements will continue into the future.215 As a result, artificial
intelligence could potentially, in the very near future, replace many, if not
most, human tasks and lead to dramatic increases in inequality.216 Thus,
these Al pessimists assert, we need wide-ranging and intrusive new regulations to assure that artificial intelligence does not cause systemic harm to our
economy and society. 217 Another related strand of this critique claims that,
even if artificial intelligence does not present a risk of surpassing the broad
cognitive capacities of humans currently, it may do so in the future. And if
that is the case, then regulators need to address these future harms now, not
wait for them to materialize. In other words, they should adopt something
akin to a precautionary principle: regulators should take anticipatory action
to protect people from plausible harm even before firm empirical evidence
of that harm materializes. 21 1

214 Notable proponents of the precautionary critique include Elon Musk (who
has claimed
that artificial intelligence is "potentially more dangerous than nukes"), Stephen Hawking (who
has said that "the development of full artificial intelligence could spell the end of the human
race."), and, sometimes, Bill Gates (who has said that "a few decades after [machines are
doing most jobs] the intelligence is strong enough to be a concern . . . and [I] don't understand
why some people are not concerned."). See Cade Metz, Mark Zuckerberg, Elon Musk and the
Feud Over Killer Robots, N.Y. TuIEs, June 9, 2018, https://www.nytimes.com/2018/06/09/
technology/elon-musk-mark-zuckerberg-artificial-intelligence.html; Rory Cellan-Jones, Stephen Hawking Warns Artificial Intelligence Could End Mankind, BBC, Dec. 2, 2014, https://
www.bbc.com/news/technology-30290540; Pete Holley, Bill Gates on Dangers of Artificial
Intelligence: "I Don't Understand Why Some People are Not Concerned", WASH. PosT, Jan.
29, 2015, https://www.washingtonpost.com/news/the-switch/wp/2015/01/28/bill-gates-on-dangers-of-artificial-intelligence-dont-understand-why-some-people-are-not-concerned/?noredirect=on&utm term= .f4a50453a03e.
215 See Estlund, supra note 87, at 258 ("[T]he prospects for job destruction are eye-opening. Robotic and digital production of goods and services, coupled with advances in Al and
machine learning, is poised to take over both routine or repetitive tasks and some more advanced
tasks.").
2 16
See ERIK BRYNJOLFSSON & ANDREW MCAFEE, RACE AGAINST THE MACHINE: How

THE DIGITIAL REVOLUTION Is ACCELERATING INNOVATION, DRIVING PRODUCTIVITY, AND IRRE-

VERSIBLY TRANSFORMING EMPLOYMENT AND THE ECONOMY (2012); Laura Tyson & Michael

Spence, Exploring the Effects of Technology on Income and Wealth Inequality, in AFTER
PIKETTy: TiE AGENDA FOR ECONOMICS AND INEQUALITY 170 (Heather Boushey, J. Bradford
DeLong & Marshall Steinbaum ed. 2017); FORD, supra note 84.
217 See Estlund, supra note 87, at 254 ("This Article charts a path for reforming that body
of law [that is, employment law] in the face of justified anxiety and uncertainty about the
future impact of automation on jobs.").
218 See Stephen G. Wood et al., Wither the PrecautionaryPrinciple?An American Assessment from an Administrative Law Perspective, 54 AM. J. Comp. L. 581, 581 (2006); Lesley
Wexler, Limiting the PrecautionaryPrinciple: Weapons Regulation in the Face of Scientific
Uncertainty, 39 U.C. DAVis L. REv. 459, 463 (2006).

2020]

Artificial FinancialIntelligence

379

The precautionary critique would take issue with the claim in this Article that artificial intelligence's limited capacity today suggests that its primary harms will stem from human error and its capacity to exacerbate this
error. It would assert that this is a short-sighted view that fails to take into
account foreseeable and near-term improvements in the technology. As a
result, artificial financial intelligence presents a much greater risk to our financial system, and one that requires significant new regulatory initiatives
and structures.
This Article concedes that if we believe that artificial intelligence will
soon, or even eventually, lead to superhuman computers that will eliminate
vast swathes of the work force or, as Elon Musk fears, swarms of killer
robots, then we would need much more far-reaching reforms, both in law
and society. Thus, to a certain extent, the arguments in this Article presuppose that the most extreme predictions about artificial intelligence are implausible. At the same time, if artificial intelligence does evolve to this
super-state, our worries will be about much more than financial regulation.
But more importantly, the precautionary critique is itself susceptible to
important criticism. If we adopted a strong precautionary approach, we
might well end up stifling innovation by ratcheting up the costs of technological adoption. 219 It might also be a waste of regulatory resources-if we
end up regulating something that never comes about, or that evolves in such
a way that the regulation is ill-adapted to it, then we will have displaced
useful, near-term regulatory action with useless, long-term action. 220 A waitand-see approach can be a prudent regulatory strategy if it allows industry
participants, government bodies, and the public to gather more information
about the costs and benefits of the technology, or let the technology mature
into a more stable equilibrium. 221 In the case of artificial financial. intelligence, where financial institutions are still experimenting and learning, regu-

219 See, e.g., Michael A. Heller & Rebecca S. Eisenberg, Can Patents Deter Innovation?
The Anticommons in Biomedical Research, 280 Sci. 698, 698 (1998); Mark A. Lemley & A.
Douglas Melamed, Missing the Forestfor the Trolls, 113 COLUM. L. REV. 2117, 2125 (2013);
Anthony Falzone, Regulation and Technology, 36 HARV. J. L. & PUB. PoL'Y 105 (2013).
220 See Sharon B. Jacobs, The Administrative State's Passive Virtues, 66 ADMIN. L. REV.
565 (2014); John P. Dwyer, Overregulation, 15 ECOLOGY L. Q. 719 (1988).
221 See Stephen M. Bainbridge, Dodd-Frank: Quack Federal Corporate Governance
Round II, 95 MINN. L. REV. 1779 (2011); Paul G. Mahoney, The PerniciousArt of Securities
Regulation, 66 U. Cm. L. REV. 1373 (1999); Larry Ribstein, Bubble Laws, 40 HOUSTON L.
REV. 77 (2003); Roberta Romano, Regulating in the Dark, in REGULATORY BREAKDOWN: THE
CRISIS OF CONFIDENCE IN U.S. REGULATION 86 (Cary Coglianese ed., 2012). But see Jonathan
Masur & Eric A. Posner, UnquantifiedBenefits and the Problem of Regulation Under Uncertainty, 102 CORNELL L. REV. 87 (2016); Lisa Schultz Bressman, Judicial Review of Agency
Inaction: An Arbitrariness Approach, 79 N.Y.U. L. REV. 1657 (2004); Michael D.
Sant'Ambrogio, Agency Delays: How a Principal-Agent Approach Can Inform Judicial and
Executive Branch Review of Agency Foot-Dragging, 79 GEO. WASH. L. REV. 1381 (2011);
Glen Staszewski, The Federal Inaction Commission, 59 EMORY L. J. 359 (2009).

380

HarvardBusiness Law Review

[Vol. 10

lators would be ill-advised to launch a wholesale restructuring of their
regulation around a still-inchoate algorithm.222
C.

The Balance Critique

If Al cynics say that artificial intelligence is nothing new, and Al pessimists say that artificial intelligence is a dire threat, Al optimists have a different view.223 While acknowledging that artificial intelligence has risks,
they assert that these risks are more than outweighed by the benefits. 224
Moreover, there are also risks to not adopting artificial intelligence. On balance, the benefits of incorporating artificial intelligence into business greatly
exceed the costs. Thus, to proponents of this balance critique, rather than
simply defaulting to an anti-technology position (like the Al pessimists do),
we need to carefully balance the pros and cons, and we will find that the
pros of encouraging the growth of artificial intelligence exceed the cons of
doing so.
Again, this Article largely agrees with the substance of the balance critique. When regulating a technology, we need to take into account both the
costs of the technology and the benefits of it.225 As these critics rightfully
point out, there are harms from not adopting artificial intelligence strategies
if they can play a useful role in reducing fraud or increasing the efficiency of
capital markets. 226 Companies are likely better judges of this than govern222 See Steven L. Schwarcz, Regulating Financial Change: A Functional Approach,
100
MrNN. L. REv. 1441, 1444 (2016) ("In thinking about regulating a dynamically changing financial system, it may be more effective-or at least instructive-to focus on the system's
underlying, and thus less time-dependent, economic functions than to tie regulation to any
specific financial architecture.").
223 See, e.g., Coglianese & Lehr, supra note 187, at 1223 ("For administrative agencies,
what will distinguish the machine-learning era is not a substitution of human judgment with
some foreign and unfamiliar methodology, but rather an evolution of human judgment to incorporate fundamentally similar-albeit more accurate and often practically more usefulprocesses of decision making made possible by advances in statistical knowledge, data storage, and digital computing. The analysis offered here provides an antidote to visceral reactions
against the use of artificial intelligence in the public sector, reactions we hope will give way to
a measured optimism capable of guiding and improving the future of the administrative
state."); Steve Lohr Another Use for A.I: Finding Millions of Unregistered Voters, N.Y.
TiMsS, Nov. 5, 2018, https://www.nytimes.com/2018/11/05/technology/unregistered-voterrolls.html; Angus Loten, AI Tool Helps Companies Detect Expense Account Fraud, WALL ST.
J., Feb. 26, 2019, https://www.wsj.com/articles/ai-tool-helps-companies-detect-expense-account-fraud-11551175200.
224 See Vikram Mahidhar & Thomas H. Davenport, Why Companies That Wait to Adopt AI
May Never Catch Up, HARV. Bus. REv., Dec. 6, 2018, https://hbr.org/2018/12/why-companies-that-wait-to-adopt-ai-may-never-catch-up.
225

See COMM. ON CAPITAL MKTs. REGULATION, A BALANCED APPROACH TO COST-BENE-

ANALYSis REFORM (2013); Hester Peirce, Economic Analysis by FederalFinancialRegulators, 9 J. L. EcoN. & POL Y 569 (2013). For a discussion of the difficulties of conducting such
cost-benefit analysis in financial regulation, see John C. Coates IV, Cost-Benefit Analysis of
FinancialRegulation: Case Studies and Implications, 124 YALE L. J. 882 (2015).
26 See, e.g., Shani R. Else & Francis G.X. Pileggi, Corporate Directors Must Consider
Impact of Artificial Intelligencefor Effective Corporate Governance, Bus. L. TODAY, Feb. 12,
FIT

2020]

Artificial FinancialIntelligence

381

ment officials, given their direct involvement and their indirect financial incentives. Too often, the default view of regulators is that companies should
bear the burden of proof to show that the benefits of their innovations exceed
the costs. 227 This may be good politics, but it is bad policy.
At the same time, it is the duty of financial regulators to uphold free,
fair, and stable markets. This Article has identified several ways in which
artificial financial intelligence might undermine these pillars of financial
regulation, and thus regulators should be vigilant about the risks. More importantly, by identifying the risks and discussing ways to reduce them, we
can shift the balance of benefits and harms, making artificial financial intelligence more socially useful and less socially damaging. Presumably this is a
goal that all groups can support.

So we have rejected the mirror critique, the precautionary critique, and
the balance critique, and, in the process, rejected the cynics, the pessimists,
and the optimists. Where does that leave us? I would say with moderation.
The most radical opinions often receive the most attention today, but this
does not mean that they are the most accurate. This Article, at its core, asserts that artificial financial intelligence is important, it is new, and it is
changing finance, but also that we are well-prepared to deal with it. We have
a comprehensive body of laws and regulations attuned to dealing with new
risks to the financial system, and our regulators are properly equipped to
respond to the challenges of artificial intelligence. To be sure, they may need
to adopt new regulatory tools or gain expertise in new areas of finance. But
228
this is always the case with new financial technologies.
CONCLUSION

In the final chapter of Tell the Machine Goodnight, in a chance encounter on a train, Pearl runs into the man she had met earlier in the book, whom
the Apricity machine had told to amputate a finger. Pearl sees that the man is
indeed missing his index finger. She approaches and asks him how he is
doing. He replies, "Oh, good. You know. Getting along. As one does." The
reader is left in a state of uncertainty. Was the computer right? Was he happier as a result of taking its recommendation? Would he have been better off
keeping his finger intact? The reader simply does not know. But the story
gets at a deeper truth about artificial intelligence: thinking about artificial
intelligence is an exercise in imagination. It requires understanding how the
technology works and how it is currently being used, but it also requires us
2019, https://businesslawtoday.org/2019/02/corporate-directors-must-consider-impact-artificial-intelligence-effective-corporate-governance/.
227 See COMM. ON CAPITAL MKTS. REGULATION, supra note 225, at 9.
228 See William Magnuson, Regulating Fintech, 71 VAND. L. REv. 1167 (2018).

382

Harvard Business Law Review

[Vol. 10

to imagine how it might work in the future, and to what new uses it might be
put. While artificial intelligence is coming for finance, and, to a certain extent, has already arrived, its applications, for now, are limited and experimental. Its risks derive primarily from the failure of human, not machine,
intelligence. We would do best to keep that in mind.

Data machine: the insurers using AI to reshape the industry | Financial Times

1 of 6

https://www.ft.com/content/d3bd46cb-75d4-40ff-a0cd-6d7f33d58d7f

Insurance

Data machine: the insurers using AI to reshape the industry
Groups are building detailed customer profiles to inform pricing and try to influence behaviour

AI allows insurers such as Ping An to produce highly individualised profiles of customer risk that evolve in real time © FT montage;
Alamy, Dreamstime

Ian Smith in London AUGUST 16 2021

Take your medicine when the app tells you, do your exercise and eat well. As long as
you “show good compliance” and share the data, you will reduce your health risks —
and your insurance premium.
This is how Xie Guotong — the chief healthcare scientist at Chinese insurer Ping An —
describes its combined insurance and digital “disease management” service for people
with type-2 diabetes. Powered by artificial intelligence, it is just one example of a big
shift going on in the industry.
AI, which sifts data and aims to learn like humans, is allowing insurers to produce
highly individualised profiles of customer risk that evolve in real time. In parts of the
market, it is being used to refine or replace the traditional model of an annual
premium, creating contracts that are informed by factors including customer
behaviour.
In some cases, insurers are using it to decide whether they want to take a customer on
in the first place.
22.08.21, 17:16

Data machine: the insurers using AI to reshape the industry | Financial Times

2 of 6

https://www.ft.com/content/d3bd46cb-75d4-40ff-a0cd-6d7f33d58d7f

New York-listed car insurer Root offers potential customers a test drive, tracks them
using an app, and then chooses whether it wants to insure them. Driving behaviour is
also the number one factor in the price of policy, it said.
UK start-up Zego, which specialises in vehicle insurance for gig-economy workers
such as Uber drivers, offers a product that monitors customers after they have bought
cover and promises a lower renewal price for safer drivers.
The theory with such policies is that customers end up paying a fairer price for their
individual risk, and insurers are better able to predict losses. Some insurers say it also
gives them more opportunity to influence behaviour and even prevent claims from
happening.

New York-listed car insurer Root oﬀers potential customers a test drive, tracks them using an app, and then chooses whether it
wants to insure them © Root insurance

“Insurance is strongly moving from payment after claim to prevention,” said Cristiano
Borean, chief financial officer at Generali, Italy’s largest insurer.
For a decade, Generali has offered pay-how-you-drive policies that reward safer
drivers with lower premiums. In its home market, it also offers AI-enabled driver
feedback in an app, and plans to pilot this in other countries. “Everything which can
allow you to interact and reduce your risk, is in our interest as an insurer.”

22.08.21, 17:16

Data machine: the insurers using AI to reshape the industry | Financial Times

3 of 6

https://www.ft.com/content/d3bd46cb-75d4-40ff-a0cd-6d7f33d58d7f

But the rise of AI-powered insurance worries researchers that this new way of doing
things creates unfairness and could even undermine the risk-pooling model that is
key to the industry, making it impossible for some people to find cover.
“Yes, you won’t pay for the claims of your accident-prone neighbour, but then again,
no one else will then pay for your claims — just you,” said Duncan Minty, an
independent consultant on ethics in the sector. There is a danger, he added, of “social
sorting”, where groups of people perceived as riskier cannot buy insurance.

Behaviour-driven cover
Ping An’s type-2 diabetes insurance product is powered by AskBob, its AI-powered
“clinical decision support system” used by doctors across China.
For diabetes sufferers, the AI is trained on data showing incidence of complications
such as strokes. It then analyses the individual customer’s health via an app to
develop a care plan, which is reviewed and tweaked by a doctor together with the
patient.
The AI monitors the patient — through an app and a blood-glucose monitor — finetuning its predictions of the likelihood of complications as it goes. Patients that buy
the linked insurance are promised a lower premium at renewal if they follow the plan.

22.08.21, 17:16

Data machine: the insurers using AI to reshape the industry | Financial Times

4 of 6

https://www.ft.com/content/d3bd46cb-75d4-40ff-a0cd-6d7f33d58d7f

But AI experts worry about the consequences of using health data to calculate
insurance premiums.
Such an approach “entrenches a view of health not as human wellbeing and
flourishing, but as something that is target-based and cost-driven,” said Mavis
Machirori, senior researcher at the Ada Lovelace Institute.
It might favour those who are digitally connected and live near open spaces, while
“the lack of clear rules around what counts as health data leaves the door open to
misuse”, she added.
Zego’s “intelligent cover”, as the company calls it, offers a discount to drivers that sign
up for monitoring. Its pricing model uses a mix of inputs, including information such
as age, together with machine-learning models that analyse real-time data such as fast
braking and cornering. Safer driving should push down the cost of renewal, Zego said.
It also plans to provide feedback to customers through its app to help them manage
their risk.
“If you’re on a monthly renewing policy with us, we’d be looking at tracking that over
time with you and showing you what you can do to bring down your monthly cost,”
said Vicky Wills, the start-up’s chief technology officer.
She added: “I think this is a trend we are actually going to see more and more —
insurance becoming more of a proactive risk management tool rather than the safety
net that it has been before.”

Monitoring bias
Campaigners warn, however, that data can be taken out of context — there are often
good reasons to brake heavily. And some fear longer-term consequences from
collecting so much data.
“Will your insurer use that Instagram picture of a powerful car you’re about to post as
a sign that you’re a risky driver? They might,” said Nicolas Kayser-Bril, a reporter at
AlgorithmWatch, a non-profit group that researches “automated decision-making”.

22.08.21, 17:16

Data machine: the insurers using AI to reshape the industry | Financial Times

5 of 6

https://www.ft.com/content/d3bd46cb-75d4-40ff-a0cd-6d7f33d58d7f

Regulators are clearly worried about the potential for AI systems to embed
discrimination. A working paper in May from Eiopa, the top EU insurance regulator,
said companies should “make reasonable efforts to monitor and mitigate biases from
data and AI systems”.
Problems can creep in, experts say, when AI replicates a human decision-making
process that is itself biased, or uses unrepresentative data.

Shameek Kundu, head of financial services at TruEra, a firm that analyses AI models,
proposes four checks for insurers: that data is being interpreted correctly and in
context; that the model works well for different segments of the population; that
permission is sought from a customer in transparent communication; and that
customers have a recourse if they think they have been mistreated.

Detecting fraud
Insurers such as Root are also using AI to identify false claims, for example to try to
spot discrepancies between when and where an accident took place, and information
contained in the claim.

22.08.21, 17:16

Data machine: the insurers using AI to reshape the industry | Financial Times

6 of 6

https://www.ft.com/content/d3bd46cb-75d4-40ff-a0cd-6d7f33d58d7f

Third-party providers such as France’s Shift Technology, meanwhile, offer insurers a
service that can identify if the same photo, for example of a damaged car, has been
used in multiple claims.
US-listed Lemonade is also a big user of AI. Insurance is “a business of using past
data to predict future events,” said the company’s co-founder, Daniel Schreiber. “The
more predictive data an insurer has . . . the better.” It uses AI to speed up the process
and cut the cost of claims processing.
But it caused a social-media furore earlier this year when it tweeted about how its AI
scours claims videos for indications of fraud, picking up on “non-verbal cues”.
Lemonade later clarified that it used facial recognition software to try to spot if the
same person made multiple claims under different identities. It added that it did not
let AI automatically reject claims and that it had never used “phrenology” or
“physiognomy” — assessing someone’s character based on their facial features or
expression.
But the episode encapsulated worries about the industry building up an ever more
detailed picture of its customers.
“People often ask how ethical a firm’s AI is,” Minty said. “What they should be asking
about is how far ethics is taken into account by the people who design the AI, feed it
data and put it to use making decisions.”

Copyright The Financial Times Limited 2021. All rights reserved.

22.08.21, 17:16

Digital disruption
in insurance:
Cutting through
the noise

Contents
Preface								

Facing digital reality
		
A strategy for a digital age

1

6
18

The age of innovation

27

Capturing value from the core

38

Partnerships, scale, and speed: The hallmarks of a successful IoT strategy

50

Modernizing IT for a strategic role

55

The promise of blockchain

66

The advance of analytics

72

The value of robotic process automation: An interview with Professor Leslie Willcocks

81

Building momentum for cultural change

86

A roadmap for a digital transformation

95

Digital Quotient: Where does your company stand?

106

Preface
There is a lot of noise out there. Insurance CEOs constantly hear about digital marketing,
digital distribution, digital IT architecture, and digital attackers, as well as digital technologies
such as telematics, automation, and machine learning, to name but a few hot topics. What is
harder for them to discern is the bigger picture. What does success look like for an insurer in a
digital world, and how is it achieved?
This compendium—“Digital disruption in insurance: Cutting through the noise”—helps
paint that picture by drawing on McKinsey’s experience in the industry and that of some 30
executives whom we interviewed. Importantly, we spoke not just to incumbents but those who
are helping to force change in the industry, including for example giant technology companies,
companies that promote the use of data-collecting sensors in our homes and cars, and
newcomers to insurance. All shared their insights on what is happening in insurance and why,
and where success lies.
The compendium’s underlying premise is stark—but some executives are beginning to face up
to it. They know that staying competitive in a digital word will require far more than the addition
of a direct sales channel or a few automated processes. Even the term “digital transformation”
can underplay the response required, suggesting as it does that the change needed is purely
technological. What is actually required is a fundamental rethink of the corporation, for which
digital technology is but the catalyst. It forces companies to rethink the sources of revenue and
efficiency. It forces them to rethink the organizational and talent model. And ultimately it forces
them to rethink the business model and the role they will play in an ecosystem that cuts across
traditional industry boundaries. They will have to reinvent themselves.
Resistance to what lies ahead is futile. Insurance has been relatively slow to feel the digital
effect owing to regulation, large in-force books, and the fact that newcomers seldom have
the capital needed to take insurance risk on to their balance sheets. But the industry is not
impregnable. Companies that fail to adapt will weaken under the pressure exerted by those
that use digital technology to slash costs and get better returns on their investments. And they
will be left floundering once digital’s relentless force ultimately breaches both the industry’s
business model and boundaries. Already, in personal auto insurance, we see how sensors
fitted in vehicles will be likely to put premiums under pressure as driving becomes safer. And
we have only to glance at other industries to understand how, in a world in which data and
analytics are king, powerful new digital competitors with large customer bases in their core
businesses can rapidly invade new ones. Chinese e-commerce giant Alibaba now also owns
one of the world’s largest technology finance company, with financial services and products
that include insurance.

Acknowledging the urgency to undertake a digital transformation—both to reap its rewards and
fend off threats—is one thing. Knowing how to manage one is quite another. Ask any executive
who is the midst of the task, and they will attest that it is a formidable effort that touches every part
of the organization, and that there is no rule book that will guarantee an easy ride. This remains
virgin territory because no one in insurance has yet completed a transformation—it could take
as long as a decade. Nevertheless, lessons are emerging that will answer the burning questions
posed by those about to embark on the challenge, questions such as:
 Where should I start, with cost-cutting or growth initiatives? And should I let a thousand flowers
boom, or pick selectively?
 Do I need to rip out my IT systems and start again?
 Do I need to set up a new, digital unit, and if so, will it cannibalize my other business?
 How do I attract all that new, whizzy talent I will be needing—and will these newcomers really
understand what makes my company successful?
 Do I need a chief digital officer?
 Our heritage makes us risk averse. But now I am being told we need to experiment and
innovate. How do we change—safely?
This compendium explores the answers to those questions. We hope it will help executives
to understand where value lies in a digital world, at the same time as offering a clear, practical
approach for capturing it.

Tanguy Catlin
Senior partner, Boston
McKinsey & Company

Preface

Johannes-Tobias
Lorenz
Senior partner, Düsseldorf
McKinsey & Company

McKinsey would like to thank these experts who shared their views on digital
developments in the insurance industry, helping to inform the articles in this
compendium.

4

Naveen Agarwal

Brad Auerbach

Sandeep Bakshi

Andrew Brem

Chief customer officer
Prudential

US industry manager
Facebook

CEO
ICICI Prudential

Chief digital officer
Aviva

Raising your digital quotient

Matthew Donaldson Jennifer Fitzgerald

Eric Gewirtzman

CEO
BGL Group

CEO
BOLT

CEO
PolicyGenius

Stefan Heck

Caribou Honig

CEO
Nauto

Co-founder
QED Investors

Tom King

Linus Lundberg

Adam Lyons

Bill Madison

Senior director
Pegasystems

Head of enterprise
partnerships
Nest

Founder and CEO
TheZebra.com

CEO, insurance, for the
risk solutions business of
LexisNexis

Eldes Mattiuzzo

Steven Mendel

Andrew Rose

Marcus Ryu

CEO
Youse Seguros

Co-founder and CEO
Bought by Many

President and CEO
Compare.com

Co-founder and CEO
Guidewire Software

Clara Shih

Scott Simony

David Stachon

Jakub Strand

Founder and CEO
Hearsay

Head of industry
Google

CEO
CosmosDirekt

CEO
Allianz, Czech Republic

John Straw

Leslie Willcocks

Investor
Bought by Many

Professor of technology, work,
and globalization
London School of Economics
Department of Management
McKinsey Digital

5

Digital technology destroys value.
That might sound counterintuitive
given the extent to which it can make
business systems more efficient—and
companies are urged to embrace its many
possibilities. Yet new McKinsey research
shows that although digital technology
propels some companies to become clear
market winners, for many more its impact
depletes corporate earnings and the
overall value of an industry.1 Consumers,
not companies, are often the ultimate
winners.

Facing digital reality
Regulation, product complexity, and insurers’ large
balance sheets have kept digital attackers from insurers’
gates. That is changing, but in ways incumbents should
embrace. They can flourish in the digital age—if they move
swiftly and decisively.

6

Facing digital reality

So it is likely to be in insurance. For a long
time, the traditional insurance business
model has proved to be remarkably
resilient. But it too is beginning to feel
the digital effect. It is changing how
products and services are delivered, and
increasingly it will change the nature of
those products and services and even
the business model itself. We firmly
believe that opportunities abound for
incumbent insurance companies in this
new world. But they will not be evenly
shared. Those companies that move
swiftly and decisively are likely to be those
that flourish. Those that do not will find
it increasingly challenging to generate
attractive returns.

24-hour access and quick delivery, clear,
relevant information about a product’s
features, particularly in relation to
pricing, and innovative, tailored services
designed for the digital age. They have
the same expectations whatever the
service provider, insurers included.
And as Matthew Donaldson, CEO of
UK-based BGL, the company behind
the comparison site Comparethemarket,
points out, although some insurers are
holding back from the commitment
needed to meet these expectations,
demand must ultimately be satisfied.

Automation can reduce
the cost of a claims
journey by as much as

30%

The goal must be to meet customers’
expectations, which have been
transformed by digital technology.
Customers want simplicity—one-click
shopping, for example. They want

In the shorter term, fulfilling this goal is
a chance for insurers to improve profits
in their core business. Higher customer
satisfaction, driven by the improved
service and faster processing times
that digitization delivers, is itself a driver
of profit through increased customer
retention.2 At the same time, by digitizing
their existing business, carriers can
remove significant cost across the value
chain, further increasing customer lifetime
value. Automation can reduce the cost

1 Jacques Bughin, Laura LaBerge, and Anette Mellbye, “The
case for digital reinvention,” McKinsey Quarterly, February
2017.

2 Alex Rawson, Ewan Duncan, and Conor Jones, “The truth
about customer experience,” Harvard Business Review,
September 2013, hbr.org.

A triple prize: Satisfied customers,
lower costs, higher growth

Digital disruption in insurance: Cutting through the noise

7

of a claims journey by as much as 30
percent, for example.
There are revenue improvement
opportunities too. The notion that
insurance is a low-engagement,
disintermediated category in which
customer relationships can be delegated
to agents and brokers is increasingly
obsolete. Instead, digital technology and
the data and analysis it makes available
give insurers the chance to know their
customers better. That means they can
price and underwrite more accurately,
and better identify fraudulent claims.
They can also offer clients more tailored
products—auto insurance that charges
by the mile driven, for example. And they
can offer them in a more timely manner.
In an analog world, an insurer will be
unaware when a customer holding a
home insurance policy puts that home
on the market. In a data-rich digital
world, that need not be the case, and
the knowledge that a home is up for
sale becomes an opportunity to offer
new home cover, new auto cover, and
perhaps a life product to help cover a
mortgage on the new house.
Longer-term growth opportunities
reside in innovative insurance products
and protection services. Concerns
about cyber security will create demand
from companies and even households
for products that prevent and protect
against the breach or loss of data, and
damage that might ensue. And more
products fit for a sharing economy
will surely emerge—for homeowners
who suddenly become hoteliers when

8

Facing digital reality

they take a guest through AirBnB, for
example.

“Insurers of the future
will play more of a
risk avoidance role
and less of a risk
mitigation one.”
— Andrew Rose, CEO of
US insurance comparison
website Compare.com
This is all good news for insurers,
particularly at a time when low interest
rates and tighter regulation constrain
performance. But while opportunities
abound, there is no guarantee that
today’s incumbents will be the ones
to capture them. Digital is opening the
gates to new attackers that will erode
their advantages.

want to take risk on to their balance
sheets because of the capital they need
to offset it. And they have the advantage
of underwriting skills built on years of
experience and proprietary data.

insurance (Exhibit 2). They are not about to
overturn today’s value chain. But there are
longer-term trends afoot that might.

This resilience explains why the industry
as a whole lags behind many other sectors
in its digital maturity. But the situation is
changing. Money now pouring into the
industry suggests it is no longer regarded
as impregnable. Venture capitalists
globally invested $2.6 billion in insurtechs
in 2015, and nearly $1.7 billion in 2016.
(Exhibit 1). Although these newcomers are
populating every part of the value chain,
their focus to date has been on the more
easily accessible slivers of the industry—
mainly distribution, particularly in P&C

Insurers are threatened by three trends:
a shift toward preventing risk rather
than insuring against it, the increasing
power of those companies that own and
analyze data, and the investment of huge
amounts of capital in insurance-related
capital market instruments by institutional
investors seeking high returns.

Eroding advantages

Risk prevention. Digital technologies that
give rise to ever-increasing amounts of
data and ever more penetrating insights
might make for more accurate pricing

Exhibit 1

The growth of insurtechs
Insurance tech funding, $ millions
2,650

Attackers at the gate
Complex regulation was and remains
a deterrent to new market entrants.
So is the size of incumbents’ in-force
books which, coupled with customers’
tendency in P&C and particularly life
insurance not to switch providers,
makes it hard for new entrants to rapidly
capture market share. Moreover,
incumbents have the advantage of large
capital reserves, as start-ups seldom

1,690

740
223
2013

2014

2015

2016

Source: CB Insights

Digital disruption in insurance: Cutting through the noise

9

Exhibit 2

Where insurtechs are focusing
Share of innovations in Insurtech database

Number of innovations as % of total in the database¹

<5%

5-10%

8%
p&c

8% 4%

4%
17%

17%10%

10%7%

7%

health
5%

5% 3%

3%
11%

11% 8%

8% 6%

6%

life3%

3% 2%

2% 9%

5% 2%

2%

Product
development

Marketing

9%

5%

Distribution

Pricing²

Claims

¹ ~500 commercially most well-known cases registered in the database (excluding wealth management related innovations)
² Includes underwriting and policy issuance

Source: McKinsey Panorama Insurtech Database

of risk, but they also help mitigate risk,
reducing premiums. Take auto insurance.
Forward collision avoidance, blind-spot
assist, and adaptive cruise control are
already fitted in many new cars, making
vehicles safer. Already, 20 percent of
vehicles globally are expected to come
with safety systems by 2020, reducing
the number of accidents and thus the
value of personal auto insurance policies.
Entirely self-driving cars could become
ubiquitous in the next two decades, at
which point liability is likely to shift from
individual drivers to manufacturers. In the
United States, we estimate auto insurance
premiums could decline by as much as 25
percent by 2035 due to the proliferation

10

Facing digital reality

of safety systems and semi- and fullyautonomous vehicles.
The same shift toward risk prevention is
apparent in other sectors. In the home,
sensors can send an alert to the owner if
a risk of flood is detected, automatically
shutting off the water system if there is no
response, and in commercial properties,
connected devices on manufacturing
equipment can give owners early warning
of maintenance requirements. Smart
devices that monitor health are also
increasingly popular. There are two main
effects. Data from connected devices can
be used to assess risk more accurately.
But it is also a powerful tool to lower
risk—to prevent accidents in the home,

>10%

reduce maintenance and downtime, or
improve health. This logically leads to a
model whereby consumers pay not for
premiums in order to be compensated for
damages they might incur, but for gadgets
or services that predict and help prevent
that risk. “Insurers of the future will pay
more of a risk avoidance role and less of
a risk mitigation one,” says Andrew Rose,
CEO of US insurance comparison website
Compare.com. The value creation from
underwriting thus diminishes.
The power of data and its analysis. Data
and analytics are changing the basis
of competition. Leading companies
use both not only to improve their core
operations but to launch entirely new
business models. Insurers have valuable
historic data. Yet in a few years’ time, will
they be able to keep pace and still add
underwriting value when competing with
newcomers that have access to more
insightful, often real-time new data culled
from the Internet of Things (IoT), social
media, credit card histories, and other
digital records? Knowledge about how
fast someone drives, how hard they brake,
or even (more controversially) what they
get up to as displayed on social media is
arguably more revealing data on which to
assess risk than simply age, zip code, and
past accident record. (Facebook recently
moved to prevent its users’ online activity
being used by insurers in the United
Kingdom—proof of the potential power of
access to good data.)
And what if those with the necessary
data and analytical skills and platforms
that reach millions—a Google or an
Amazon—not only offered well-targeted,
tailored products, but also began to
cherry-pick low-risk customers? If they
did so in significant numbers, the insurers’

business model, whereby premiums
collected from low-risk policyholders
contribute to the claims of high-risk ones,
could fall apart.
Auto manufacturers are arguably close
to changing the game for insurers. The
fitting of connected devices as standard
in cars is not far off, potentially giving
manufacturers unique access to data that
could accurately ascertain the risk of their
customers, as well as ready-made access
to drivers in need of an insurance product.
How would incumbents fare in such an
evolving ecosystem?

Leading companies
are using data and
analytics not only to
improve their core
operations but to
launch entirely new
business models.
Institutional investors. For more than a
decade, large institutional investors have
been pouring money into insurancelinked instruments on the capital markets
in search of non-correlated returns
and higher yields in a low interest rate
environment, disintermediating reinsurers
in the process. To date, they have
focused mainly on reinsuring property
catastrophe risk—a sum of $70 billion in
2015. But now they have their eyes on the
primary market. For the moment, interest
centers on “short-tail” lines of business.
Yet ultimately, why would, say, a large
manufacturer of sensors that gathered

Digital disruption in insurance: Cutting through the noise

11

data about weather and soil conditions
to optimize agricultural productivity
not consider offering a crop insurance
product to farmers, with the backing of
investors? The data gathered would aid
risk analysis, and payments could be
triggered automatically (and cheaply)
when sensors detected damaging
weather conditions.

A large incumbent
could more than
double profits over
5 years by digitizing
existing business.
Despite these potential threats, our view is
that today’s carriers, many of which have
a century-old record of creating value
for their policyholders and shareholders,
remain in a strong position to flourish in a
digital age. For the time being, they have
expertise no one else has, making them
valuable partners in the ecosystems
that are evolving to offer consumers
both risk prevention and risk mitigation
services. They still have large balance
sheets that enable them to underwrite
large pools of risk. And they have the trust
of policyholders who need to know their
insurance company will still exist when
they make a claim or their policies mature,
perhaps decades from now.

of just one or two innovation cycles. Retail
music, book stores, travel, and media
are some of the high-profile sectors that
have already felt its force, transforming
their economics and sometimes toppling
what were once industry heavyweights.
The question for incumbents is therefore
whether they are nimble enough to rise
to the opportunities that digital offers.
The evidence that they will need to move
quickly is compelling.

could more than double profits over
the course of five years. In the longer
term, however, earnings from traditional
business will face headwinds as driving
becomes less risky owing to the use of
sensors and telematics or because, in
the case of autonomous cars, liability
is transferred to manufacturers. Fifteen
years on, profits for traditional personal
lines auto might fall by 40 percent or more
from their peak (Exhibit 3).

Second, in a digital economy, the
effects of a shrinking economic pie are
compounded by the fact that the pie
will not be evenly divided—the result of
economies of scale and network effects.
Hence, not all carriers will be able to
sustain the performance described in the
analysis above. For many, digital’s threats
might well outweigh the opportunities.
Again, the signs are already apparent. In
direct auto insurance in Spain, Germany,

Uneven distribution of rewards
First, digital diminishes value. McKinsey’s
global survey of a wide range of industries
has shown that digital technology
shrinks revenue growth at an average
rate of 3.5 percent a year and growth in
earnings before interest and tax (EBIT) at
an average rate of 1 percent a year. For
some industries, the figure is as high as
12 percent for revenue and 10 percent for
EBIT.
Our analysis of auto cover, the insurance
segment that has been first to feel digital’s
impact, suggests a similar dynamic
is unfolding in the insurance industry.
US auto insurers have already lost on
average $4.2 billion in underwriting profit
a year over the past five, with expenses
and losses consistently outweighing
premiums. They should expect further
annual profit declines of between 0.5
and 1 percent if they fail to use digital
technology to improve efficiency and
effectiveness.

Exhibit 3

Profit projection for an auto insurer digitizing its business
Future profits as a percentage of today’s profits

220-300

Can be augmented through
innovation in new coverages and
value-added services

20-60

75-275

60-100

120-200

15-55

100

Today’s
profits

Growth, loss
and expense
ratio
improvements1

2025
Profits2

Impact from
improved
vehicle
safety3,4

Shift in
Loss and
liability to
expense ratio
commercial improvements4
lines4

Short-term improvement

2035
Profits2

Long-term decline

1 Assumes a 3 to 5 percentage point improvement in loss ratio, a 2 to 4 percentage point improvement in operating expenses, and a 6 to 8 percentage point improvement
in direct sales conversions

But for many carriers, the window of
opportunity is narrow. Once cracks
appear, digital technology has the power
to break business models within the space

12

Facing digital reality

In the shorter term, corrective measures
could lead to huge profit improvements.
By digitizing existing business, our
research suggests, a large incumbent

2 Includes growth in investment income as well premiums. Investment income modeled as a flat percentage of premium in each year
3 Includes impact of semi- and fully autonomous vehicles
4 Assumes a 25 percent reduction in premiums as a result of telematics and sensors and a 50 percent risk transfer to commercial product liability

Source: Digital and Auto Insurers Value at Stake Analysis, McKinsey, 2016

Digital disruption in insurance: Cutting through the noise

13

and the United States, a single player has
captured the lion’s share of profits, up to
70 percent, leaving a long tail of sub-scale,
often unprofitable carriers competing for
the remainder (Exhibit 4).
Third, the winners will be those that move
decisively. Our cross-industry research
showed that those companies that
initiated disruption fared best, generating
revenue and EBIT growth that was on
average between one and two percentage
points higher than that of more ad hoc
responders. These companies made big

bets—to innovate products or reshape
the value chain, for example—rather than
following in others’ wake. In insurance, this
is borne out by the companies featured
in Exhibit 4: HUK24, Direct Line, and
Progressive were all first movers.

its impact to date in industry after industry, it
would be foolhardy to bet against it.

What it takes to transform rapidly and
at scale

A similar dynamic is likely to play out
across the industry. Digital technology
will take longer to disrupt more complex
business lines, such as life insurance, and
technological innovation may disrupt them
in ways we cannot yet foresee. But given

Exhibit 4

The “winner-takes-all” effect
Direct auto insurance underwriting profit

Market leader

Germany, 2015

Spain, 20151

US, 20152

€, million

€, million

$, million

83

48

Players 2-4

Rest of industry

The new value drivers

499

394

76
Direct Line

25

Progressive

56

347
105

7
Profits

Losses

Total

Profits

-15
1 Does not include “other technical results”
2 Includes results only for direct U.S. auto writers Progressive, USAA, GEICO and Amica
Source: INESE, McKinsey Insurance Database Germany, AM Best (statutory filings)

14

Facing digital reality

Losses

Total

Profits

Losses

Total

— Matthew Donaldson,
CEO, BGL Group
(Comparethemarket)
largest profits, and insurers must fight for
them. Their success will depend upon
offering superior products and services.
Technical underwriting skills alone will not
suffice.

Technological leadership and innovation.
Winning companies will need to do more
than follow technological trends and
innovation. They will need to lead them.
Innovation is a vital component of a digital
transformation.

Efficiency (cost savings) and effectiveness
(higher returns). Digital technology puts
margins under pressure as premiums fall
under the weight of price competition and
as new ways of mitigating risk emerge.
Under these conditions, insurers will need
to harness digital to make their operations
more efficient, aggressively lowering costs.
They will also need to make them more
effective by, for example, improving the
accuracy of their pricing and underwriting
to improve loss ratios.

Customer ownership. Incumbents have
not had to worry much about customer
ownership. Their only competitors have
been other insurers, and most have felt
secure enough to cede customer contact to
intermediaries. Today, however, customer
access and “ownership” are keys to the

Scale and network effects. In a digital
world, initial investments are sizeable but
marginal costs are close to zero. Scale
therefore matters. It also delivers network
effects, helping to build a company’s
access to more and better data, talent, and
partners to the extent that it becomes a

Success will be grounded in recognizing
the drivers of value in a digital age. There are
five of these.

64

HUK24

Against this backdrop, we interviewed
some 30 executives in incumbent and
attacking companies to understand their
views on how the industry is changing
and how to respond. The single message
most constantly repeated was the
need for incumbents to accelerate their
response (see box, “The need to commit
to speed”). Most know they cannot afford
to wait until evolving technologies turn the
market upside down and the competitive
advantages they enjoy today evaporate.
If history tells them anything, it is that
they need to get ahead of the curve. And
they will need to do so at scale, ultimately
transforming the entire business. What
holds them back, however, is deciding how
to address the challenge given its enormity.

“I believe the
consumer will win
and that the desire for
low-cost, transparent,
high-quality digital
services will have to
be met.”

Digital disruption in insurance: Cutting through the noise

15

VOICES: The need to commit to speed
“There are times when we talk to carriers about integrating a line of
code into their app to integrate more into Facebook, and the answer
we get is, ‘Well, our next release cycle isn’t for another eight months.’
The ability to speed up those release cycles is a variable that we see
with those carriers that are not just talking the talk, but taking action.”
—Brad Auerbach, US industry manager, Facebook
“You have to believe that tomorrow somebody’s going to attack you.
And you have to be acting very, very fast. The second that you slow
down, somebody’s going to pass you. Insurance companies operate
on slower timescales. You can’t do that. The market will pass you by.”
—Andrew Rose, president and CEO, Compare.com
“Companies need to commit to speed. Insurance is a highly regulated
industry and it is not easy to move quickly, but the fact is consumers
are moving at exceptional rates. So the companies that will stand
out are the ones that are going to find ways to move a bit faster, at the
pace of the people they’re insuring.”—Scott Simony, head of industry,
Google
“We see some carriers that understand this is the beginning of a
reinvention of the auto insurance model, but we also see many that are
still scared of technology, a bit like the utility world was a few years ago,
where people said, ‘You know what, I’m fine running my coal plants, I
don’t want to know about all this renewable technology because it’s
only going to hit in the generation after I retire.’ But car makers are
adopting the technology quite rapidly. Five, ten years out we’re going to
see some very, very major effects.”—Stefan Heck, CEO, Nauto
“Insurance companies that are really good at risk management are
thinking traditionally—that if you spend enough time, one year, two
years, thinking and planning, the outcomes you generate would
be [the result of ] the time spent. But the pace of change is so fast
that by the time you have thought through things, the market may
have already moved on.”—Naveen Agarwal, chief customer officer,
Prudential

barrier to entry for others. Some companies
have built hyper-scale data platforms that
enable them to blur traditional industry
definitions by spanning product categories
and customer segments, creating new
ecosystems and value chains in the
process.
Speed and agility. The strength of an
insurer’s in-force book will not protect it
indefinitely. Incumbents need to move
quickly to compete with digital competitors
that have the agility to keep pace with
evolving technology and customer needs.
That means letting go of slow decisionmaking processes and outdated ways of
working, and adopting a new culture and
talent base that is more comfortable with
experimentation, testing and learning, and
sometimes even with failing.

Insurers should not underestimate the
changes that digital will bring to their
industry and the challenges they will
pose. Neither should they overlook the
significant short-term profit improvements
that are within their grasp if they digitize
their core businesses, nor shy away from
innovating to be part of an exciting future
that is unfolding for the industry. If they act
decisively, they will be among its leaders.

Tanguy Catlin is a senior partner
in McKinsey’s Boston office, where
Christopher Morrison is an associate
partner. Johannes-Tobias Lorenz is a
senior partner in the Düsseldorf office, and
Holger Wilms is an associate partner in the
Washington, DC, office.

A roadmap for the future
These new value drivers will inform the
roadmap insurers chart to transform
their businesses and secure their future
competitiveness. They will shape their
strategy, helping them to understand the
forces that are disrupting the industry.
They will make clear the huge value to
be created by digitizing their current
businesses, as well as the imperative to
innovate. They will demonstrate the need
for significant investments in IT and a
change in perspective whereby IT becomes
a strategic function, not a cost center. They
will make plain the new capabilities required
to take full advantage of IT’s potential,
including automation, advanced analytics,
and blockchain. And they will highlight the
importance of culture and talent change if
the transformation is to be successful.
  

16

Facing digital reality

Digital disruption in insurance: Cutting through the noise

17

The verdict is clear: those insurance
companies with the most advanced
management practices related to digital
strategy, capabilities, culture, and
organization outperform their peers.1
Yet relatively few incumbents have so
far defined a comprehensive digital
strategy—the foundation from which
all else logically follows if they are to
compete in a digital world. Instead, they
package together tactical or incremental
initiatives that individually drive modest
performance improvement—some
digital marketing, a new sales channel, or
some degree of automation, perhaps—
while leaving significant value potential
untapped and their futures in doubt.

A strategy for a digital age
Few insurers have defined a comprehensive digital strategy
fit to withstand attackers at the gate. The starting point is to
understand the sources of disruption.

Why? Part of the answer lies in the extent
to which carriers have been protected by
regulation and the strength of their in-force
books. In addition, CEOs with limited
tenures might be wary of upsetting what
has served them relatively well—and are
likely to be more circumspect when the
future is so uncertain. With competitive
landscapes changing fast, it can be hard
to know just how digital technology will
play out, and hence where to place big
bets. Yet hesitation is not an option. In
insurance, as in other industries that have
felt the force of digital disruption, those
that move fastest to adapt are likely to take
a disproportionate share of the profits.
Hence, a means of discerning clearly the
sources of opportunity and disruption
in digital technology lies at the core of a

1 Tanguy Catlin, Ido Segev, and Holger Wilms, “The
Hallmarks of Digital Leadership in P&C Insurance,”
McKinsey & Company, August 2016.

18

A strategy for a digital age

digital strategy, and is critical to building a
leadership position.

Building a digital strategy
The definition of a digital strategy is no
different from that of any other strategy.
It is a set of integrated, hard-to-reverse
choices, made for the future, in the face of
uncertainty, with the purpose of creating
and capturing economic surplus.2

With competitive
landscapes
changing fast, it can
be hard to know
just how digital
technology will
play out, and hence
where to place big
bets.
The building blocks of a digital strategy
likewise resemble those of any other
strategy: a diagnosis of where and why a
company makes money in the present,
a forecast of how that might alter in the
future, an understanding of the potential
pathways to success, a portfolio of
initiatives, and then a commitment to
driving change.
2 Frederick W. Gluck, Stephen P. Kaufman, A. Steven
Walleck, Ken McLeod, and John Stuckey, “Thinking
strategically,” McKinsey Quarterly, June 2000.

Digital disruption in insurance: Cutting through the noise

19

All these considerations will transform
certain aspects of how companies
manage their strategies, even though
the foundations remain the same. In the
first instance, companies need to be
bolder. A McKinsey survey of more than
2,000 executives in industries affected
by digital technology shows that the
companies with the highest revenue
and earnings growth looked for digital
opportunities across all elements of their
business model, not just one or two,
and either led the disruption or were fast

20

A strategy for a digital age

The digital tipping point
To stay competitive, incumbents’ strategic focus should shift
from digitizing the existing business model to disrupting it
as digital technology takes hold

tipping point

1. To what extent
will digital
technology
transform the cost
structure of the
business?
2. Will digital
technology disrupt
supply and
demand?

Current position of most carriers in segment.

Advanced
incumbents
adapt to new
model
Early adopters
embrace the new
models

New trends
emerge

3. Will digital
technology give
birth to new value
propositions and
markets?

Mainstream
customers
adopt

personal
insurance

Companies that procrastinate over such
bets risk disappearing. In insurance, as
in other industries, it takes a while for
customers and companies to embrace
digital technology, but as the pace of
change accelerates incumbents’ scope
to adapt diminishes. There comes a
tipping point where those that have not
adapted their strategies fade away—as
in traditional print media, for example.
The insurance industry might have been
relatively slow to feel the digital effect,
but personal lines in P&C cover look set
on a steep trajectory toward the tipping
point, with small commercial lines
just behind. Life insurance and large
commercial insurance, with longer-term,
often more complex contracts, have
further to go (see Exhibit 1).

Exhibit 1

life
insurance

The prerequisite of
a digital strategy is
an understanding
of the threats and
opportunities that
digital technology
poses.

followers. These leaders made bets on
digital processes across the value chain,
on innovative products, and on new
business models.

commercial
insurance

What is different in a digital age is the
speed and potential magnitude of
that change, upending old business
models and rapidly building entirely
new ones. Circumventing the need
to build traditional fixed assets, the
likes of Amazon, Netflix, Uber, Airbnb,
and a host of fintechs have disrupted
incumbents in the space of a few years
by using digital technologies, data, and
analytics to create value without owning,
respectively, physical shops, cable
connections to viewers’ homes, car
fleets, hotels, or bank branches.

Laggard
incumbents die

4. Will digital
technology give
birth to hyper-scale
platforms?

Innovative
startups begin to
disrupt business
models

time

Second, companies need to review their
strategies frequently as technology,
consumer behavior, and competitors
evolve ever more rapidly. The five year
strategic review—once a staple of
board-level strategies—is increasingly
outdated. Recall that five years ago, the
iPad, now ubiquitous, had been on the
market for only 18 months, Netflix stock
was taking a beating after the company
suggested it would spin off its DVD
delivery business, and Spotify had just
launched in the United States.
Third, companies need to build a wider
range of strategic options because
conditions can change so quickly.

Focus on digitizing
existing business model

And fourth, when conditions do change,
they will need the discipline and agility
to reallocate management time and
resources swiftly. As Klaus Schwab,
chairman of the World Economic Forum,
memorably said, “In the new world, it is not
the big fish which eats the small fish, it’s
the fast fish which eats the slow fish.”

Focus on innovative products,
services, and business models

The catalysts of disruption
The prerequisite of a digital strategy is
an understanding of the threats and
opportunities that digital technology
poses. A review of what peers and
newcomers are up to can help in this
regard and presage what the future might

Digital disruption in insurance: Cutting through the noise

21

hold. The problem here, however, is that
there are hundreds of insurtechs to track,
with more appearing as venture capital
pours into the industry (to the tune of $1.7
billion). They cannot all be monitored,
and it is a sure bet that although some will
succeed, most will vanish. It is therefore
important to focus on the nature of the
disruption rather than on the would-be
disruptors, with a view to getting ahead
of it—in other words companies must
understand both what is happening,
and why. Only in this way, according to
Tom King, senior director at US software
company Pegasystems, will insurers be
able to respond promptly to changes in
the market.
Our research suggests that digital
technology can disrupt in four, nonmutually exclusive, ways. It can transform
the cost structure of a business system.
It can disrupt supply and demand. It
can create new value propositions and
markets. And it can create hyper-scale
digital platforms. There are thus four
questions companies should ask in order
to start building a strategy.
1. To what extent will digital technology
transform the cost structure of the
business system?
Netflix took movie rentals and rethought
the business around them, then went
from DVD delivery to owning one of the
world’s largest video streaming services.
In a digital world, all businesses are likely
to be disrupted if they rely on a physical
distribution network and involve manual
processes that can be automated. It is
easy to see how traditional insurance
models, often reliant on agents with

22

A strategy for a digital age

commissions, could be attacked by
companies that are able to automate
advisory processes and apply advanced
analytics to improve pricing and
underwriting. McKinsey estimates that
up to 40 percent of P&C and life insurers’
expenses are locked up in their top 20 to
30 core end-to-end processes—costs
that digitization can reduce, and in some
cases, eliminate.

“It doesn’t matter how
much business you
sell today, it’s whether
or not you can identify
where [future] profits
and losses lie, and
what you need to
jettison.”
– Tom King, senior director
at US software company
Pegasystems
2. To what extent will digital technology
disrupt supply and demand?
In the analog world, economics can make
it hard to cater precisely to individual
demand. Think of the hefty package of
supplements bundled together in Sunday
newspapers. Most consumers do not
read everything, but the economics of
distribution mean they get it anyway.

Digital technology can cater to demand
more precisely so that customers are
no longer obliged to buy elements of a
package they do not want. iTunes makes
it unnecessary to buy a whole album,
for example. This unbundling makes
businesses vulnerable to disruption,
particularly if they cross-subsidize parts
of their offering, as insurers do, with direct
sales channels covering the cost of more
expensive agency channels.
Aware of what is afoot, some carriers,
such as Progressive, enable customers to
“name their price” and choose elements
of a policy that fit their budget—the level
of deductibles, for example. Some offer
pay-as-you-go auto insurance whereby
drivers are charged by the mile. And
some use data on, for example, driving
habits, to price products in a way that
more precisely reflects an individual’s
risk. These developments amount to an
“unbundling” of coverage, better matching
the protection provided to the protection
required.
Digital technology also has the power
to unleash supply. YouTube has made
it easy and inexpensive for millions of
individuals to become published video
producers, unlocking a supply of content
that previously would have been too
costly to distribute. In insurance, complex
regulation and capital requirements have
restricted supply in primary markets as
start-ups seldom want to take insurance
risk on to their balance sheets. But startups are targeting accessible slivers of
the industry, primarily marketing and
distribution. And institutional investors
are hovering. They have already

Digital technology
can cater to demand
more precisely so
that customers are no
longer obliged to buy
elements of a package
they do not want.
poured money into insurance-linked
instruments on the capital markets
in search of non-correlated returns
and higher yields, disintermediating
reinsurers in the process. Some are now
investing in primary markets, a move
that digital technology could accelerate.
It is conceivable, for example, that a
manufacturer of sensors that gather
data about weather conditions in order
to optimize fertilization could turn to
investors to back an insurance product for
crops, using the same sensors to indicate
whether weather conditions were harsh
enough to damage them.
3. To what extent will digital technology
give birth to new value propositions
and markets?
There are myriad ways of using digital
technology to improve value and offer new
propositions, such as making purchases
simpler and faster, adding fresh elements
to a product or service, using data
and analytics to make products more
relevant, or removing costs incurred by
intermediaries. Examples are emerging
of carriers using it to reward consumers

Digital disruption in insurance: Cutting through the noise

23

with benefits for behaving in a way that
aligns with their own interests—such
as US insurer John Hancock offering
customers discounts on products and
services, as well as lower premiums, in
return for leading healthy lifestyles. Some
digital attackers are making it possible
to buy complex products such as life
insurance online, while others are using
internet crowd sourcing to negotiate better
deals with insurers for “long-tail” insurance
products. Policies for pug dogs and
diabetic travelers fall into this category.

New value
propositions can lead
to the establishment
of new markets, by
matching supply
and demand in
pioneering ways.
And there are new products for new
risks—protection against cyber risk, for
example, or cover for “sharing economy”
risks such as those to which car owners
are exposed when they decide to become
cab drivers for Uber.
Some value propositions are emerging
that threaten to undermine the existing
insurance model. The more real-time data
becomes available, from sensors in cars
or on drones, devices installed in homes,
or monitors worn on our bodies, the more

24

A strategy for a digital age

companies can learn from the analysis of
that data and the more it will be possible to
mitigate risk, reducing the need to insure
against it. That hits the volume of demand,
but risk mitigation becomes a new value
proposition in the process.
New value propositions can also lead
to the establishment of new markets,
by matching supply and demand in
pioneering ways. The likes of Uber, Lyft,
and the Chinese ride-sharing company
Didi Chuxing use digital platforms with
location-based mapping technology
to match would-be passengers with
the drivers in closest proximity, along
with analytics to make dynamic pricing
adjustments and encourage drivers to
meet demand in peak periods. It is a far
cry from passengers trying to hail a taxi
in the street. In insurance, online price
aggregators have established markets
to help consumers compare prices and
bypass the traditional agent distribution
model.
4. Will digital technology give birth to
hyper-scale platforms?
Digital technology can give rise to
companies that build platforms on a
massive scale. Their size, the huge
amounts of data they amass, and the
depth of analytical talent they deploy—
along with the network effects they
generate—are hard for others to match
and thus create barriers to entry.
Moreover, these companies’ skills and
capabilities enable them to blur traditional
industry definitions by spanning product
categories and customer segments and

inventing new value chains. For example,
Uber has signed a deal with Volvo to
invest in the development of self-driving
taxis in the United States; testing began in
Pittsburgh in September 2016.3 Apple has
used its unique data, infrastructure, and
product platform to push into the world
of finance with Apple Pay. And Chinese
e-commerce giants Alibaba, Tencent, and
JD.com have leveraged their volumes of
data to offer microloans to the merchants
that operate on their platforms. By using
real-time data on merchants’ transactions
to build its own credit scoring system,
Alibaba’s finance arm has been able to
achieve better non-performing loan ratios
than traditional banks.4
Insurers will need to consider what
their role might be in the ecosystems
developing around these data platforms,
and where value lies in owning and
analyzing data.5 Will, say, a large car
manufacturer that fits sensors as
standard in vehicles amass enough data
to dominate an ecosystem that brings
together insurers and other service
providers such as telecom companies,
repair shops, road side assistance,
telematics providers, and legal services?

A heat map for capturing value
The process of understanding these
forces and analyzing the value at stake will
3 “Uber and Volvo to develop self-driving cars,” Financial
Times, August 18, 2016.
4 China’s digital transformation: The internet’s impact on
productivity and growth, McKinsey Global Institute, July
2014.
5 Nicolaus Henke, Jacques Bughin, Michael Chui, James
Manyika, Tamim Saleh, Bill Wiseman, and Guru Sethupathy,
“The age of analytics: Competing in a data-driven world,”
McKinsey.com, December 2016.

reveal the need for a portfolio of initiatives
that grapple simultaneously with two
strategic imperatives.
The first is the need to capture short-term
value. In the early stages of disruption,
digital technology invariably starts to
transform the cost structure of the
business system and disrupt supply and
demand, posing opportunities and threats
to incumbents. To respond, they will need
to digitize their businesses in order to
cut costs, grow revenues, and improve
the customer experience. Essentially,
however, the business model will remain
the same.

By understanding
the catalysts for
disruption and
regularly reviewing
their businesses,
companies will be
able to lead the wave
of disruption as it
gathers strength, not
drown in it.
Drawing up a heat map that examines
the value at stake throughout every
business line will indicate the extent of
the opportunity—the cost savings an
auto carrier could make by digitizing and

Digital disruption in insurance: Cutting through the noise

25

automating the claims process, say—as
well as the threat if it fails to respond—the
fall in profits that would ensue if customers
were to gravitate toward price-driven
aggregators and comparison sites,
for example. The “hot spots” will help
a company decide where to prioritize
initiatives, although this will depend also
upon whether it has the capabilities to
pursue them, and upon regulatory issues.
The second strategic imperative will be
to look beyond today’s business for fresh
sources of value. Pondering the potential
for new value propositions and markets,
and for hyper-scale platforms will suggest
how digital technology might disrupt
not just elements of the value chain but
the entire business model. The higher
up the digital curve a business line rises,
the more imminent such disruption is
likely to be, and the greater the need for
innovation. The exhibit shows how the
strategic focus shifts as digital’s influence
on an industry grows. In P&C lines it is
already apparent that the traditional model
is being reshaped by data and analytics
that make it easier to mitigate the risks we
insure against today. By understanding
the catalysts for disruption and regularly
reviewing their businesses, companies will
be able to lead the wave of disruption as it
gathers strength, not drown in it.
  

innovating for the future risks cannibalizing
profits in the here and now, along with
organizational upheaval.
The answer lies not in reverting to a
strategy of incremental improvement.
Competition in a digital age rules this
out. Rather, it entails fully grasping where
value lies, in order to shape and sequence
initiatives in ways that meet strategic
imperatives while maximizing quick paybacks to protect the performance of the
business. Understanding the catalysts
of change has to be the starting point,
helping to reveal where value-creating
opportunities lie and where value is at risk,
and ensuring companies disrupt before
they are disrupted.
Tanguy Catlin is a senior partner
in McKinsey’s Boston office, where
Christopher Morrison is an associate
partner, and Kurt Strovink is a senior
partner in the New York office.
The authors would like to thank Jacques
Bughin, Laura LaBerge, Jay Scanlan,
and Ido Segev for their contributions to
this article.

The age of innovation
Insurers have a choice: be disrupted or be the disruptor
with new products, services, and business models.

Delivering on these imperatives will prove
a hard balancing act for CEOs, faced
with the constant pressure of the next
earnings report. Although digitizing the
existing business will reap rewards, it can
require significant investment that pays
off after several years. At the same time,

26

A strategy for a digital age

Digital disruption in insurance: Cutting through the noise

27

Digital technology is disrupting industry
after industry—and quickly separating
winners from losers. The spoils are going
to the boldest innovators. A McKinsey
survey of more than 2,000 executives in
industries affected by digital disruption
shows that the companies with the
highest revenue and earnings growth
led the disruption or were fast followers,
making big bets across their businesses
on innovative products, digital processes,
and even entirely new business models.
Most insurers, though, do not have
innovation in their DNA. Regulation has
curbed incumbents’ ability to experiment,
while limited competition has given them
no particular need to do so—the size
of their in-force books makes it hard for
new entrants to build market share, and
start-ups seldom want to take risk on
to their balance sheets because of the
capital required to offset it. But innovate
they must. Although there is significant
opportunity to capture value in the
short term by digitizing their current
business, they will get left behind if they fail
simultaneously to use digital technology to
innovate and build new business.
Exhibit 1 shows where insurtechs are
concentrating their innovation efforts.
To help companies think through
where innovation lies, we look at three
broad areas—new kinds of risk, new
approaches to underwriting, and new
value propositions. And we discuss how
companies are organizing themselves to
develop ideas and accelerate innovation.

Cybercrime
Companies today run on data, which
makes cyber insecurity a major concern.
An intrusion can not only disrupt business
but also cause great harm to a company’s

“It’s hard for big
carriers to innovate as
they have so much to
contend with already
—industry headwinds,
legacy issues. But
they need to be in the
game, right now.”

reputation, particularly if customer
information such as credit card data is
compromised. Consumers too are at
risk, from identity theft, loss of financial
assets, and unauthorized credit card
use. Opportunities for carriers include
prevention services and insurance
integrated into the offerings of software
providers (see box, “The cybersecurity
opportunity—that few are seizing”).
Global supply chains

28

The age of innovation

Leading trends among insurtechs
Innovations as % of database total1

9

Emerging ideas

— Caribou Honig, cofounder
of QED investors

New risks
Insurers have an immediate opportunity
to write cover for new types of risk that are
emerging in a digital age.

Exhibit 1

Digitization and ubiquitous data
communications have enabled
companies to build global supply chains.
These complex networks make it possible

1

Big data/machine learning

20

2

Software as a service/cloud

21

3

Usage-based insurance

13

4

IoT

12

5

Digital/Roboadvisory

10

6

Gamification

9

7

Peer-to-peer insurance

4

8

Blockchain

4

9

Micro-insurance

3

1 ~500 commercially best-known cases registered on database. Innovations focusing purely on insurance
Source: McKinsey Panorama Insurtech database

for companies to source supplies,
manufacture goods, and sell their
wares anywhere in the world. But the
rising complexity of supply chains also
multiplies risk. There are more points of
vulnerability, and disruption in any part
of the chain can quickly affect the entire
business. There is thus growing
demand for equally sophisticated
supply chain cover.

Digital technology not only creates
the risk, it also provides many of the
solutions. Using the connected sensors
and monitors that comprise the Internet
of Things (IoT), it is possible to track the
location of inventory and finished goods
as they travel on trucks, ships, and
planes. Predictive analytics can then
be applied to data on claims, weather,
and other factors to enable insurers to
underwrite the supply chain risk more
precisely.

Digital disruption in insurance: Cutting through the noise

29

The cybersecurity opportunity—that few are seizing
Cybercrime presents rapidly multiplying risks for businesses and consumers. Having
almost quadrupled between 2012 and 2015, from $112 billion to more than $400 billion,1
the estimated cost of cyber breaches is projected to reach $2 trillion in 2019, or almost as
much as India’s GDP for 2015.2
Yet the insurance industry has not leaped at the opportunity to sell protection against this
new risk. The global insurance pool in 2015, according to Lloyds, was just $2.5 billion.
Part of the problem is demand; awareness of the risk remains limited. There are also
supply-side issues. Insurers are unsure how to model cybersecurity risk and still have
not decided what they can cover economically. Few have written “full” cyber cover to
compensate customers for all possible losses, including data theft, business disruption,
property damage, and personal injury, and a lack of reliable information on historical
breaches makes pricing difficult. Moreover, there are few standards for cover and the law
differs according to jurisdiction. Perhaps most important, technology and the capabilities
of hackers continue to evolve more rapidly than cybercrime protection methods.
Nonetheless, a risk this large should be the basis for a successful line of business
for companies that are able to innovate. They would need to invest in understanding
the drivers of cyber risk, which would require them to hire experts who understand
the technical issues as well as the underwriting process, or enter partnerships
with organizations that have those capabilities. They would also need to develop
comprehensive histories of cybersecurity breaches and create compliance frameworks
to measure enterprise risk. Given the magnitude of the risks involved, though,
incumbents with strong balance sheets could have an advantage in cybercrime
insurance.
1 State of Security Survey, Symantec (2013); Lloyds of London; World Economic Forum.
2 Juniper Research.

New solutions are emerging. For car
rides, Uber supplies drivers with limited
liability cover when its app is turned on
and a driver is available. Its commercial
cover kicks in when a fare enters the
car. For drivers of BlaBlaCars (a service
that operates in France and the United
Kingdom), Axa offers a combined
personal and commercial package.
Various forms of cover are emerging for
homeowners participating in Airbnb and
other short-term home rental platforms
such as Alterkeys and 9Flats.com. The
platforms offer protection for damage by
tenants that cannot be resolved by the
owner, but with significant exclusions.
Carriers such as US-based Proper, which
have long offered insurance to owners
of vacation rental properties, are adding
cover for short-term rentals. Still, most
traditional homeowner policies do not
cover commercial uses of properties. As
the sharing economy grows, there will
surely be more opportunities to innovate
and provide relevant insurance products.

New underwriting approaches
Digital technologies enable new ways to
provide traditional cover and underwrite
traditional risks, often by using individual
rather than group data. They are also
being used to reach new customers.

“We ... create
communities of
individuals, on
whose behalf we
negotiate with the
insurance industry to
bring them a better
deal than they could
get on their own.”
—Steven Mendel, founder
and CEO of Bought By
Many
problem. For example, they are enabling
a form of low-cost, micro-crop insurance
for farmers in emerging economies that
does not require claims adjusters to trek to
remote locations to settle claims. Instead,
insurers use data analytics to determine
if severe weather, low rainfall, or other
factors would have damaged crops, and
pay claims based on their analysis. This
vastly reduces settlement costs, making
it possible for insurers to offer affordable
policies to farmers in the developing world.

Micro-insurance
On-demand insurance
The sharing economy
New kinds of risk are emerging from the
sharing economy that has grown from
digital technology’s capacity to match
supply and demand. Online platforms
such as Uber and Airbnb enable

30

The age of innovation

consumers to “share” unused capacity
(a car ride, the use of a spare room) for
a fee. This turns a car owner into a cab
driver and a homeowner into a hotelier,
and alters the nature of the insurance
cover that the driver and homeowner
require.

Traditional, loss-based insurance can
be prohibitively expensive to provide
for small amounts of cover. New data
streams and data analytics address this

In addition to facilitating the underwriting
of small amounts of cover, real-time data
can enable the provision of “episodic” or

Digital disruption in insurance: Cutting through the noise

31

on-demand cover for short periods. Sure,
for example, is a mobile app for episodic
travel accident insurance bought on the
spot. Travelers look up their flights, enter
their personal data, and purchase cover
for the duration of the flight.
European telecom operator Tele2
offers travel insurance in partnership
with Gjensidige, a Nordic insurer, for
motorists whose insurance extends only
to domestic travel. When a driver crosses
a border—from Poland to Germany, say—
the insurer issues a text message offering
episodic cover while the vehicle is out of
Poland. Another start-up, San Franciscobased Trov, has an app that enables
consumers to buy short-term insurance
on demand against loss or damage for
items such as sports equipment and
computers. If they are about to take a ride
on an expensive bike or take a laptop on
a vacation, the app can be used to switch
the cover on and off. Another emerging
form of on-demand insurance is usagebased or pay-as-you-go cover—auto
insurance by the mile, for example.

will be donated. The idea is that peer
group members who share an interest
in maximizing contributions to their
causes will not attempt to inflate claims.
One of the company’s executives is
behavioral scientist Dan Ariely, who says
the Lemonade approach removes the
conflict between carrier and the insured
that is inherent in traditional insurance. As
a result, he says that the company, which
began offering policies in September
2016, will be able to pay claims quickly
because it has less need to hold back
payment until they can be verified.

“The big difference
in insurance in the
future is going to be
service.”
— Eldes Mattiuzzo, CEO of
Youse Seguros

Peer-to-peer insurance
Several start-ups have created peer-topeer insurance services that aggregate
customers for a group purchase.
Lemonade, a New York-based start-up
that has recruited veteran insurance
industry executives, organizes peer
groups around charitable and social
causes. Consumers who purchase
homeowner or renter insurance on
Lemonade’s online platform designate a
cause to which unspent premium money

32

The age of innovation

Bought By Many, a UK-based insurance
distribution company, groups those with
similar insurance needs—diabetics, for
example, who often have trouble getting
travel insurance, or owners of particular
breeds of pet. “We use a combination of
search engines and social media to create
communities of individuals, on whose
behalf we negotiate with the insurance
industry to bring them a better deal than
they could get on their own,” says the
company’s founder and CEO, Steven

Mendel (see “Playing to connectedness:
An interview with Steven Mendel of
Bought by Many”).
Personalized pricing
Digital technologies increasingly enable
carriers to assess risk on the basis of
data about specific consumers, rather
than general population data. Telematics
collect real-time information about an
individual’s driving habits to inform the
pricing of auto cover, while data from
wearable devices such as fitness bands
and apps that monitor adherence to
medical treatment can inform life cover—
services that Sureify, a tech start-up,
uses to assist carriers underwriting
personalized term life cover. Some carriers
have experimented with using social
media data as a basis for underwriting
and pricing decisions—but have met
opposition from platform owners.

New value propositions
In the digital era, traditional insurance
models are threatened by the availability
of reams of data, much of it real-time,
that help mitigate risk. One of the
biggest challenges on the horizon is the
development of autonomous vehicles
and advanced driver assistance systems
(ADAS). These technologies will put
passenger cars and other vehicles fully
or partially under computer control,
reducing premiums as driving becomes
safer, and ultimately shifting liability from
the driver to the car manufacturer or its
software vendor. ADAS systems, ranging
from adaptive cruise control to traffic
sign recognition, are already becoming
common on passenger cars (Exhibit 2).

Stefan Heck, CEO of Nauto, a US-based
start-up that provides autonomous vehicle
technology, believes that as a result, some
70 percent of loss events will disappear
in the course of ten years (see “Once in
four-generation change: An interview with
Stefan Heck of Nauto”).
The same shift toward risk prevention
exists in other business lines. Sensors in
the home and devices that monitor our
health reduce the likelihood of accidents
or sickness. Accordingly, insurers are
beginning to offer new services, often
in conjunction with partners, in the
ecosystems that are growing around new
data. “The big difference in insurance in
the future is going to be service,” says
Eldes Mattiuzzo, CEO of Youse Seguros,
the online insurance sales platform of
Brazilian carrier Caixa Seguradora.

“There isn’t one size
fits all. Depending
on our situation, we
will partner, we will
invest, we’ll build
ourselves. And that
gives us all ways to
plan.”
— Andrew Brem, chief digital
officer of Aviva

Digital disruption in insurance: Cutting through the noise

33

assistance services, car repair
workshops, rental car services and
more—all of which can be instantly
accessed via a mobile app (Exhibit 3).
Home insurers might become part of an
ecosystem centered on an app that helps
home buyers take out insurance, and also
values the property, predicts utility costs,
offers smart-home devices to monitor fire
or flood risks, sends storm alerts, and, if a
problem is detected while the homeowner
is away, offers to send out an inspector or
repair person.

Exhibit 2

Installation rates of ADAS1 technology
Passenger cars

Europe
70

70

65

65

60

60

55

55

50

50

45

45

40

40

35

35

30

30

25

25

20

20

15

15

10

10

5

5

Japan

To seize the opportunities and overcome
the threats implicit in digital disruption,
incumbents have no choice but to
innovate. Innovation must become a core
capability.
We see three ways for insurers to develop
new ideas and accelerate innovation:
by forming strategic partnerships, by
investing in start-ups that have digital
expertise, and by creating in-house

Exhibit 3

An auto insurance ecosystem

0

0
2010

North America

How insurers can develop ideas for
innovations

2011

2012 2010 2013 2011 2014 2012 20152013 2016 2014 20172015

2016

Traditional
claims
assessors

2017

OEMs
Repair
shops

1 Includes installation of any of the following technologies: adaptive cruise control, collision mitigation, lane departure warning, blind spot detection, intelligent lighting,
night vision, traffic sign recognition.
Source: McKinsey estimates; press

Telematics
providers

Liberty Mutual, for example, is
collaborating with Nest, a manufacturer
of smoke detectors and other connected
home products, to reduce homeowner
risk. The insurer provides Nest smoke
detectors to policyholders who agree to
let the company check every month via
wifi whether the batteries are working.
The homeowner gets discounted cover in
return. Linus Lundberg, head of enterprise
partnerships at Nest, foresees a wealth of
opportunities to build insurance products
around the many connected products that

34

The age of innovation

are emerging—a “one-plus-one-is-three
proposition” is how he describes it. “There
are products that we can provide, and a
set of insurance products, so the value
goes beyond reacting when something
bad is happening, to helping customers
prevent it from happening in the first
place.”
In time, an auto insurer might be part
of an ecosystem that includes not
just telematics providers and car
manufacturers, but also roadside

Insurance
companies

Core IT platforms/systems

Law
firms

Rental cars

Roadside
assistance

SOURCE: McKinsey

Claims solutions
and analytics

Police and
court

Payment
providers
McKinsey & Company | 2

Digital disruption in insurance: Cutting through the noise

35

expertise. They can use all three
approaches, but are likely to emphasize
one or another for strategic reasons.
Andrew Brem, Aviva’s chief digital officer,
explains it thus: “There are some things we
want to do ourselves from scratch, and we
have the capabilities, but sometimes we
take equity investments. There isn’t one
size fits all. Depending on our situation,
we will partner, we will invest, we’ll build
ourselves. And that gives us all ways to
plan.”

“We’ll see a dramatic
reduction in accidents
as real-time collision
warning and
increasing automation
come into vehicles—
by 70 or 80 percent in
the long term.”
— Stefan Heck, CEO of Nauto
Strategic partnerships
For most insurers it would be unrealistic
to pursue innovation entirely under their
own steam. Partnerships can help them
rapidly provide new types of policies or
ways of selling them, gain expertise, and
play in ecosystems beyond the insurance
industry (see “Partnerships, scale, and

36

The age of innovation

speed: The hallmarks of a successful IoT
strategy”).
Allianz, for example, has set up a joint
venture with Chinese internet giant Baidu
that enables it to use data on consumers’
online behavior to create customized
offers. If an individual orders a plane ticket,
for instance, the system will automatically
send an offer for flight insurance. This
not only gives Allianz a new way to sell
insurance, it also grants the company
access to the vast Chinese market, which
it had trouble cracking on its own. The
Baidu partnership will, says Allianz CEO
Oliver Bäte, enable the insurer to “jump the
S-curve” in China.
AIG, meanwhile, has formed strategic
partnerships with IBM and other
technology vendors to boost its expertise
in risk analytics and cybersecurity.
Insurers’ need for technology capabilities
is likely to be a prime reason for embarking
on partnerships.
Investing in start-ups
Whether through direct or venture
investment, carriers can buy into new
companies to learn more about emerging
technology and its applications. AIG,
for example, has invested in Human
Condition Safety, a provider of wearable
devices aimed at maintaining workplace
safety. Munich Re’s equipment insurance
subsidiary Hartford Steam Boiler (which
already uses drones for site inspections)
has invested in Augury, which uses
sensors and analytics to monitor heating,
ventilation, and air conditioning systems,
improving maintenance and helping to
prevent break-downs.

Some insurers are funding technology
incubators. Swiss Re, for instance,
has set up an insurtech accelerator in
Bangalore, India, to help start-ups develop
products and services. Technology under
development ranges from data analytics
for predicting health outcomes to artificial
intelligence for customer engagement.

  

Insurers not only learn about new
technologies from these investments, they
also gain exposure to more agile ways
of working. In other words, working with
start-ups helps older companies build a
digital culture. Caribou Honig, founding
partner of QED investors, which supports
high-growth, data-led businesses, believes
working with start-ups is essential to “be in
the game.”
In-house innovation factories

This is the age of digital disruption. Across
industries, insurgents with digitally
enabled business models are challenging
incumbents and their established business
models. The incumbents have a choice: be
disrupted or be the disruptors. Those that
prosper in the digital future will be those
that choose to be disruptors and invest in
innovation today.
Alex Kazaks is a partner in McKinsey’s
San Francisco office, Parker Shi is a senior
partner in the New Jersey office, and
Holger Wilms is an associate partner in the
Washington, DC, office.
The authors would like to thank Olga
Yurchenko, an engagement manager in the
Boston office, for her contribution to this
article.

Our view is that innovation is too important
to be outsourced entirely. Accordingly,
companies need to get very good at taking
ideas themselves and figuring out how to
commercialize them, roll them out on a
large scale, and integrate them with existing
processes, functions, and lines of business.
One way to improve in-house innovation
is to build dedicated labs. These units
are set up with a mandate to coordinate
the development of ideas and support
the scaling-up of the most promising
ones. AXA, MetLife, and Aviva have all
launched labs in Singapore, where the
government has backed the development
of an insurtech industry and companies
have access to the growing Asian market.
AXA is looking at innovation in data storage
and analysis, while MetLife’s LumenLab
focuses on innovations for healthy living.

Digital disruption in insurance: Cutting through the noise

37

There is plenty of talk about how digital
technology will affect consumers’ need
of insurance. The advent of autonomous
cars will reduce the requirement for auto
insurance, for example, while monitoring
by the Internet of Things will lead insurers
into businesses that help consumers
mitigate risk rather than simply protect
against it.

Capturing value from the core
Insurers’ existing customers, brands, data, and technical
skills are valuable business assets if they can be catapulted
into the digital age.

Without doubt, insurers must take a hard
look at what the future might hold and
strategize accordingly. But in the nearer
term, customers’ insurance needs will
change less radically than the ways in
which those needs can be met with digital
technology—and there is considerable
value to be had from a carrier digitizing
existing business as a result. We estimate,
for example, that a typical large auto
insurer could, over a five-year period, more
than double profitability by harnessing the
power of digital to attract and satisfy more
customers, while simultaneously cutting
operating costs and improving pricing
and underwriting accuracy. (See “Facing
digital reality” for further details of this
analysis.)
It is crucial such value is captured, as it
is only by digitizing the core that insurers
will be in a position to compete over the
long term, however the industry evolves.
Doing so will generate the funds for future
investment as well as build the skills and
capabilities that will be the hallmark of
successful carriers.
Capturing this shorter-term value is no
easy task. Existing customers, brands,

38

Capturing value from the core

data, and technical skills are valuable
business assets, but they need to be
catapulted into the digital age. That
will require the reinvention of the core
business and the rethinking of decadesold beliefs and practices, with more rigor
and determination than most insurers
have shown in the past. Simply hooking
digital assets—a digital sales channel or a
snazzy new service app—on to an analog
business model does not make a digital
business.

Existing customers,
brands, data, and
technical skills are
valuable business
assets, but they need
to be catapulted into
the digital age.
Instead, carriers need to digitally redesign
entire customer journeys—from the
moment a customer considers taking out
a new policy to the moment of purchase,
for example, or from the moment a
customer needs to make a claim to the
moment of reimbursement. That in turn
will require an integrated approach: the
digitization of customer-facing processes
and the seamless automation of back-end
ones.

Digital disruption in insurance: Cutting through the noise

39

Redesigning customer journeys
Central to capturing value from the core
business is recognizing how digital can
drive a fundamental shift in the way
companies interact with customers. No
longer do customers have to contend
with what, from their perspective, are
slow and frustrating processes defined
by a carrier’s internal functional silos
and technical limitations. Instead,
digital technology and the redesign of
customer journeys can help them to
move quickly and seamlessly across
channels and touchpoints, and deliver
personalized communications. Indeed,
digitization in other industries has led
customers to expect nothing less than
this level of ease and convenience.
The redesign of a customer journey
model has three components. The first
is design thinking—putting customers
at the center of the business and
considering how best to meet their
needs and how they interact with
the business at each stage of each
journey they embark upon. The newly
designed journey is enabled by the
second component, automation and
analytics. These are used to anticipate
customer demands, shorten waiting
times, personalize experiences, and
automate simpler customer interactions
(a small auto claim, for example),
while significantly reducing costs and
complexity for carriers. A rapid launch of
the redesigned journey is then ensured
by the third component, agile working
methods, which are deployed across
the business, not just in IT. All these

40

Capturing value from the core

components need to be addressed to
improve the underlying value levers of
the insurer’s business model.

“We’ve changed from
knowing everything
upfront to trying
and testing. Where
we used to have
everything ready and
done before we put it
into action, now we
can put it into action
and learn on the way.”
— David Stachon, CEO
of German direct insurer
CosmosDirekt
The outcome is threefold: higher
customer satisfaction, greater efficiency,
and greater effectiveness. Our work
suggests that in a claims journey for auto
insurance, for example, digitization can
raise customer satisfaction by between
10 and 15 points, improve claims
adjustment expenses by as much as 30
percent, and increase the accuracy of
payments—by cutting down on fraud,
for example—by around 4 percentage

points. (Exhibit 1 explains what underpins
these figures.) In the past, trying to pull
off this hat trick seemed an impossible
task, but not today. As Oliver Bäte, CEO of
Allianz, said in a speech recently: “We can
meet customer expectations that were too
expensive in the past. We can customize
and individualize things and we can make
them more flexible. Flexibility used to be
the opposite of efficiency, and that is the
paradigm that is disappearing because
you can offer a very efficient solution at
very low cost.”
The fundamentals of a redesign
Before considering the process for
redesigning customer journeys,
companies need to take on board the
fundamental elements that support it:
customer empathy, the marriage of form
and function, an iterative approach, and
agile, cross-functional teams.
Customer empathy. Good design is based
on an understanding not only of what
customers say they want, but of what
might go unimagined if they are bound
by the present. Henry Ford put it like this:
“If I had asked people what they wanted,
they would have said faster horses.”
What they really wanted, of course, was
something that was not just faster but
more comfortable and capable too.
It is customer empathy that enables digital
companies to move beyond incremental,
me-too improvements to drive stepchanges in customer experiences—
perhaps mapping the progress of an
incoming tow truck to relieve customer

anxiety after a car accident. The success
of many new, digital insurance companies
lies not so much in the digital tools they
deploy but in the experience those tools
enable: a faster, more transparent, and
more intuitive approach to shopping for
and servicing insurance.
The marriage of form and function.
Good design must also marry form (the
customer experience) with function (the
value to the business). The design process
needs continually to check that both
are being met. Reducing time spent on
the phone for simple sales and service
transactions is an example of where
customer and business interests meet in a
marriage of form and function.

A typical large
auto insurer could
more than double
profitability over 5
years by harnessing
the power of digital.
An iterative approach. In a digital era,
capturing value demands an iterative,
speedy development of processes and
services to keep pace with changing
technology and customer expectations. It
is important that insurers feel comfortable
with the idea of testing products and

Digital disruption in insurance: Cutting through the noise

41

Exhibit 1

Effect of digitization on customer satisfaction, efficiency and
effectiveness for an auto insurance claim

Greater efficiency

A

move from: High costs and low scalability

<10% of customers report claims online and their data has be
entered into the insurer’s systems manually by a claims handler

efficiency
BReduce
claims-adjustment

A customer satisfaction
1

Increase NPS by

expenses by

~10-15 ppt

Notification
of loss

~20-30%

Claim
management

60% of claims reported online by customers and agents with
automatic feed into insurer’s systems

B

C

40% of cases require on-site
appraisal of ~2 hours
20% of calls to call center are
for status requests

100% of payments
triggered manually

Loss assessment/
repair

Claims
settlement

30% of claims use digital tool
for remote damage
assessment or appointment
selection

50% of loss payments
triggered and executed
automatically

50% drop in status request
calls owing to use of status
messages

to: 20-30% loss-adjusted expenses reduction

effectiveness
C
Improves accuracy of claims
payments by

~4%
Greater effectiveness
A

Higher customer satisfaction
A

B

B

move from: High costs and low scalability
C

move from: Below-market satisfaction levels
C
Wait up to 5 min in claims
hotline and take ~20 minutes
to report a claim

Several touchpoints with
claims handler, claims
adjuster and repair workshop

No transparency on status
of claim

Wait up to 20 days for
processing and payout

Notification
of loss

Claim
management

Loss assessment/
repair

Claims
settlement

Report of claim online within <3
min

Receive online a choice of
options for claim

Appointment with repair
workshop/claims adjuster
scheduled online

Real-time processing and
payout of cash settlement:
settlement takes 2 hours

15% of high- severity cases
detected within 10 days

Straight-through processing
reserved for glass claims only

<5% of cases steered into
partner network

Fraud analysis conducted
sporadically at specific
touch-points

Notification
of loss

Claim
management

Loss assessment/
repair

Claims
settlement

65% of high- severity cases
detected within 10 days

50% of claims automatically
routed into target process

>50% of cases steered into
partner network

Continuous fraud monitoring

Self-service damage
assessment via app in <5 min
at any time and location

to: Increase in customer satisfaction

1

42

Full transparency on claim
status via push messages/app

to: Reduction in claims payment of up to ~4%

Net promoter score measures the loyalty between a company and its customers

Capturing value from the core

Digital disruption in insurance: Cutting through the noise

43

experiences with customers as they are
developed—rather than waiting until
they are complete—in order to obtain
immediate feedback and ensure the
solution delivered is the one customers
want. The aim is to bring a prototype,
known in venture capital and start-up
jargon as a “minimum viable product”
(MVP), to market in months rather than
years. The MVP is then refined in a series

of tests with users. It is easy to imagine
the Swiss army knife being developed in
this way—first the simple blade, then a
bottle opener, then scissors and file, and
eventually an entire suite of tools.
David Stachon, CEO of German direct
insurer CosmosDirekt, explains how
the test-and-learn approach speeds
progress. “We’ve changed from knowing

Digital distribution and claims in P&C
Lemonade, a New York-based start-up that offers insurance for renters, uses a
conversational chatbot powered by artificial intelligence to recreate the experience of
texting or messaging with an agent to deliver tailored sales recommendations, followed by
instantly issued policies. When a claim occurs, the same chatbot is used for first notice of
loss and damage assessment; in some cases it will issue payment in under three seconds.
Lemonade aims to use digital technology to improve the customer experience and keep its
expense ratio down, passing savings on to customers.

Digital distribution in individual life
Haven Life, a direct-to-consumer life insurer started by MassMutual, uses digital
technology to offer medically underwritten term life insurance quickly and at low cost. By
tapping into other data sources such as prescription history and motor vehicle records,
Haven can issue policies without asking customers to undergo unpleasant, lengthy, and
costly (for the carrier) medical tests. And by going directly to the consumer it eliminates the
distribution expenses associated with agents—often more than 100 percent of the first
year’s premium.

44

Capturing value from the core

everything upfront to trying and testing.
Where we used to have everything ready
and done before we put it into action, now
we can put it into action and learn on the
way. That’s a huge paradigm shift.” The
methodology also avoids costly mistakes,
as wrong moves can be quickly corrected
with early feedback. But insurers will need
to embrace failures in order to learn from
them, and recognize that the journey
is never really complete: it undergoes
constant iteration.
Agile, cross-functional teams. In a
rapidly changing environment, insurers’
various functions—risk, underwriting,
claims, marketing, and sales—offer deep
expertise but are often too rigidly siloed to
respond quickly. Moreover, in a functional
set-up, no one owns the full customer
experience. It can take several weeks
and many working sessions to create a
complete view of it, and still not everyone
will be committed to its improvement
given the various performance metrics
used. The solution is cross-functional
teams whose common goal is to remove
customer pain points and capture the
business opportunity.
Adopting an agile approach, these
teams work in “sprints” to meet specific,
agreed development targets week by
week, incorporate regular user feedback,
and hold daily meetings to ensure
progress is transparent and deadlines
are met. Regular review meetings
with other stakeholders from affected
business functions help identify areas
for enhancement. In this arrangement, IT

and the business work closely to splice
business and customer. IT’s role thus
becomes strategic—it is no longer a
support function. (See “IT moves center
stage” for more on this topic.)

The approach
These fundamentals are all reflected in
the redesign of a customer journey. There
are three stages in the redesign: define,
design, and deliver (Exhibit 2). The first
stage, define, is about understanding
what customers want and why, and how
the business will benefit from meeting
their expectations. For simple claims,
for example, customers might seek
assurance that their case is being fasttracked without their having to call the
adjustor to check progress; the adjustor
saves time as a result. These customer
needs are uncovered by mapping current
customer journeys and identifying
opportunities and pain points, an exercise
that can be achieved within three to five
weeks through ethnographic market
research and close customer contact. A
company might not choose to fix all the
opportunities and pain points identified,
but it does need to address the highest
priorities. This stage forms the foundation
of cross-functional collaboration that will
mark the new way of working.
Next comes the design phase. For the
time being, it ignores constraints such
as immature technology or regulatory
limitations and instead focuses exclusively
on the customer need and the business
objective (“maximum aspirational

Digital disruption in insurance: Cutting through the noise

45

Exhibit 2

Exhibit 3

Capturing value from the core by redesigning customer journeys
Define, design, and deliver the minimum viable product within six months

1. DEFINE

2. DESIGN
1 month

4 months or less

Define digital business
model and customer needs

Design a clean-sheet
customer journey. Test it
continuously

Release the MVP

Map existing customer journey
Identify the key value levers
that will define a new, digital
business model
Identify customer pain points
as well as opportunities to
delight customers

Develop a customer journey
that addresses customer and
business needs, using a
“maximum aspirational
proposition”
Test and iterate the journey
with customers
Prioritize key features for
MVP release

proposition”). To extend the claims
example, the team might decide that a
lightweight chatbot based on artificial
intelligence is the best way to accelerate
customer response times. It would test
the concept with customers, refining it
several times, and then break it apart
into a set of discrete features. Each
is assessed for feasibility (business,
technical, and regulatory) in order to
prioritize those features that can be
developed immediately (the MVP). In this
case, instead of a chatbot, adjustors might
first use a texting platform, augmented
by automatically generated status

46

Capturing value from the core

From

3. DELIVER

1 month

How current customer journeys could be redesigned

Subsequent releases build on
MVP (e.g., Version 2.0, 3.0) to
deliver additional business
and customer value

Rapidly develop MVP features
using agile methodology –
constantly test with customers
Release product and track
customer and business impact

To

auto insurance claims
Customer waits on hold to report claim
via phone

Instant claims reporting through responsive mobile
site or carrier app

Lengthy first notice of loss call to give
information on claim

Automatic data collection based on a simple Q&A,
photo upload, and sensors

No transparency on status of claim

Proactive updates sent via email, SMS, and online
messengers (e.g., WhatsApp, Facebook)

Communication with human adjustor to check
for status and share additional information via
phone and email during normal business hours

Communication with chatbot via digital channels

Payments triggered manually, and often paid
via check

Payments triggered automatically and deposited
directly into customer’s bank account

Prioritize next set of features
in a prioritized backlog
(roadmap) beyond MVP

life insurance purchasing

updates, while the tech team works on
the advanced analytics-driven chatbot for
subsequent releases.
In the third phase, deliver, the crossfunctional team embarks upon one- to
two-week development sprints with a
commitment to release the MVP to market
within three to four months. This requires
close coordination between business
and IT, often using an agile development
method that requires a strong product
“owner” who is empowered to make
decisions about the scope and form of
the solution, a “scrum master” who leads

Initiated by a cold call from an agent

Triggered direct-to-consumer based on customer’s
life event

Confusing set of complex policy variations

Simple, tailored set of product options based on
customer’s unique needs

Blood- and urine-based underwriting

Fluids-free underwriting, informed by public
and private databases (accessed with
customer’s consent)

Lengthy underwriting duration

Instant underwriting, quoting and policy issuance

homeowners’ insurance renewal
Paper-based renewal notification at time
of renewal

Proactive communication well ahead of renewal

No explanation for change in premium

Simple explanation of changes to policy and
additional cross-sell options

Phone call required to learn more or request
complex changes

Interactive, web-based process to make changes
to coverage (all pre-approved)

Manual renewal submission with a new
underwriting process

Automatic and instant renewal

Digital disruption in insurance: Cutting through the noise

47

the charge for a new way of working, a
team of four to six full-stack developers,
experience and visual designers, and
representatives from relevant business
functions. Once released, often to a
limited set of end-users at first, customer
feedback is gathered. This, along
with confirmation that the underlying
technology is stable, is used to develop
version 2.0 almost immediately with the
next set of prioritized features.

Scaling up
Almost every customer- or agent-facing
touchpoint is part of a journey that could
be digitized to some degree to make
it more satisfying for customers, more
efficient, and more effective. Exhibit
3 describes a handful of journeys ripe
for redesign, with examples of how
the customer experience could be
transformed.

aggressive target would be to redesign
25 to 35 journeys within three years,
accelerating the pace of the roll-out during
that period as the redesign approach
improves with experience.
To support the roll-out, IT infrastructure
often needs upgrading to include modern
technology stacks, cloud architecture,
automated testing, reusable application
program interfaces (APIs), and a flexible
middle layer that links customer-facing
applications to underlying systems. New
hires and new partnerships will also be
required to build the necessary skills, and
different organizational structures will
have to be considered. Some companies
choose to set up a separate division to
lead digital initiatives, believing they need
the distance, space, and a degree of
autonomy from the old business in order
to flourish.
  

The question thus becomes, which
customer journeys should be tackled
first? The choice will differ by organization,
but it is important to prioritize journeys
that will demonstrate early impact and so
gather enthusiasm and support for more
investment.
Generally, the choice and sequencing are
guided by the value likely to be captured
and the feasibility: is the IT architecture
in place, and are there enough people
with the right capabilities? This will shape
the roadmap for the coming months. An

48

Capturing value from the core

As technology evolves, so will the extent to
which customer journeys are transformed
to include an array of products and
services. Home insurers, for example,
might become part of an ecosystem
centered on a mobile app that not only
helps home buyers take out insurance,
but also values the property, predicts
variable costs such as annual energy
charges, helps the homeowner catalog
possessions against any future claim,
offers smart-home devices to monitor fire
or flood risks, and, if a problem is detected

while the home owner is away, offers to
send out an inspector. The app could even
send alerts of pending storms, advise on
precautions that might be taken to protect
the home, and offer a snow removal or
repair service once the danger passes.
Redesigning customer journeys is not
therefore simply a way of creating value
from insurers’ core business today. It
also prepares them for the future. The
cost savings the process delivers will
be essential if insurers are to compete
with low-cost digital attackers and invest
in innovative products and services.
Just as importantly, it equips today’s
insurers with the means to adapt swiftly
and continuously to changing customer
needs—whatever the shape of tomorrow’s
insurance industry.
Johannes-Tobias Lorenz is a senior
partner in McKinsey’s Düsseldorf office,
Pradip Patiath is a senior partner in
the Chicago office, and Christopher
Morrison is an associate partner in the
Boston office, where Ido Segev is a
partner.

Digital disruption in insurance: Cutting through the noise

49

Insurers have always offered “virtual”
products and based their success on a
data-driven business model. Information
technology has thus been essential to
their operations. Yet the industry has been
slow to adopt digital technology and, in
particular, to grasp the benefits arising
from the Internet of Things (IoT). If it is to
do so, it needs to put its foot firmly on the
accelerator.
The soaring number of internet-connected
devices that constitutes the IoT signals
their influence. In 2010, there were 12.4
billion. By 2025, it is estimated there will
be more than 50 billion. These devices,
equipped with sensors and activators
and attached to all manner of objects or
worn by people, can convey vast amounts
of data back to companies in real time
and enable virtually immediate analysis
and response, often without the need for
human intervention. The way companies
in many industries operate is changing
because of them.

In the energy industry, for example, the
IoT is being applied to the maintenance of
wind turbines to improve their repair speed
and reliability. In agriculture, sensors that
monitor soil humidity and trigger irrigation
are raising productivity. For insurers too,
the IoT presents an array of opportunities,
particularly in relation to the way they
interact with customers—but it also poses
a threat to existing business models. A
winning IoT strategy will depend upon the
partnerships and scale insurers can build,
and the speed at which they do so.

The emergence of ecosystems
At present, there are four primary areas
for insurers considering an IoT strategy:
connected cars,1 connected health,
connected homes, and IoT in commercial
lines. The IoT can enhance existing
business models in each and allow for
more accurate risk assessment. For
example, auto insurers used to price
1 For more detail, see “Shifting gears: Insurers adjust for
connected-car ecosystems,” McKinsey.com.

Partnerships, scale, and speed:
The hallmarks of a successful
IoT strategy

Since 2008
connected devices
have outnumbered
people1
50+ billion by 2025

The Internet of Things both promises to enhance and
threatens to undermine insurers’ business models. A threepronged strategy is needed to secure its benefits.

2005
1

50

Partnerships, scale, and speed: The hallmarks of a successful IoT strategy

2006

2007

2008

2025

Source: Statistisches Bundesamt, Deutsche Bundesbank, Prognos, Digital Sociey Study, Thomas Nipperdey, McKinsey.

Digital disruption in insurance: Cutting through the noise

51

policies on the basis of proxy variables
such as the age, residence, and credit
score of a driver. Today, they can price
on the basis of real usage and driving
behavior, such as how fast a vehicle
is being driven and whether it is being
driven at night. In a commercial setting,
insurers can now know whether a
business owner is following required
safety and maintenance procedures.

While offering
plenty of potential
to enhance the
business model,
connected devices
also challenge it.
On top of the core business of offering
insurance policies, connected devices
also give insurers the opportunity to
interact more often with their customers
and to offer new services on the basis
of data collected—a step change in an
industry where customer relationships
are often delegated to an agent or
broker, and customer touchpoints tend
to be limited to annual renewals and
occasional claims.
While offering plenty of potential to
enhance the business model, however,
connected devices also challenge it.
The auto industry—the most mature
sector in terms of its adoption of
connected devices—illustrates the

52

point. Cars are increasingly equipped
with sensors that, besides monitoring
a driver’s behavior and vehicle usage,
can collect other vehicle data such as
oil temperature, brake wear, and tire
pressure. A host of new applications
are thus enabled that meet customer
demands for convenience, safety,
and security. And as their number
grows, an ecosystem forms around the
connected car, involving automakers,
telecom companies, sensor and chip
manufacturers, digital platform giants
such as Uber, academic institutions
and standards-making bodies, and, of
course, insurers.
The emergence of this connected-car
ecosystem changes the competitive
landscape for all participants, but
particularly for insurers. Connected cars
have fewer accidents and breakdowns—
the new technology increasingly
prevents them. Hence, premiums
fall. This downtick is potentially
aggravated by significant changes in
risk distribution. Connected devices can
separate out the high-risk customers
from the lower-risk ones, so the insurer’s
focus moves to predicting and managing
individual risks rather than communities
of risk and to developing new actuarial
models. Moreover, careful drivers might
expect significant discounts on their
insurance premiums that will be difficult
to balance with price increases for
higher-risk drivers. These developments
are expected to put pressure on hitherto
stable revenue streams.
The loss of these risk-based revenues
could well be offset by the emergence of

Partnerships, scale, and speed: The hallmarks of a successful IoT strategy

new, service-based revenues, however.
Insurers could offer risk-prevention
services, alerting drivers that their car
needs a service, for example, or finding
smart parking solutions. They could
even offer proprietary data and analytics
solutions to third parties, such as media
agencies that focus on location-based
advertisements.
Yet, notwithstanding assets such as
proprietary data, long-established
customer relationships, and analytical
capabilities, insurers might not be in the
best position to tap the IoT. To access
the valuable data from sensors upon
which new, hybrid insurance models
depend, they will probably have to enter
partnerships with the companies that own
the data, such as auto manufacturers and
health equipment producers—and these
companies might have better contacts
with their customers than insurers do.
In that case, auto manufacturers that
fit monitoring devices to every car as
standard, or telecom companies that
upgrade buildings with smart home
sensors, could become gatekeepers
to insurance customers. At the same
time, companies outside the insurance
industry are building risk-related data and
analytics, alongside service capabilities.
In other words, the IoT could undermine
insurers’ two hitherto critical competitive
advantages—their underwriting skills and
their customer access.

Becoming an attractive partner
What will it take for insurers to succeed in a
connected world? Carriers should start by
asking themselves three questions. Can I

find the right partner? Can I build enough
scale? And can I move quickly enough?
The question of finding the right partner is
closely related to the question of building
sufficient scale. Any partner will be need
to be sizeable. That is because very large
amounts of sensor data will be required,
on top of the proprietary data insurers
already have, if meaningful insights are
to be extracted from it, especially to
get to sufficiently long claims histories
in order to assess risks. At present, in
respect of connected cars, for example,
many sensor systems are of limited value
because they have neither sufficient
geographic coverage nor a link to data on
actual claims frequency or severity.

Insurers need to make
themselves attractive
potential partners.
Insurers can enhance their chances of
finding the right partner by considering
carefully how they position themselves
within an IoT ecosystem. For example,
consumers are increasingly suspicious
of companies collecting their data; thus
insurers can present themselves as
trusted and reliable collaborators. They
can also highlight their capabilities in risk
assessment. Yet ultimately, the most
attractive insurers in the ecosystem will
be those keen to build risk mitigation
capabilities too, and to help provide
services such as roadside assistance and
medical assistance.

Digital disruption in insurance: Cutting through the noise

53

This leads to the third question: can I move
quickly enough? Before long, the IoT will
reach a tipping point where insurers not
yet in the game could find themselves
locked out. Unless they move fast, they
might find it hard to secure a partner with
the necessary mass of data and customer
access. An auto manufacturer might
need only one insurance partner, after all.
Similarly, in the connected home market,
those with the data are likely to be picky.
In the end, this could be a winner-takesall situation in which first movers shape
the market and sustain a competitive
advantage.

partner in the Munich office, and Anand
Rao is a digital vice president in the
Chicago office.
The authors would like to thank Simon
Behm and Thomas Schumacher for their
contributions to this article.

Insurers therefore need to make
themselves attractive potential partners.
That means defining a compelling value
proposition and building the critical
capabilities: next-generation IT that can
interact with multiple external systems,
advanced analytics that connect an
insurer’s data with insights from partners
in the various ecosystems, the ability to
integrate coverage and service solutions,
and digitally native talent experienced
in agile and test-and-learn modes of
working.
The inevitable uncertainty that still
surrounds the development of the IoT
should not prevent insurers from taking
bold, urgent action. The fast lane is the
place to be.
Markus Löffler is a senior partner in
McKinsey’s Stuttgart office, Christopher
Mokwa is an associate partner in the
Cologne office, Björn Münstermann is a

54

Partnerships, scale, and speed: The hallmarks of a successful IoT strategy

Modernizing IT for a strategic role
IT has long been seen as a cost of doing business
by insurance companies. In a digital era, it must be
modernized and recast as a strategic one.

Digital disruption in insurance: Cutting through the noise

55

Insurers’ success has always depended
upon their ability to analyze data, and
thus to price and underwrite policies
accurately. The purpose of IT has been
to support these capabilities and as such
it has been regarded as a cost of doing
business. In a digital environment, this
relationship and attitude have to change.
While the successful insurers of the future
will still excel at the analysis of large data
pools, their IT functions will move toward
playing a strategic role. In the words of
Danny Dagher, group chief information
officer of regional universal banking group
Bank Audi, “There are many insurance
companies that run IT as a support
function. [In today’s environment,] that will
kill them.”

With so much realtime data being
generated in a
connected world,
digital technology
is pushing insurers
toward new types
of business that
help consumers
mitigate risk rather
than simply protect
against it.

56

Modernizing IT for a strategic role

The reason is that technology is defining
the winning business model in insurance,
as in other industries. It has set a high
bar for service—with customers now
expecting simplicity, speed, transparency,
and customization—while reducing the
cost of that service. At the same time, with
so much real-time data being generated
in a connected world, digital technology
is pushing insurers toward new types of
business that help consumers mitigate
risk rather than simply protect against it.
If IT is to sit at the center of a new business
model, insurers will need to make two
commitments. First, they will have to
invest heavily to build IT capabilities and
modernize core platforms. For some
incumbents, that might mean as much
as 10 percent of a single year’s premiums
spread over a five-year period, depending
on the starting point and the extent of
the modernization needed. That level
of spending might be hard for some to
contemplate, not least because premiums
are destined to fall as a result of digital
technology (see “Facing digital reality”).
But they should bear in mind that wise
investments to upgrade IT can ultimately
lower their IT and operating costs relative
to those of their peers and bring efficiency
in IT to the level required in today’s market
(Exhibit 1).
Second, insurers will need to commit
to new ways of working. That means
a differentiated approach to IT
development, and a more collaborative
IT operating model that changes not only
the way business leaders work with IT,

Exhibit 1

Comparison of costs incurred by modern and legacy IT systems
Percent (average of Q1 2016 of a sample of life insurance companies)

IT costs per GWP1

Operating costs per GWP1

1.8

Legacy IT

Modernized IT

0.8

1.4

0.8

-58%

-43%

1 Gross written premium
Source: McKinsey’s insurance cost benchmark Q1 2016 Life insurance sample, expert interviews

but also how they think about IT. Without
this change, large investments could be
wasted.
An understanding of what IT needs to
deliver in a digital age reveals why these
commitments are so important.

The deliverables
There are four elements of a highperforming digital insurance business
(Exhibit 2).
A digital portfolio of products and
services within an ecosystem of partners

The products and services that digital
technology enables constantly evolve. A
leading-edge portfolio already includes
features such as dynamic pricing,
whereby prices are instantly adjusted
based on predictions relating to claims or
client churn, for example, and real-time
customization of products from a set of
both mandatory and optional product
modules, again allowing for dynamic
pricing.
New types of product will emerge as
digital technology alters the nature of the
industry. Access to ever more real-time
data, particularly via the Internet of Things,

Digital disruption in insurance: Cutting through the noise

57

A large European insurer has modularized its auto insurance to enable customers to
tailor policies to their needs—either by choosing one of three pre-defined packages or
by assembling a policy from a range of modules including roadside assistance, rental car
guarantee, and compensation for loss in value. Because the dozen or so modules are
standardized and individually priced, the straight-through processing (STP) rate for the
issuance of policies is close to 100 percent, delivering considerable cost savings, while the
average new-business premium per contract has risen by 6 percent.

means more accurate assessment of
risk, but also less risk. Sensors in the
home can warn of the danger of fire,
sensors in the car can help prevent
accidents, and sensors worn on the
body can alert physicians to health
problems. In addition, the data gives rise
to new services that can be combined
with insurance products—a medical

check-up, or an automatically triggered
appointment for repairs when a fault is
detected on a car.
Importantly, in a connected world,
insurers will need to complement
their proprietary data with data from
other industries and external sources.
For example, car manufacturers will

arguably have more insight into a driver’s
risk than insurers do once sensors
become fitted to vehicles as standard.
Insurers will therefore no longer be able
to rely solely on their underwriting skills,
but will need to partner with companies
from other industries, such as auto
manufacturers and telecoms operators,
to become part of the ecosystem forming
around the data stream, offering products
and services of which insurance is but one
component (see “Partnerships, scale, and
speed: The hallmarks of a successful IoT
strategy”).

required to implement them either in the
product system or other core systems.
IT will also need to manage quite
different relationships. Under the
traditional insurance model there was
clear differentiation between a carrier’s
customers and suppliers. But that line
is blurring as insurers increasingly offer
value-added services that are provided by
external partners, in addition to traditional
insurance products, and an ecosystem of
partners takes shape.

A Dutch insurer has partnered with a leading technology company to develop an internet
platform for the remote monitoring of chronically ill patients, aimed at containing costs and
increasing customer satisfaction by encouraging healthier lifestyles. In a similar effort, South
African insurer Discovery has developed the Vitality platform, now available globally through
partnerships—with John Hancock in the United States and AIA in Singapore and Australia,
for example. It encourages its more than 5.5 million members to lead healthier lifestyles, and
in return offers discounts on a range of products and services.

Exhibit 2

Four elements of a high-performing digital insurance business

1

Digital portfolio of
products & services with
an ecosystem of partners

Advanced analytics

3

An omnichannel
customer experience

4

Automated operations

Products are fully digital, with
dynamic pricing

Customized offerings identified to
meet customer needs

Full spectrum of digital channels in
place beyond simple website

Process landscape automated and
integrated across the organization

Modular product structure enables
real-time customization

Advanced analytics used across the
value chain to prevent high-cost cases,
identify market micro-segments, and
enable interactive and customized
underwriting

Advantages of all channels leveraged
in a targeted manner to increase sales
and retention

Response time to customers
quickened and waste and costs in
operations reduced

Value-added services offered beyond
pure insurance, such as predictive
maintenance with auto policies

58

2

Modernizing IT for a strategic role

Increased sales and retention through
an optimized channel mix

IT capabilities clearly contribute to the
value of such digital portfolios. Technically,
product systems need to be flexible
in order to map modularized product
structures, for example, and must lend
themselves to being integrated, alongside
other systems in the IT landscape, with
those of external partners to enable
joint development, testing, and release.
Organizationally, IT needs to support the
business to bring new products to the
market within weeks rather than months,
and with little or no additional IT effort

An omnichannel customer experience
Whatever products and services an
insurer offers, customers want to
access them across a range of channels
where they enjoy the same high-quality
experience that they are used to from
other industries, such as retail. And
they want to be able to switch from one
to another without the disruption of
having to repeat themselves or re-enter
data. Companies that fail to provide
this omnichannel experience will lose
customers to competitors that do.

Digital disruption in insurance: Cutting through the noise

59

Advanced analytics
A US insurer has launched a mobile app that enables customers to get an instant quotation
for auto insurance by taking a snapshot of their driver’s license, to report vehicle damage by
sending photos, and to find a service center for repairs. Claims processing time has fallen
by up to 20 percent as a result. A European insurer has launched a similar app for mobile
quotations and underwriting; cycle times for policy issuance have fallen from three weeks
to three minutes.

The implications for insurers are clear.
They need round-the-clock platforms for
all channels, with functionalities available
to customers, sales partners, and
external partners on multiple devices
and user front-ends. They need to equip
the salaried salesforce and tied agents
with mobile devices and applications
that ease the sales process with existing
and potential customers. And they
need to provide those customers with
self-service tools that enable them to
acquire real-time quotations, make
administrative alterations to policies
(such as changing an address or direct
debit information), or notify a claim.
Automated operations
The automation of processes increases
customer satisfaction while reducing
operating costs, and touches every step

in the value chain regardless of the line
of business or channel. The generation
of sales leads and the processing of
high-frequency, low-cost claims are
just two candidates ripe for automation.
Increasingly, however, insurers will need
not only to automate basic processes
further, but also to deploy robotics with
artificial intelligence and advanced
analytics to make better decisions,
faster.
Achieving a high degree of automation
requires profound changes to IT
architecture because every layer
is affected. For example, policy
administration and claims systems will
need to be overhauled, be it in response
to a higher overall level of IT intensity, the
introduction of novel robotics and script
systems, or upgraded workflow engines.

A Scandinavian insurer has rigorously automated the claims handling process. For first
notice of loss, it deploys “smart” scripts to capture the fields relevant for STP; in claims
handling it checks coverages through a rules engine and calculates costs upfront through
a data-based inspection system. In these ways, the insurer has been able to achieve STP
rates in claims of up to 30 percent in auto insurance and 60 percent in health.

60

Modernizing IT for a strategic role

Insurers increasingly employ advanced
analytics to help them make better
decisions. Some auto insurers, for
example, use credit scores to assess
risk more accurately, as analytics have
revealed that people who pay their bills on
time tend to be safer drivers. And some
life insurers are using social network and
geographical data to reduce fraud by up to
25 percent. Ultimately, advanced analytics
will become a capability that sits at the
core of the way business is conducted
across the value chain, further driving the
level of automation.

value-creating insights via predictive
models or machine learning.

A strategy for building nextgeneration IT
Delivering on all this is replete with
challenges. There is the technical
challenge of overcoming the drag of
legacy systems and the practical one of
hiring new talent—both of which may be
familiar to some insurers. Yet if the full
strategic value of IT is to be realized, new,
often unfamiliar ways of working and
thinking will be required too.

A large European insurance group has developed a statistical model to predict and reduce
customer churn. By analyzing variables such as the price paid for a policy, the percentage
price increase year on year, and how long the policy has been held, it can identify those
customers most likely to leave. It then reverse-engineers competitors’ prices and optimizes
its own prices accordingly. In addition, having identified those clients most at risk of leaving,
it is able to concentrate agents’ efforts on retaining them. As a result, renewal rates have
increased by up to 7 percentage points and bottom-line profits by as much as 5 percent.

Capturing the technology’s potential
hinges on the ability to administer and
analyze data (whether from internal
or external sources) in a consistent
manner across all channels. Both will
require significant changes to existing IT
architectures. These include establishing
a master data-management system that
gives a consolidated view of all data, in
particular customer and product data, and
the deployment of big data and advanced
analytics systems that integrate data
sources and provide platforms to generate

We see four key components of a strategy
to modernize IT for the digital age.
Systematic building of new capabilities
With most incumbent insurers, there is a
gap between the capabilities they have
and the capabilities they need. A clear plan
is required to bridge the gap, based on a
grasp of the present state and the target
state over the next three to five years.
Areas must be prioritized and initiatives
agreed.

Digital disruption in insurance: Cutting through the noise

61

Beyond exceptional general capabilities
such as fast decision-making, the ability
to learn and react, and strong central
steering, the essential qualities needed to
keep pace with digital leaders are rigorous
discipline in IT execution, world-class
agile IT engineering, a scalable cloud
infrastructure, and a single, open, and
flexible application architecture.

“Think of all the
informational assets
you have as an
insurer ... if that’s all
hard coded, and if it
takes massive capital
expense and effort in
order to make even
superficial changes
to that environment,
then you are, in a
sense, suffering from
strategic lock in.”
—Marcus Ryu, CEO of
Guidewire
Fresh talent will be required to strengthen
existing capabilities and build new ones.
Traditional methods of recruiting via
agencies, job listings, or internal referrals

62

Modernizing IT for a strategic role

might have to be augmented by searches
among developer communities, via
participation in technology conferences
and other events, or by establishing
partnerships with software providers. In
turn, developers will expect prospective
employers to check their contributions
to open code communities, not rely on
interviews.
A differentiated approach to IT
development
Digital attackers can build their IT
capabilities from scratch, aiming precisely
at specific emerging opportunities.
Incumbents have the advantage of large
policyholder books of business, but are
burdened by system environments which
were designed for traditional operating
models and are challenging to adapt
to contemporary digital preferences.
Marcus Ryu, the CEO of Guidewire, a
US software provider for P&C insurers,
describes the situation as “strategic
lock-in.”
That said, these legacy assets still have
considerable value, which needs to
be maximized. Often, the solution is a
bimodal approach comprising “digital IT”
and “foundational IT.” In respect of the
first, the innovative features and products
demanded by customers are released
quickly by replacing the waterfall method
of software development—whereby
software is developed, tested, and
deployed in a strict sequence—with agile
methods whereby teams work in sprints
to meet weekly development targets.
Features are tested with customers and
refined and refreshed in rapid iterations.
Meanwhile, the development and

management of foundational IT—the
parts that support business capabilities
requiring less agility and speed—can
be approached in a traditional, more
structured manner to ensure the stability
and reliability of systems, and cost
efficiency. Our experience is that the
division of digital IT and foundational
IT should be made according to
business capabilities and where speed
will differentiate a company. Hence
customer portals, social management,
and customer relationship management
typically belong to digital IT, while risk
management, fraud management, and
accounting belong to foundational IT with
longer release cycles.
Some believe the bimodal approach
has drawbacks, arguing that agile ways
of working should be introduced as
broadly as possible. Our view is that
insurers should indeed switch to an agile
development approach wherever they
can, but the fact remains that releases in
foundational IT domains do not need to be
as frequent as those in digital IT domains.
A bimodal approach is therefore an
effective way to ensure that investments to
accelerate IT delivery are directed where
they will be most valuable.
That said, establishing a bimodal
approach requires time, careful
consideration, and commitment because
it involves radically evolving IT, an agile
collaboration culture (see below),
modern engineering methods such as
DevOps, increased use of services and
microservices, improvements to the
organizational set-up, and the honing
of talent.

“Moving toward an
agile methodology ...
means development
times are shorter.
And it includes—this
is the important
part—getting the
business people
involved in the
development of any
new solution.”
—Tom King, senior director
at US software company
Pegasystems
Modernized core platforms
For many incumbent insurers, there
is no getting away from the need to
overhaul their core platforms. Written
using decades-old, common businessoriented language (COBOL) or PL/I,
these monolithic, batch-processing
systems usually cannot deliver the speed,
agility, and flexibility required by a digital
business. They can present difficulties in
terms of operations and scalability, and
are too costly.
These insurers are left with three choices:
build a new core insurance platform
themselves, refactor the existing one

Digital disruption in insurance: Cutting through the noise

63

(by modernizing code or streamlining
the system architecture, for example),
or replace it with standard software.
The choice will depend on a range of
considerations, including the state of the
legacy systems, the level of ambition, and
the level of resources. Those wanting
to lead in processes and product
innovation, and able to invest accordingly,
might choose a proprietary platform.
This was the choice made by a global
insurance group seeking a platform
with a common core and country- and
entity-specific customizations in order
to promote common practices globally
while maintaining local flexibility. Insurers
with relatively stable and modern systems
that need to be able to support digital
technology might choose to refactor.
And those aiming for lean, standardized
processes and products might find
standard software to be the right solution.
A Benelux insurer that found itself with a
very expensive legacy system unsuited
to digital modification, and wishing to
institute lean processes, chose standard
software as its best option.
A collaborative IT operating model
The digital operating model is defined
by agile ways of working and by
collaboration—internally across the
business and externally with partners and
vendors.
Internally, a digital-ready operating model
is one in which IT works closely with all
other parts of the business. Yet most
insurers still organize themselves around
functions such as risk, underwriting,
claims, marketing, and sales. While these

64

Modernizing IT for a strategic role

functions have deep expertise, they are
too rigid to respond to rapid change.
Moreover, in a functional set-up, no one
really understands the entire customer
experience.

For many incumbent
insurers, there is no
getting away from
the need to overhaul
their core platforms.
Cross-functional teams organized
around products solve this problem.
Their combined expertise means they are
able to deliver the products and services
customers want and at the pace required
in a digital world, particularly if team
members are located in the same place
(often digital “garages” or “factories”),
they are empowered to make their own
decisions, and the entire team (not just
the IT people) adopts an agile approach to
its work. Flexible funding—replacing the
conventional one-off, annual budgeting
process—ensures that investment is
directed incrementally at projects that
show most promise.

might be needed too. If agile models
are to succeed, vendors might need to
work differently, in closer cooperation
with insurers. Insurers are therefore likely
to have to consolidate the number of
vendors with which they work. In addition,
contracts that fix prices, scope, and
budget might need to be replaced with
contracts that reward success.
This type of operating model requires
cultural change within IT and the
businesses. Leaders in both need to
help build an understanding across
the organization of how IT can define a
product’s value to customers, and how
agile ways of working can deliver that
value. This is particularly true for intangible
insurance products.

and culture. While near-term benefits can
be captured within six to 12 months, a
wholesale upgrade to the next generation
of IT capabilities can take as long as five
years and will require a company’s full
commitment and significant investment.
The effort will bring a reward beyond lower
overall unit costs: an IT function equipped
to play the strategic role crucial to an
insurer’s success in a digital world.
Krish Krishnakanthan is a partner
in McKinsey’s New York office, Jens
Lansing is an associate partner in the
Düsseldorf office, Markus Löffler is a
senior partner in the Stuttgart office, and
Björn Münstermann is a partner in the
Munich office.

Working in cross-functional teams will
help alter thinking. But for business
leaders to contribute to the collaborative
environment, and understand the
constraints and potential of IT, some
formal training is often required. One
large European insurance group has set
up an IT literacy program to educate and
update business line managers, while all
newly appointed top business managers
must take a three-day training module to
help them understand and capture IT’s
strategic value.
  

Externally, IT must facilitate collaboration
with new partners—auto manufacturers,
telecom companies, sensor and chip
manufacturers, or digital platform giants
such as Uber—by enabling the integration
of systems and processes. Yet some
re-evaluation of existing relationships

Building next-generation IT capabilities
is no small undertaking. The process
touches all dimensions of a company’s
IT—architecture, application landscape,
infrastructure, supporting processes,
and operating model—as well as skills

Digital disruption in insurance: Cutting through the noise

65

Many have likened the revolutionary
possibilities of blockchain technology to
those of the internet, such is its perceived
capacity to transform the ways in which
people and businesses cooperate.

Investors put
more than

$800M
into blockchainrelated start-ups
between 2014 and
2015.

The promise of blockchain
Blockchain has huge potential to enhance insurers’
business model, but is also being used by digital start-ups
to attack it. Hence the imperative for incumbents to start
exploring this nascent technology.

66

The promise of blockchain

Sensing this, investors put more than
$800 million into blockchain-related startups between 2014 and 2015. Perhaps
even more indicative of its disruptive
potential, in late 2016 four European
insurance giants, Aegon, Allianz, Munich
Re, and Swiss Re, set up a combined
pilot project known as B3i to explore the
nascent technology.1

up, because as well as demonstrating
potential to enhance insurers’ current
business model, blockchain is being used
by digital start-ups to attack it.
Blockchain is a shared, public ledger of
records or transactions that is open to
inspection by every participant but not
subject to any form of central control. The
Economist newspaper has described
it as a machine for building trust.2 In
the case of the virtual currency Bitcoin,
arguably its most famous application,
it tracks transactions and facilitates
money transfer, while preventing doublespending, without the need for a bank.
But blockchain lends itself to many other
systems for keeping static records (of
land titles, for example), for registering
dynamically the exchange of assets,
and for making payments such as ticket
purchases. It is also a platform for “smart
contracts”—computer programs that
automatically initiate certain actions when
predefined conditions are met.

How it works
While blockchain technology can be used
in different ways, a blockchain solution
generally builds on four features.

The insurance industry in general,
however, lags behind other industries,
such as banking, in terms of the interest it
has so far expressed. It will have to catch

Decentralized validation. When a
transaction such as a ticket sale occurs,
new data blocks describing it are added
to a chain only after consensus is reached
among the relevant participants on the
validity of the action—for example, when

1 See also http://www.mckinsey.com/industries/financialservices/our-insights/beyond-the-hype-blockchains-incapital-markets.

2 http://www.economist.com/news/leaders/21677198technology-behind-bitcoin-could-transform-howeconomy-works-trust-machine.

Digital disruption in insurance: Cutting through the noise

67

the seller is validated as the owner of a
ticket that is sold.

smart contract, or registered data in the
blockchain.

Redundancy. The blockchain is
continuously replicated on all or at least
a group of nodes in a network. As a
result, no single point of failure exists.

Opportunities for insurers

Immutable storage. Blockchain
confounds hackers because to tamper

In a digital world,
winning companies
meet exacting
consumer needs—for
tailored products,
simplicity, and
transparency, for
example.
with data they would have to alter not
just one block in a chain but also all
successive blocks and the majority of
their replications. In addition, data is
registered in the blockchain with a digital
fingerprint that includes a date and time
stamp; any attempt to change data
would be apparent because the new
digital fingerprint would not match the
old one.
Encryption. Digital signatures based on
pairs of cryptographic private and public
keys enable network participants to
authenticate which participant owns an
asset, initiated a transaction, signed a

68

The promise of blockchain

With these characteristics, blockchain
can help address some of the key
challenges that many incumbent
insurers face in a digital age, including
the need to understand and meet
customer needs more fully and to cut
costs by making operations more
efficient. There follow some examples of
the way blockchain might be applied.
Meeting customer needs
In a digital world, winning companies
meet exacting consumer needs—for
tailored products, simplicity, and
transparency, for example. Insurers
traditionally have had little opportunity
to understand such needs, interaction
with customers being limited to buying
a policy or making a claim, processes
that might anyway be delegated to
brokers and agents. This explains both
the threat and often the success of
digital attackers that make customer
satisfaction their priority.
Blockchain can help insurers in this
both by sparing clients the frustration
of repeatedly having to provide data
for verification purposes—a copy of a
passport, for example—and by reducing
privacy concerns. No longer will it be
possible to pass that data on to a third
party without the client’s permission.
For instance, UK start-up Tradle is
working on a blockchain solution that will
enable financial institutions to conduct
the know-your-customer (KYC) checks

required by regulators to prevent money
laundering—a process that is otherwise
expensive and time-consuming for
institutions and annoying for clients if they
have to offer up the same information
about their identity and source of wealth
to different institutions. Once KYC data is
verified, the customer can use a private
key to grant companies in the network
access to the encrypted data whenever it
is needed.

process of verifying the delay can be a lot
of effort for relatively little reward.)

In addition, blockchain provides greater
transparency and hence perceived
fairness in respect of tariffs and claims
handling. Another UK start-up, InsurETH,
is working on a peer-to-peer flight

Fraud prevention

An estimated

5-10%
of all insurance
claims are
fraudulent.
insurance policy built on blockchain with
smart contracts. The contracts initiate
payouts to the holders of insured tickets
when cancellations or delays are reported
from verified flight data sources, making
the claims and payments process quick
and easy. (Although many travelers could
claim compensation for flight delays under
their usual insurance, few do so as the

Similarly, smart contracts could trigger
the claims and payments processes for
damage caused in the home or to a car
and detected and verified by sensors
linked to the Internet of Things, doing
away with quibbling about the causes
of damage and phone calls to chase the
progress of a claim.

An estimated 5 to 10 percent of all
insurance claims are fraudulent, costing
US non-health insurers more than $40
billion a year according to the FBI. By
serving as a cross-industry, distributed
registry of external and customer data,
blockchain can be used to identify fraud.
It can, for example, expose falsified
damage or theft reports by validating the
authenticity, ownership, and provenance
of goods, authenticating documents such
as medical reports, checking police theft
reports and claims histories, and verifying
identities.
It is clear that extensive cooperation
between insurers, manufacturers,
customers, and other parties will be
needed to unlock Blockchain’s full
potential. Blockverify, a UK start-up, is
building a system that will enable users
to check for fraudulent transactions,
counterfeiting, or theft relating to
goods such as personal electronics,
pharmaceuticals, and luxury items. It
works by labeling products and then
storing their history and supply chain

Digital disruption in insurance: Cutting through the noise

69

activity in a blockchain. Everledger,
also based in the United Kingdom, has
devised a similar application, used to verify
diamonds and transactions relating to
them, and targeted at helping insurers, law
enforcers, and those in the diamond trade
to detect fraud.
Efficiency
Underlying many of these use cases is
another clear opportunity for insurers—to
reduce operational and administrative
costs. Automated verification of
policyholders’ identity and contract
validity, the auditable registration of
claims and data from third parties such
as doctors, the underwriting of smart
contracts, and the automation of claims
procedures all reduce costs while
speeding up processes.
The lower handling costs of a smart
contract could feasibly help open up new
growth markets. In emerging markets,
blockchain and smart contracts could be
used to offer micro-insurance to farmers,
for example, triggering payments to them
when drought conditions are verified by
a reliable meteorological source. And
insurers could potentially save the many
millions currently spent chasing down
fraud.

The way ahead
Blockchain clearly facilitates innovative
business models and promises cost
advantages to insurance companies and
their customers. Various barriers impede
its widespread adoption, however.
Scalability is the first challenge. The
technology’s consensus-based

70

The promise of blockchain

validation mechanism, its continuous
replication, and the ever-growing amount
of stored data means that the larger the
blockchain grows, the greater become the
requirements for storage, bandwidth, and
computational power. That leads to a risk
of centralization if the blockchain becomes
so large that only a few nodes are able to
process a block.

The lower handling
costs of a smart
contract could feasibly
help open up new
growth markets.
Second, recent incidents have shown
that for all blockchain’s security attributes,
it is not impregnable. For example,
hackers stole $65 million from Bitfinex,
a cryptocurrency exchange. Such
threats are not as well understood as
those related to conventional database
architectures.
Standardization is a third challenge. To
realize sustainable benefits from an open
or partially shared and distributed system,
some standardization will be necessary.
The current absence of industry
standards—which the B3i project is
seeking to address—reflects the newness
of the technology. A distributed system
that sometimes depends on collaboration
between competitors, suppliers, and
others will take time to evolve. So will
the resolution of legal and regulatory
issues. Thus there is a high risk of initiating

inefficient solutions, and investment
decisions will need to be taken carefully.
But the obstacles should not deter
insurers given that new companies are
rapidly embracing the technology and its
cost advantages. At their core, insurance
companies collect premiums, pool the
money, and reassign it to those with a
valid claim. Blockchain means all this can
now be automated and today’s insurers
potentially disintermediated—by the likes
of InsurETH, for example, or Dynamis,
a start-up that is using smart contracts
to offer peer-to-peer supplementary
unemployment insurance. In the latter
case, it is other policyholders on the
network who validate both the application
for insurance and the claim, using social
media.

involved in a transaction then insurers’
current transaction models are likely
to suffice. Moreover, it is unlikely to be
beneficial if no intermediary is needed,
or a trusted one already exists. But in
transactions involving multiple parties,
perhaps with competing incentives, where
an iron-clad record of data is needed, and
no central trusted authority is available
or needed—then blockchain technology
holds out huge promise, which insurers
would be wise to explore.

Matt Higginson is a partner in
McKinsey’s New York office, JohannesTobias Lorenz is a senior partner in the
Düsseldorf office, Peter Braad Olesen is
an associate partner in the Copenhagen
office, and Björn Münstermann is a
partner in the Munich office.

These examples pose no immediate great
threat to incumbents’ business. But they
should alert incumbents to blockchain’s
disruptive potential, and to the need
for them to help shape the blockchain
insurance ecosystem. The starting point
is to develop a thorough understanding
of how the technology can address
customers’ needs as well as their own,
and to identify potential applications.
That will mean working with consortia,
technology experts and start-ups,
regulators, and other market participants
to address the challenges. Incumbents
can learn from the start-ups and might
consider partnering with or acquiring
companies that are entering the insurance
market with blockchain-based products
and processes.
For the time being, it is important to bear
in mind what blockchain can and cannot
facilitate. If a limited number of parties are

Digital disruption in insurance: Cutting through the noise

71

The use of data and analytics to
underwrite risk is nothing new for
insurance carriers. Yet in a digital world,
it is revolutionizing their business.
An industry in which 80 percent of all
auto insurance claims are adjudicated
automatically, and 80 percent of all life
insurance policies are issued straight
through without requiring any of the
usual health checks, is no distant pipe
dream. Neither is one in which the cost
of acquiring a customer falls by as much
as 70 percent because of precision
marketing and personalization. Such is the
power of analytics.

The advance of analytics
Harnessing the potential of burgeoning data and
computer power to add value must become ingrained in
insurers’ every activity.

72

The advance of analytics

The convergence of several technology
trends is behind this revolution. The
volume of data continues to double
every three years as information pours
in from digital platforms, wireless
sensors, virtual reality applications, and
billions of mobile phones. Data storage
capacity has increased, while its cost
has plummeted. And data scientists now
have unprecedented computing power
at their disposal, giving birth to ever more
sophisticated algorithms. As a result
machine and deep learning are on the
horizon (see box, “Analyzing analytics”).
“We’re moving from computer science,
where computer coders write very explicit,
line-by-line instructions, toward starting to
train machines to look for information that
could be valuable,” says Scott Simony,
head of industry at Google.
Yet data and technology alone do not
deliver value, as too many companies
have discovered to their cost. While some
are seeing good results, others admit they

have seen little effect to date from their
investments in analytics.1

We only have to
glance at other
industries to
understand how
powerful new
competitors with
large customer bases
can rapidly invade
other sectors.
It is important that this changes quickly,
as those slow to adopt the technology at
scale will surely struggle to compete. They
will struggle against other insurers that use
analytics to improve their core business
by streamlining internal processes, raising
revenue and cutting costs in the process.
And they will struggle in the longer term
as data and its analysis begin to break
down business models and industry
boundaries. In personal auto insurance,
we can already see how data from sensors
fitted to vehicles will put premiums under
pressure as driving becomes safer. And
we only have to glance at other industries
to understand how, in a world in which
data and analytics are king, powerful new
competitors with large customer bases for
1 See “The age of analytics: Competing in a data-driven
world,” McKinsey.com.

Digital disruption in insurance: Cutting through the noise

73

Exhibit

their core businesses can rapidly invade
other sectors. Chinese e-commerce
giant Alibaba also owns one of the world’s
largest technology finance companies,
which include among its services
insurance.

Analytics at work in claims management
1

Case evaluation

overview
Advanced analytics enables carriers to use historical

analytics use cases

main impact

Fraud prediction

Claims costs

Total loss prediction

Handling costs

data to create robust data sets
The data sets identify patterns in case characteristics
(accident details, vehicle type, presence of bodily
injury) that offer predictive markers that can be used to

Litigation prediction

Claims costs

Severity prediction for BI cases

Claims costs

identify similar cases
Once these markers have been defined, carriers can
apply them to incoming cases to guide handling

2 Case segmentation
overview

analytics use cases

Using the insights from case evaluation, carriers can

Identification of straightthrough processing cases

Handling costs

Assignment of cases to
handler units

Handling costs

determine a subset of cases ideal for straight-through

main impact

processing vs. those that require specialized processing.
Those requiring specialized attention are assigned to
claims handlers (for example, by complexity)

3 Case management
overview

analytics use cases

The outputs of case evaluation also enable

Prediction of success in
case steering

Claims costs

Medical treatment

Claims costs

Reserve prediction

Claims costs

automation of case steering, so that carriers can

main impact

direct customers to preferred garage networks to
repair their vehicle, for example, or use an app to
make self-service claims

74

The advance of analytics

Here then, is how companies can move
quickly to build their analytics muscle
across the organization, avoiding
common problems and ensuring their
investments translate into business value.
There are four phases.

Phase one: Building insights
The starting point is to be clear about how
analytics can deliver insights and add
value, and choose the use cases that will
demonstrate this. Too often, companies
give scant thought to the business
problem they are trying to solve, instead
getting carried away with refining data,
gleaning perfect insights, or investing
heavily in technology infrastructure. The
exhibit shows how analytics can be put to
work in claims management.
It is also important to understand what
analytics can and cannot do. It cannot, for
example, predict outcomes with pinpoint
accuracy, particularly in low-frequency,
high-severity, or shock-prone lines of
business. For instance, the market for
directors and officers liability insurance
endured waves of litigation over the past
decade—and subsequent spikes in
claims—resulting from events such as
the financial crisis and new regulations
governing options backdating. It would
have been difficult to predict any of these
events with analytics.

Some life insurers
are using social
network and
geographical data to
reduce fraud by up
to 25 percent.
But their use can significantly improve
predictive capabilities, unearthing insights
upon which carriers can act. Some auto
insurers now use credit scores to assess
risk more accurately, analytics having
revealed that people who pay their bills
on time tend to be safer drivers. Some
life insurers are using social network and
geographical data to reduce fraud by up
to 25 percent. And some companies are
using data on insurance agents—their
behavior, previous sales, regional location,
and training undertaken—to predict how
likely each one is to sell multiple products,
and which specific products they would
be most successful at selling, leading to
a 20 to 25 percent increase in sales. As
machine learning technology develops,
it will be applied not only to predicting
events and forecasting outcomes, but
also to classification (including identifying
images or making associations between
data) and generation (from interpolating
missing data to generating the next frame
in a video sequence, for example).
Mining internal and external data
In some cases, organizations struggle to
develop convincing use cases because
data quality is poor. Many cannot yet

Digital disruption in insurance: Cutting through the noise

75

master their internal data, which remains
disaggregated, unstructured, and
generally underused, requiring substantial
effort to be brought into working condition.

Leading
organizations find
ways to make sure
the businesses
work alongside the
analytics function,
and involve top
management.

changes to IT architectures are likely to
be required. These include establishing
a master data-management system that
gives a consolidated view of all data, in
particular customer and product data, and
the deployment of big data and analytics
systems that integrate data sources and
provide platforms to generate valuecreating insights via predictive models or
machine learning.

Phase 2: Capturing value
Here the focus shifts from proof of
concept to adoption, the goal being for the
businesses to lead demand for analytics.
That is unlikely to happen unless the
front line is involved from the outset and
performance measurements are chosen
carefully.
Involving the front line

Accomplishing this should perhaps be a
priority before a company begins mining
external data. An additional challenge
is to collect, integrate, and analyze
unstructured data such as web content,
network data, images, text, and audio and
video recordings.
Many incumbents struggle with switching
from legacy data systems to a nimbler
and more flexible architecture to store and
harness big data (whether from internal
or external sources). But capturing the
potential of analytics hinges on it. At the
outset, companies should bear in mind
the business case they are making,
and that the very latest technology and
significant upfront investment are not
always needed. Before long, though,

76

The advance of analytics

When companies falter in their use of
analytics it is often because the old way of
working still prevails: that is, build a model
(often based on unclear assumptions
about the variables that have most
predictive impact on the outcome) and
roll it out, regardless of whether people on
the front line understand precisely how to
apply it. They might not know, for example,
whether the model’s recommendation is
binding or if there is flexibility to deviate
from it. Not surprisingly, efforts at adoption
can meet resistance.
Instead, front-line employees need to be
involved at each stage of the development
process, from establishing the business
case to deciding what data to draw upon,

Analyzing analytics
Analytics has emerged from four trends. First is the exponential growth in data that a digital
world enables, including structured data that is machine readable and easily loaded into
databases and queried, and unstructured data such as video, text, social media, and
employee emails that is harder to collect, analyze, and process. In the past 18 months
alone, more data has been generated globally than in the entire previous history of mankind.
In the next five years, the amount generated will be three times more than has been
cumulatively generated to date.
The second trend relates to revolutionary advances in computer technology and to
analytics techniques, such as machine learning, that rely on automated, computer
program-driven pattern recognition. These techniques are far more predictive than
generalized linear modeling. With machine learning, algorithms “learn” from data and adapt
to new circumstances without being explicitly reprogrammed. The concept is to give the
algorithm “experiences” (training data) and a generalized strategy for learning, then let the
algorithm identify patterns, associations, and insights from the data—in short, to train the
system rather than program it.
Deep learning, a frontier area of research within machine learning, uses neural networks
with many layers (hence the label “deep”) to push the boundaries of machine capabilities.
Data scientists working in this field have recently made breakthroughs that enable
machines to recognize objects and faces, to beat humans in challenging games such
as chess and Go, and even to generate natural language. Digital giants such as Google,
Facebook, Intel, and Baidu, as well as industrial companies such as GE, are leading the
way in these innovations, seeing machine learning as fundamental to their core business
and strategy.
The third trend is the shift from batch processing to real-time processing, monitoring, and
visualization of data feeds. This trend will continue to change the behavior of the insured
and affect the operations of many core insurance functions such as underwriting and
pricing, claims, billing, and customer relationship management.
Finally, flowing from all this, is a complex ecosystem of new analytics vendors and solutions
that enable carriers to combine data sources, external insights, and advanced modeling
techniques in order to glean insights that were not possible before.

Digital disruption in insurance: Cutting through the noise

77

how to integrate the output into working
patterns, and what new skills might be
needed. One large insurance carrier saw
a 30 percent increase in adoption rates
when front-line employees joined a crossfunctional team engaged in defining use
cases. They participated in workshops
to define hypotheses on the variables
with most predictive power, worked on
understanding and refining modeling
output, and finally integrated the output
with the business process.
The integration element is particularly
important and often particularly
challenging, given that it involves a
significant shift of mind-set away from
the old method of working. How will data
that reveals insights be presented? It is no
point sending quantities of it to the person
required to use it. Carriers will need to be
creative so that data is in a form that is
self-explanatory and prescriptive. It is also
important that analytics becomes part
of the work process, rather than being
an additional, separate task that busy
people are unlikely to complete. Better
that it be integrated directly into core tools
being used for, say, customer relationship
management and pricing.
Performance management
Early on, organizations are
understandably keen to see a return on
their investments. But too much focus on
certain metrics can impede progress. It is
hard, for example, to isolate the financial
impact of an analytics initiative from
that of other business initiatives such as
efforts to improve customer retention
based on digital marketing or strategic

78

The advance of analytics

projects—and trying to do so can become
an exercise in false precision. Diligently
tracking the impact of use cases in terms
of their adoption and satisfaction might
prove a better measure of early progress,
as well as an indication of when version 2.0
or 3.0 is needed. Comparing outcomes for
those who use the new models and those
who do not is also a helpful gauge.

The end-state is one
in which analytics
shifts from being
regarded as a business
aid to being seen as
a capability that sits
at the core of the way
business is conducted.
Phase 3: Achieving scale
The application of analytics often begins
within the pricing and underwriting
functions. Employees here are relatively
accustomed to modeling and datadriven analyses, and the potential to
improve previous practices should be
clear—be it by finding new variables,
exploring new modeling techniques, or
further automating processes. Eventually,
however, it needs to be deployed in all
businesses and functions. To reach that
point efficiently, leading organizations
use heat maps that indicate where to

prioritize efforts. They also find ways to
make sure the businesses work alongside
the analytics function, and involve top
management.

companies need a CoE with teeth to come
up with ideas and recommendations,
as well as businesses and domains that
shape and approve the CoE’s agenda and
the costs allocated to it.

Prioritization
Direct involvement of top management
The heat map should be drawn up on
the basis of three dimensions: the value
that analytics can deliver, their feasibility
(drawing on a large number of different
systems to collect data will make it
harder to capture value from a use case,
for example), and strategic relevance.
Importantly, the map needs to be updated
at least once a year to align with changing
strategic priorities and feasibility based on
the technology and data lessons learned
in the previous year.
Balancing business engagement with a
strong analytics function
As carriers master the execution of
use cases, so a permanent center of
excellence (CoE) needs to take shape
to support the businesses. Carriers can
wrestle with how best to position the CoE.
Should it be autonomous with its own
reporting and profit-and-loss statements?
Or should it function as an on-demand
resource? The advantage of the former is
that the CoE is likely to be more proactive
in developing analytics initiatives across
the organization and more accountable
for their success. The latter has the
advantage of more closely aligning the
CoE with the businesses’ agenda.
The best approach probably lies
somewhere between the two, making
sure there is strong business and analytics
leadership. Whatever structure chosen,

As the CoE scales up, senior management
needs to make clear that analytics
is a corporate priority, paying close
attention to the portfolio of initiatives and
understanding how it will achieve impact.
To promote take-up, executives can
encourage line leaders to contribute to
the pipeline of analytics ideas as part of
the annual planning process. And, while
understanding that returns on investment
might not be obvious within the first
few quarters, executives can highlight

An industry in which
80% of all auto
insurance claims
are adjudicated
automatically, and
80% of all life
insurance policies
are issued straight
through without
requiring any of the
usual health checks,
is no distant pipe
dream.

Digital disruption in insurance: Cutting through the noise

79

quick wins and celebrate successes
that will prove the concept and maintain
momentum.

Phase 4: The analytics-driven
organization
The end-state is one in which analytics
shifts from being regarded as a business
aid to being seen as a capability that sits at
the core of the way business is conducted.
Indeed, it will become so ingrained in
daily work practices that the CoE is made
redundant. Various functions—claims,
distribution, underwriting—might still
exist, since the practical activities and
the skills required for them differ. But the
core decision-making and the analytics
engine that supports decisions are likely
to converge at a single point. When
that point is reached, all business and
strategy decisions are made with data and
analytics at their center.
At this stage it will make no sense
to measure success by returns on
investment. The business metrics
themselves become the markers of
success, be it price adequacy or loss,
expense and combined ratios, or the
quality of new-business growth. In
addition, analytics will firmly shape the
organization’s talent strategy, becoming
an integral part of multiple roles.

and the use cases emerging dictate
that gradual improvement is no longer
an option. Analytics will soon become
a core corporate capability, and those
carriers that leap ahead and bring it to
insurance are likely to capture an unrivaled
competitive advantage.

Ramnath Balasubramanian is a partner
in McKinsey’s New York office, where
Khushpreet Kaur is an associate partner
and Ari Libarikian is a senior partner.
Paolo Moretti is a senior partner in the
Milan office.

The value of robotic process
automation: An interview with
Professor Leslie Willcocks

  

While most carriers have taken up
analytics, they have barely begun to tap its
potential. Yet the intensity of competition

80

The advance of analytics

The professor of technology, work, and globalization at the
London School of Economics’ Department of Management
talks about robotic process automation—its impact on
work, the strategic and financial benefits, and how to
capture them.

Digital disruption in insurance: Cutting through the noise

81

McKinsey: Can you start by defining
robotic process automation (RPA)?
Leslie Willcocks: RPA takes the
robot out of the human. The average
knowledge worker employed on a back
office process has a lot of repetitive,
routine tasks that are dreary and
uninteresting. RPA is a type of software
that mimics the activity of a human being
in carrying out a task within a process.
It can do repetitive stuff more quickly,
accurately, and tirelessly than humans,
freeing them to do other tasks requiring
human strengths such as emotional
intelligence, reasoning, judgement, and
interaction with the customer.
There are four streams of RPA. The first
is a highly customized software that will
work only with certain types of process
in, say, accounting and finance. The
more general streams I describe in terms
of a three-lane motorway. The slow lane
is what we call screen scraping or web
scraping. A user might be collecting data,
synthesizing it, and putting it into some
sort of document on a desktop. You
automate as much of that as possible.
The second lane in terms of power is a
self-development kit where a template
is provided and specialist programmers
design the robot. That’s usually
customized for a specific organization.
The fast lane is enterprise/enterprisesafe software that can be scaled and is
reusable.
You can multi-skill each piece of
software. It’s lightweight in the sense that
you don’t need a lot of IT involvement
to get it up and running. Business
operations people can learn quite

82

quickly how to configure and apply the
robots. It’s lightweight also in that it
only addresses the presentation layer
of information systems. It doesn’t have
to address the business logic of the
underlying system or the data access
layer.

One major benefit of
RPA is “a return on
investment that varies
between 30 and as
much as 200 percent
in the first year.”
McKinsey: How is RPA different from
cognitive intelligence?
Leslie Willcocks: RPA deals with
simpler types of task. It takes away mainly
physical tasks that don’t need knowledge,
understanding, or insight—the tasks
that can be done by codifying rules and
instructing the computer or the software
to act. With cognitive automation, you
impinge upon the knowledge base that
a human being has and other human
attributes beyond the physical ability to
do something. Cognitive automation can
deal with natural language, reasoning,
judgement, with establishing context,
possibly with establishing the meaning of
things and providing insights. So there is a
big difference between the two.
In addition, whereas RPA is pretty ripe as
a technology, cognitive automation isn’t.

The value of robotic process automation: An interview with Professor Leslie Willcocks

I’ve not seen a wave of powerful cognitive
automation tools appear in the market and
not many companies are using them yet.

McKinsey: What are the business
benefits of RPA?
Leslie Willcocks: The major benefit we
found in the 16 case studies we undertook
is a return on investment that varies
between 30 and as much as 200 percent
in the first year. But it’s wrong to look just at
the short-term financial gains—particularly
if those are simply a result of labor savings.
That approach does not do justice to the
power of the software because there are
multiple business benefits.
For example, companies in highly
regulated industries such as insurance
and banking are finding that automation is
a cheap and fast way of applying superior
capability to the problem of compliance.
You also get better customer service
because you’ve got more power in the
process. A company that receives lots
of customer enquiries, for example, can
free staff to deal with the more complex
questions.
There are benefits for employees, too.
In every case we looked at, people
welcomed the technology because
they hated the tasks that the machines
now do and it relieved them of the rising
pressure of work. Every organization we
have studied reports that it is dealing with
bigger workloads. I think there will be an
exponential amount of work to match
the exponential increase in data—50
percent more each year. There is also
a massive increase in audit regulation
and bureaucracy. We need automation

just to relieve the stress that creates
in organizations. One online retailer
measures the success of RPA in terms
of the number of hours given back to the
business. So it’s not just the shareholders,
the senior managers, and the customers
who benefit but also employees.

McKinsey: Can you describe a process
where you have seen RPA in action?

To get started with
RPA, “you have to
pick the right process.
It has to be stable,
mature, optimized,
rules-based,
repetitive, and usually
high-volume.”
Leslie Willcocks: In an insurer we
studied, there was a particular process
where it used to take two days to handle
500 premium advice notes. It now takes
30 minutes. It worked like this: a range of
brokers would write business for clients,
and there was a central repository into
which the business written had to go, and
a process that someone had to manage
to get the premium advice note from the
broker into the repository. A number of
operations had to occur for that advice
note to be fully populated by all the data,
and the process operator might find that
the data had not been completely filled
out, perhaps because the advice note

Digital disruption in insurance: Cutting through the noise

83

wasn’t structured very well. So the data
had to be structured to standardize it so
that it could be a common document
like all the other advice notes. And if any
data was missing, that person might
have had to go back to the broker, or add
things from the systems of record in the
back office. Then, once the note was
complete and signed off by the process
operator, it went into the repository.

“In an insurer we
studied, there was
a particular process
where it used to take
two days to handle
500 premium advice
notes. It now takes
30 minutes.”
Now a lot of that sort of work can be
automated. But some of it requires
human intervention, human reasoning,
judgement. So an RPA engineer would
look at that type of process and say,
“Which bit can we automate?” The
answer is not everything—it can’t
structure the data. There may at
some stage be cognitive automation
technology that could structure the data
but RPA can’t, so the human being has
to structure the data at the front end
and create a pro forma ideal advice
note. Clearly, the RPA can’t deal with
exceptions either. The engineer has to
intervene and look at the exceptions and
create a rule to deal with them, so that

84

gradually you educate and configure
the RPA to do more and more work.
Eventually it can do 90 or 95 percent of
the work and very few exceptions have
to be dealt with by a human.

McKinsey: What are the most
important considerations for those
wishing to adopt RPA?
Leslie Willcocks: The most important
consideration is strategy. You can use
automation tactically for cost savings.
But if you use RPA as a broader strategic
tool, you get a lot more out of it. That’s
number one. Number two concerns
the launch. You need to get the C-suite
involved and appoint a really good
project champion, and you have to
pick the right process. It has to be
stable, mature, optimized, rules-based,
repetitive, and usually high-volume. Start
with a controlled experiment on a visible
bottleneck or pain point.
The third consideration is change
management—persuading the
organization to change and adopt
automation. It is a key issue from the
outset. And the fourth is building a
mature enterprise capability for RPA.
Long-term users have built centers of
excellence over time, usually within
business operations, and developed
skills and capabilities within that center.
They have people who assess the
feasibility of a proposal from a business
unit. They have people who configure
a robot, install it, and develop it, and
controllers who switch it on and off,
and plan its work and how it fits with
human work. They have some sort of
continuous improvement capability and
relationships with IT, governance, and

The value of robotic process automation: An interview with Professor Leslie Willcocks

security. Organizations signing up to RPA
now should probably think about building
a center of excellence immediately.

McKinsey: How do companies choose
whether to implement an IT solution or
RPA? And how do the two departments
work together?
Leslie Willcocks: When organizations
consider proof of concept for RPA, they
look at the business case and compare
it to an IT solution. Often that’s pretty
unflattering for IT. In one organization we
looked at, the return on investment for
RPA was about 200 percent in the first
year and they could implement it within
three months. The IT solution did the same
thing but with a three-year payback period
and it was going to take nine months to
implement.

“In the longer term,
RPA means people
will have more
interesting work.
For 130 years we’ve
been making jobs
uninteresting and
deskilled.”

tool is usable, cheap, and doesn’t require
much IT skill to implement it’s a no-brainer
for the average operator in a business unit.
The reason IT gets worried is that they
know the disruptive, potentially disastrous
effects of people playing around with IT in
the organization and not understanding
how it’s going to upset infrastructure,
governance, security, and all the important
touchpoints that IT is held responsible for.
So it’s not surprising to find IT functions
in denial about RPA and what it can do.
It’s crucial therefore that IT is brought on
board early.

McKinsey: What do you think will be
the long-term impact of robotic process
automation?
Leslie Willcocks: In the longer term, RPA
means people will have more interesting
work. For 130 years we’ve been making
jobs uninteresting and deskilled. The
evidence is that it’s not whole jobs that
will be lost but parts of jobs, and you can
reassemble work into different types of
job. It will be disruptive but organizations
should be able to absorb that level of
change. The relationship between
technology and people has to change in
the future for the better and I think RPA
is one of the great tools to enable that
change.

Professor Leslie Willcocks was
speaking to Xavier Lhuer, an associate
partner in McKinsey’s London office.

In addition, many business operations find
going through IT frustrating because it’s so
busy. Often the business wants something
relatively small, but the IT function has
bigger fish to fry and the business has to
go to the back of the queue. So if an RPA

Digital disruption in insurance: Cutting through the noise

85

Introduction
Few CEOs need convincing that a digitally
enabled transformation of their companies
is the path to lower costs, growth, and
perhaps even survival as technology
and changing customer expectations
usher in new competitors, new value
drivers, and new business models. Nor
do they need telling that at the heart of a
digital transformation lies a cultural one,
equipping them to support new ways of
thinking and working. Rare is the CEO
who does not have cultural change high
on his or her agenda. But making that
change can seem a daunting task. Indeed,
McKinsey research has shown that 46
percent of financial services executives
feel cultural or behavioral change is the
biggest challenge they face in pursuing
their digital strategies.
Perhaps not surprisingly then, insurers
scored poorly when we measured their
cultural preparedness for a digital world
(see “Measuring your digital maturity”).

Building momentum for cultural
change
Being told to abandon old ways of thinking and working
and embrace without delay a new, and seemingly riskier,
digital culture can be unnerving for insurance companies.
But there are certain actions insurers can take to kick-start
change while minimizing the risks—and they do not have to
alter everything at the same pace.

86

Building momentum for cultural change

Cultural change is of course hard for any
long-established organization. And so it
is with insurers, the largest of which often
have a century-old record of creating
value for policyholders and shareholders.1
Unlike digital newcomers to the industry
that are building up a new business,
incumbents suspect change might
undermine the health of their existing one.
But beyond a general reluctance to
tamper with approaches that have served

1 Average age of the top ten P&C and top ten life insurance
companies in the United States based on 2015 premiums,
SNL Financial.

them well, there are more specific reasons
why cultural change can be particularly
hard for insurers to contemplate. To begin
with, the industry is highly regulated,
making insurers extremely cautious about
changing the way they work. There are
also certain aspects of a digital culture
that seem designed to undermine the
very things that have made insurance
companies so successful in the past.

“The companies that
will stand out are the
ones that are going to
find ways to move a
bit faster, at the pace
of the people they’re
insuring.”
— Scott Simony, head of
industry, Google
For example, a digital culture demands
an unswerving focus on customer needs.
And while there are exceptions, most
insurers have built their success on the
products they offer and their underwriting
skills, and by focusing on agent and
broker relationships—not customers. A
change of focus will therefore be hard
not only culturally, but also operationally:
administrative systems that are built

Digital disruption in insurance: Cutting through the noise

87

around policies rather than customers
will need to be reconfigured, for instance.
And disturbing the long-established
intermediated distribution system carries
risks when 84 percent of sales in US
P&C and 90 percent of US life policies go
through agents or brokers.2, 3

25%

~

of people who shop for
auto insurance in the US
buy online directly from
the carrier.
Another digital mantra is experimentation
with new products and services—
requiring an ability to test and learn quickly
and a willingness to fail sometimes in order
to keep pace with market change. But the
idea of experimenting can make insurers
feel distinctly uncomfortable. They
spend a great deal of time meticulously
planning to ensure nothing they do
falls foul of regulatory or compliance
requirements, while the job of actuaries
is to be absolutely certain about the
carrier’s predicted losses. Will a new
culture that demands more speed and

2 Market Share Report, Independent Insurance Agents and
Brokers of America, 2016.
3 Fritz Nauck, Kia Javanmardian, Brad Mendelson, Jonathan
Godsall, “Rethinking U.S. Life Insurance Distribution,”
McKinsey & Company, May 2016.

88

Building momentum for cultural change

experimentation put their value and
brands at risk?

Personal lines insurance has felt the
greatest impact from digital technology.
About 25 percent of people who
shop for auto insurance in the United
States, for example, buy online directly
from the carrier,4 with several direct
underwriters enjoying high growth and
profitability as a result. In the United
States, carriers that mostly sell directly
have the lowest combined ratios (losses
and loss-adjustment expenses divided

Of course, fear of change is no reason for
maintaining the status quo; history is full
of the corpses of companies that failed
to keep ahead of industry disruption.
Moreover, building a digital culture does
not mean destroying the skills and values
that have sustained the company. Rather,
it is about renewing that heritage with new
ways of thinking and working.
In addition, not everything has to alter
at the same pace. It is important to
distinguish between those segments of
the industry that are being transformed
quickly due to digital technology, where
cultural adjustment is thus urgent, and
those where change is slower. With these
parameters drawn, cultural shifts become
a less unnerving prospect. We do not
pretend there is an obstacle-free method
to instilling new ways of working and
thinking, and a digital culture will need to
take hold across the entire organization
before long. Nevertheless, certain actions
can kick-start change, and build support
and momentum for more.

by earned premiums) and enjoy some
of the highest growth in direct written
premiums (Exhibit 1). Arguably, their
success stems from their digital culture:
they have moved swiftly to embrace
technological innovation and focus on
changing customer needs. The outcome
is a high level of automation that enables
them to cut costs and price keenly, and a
determination to make buying insurance
easy for customers. Personal lines
insurers that fail to act similarly will surely
struggle to compete.

The business

4 J D Power US Insurance Shopping Study, 2016.

Exhibit 1

Direct sales can enhance growth for personal lines insurers¹
The top 15 personal lines carriers by size in 2015²
Carriers with smaller
proportion of direct sales

combined ratio
%, 2005-2015

Carriers with a large
proportion of direct sales

90

95

Progressive

Geico

Where to start?
USAA

100

Wholesale, rapid change is neither
necessary nor possible. Culture, by
definition, takes time to root. To know
where to concentrate their efforts, insurers
should first consider how quickly digital
technology will affect different business
lines, then different functions within those
businesses. With this clear, they need to
improve those elements of a digital culture
where they are weakest.

top quartile

High Growth/Low Combined Ratio
105

110
-0.5

0

0.5

1.0

1.5

2.0

2.5

3.0

3.5

4.0

4.5

5.0

5.5

6.0

6.5

8.5

9.0

dwp growth

%, CAGR 2005-2015
¹
²

Figures refer only to the direct written premium (DWP) growth and combined ratio of the personal lines of each insurer
Size of the bubble represents size of 2015 personal lines DWP

Source: AM Best, McKinsey analysis

Digital disruption in insurance: Cutting through the noise

89

Small commercial and simple term
life policies will be next to go the
direct route, both as customers grow
increasingly comfortable using virtual
channels and as the combination of
more data and technology enables
insurers to underwrite a large share of
these risks automatically, limiting the
need for intermediaries. Movement
is already apparent in the life
segment. Jennifer Fitzgerald, CEO of
PolicyGenius, a US-based aggregator of
term life quotes that aims to make buying
a life policy simple for consumers,
says people cannot understand why,
if they can do something as seemingly
complicated as their tax returns on their
own, they cannot figure out how to buy
a life insurance policy unaided. Haven
Life, a direct term life carrier in the United
States, offers an online application
process that takes less than 20 minutes
and makes an immediate decision on
term coverage up to $1 million.
Direct insurance for small commercial
is still rare, but a McKinsey survey of
more than 1,500 customers with small
commercial policies showed 60 percent
would be interested in buying directly.
Large commercial and specialty policies
will be the last to feel digital’s pull given
their complexity, the fact that brokers
fully control distribution, and the lesser
price elasticity of buyers compared to
other segments.
The function
It is already the case that consumerfacing functions such as marketing,
customer service, and claims can fulfill

90

Building momentum for cultural change

customers’ expectations only if they
are strongly digitally enabled. Because
these areas lend themselves to digital
experimentation, bringing about
change should not be overly difficult.
In marketing, for example, testing
messages and channels in order to
find out what is most effective presents
little risk for an insurer and can produce
answers quickly if A/B tests are used
(whereby two versions of a web page or
app are tried out to decide which one
performs better).

In the US, carriers
that mostly sell
directly have the
lowest combined
ratios and enjoy some
of the highest growth
in direct written
premiums.
Before long, however, companies will
need to be prepared to broaden their
change efforts to wherever the adoption
of digital technologies will enhance
competitiveness. In underwriting auto
insurance, for instance, real-time data
from the Internet of Things is leading to
more accurate pricing and risk selection
based on factors such as how fast a
person is driving or how hard they are
braking.

Strengths and weaknesses
Our research, as described in the box
below, suggests there are certain cultural
attributes that underpin a mature digital
environment and help drive superior
performance: an appetite for risk, a testand-learn mind-set, organizational agility,
and a desire to collaborate internally
and externally. Often, of course, these
cultural attributes are nurtured by certain
management or organizational practices.
Is the leadership team a good role model,
for example, or are functions set up in a
way that makes collaboration possible?
Various tools exist to help a company
ascertain its cultural starting position and
to indicate what needs to change and
what does not. These include McKinsey’s
Digital Quotient® and the Organizational
Health Index.

How to start
There are myriad ways to achieve a digital
culture, and the path each company
chooses will be unique. In general, there
are a number of actions companies can
take to kick-start change and speed them
on their way. Here we describe some that
reinforce three particular traits of highperforming digital companies—customercentricity, collaboration, and comfort with
(calculated) risk-taking.
Customer-centricity
Most businesses make decisions by
considering the business case and what
competitors are up to. Customer-centric

companies expand the framework
for decision making, putting the
customer’s point of view among their top
considerations. A question on the table
should always be, “How does this create
value for the customer?”

“You’ll be penalized
if you fail over a long
period of time, so fail
fast.”
— Eric Gewirtzman, CEO of
US online insurance agency
Bolt

At Amazon, for example, internal
presentations addressing business
problems are known as “working
backwards documents.” They start by
identifying how a proposed solution would
help improve the customer experience, be
it a better price, improvement in service,
or increased selection. Only then does the
presenter work backwards to present the
business case. It is a mind-set that some
insurance incumbents are endeavoring to
enforce. Sandeep Bakshi, the CEO and
managing director of Indian life insurer
ICICI Prudential, insists decisions made
by employees, whatever their rank, must
have one of three outcomes: improved
customer experience, more business, or
less risk.

Digital disruption in insurance: Cutting through the noise

91

Many other businesses are engaging
customers in the product development
process, as there is no point asking what
they think of a new product or service
once it has been launched. If they are
dissatisfied, the development has been
a waste of time and money. Customer
needs should be understood at the outset
and feedback sought continually as the
product is developed.

Aviva has an internal
app that connects
employees to the
digital insight the
company has about
its consumers,
including live feeds
from social media or
curated calls from its
contact centers.
The more people reached by that
feedback, the better. To this end, Aviva has
an internal app that connects employees
to the digital insight the company has
about its consumers, including live feeds
from social media or curated calls from
its contact centers. “The purpose is to
give our people the ability to nibble on
real consumer feedback in an entirely raw
fashion without making a huge event of it,”
says chief digital officer Andrew Brem. “So
if you’ve got two minutes, you could read a
few tweets about what our customers are

92

Building momentum for cultural change

really saying to us or hear a few calls. The
idea is just to get our people connected
with our customers.”
A sure way to quicken a shift toward a
customer-centric culture, of course, is to
link employee compensation to metrics
that promote it—for example, metrics that
measure customer satisfaction directly,
or relate to other attributes of highperforming digital companies that affect
customers indirectly, such as speed to
market.
Collaboration
Collaboration is key not only because
it improves customer understanding
and decision making, but also because
it does so quickly. Our research shows
that more than 70 percent of insurers
take from six months to more than a year
to move a digital initiative from idea to
implementation. That is too slow. Scott
Simony, head of industry at Google,
explains why. “Insurance is a highly
regulated industry and it is not easy to
move quickly—but the fact is consumers
are moving at exceptional rates. So I’d
say that the companies that will stand out
are the ones that are going to find ways to
move a bit faster, at the pace of the people
they’re insuring.”
The way to achieve this pace and cut
development time dramatically is to set
up small, cross-functional teams that
take an agile approach to their work. In
a functional set-up, no one owns the full
customer experience and it can take
many work sessions to cobble together a
complete view of it. But a cross-functional
team, focused on the single goal of

improving the customer experience, can
do that rapidly.
The team, located together and working
in sprints to meet specific weekly
development targets, introduces early
prototypes or minimum viable products
(MVPs) that satisfy some—not all—
customer needs and can be improved
with customer feedback. If the team is also
empowered to make decisions without
seeking higher authority, it can cut delivery
time to as little as three to four months.

“It’s really hard to
stop a prototype
because it’s touchable,
feasible.”
— John Straw, Investor,
Bought by Many

Pure digital companies such as Spotify
were among the first to adopt this agile
approach, and insurance companies are
increasingly following their lead. John
Straw, an entrepreneur with investments
in the insurance industry, and formerly
the chairman of the digital advisory
board at UK travel agent Thomas Cook,
recalls his experience building a new
insurance website for the company.
“It was the prototyping part that made
the big difference. Rather than put the
plans through a committee, I took some
of my budget and went to a WordPress

developer and said, ‘Build me a working
prototype of the new insurance website.’
It took four weeks. I then took it to the
innovation committee, and it was relatively
simple from there. It’s really hard to stop
a prototype because it’s touchable,
feasible.”
Risk taking
On the subject of experimentation, the
inventor Thomas Edison is reputed to
have said, “I haven’t failed, I’ve just found
10,000 ways that won’t work.”
In a digital age, insurers need the same
mind-set. Concern over the costs of
failure can be minimized by the use of the
test-and-learn approach encapsulated
in MVPs—the frequent gathering of
feedback means a company will not
travel far in the wrong direction before
correcting course. United Services
Automobile Association, a US-based
insurer, now tests some 8,000 ideas each
year, generating roughly 250 patents.
Yet a culture that understands the value
of calculated risk-taking is one that also
accepts failure, and learns from set-backs.
Some organizations openly celebrate the
lessons learned in order to encourage
their employees to take risks.

Organizational changes and the role
of the CEO
The way a company chooses to organize
itself can significantly affect the pace of
cultural change. There are many options.
For example, some companies tackle the
cultural challenge from within, in the belief
that this is the only way it will take hold,
while others set up a separate division

Digital disruption in insurance: Cutting through the noise

93

for digital initiatives on the basis that they
need distance and a degree of autonomy
from the old business to flourish. That
division will look more like a start-up, with
its own goals, new digital talent, agile
processes, and the autonomy to act
toward these goals.

United Services
Automobile
Association, a USbased insurer, now
tests some 8,000
ideas each year,
generating roughly
250 patents.
Youse Seguros, the online insurance sales
platform of Brazilian insurance company
Caixa Seguradora, was set up in this way.
According to CEO Eldes Mattiuso, “It
was an essential move. You have to start
from scratch. You have to forget about the
rules of the old company and think like a
start-up. If I’d had to follow the traditional
product development procedures it would
have proved impossible to move quickly,
or to use the cloud, for example. It would
have taken us a year and a half to launch
a single product.” Eventually, once the
new culture takes hold, the division can be
reintegrated.

94

Building momentum for cultural change

Aside from these considerations, or
other actions a company might take,
the element that underpins all efforts
to embark upon cultural change, and
sustain it, is the commitment of the
CEO and the leadership team. It falls to
them to explain to the organization why
cultural change is so important and to
model the required behaviors. Some
gain inspiration and conviction for this by
visiting other companies around the world;
Dean Connor, CEO of Sunlife, takes his
management team to Silicon Valley once
a year, for example. Others spend time
with customers, then share what they
have learned, perhaps in a live-streaming
interview, or underscore the importance
of a changed culture in every meeting.
Whatever the specific tactics, it is the
demonstration of senior commitment that
is the surest way to bring about change.
Everything emanates from there.

Tanguy Catlin is a senior partner in
McKinsey’s Boston office. Somesh
Khanna is a senior partner, and Julie
Goran is a partner, both in the New
York office.

A roadmap for a digital
transformation
No insurance company has yet completed a digital
transformation—one that fully harnesses the power
of digital technology to rethink every aspect of the
organization. But a number of carriers are making
remarkable progress, indicating the direction others
should take.

Digital disruption in insurance: Cutting through the noise

95

The future of insurance will be digital.
That much is certain. The industry might
have been slow to feel digital technology’s
impact, protected by regulation, the size
of companies’ in-force portfolios, and
customers’ tendency to stay put with their
insurers. But the pressure is mounting. In
auto insurance, a handful of direct carriers
already enjoy the lion’s share of profits.
Disruption of other lines of business
will surely follow. Distribution channels,
products, underwriting technology,
competitors, and even business models
will shift as technology attacks market
inefficiencies and customer expectations
evolve.
Most insurers are responding to some
degree, albeit often cautiously. Some
see how digital technology will transform
pieces of the business, but find it harder
to envisage how the entire value chain
and business model might change.
They therefore content themselves
with investing in a new sales channel,
launching a service app, or automating
a few processes. At other carriers,
executives believe a transformation will not
be completed on their watch, because the
magnitude of change required will leave
no part of the organization untouched and
could take up to a decade. So why bet on
an uncertain future and risk cannibalizing
existing profits or alienating distributors
when they face more pressing issues,
such as regulatory compliance?
A growing number of executives, though,
are facing up to digital reality. They know
that digital technology can significantly
improve the performance of their current
business. They know that first-movers

96

A roadmap for a digital transformation

have an advantage. And they are keenly
aware that digital can give birth to entirely
new business models that shake up
sectors, leaving companies that fail to
adapt struggling to survive (newspapers
are a case in point). They have therefore
taken steps toward transforming their
businesses.

The CEO cannot
simply sanction a
digital transformation;
he or she must
communicate a vision
of what needs to be
achieved, and why.
They are far enough advanced to know
that each stage of the transformation will
present challenges. The first will occur at
the outset, when the CEO must set the
company on the right course for success.
More will present themselves during
the first six to 18 months—the launch
and acceleration phase—when initial
changes have to start taking root, and yet
others will arise during the long haul of
subsequent years, when digital initiatives
need to be scaled across the enterprise
and digital capabilities and new ways
of working become the lifeblood of the
company. Already, the industry’s digital
pioneers are meeting these challenges
and demonstrating to fellow CEOs ways

in which they can be overcome. And from
these early efforts and successes a set of
ten guiding principles is starting to emerge
(Exhibit 1).

1. Secure senior management
commitment
Any transformation will be dead in the
water if it does not have the commitment
of the CEO and the leadership team.
That statement seems almost glib,
given how often CEO commitment is
positioned as the solution to any major
challenge. But the CEO cannot simply
sanction a digital transformation; he
or she must communicate a vision of
what needs to be achieved, and why,
in order to demonstrate that digital is
an unquestionable priority, make other
leaders accountable, and make it harder
to back-track. Hence, in 2015, Allianz
announced that a key strategic growth

Defining value
To set a digital transformation on the right
course a company must place it at the
core of its agenda, and understand the
magnitude of that undertaking. It is not for
the fainthearted, but CEOs are heading
in the right direction if they grasp the
fundamental importance of heavyweight
management commitment, are willing
to make significant investments, and set
clear, ambitious targets.

Exhibit 1

10

guiding principles
of a digital transformation

stage 2. launch & acceleration

stage 3. scaling up
8

Sequence initiatives for quick returns

9

Build capabilities

10 Adopt a new operating model

stage 1. defining value

4

Start with lighthouse projects

5

Appoint a high-caliber launch
team

1

Secure senior management
commitment

6

2

Set clear, ambitious targets

7

3

Secure investment

Organize to promote new, agile
ways of working
Nurture a digital culture

Digital disruption in insurance: Cutting through the noise

97

initiative was to become “digital by
default”—indicating the extent of the
changes ahead. Similarly, ING branded
its transformation “Fast Forward.”

provocative, disruptive, ambitious, and
often uncomfortable sponsorship to be
successful.”
2. Set clear, ambitious targets

It’s not enough
just to have CEO
sponsorship. It needs
to be provocative,
disruptive,
ambitious, and
often uncomfortable
sponsorship to be
successful.”
– Andrew Brem, chief digital
officer, Aviva
With the vision set, results are then
achieved through relentless daily
engagement. Andrew Brem, chief digital
officer of Aviva, says CEOs need to be
“single-minded and aggressive” about
driving the transformation. “There’s no
way you can do digital transformation
by halves,” he comments. “Our CEO
is chirping in my ear the whole time.
He is very activist. He bases himself
in our garage frequently. He drops
into meetings. He just starts talking
to people. It’s not enough just to have
CEO sponsorship. It needs to be

98

A roadmap for a digital transformation

To set the organization’s sights at the
right level, investments need to be
linked to clear, ambitious targets. This
helps on three fronts. First, it signals the
magnitude of what digital technology
can deliver. Without targets, people who
find it hard to accept that the old ways of
doing things were massively inefficient
might be content to sign up for a 10
percent improvement in cycle time, for
example, when 100 percent is possible.
External benchmarking can help in this
respect by reinforcing the conviction that
cutting the time it takes to, say, process
a claims submission from 90 minutes to
20 is not good enough if someone else
has reduced it to four. A company can
be certain that if it does not match that
benchmark soon, others will.
Second, setting clear targets at the
outset prevents back-sliding when the
going gets tough. And third, it imposes
discipline on the process of deciding
which initiatives to pursue for maximum
impact.
Targets are needed for each source
of value creation—cost savings,
revenues, improved performance of
agents, and satisfaction of employees
and customers—and for new ways
of working and the new capabilities
required. They can be set, for example,
for the frequency of releases, the
percentage of processes that will

be automated, the percentage of
transactions that will be migrated from
one channel to another, the fraction of
new code that will be tested automatically,
the level of personalization that will be
achieved, and the number of campaigns
that will be run each month.

An insurer with
premiums worth
more than $5 billion
should expect to
hire between 20 and
100 new specialists
during the first
18 months of a
transformation.
3. Secure investment
Digital transformation is likely to require
significant investment. European insurer
Axa, for example, invested €950 million
over just two years. Our experience
suggests that in IT alone, companies with
outdated systems might need to double
their current spending over a five-year
period. That investment is likely to result
in lower profits for a while—but without
it there is a serious risk to profits in the
longer term. Importantly, companies
will need to allocate investment both
to improve the current business and to

build new businesses as the insurance
model evolves. To acquire expertise in
new fields and keep abreast of innovation,
for instance, insurers will need to invest
in partnerships or a venture capital arm,
perhaps both, as well as in their own
innovation labs.

Launch and acceleration
It is easy to launch change initiatives. It is
hard to keep them afloat and spawn more.
Often companies decide to fund several,
assign people, even set up separate
units. But then the initiatives fail to take
off and the old ways of doing business
continue much the same—at which point
executives wrongly conclude there is no
urgency as the market is not ready for
change.
To ensure early efforts thrive and build
momentum, companies should consider
carefully which projects to start with
and support them with the necessary
resources. Prerequisites include a highcaliber launch team often led by a chief
digital officer (CDO), consideration of
organizational structure, and the nurturing
of a digital culture.
4. Start with lighthouse projects
To win early support, companies should
start with projects that offer potential for
significant rewards with manageable risk.
Such projects include customer services
activities and the redesign of the claims
process, from the moment a customer
needs to file a claim to the moment
of reimbursement. Customers will be
delighted, cost savings can be as high as

Digital disruption in insurance: Cutting through the noise

99

40 percent, and effectiveness, measured
in return on investment, can rise by as
much as five percentage points.
5. Appoint a high-caliber launch team
The importance of securing a highcaliber launch team, often under a
CDO, cannot be overstated. A CDO
can prove invaluable in co-ordinating a
transformation—avoiding duplication by
devising a methodology for the redesign of
customer journeys that can be replicated
across the organization as digitization
efforts are extended, for example. He
or she can also ensure the appropriate
technology and skills are in place, decide
the sequence of the transformation,
monitor progress against targets, and
ensure that tactical day-to-day priorities
get the attention they need. But the
role of CDO is a temporary one. At the
end of the nineteenth century, many
companies employed a chief electricity
officer to ensure supplies of what was a
new industrial commodity. A few years
later, none did. Key recruits to the launch
team include designers to contemplate
customers’ unmet needs and inform the
creation of experiences, products, and
services; data scientists; scrum masters
to facilitate agile development; and
developers who can work in the modern
IT environment. Roughly, an insurer with
premiums worth more than $5 billion
should expect to hire between 20 and 100
new specialists during the first 18 months
of a transformation.
That is not a huge number, but the
competition for digital talent and the
advantage technology companies have
in attracting it makes finding people of the

100

A roadmap for a digital transformation

highest caliber a considerable challenge.
The scarcity of elite data scientists, for
example, has been a factor in some
insurers’ acquisitions of cutting-edge
artificial intelligence start-ups; $5 million
to $10 million per employee can be
commanded in these so-called “acquihire” deals.

“We have an
advantage when it
comes to culture. We
are a tech company
in the insurance
space, not an
insurance company
that plays with
technology.”
– Adam Lyons, founder and
CEO, TheZebra.com
One way to meet the challenge is to start
by hiring a renowned expert to serve as
an anchor hire, who will help to attract
others, on the basis that they will be drawn
to him or her more than they would be to
an insurer per se. Some companies go
further than hiring individuals and acquire
agencies that specialize in design thinking.
To help satisfy the expectations of their
ambitious recruits, companies might have
to adapt their traditional value proposition,
based on span of control, with a different

kind that promises empowerment in their
work on high-impact digital initiatives.
“The talent piece is essential,” says
Andrew Brem. “I’ve hired an entirely
new digital team. I’ve brought in people
from the world of gaming, from travel,
from retail, from pure digital. And they’ve
bought in a lot of people too. There are
some particular skills I’d call out. One
would be digital production design.
Another would be digital marketing on the
social side. And another would be data
analytics, particularly on the customer
side rather than risk.”
People leadership skills are essential too.
Transformation is not just about tipping
everything upside down, reinventing
products, and disrupting value chains. It
is partly about balancing old and new and
integrating fresh talent with old, valued
hands. As Clara Shih, founder and CEO
of “advisor marketing cloud” company
Hearsay has observed, digital-savvy
hires from outside the industry might ace
building a digital-direct, e-commerce
business, but are often ill-equipped to
modernize insurers’ existing channels,
where huge, value-creating opportunities
await. “The reason traditional agency
distribution hasn’t innovated is because
it’s very hard to find someone steeped in
digital who also understands field sales,
and vice-versa,” she says.
6. Organize to promote new, agile ways
of working
The way a company organizes itself is key
to a successful launch. Setting up a digital
unit independently of the organization will
promote new ways of working essential
for digital success, such as agile product

development, test-and-learn methods that
speed progress while keeping the focus
on customers, and cross-functional teams
that pool specific types of expertise.

“The reason
there hasn’t been
more innovation
within traditional
distribution ... has
been that it’s very
hard to find someone
with a digital
skill set who also
understands field
sales and vice versa.”
– Clara Shih, co-founder and
CEO of Hearsay Social
A digital unit can also help attract and
retain those specialists, while offering
them freedom from incumbents’
organizational constraints and the support
of like-minded colleagues. If such people
are simply parachuted into the existing
structures of incumbents they can
become bored and frustrated at the pace
of change. They need to be empowered to
make a swift impact, which often means
giving them authority to make their own
decisions.

Digital disruption in insurance: Cutting through the noise

101

Separating a digital component from
the rest of the organization is not entirely
the answer, however. To begin with,
newcomers can (unintentionally) run
roughshod over what is valuable in an
incumbent: the reason many insurance
companies have been around for more
than a century is that they excel at what
they do. They can also start to create
channel conflict, particularly if innovations
threaten to cannibalize revenue streams.
The digital unit therefore needs to be
reintegrated at some stage, and that
becomes more difficult as time passes.
Whatever the choice, the ultimate goal has
to be to enmesh the old and the new.

McKinsey research
has shown that 46
percent of financial
services executives
feel cultural or
behavioral change
is the biggest
challenge they face in
pursuing their digital
strategies.
7. Nurture a digital culture
We have touched upon how digital ways of
working and thinking—fast, collaborative,

102

A roadmap for a digital transformation

empowered—will be the default mode
of new recruits with digital skills. These
methods also need to take hold across the
organization, and now is the time to start
nurturing them.
So much needs to change. A focus on
customer needs rather than process
and procedure, continuous customer
feedback, comfort with testing and
learning and hence with occasional
failure, and collaboration—all are vital.
But insurers can be made to feel they are
being asked to jettison the things that
have made them successful and adopt an
untested culture. No wonder McKinsey
research has shown that 46 percent of
financial services executives feel cultural
or behavioral change is the biggest
challenge they face in pursuing their digital
strategies.
They are not, of course, being asked to
abandon the traits that have made them
successful, but to renew their heritage
with innovative ways of thinking and
working (see “Building momentum for
cultural change”). Brad Auerbach, US
industry manager at Facebook, describes
it as recalling what initially made them
successful. And there are relatively easy
ways to kick-start change and gain
support. For example, rather than making
decisions by considering the business
case or what competitors are doing, insist
that the starting point is “How does this
create value for the customer?” Moreover,
change can begin in areas where there are
fewer risks—in marketing, for example, by
testing messages and channels to find out
what is most effective.

“Agile principles
are now standard
operating procedure
for software design,
but they’re also
applicable any
time you need to
orchestrate a large
number of people
to get something
complex and multifaceted done over an
extended time frame.”
— Marcus Ryu, co-founder
and CEO at Guidewire
Software
Scaling up
At the 18-month point, companies
should be making good progress. They
should have a handful of initiatives up and
running and be starting to capture value.
But just when everything seems under
control is also the time to supercharge the
transformation and do everything on a
grander scale. The thoughtful sequencing
of subsequent initiatives is key to this. In
addition, close attention will need to be

paid to building more capabilities. And to
reap the full rewards of a transformation,
eventually an entirely new operating model
will be required.
8. Sequence initiatives for quick returns
Sequencing with a view to quick returns is
key to building scale fast. The more value a
transformation captures as it progresses,
the more it becomes self-funding and the
greater the support it garners. Often a
company’s approach is to let a thousand
flowers bloom. But this spreads scarce
resources thinly. Moreover, transformation
incurs costs at a time when competition
is probably putting pressure on margins.
Hence the imperative to thoughtfully
pursue a manageable number of digital
initiatives to tend the performance of the
core business while cultivating future
sources of growth (see “Capturing value
from the core”).
Initiatives that are strategically important,
pay back quickly, and reduce complexity
are the ones to prioritize. This almost
always means looking for ways to cut
costs—a counterintuitive notion for many
executives who tend to focus on digital
technology’s growth potential. But context
matters. A company’s financial pressures
will shape the sequencing to some
degree. So will its IT, if legacy systems
restrict initial choices. And companies
need to be flexible. It could prove hard to
recruit the particular people needed, while
technology and customer behavior will
continue to evolve.
Tracking returns is essential to ensure
all available value is captured. Often,

Digital disruption in insurance: Cutting through the noise

103

targets can be raised during the course of
the transformation as prototypes reveal
greater productivity improvements than
have been assessed on paper. And when
initiatives are successful and deliver the
intended financial benefits, the board and
top team should be emboldened to push
to achieve more. But while concentrating
effort and attention on what works well
matters, so does letting go of what does
not.
9. Build capabilities
By now it will be apparent that insurers
will have to invest in more than just digital
technologies themselves to scale up
digital initiatives. Marcus Ryu, co-founder
and CEO at Guidewire Software, contends
that it is only by modernizing core
operating platforms — most importantly
policy administration, billing, and claims
systems — that insurers can externalize
the data and business logic necessary to
deliver a satisfying digital experience for
the policyholder or distribution partner.
Skills as well as systems will need to be
boosted. But if a company struggles to
hire 20 to 100 new people for the launch
team, how should it go about hiring several
hundred? Searches are likely to extend to
developer communities and to technology
conferences and similar events. The quest
for talent might even lead companies
to establish partnerships with software
providers.
A huge internal training job will be
needed too. Business leaders will need
to understand IT’s strategic value—the

104

A roadmap for a digital transformation

reason one large European insurance
group has set up an IT literacy program
to educate and update business line
managers, while all newly appointed top
business managers must take a three-day
training module to help them understand
and capture IT’s strategic value (see
“Modernizing IT for a strategic role”).
Ultimately, however, it will be important
to help all employees rethink the way
they work, as the end result of a digital
transformation is the establishment of a
company-wide agile operating model.
10. Adopt a new operating model
Whatever structures a company chooses
initially, it will reach the stage when only
a fundamental organizational redesign
will do. Silos drawn along functional lines
have always been a drag on collaboration
and performance in large organizations.
In the digital age, when companies need
to reinvent the way they work on the fly,
an inability to connect all parts of the
organization to share data, expertise, and
talent can be crippling.

The only way forward
for a company is to
learn as it goes and
figure out how to
apply lessons as scale
is built.

That is why companies will have to lean
away from a traditional matrix structure
with rigid functional boundaries if the
transformation is to succeed. They will
need a network structure, organizing
around sources of value, with product
managers empowered to make decisions
with implications that cut across functions.
Teams will not be permanent. They will be
dissolved when they capture the value at
stake, then regroup around new sources of
revenue growth or cost reductions. Some
companies call them scrum teams, others
tiger teams, portfolios, or tribes. Whatever
the label, the ossified matrix is giving way
to a more agile one. In other words, the
entire organization, not just IT, will adopt an
agile approach to working. “Agile principles
are now standard operating procedure for
software design,” says Marcus Ryu, “but
they’re also applicable any time you need to
orchestrate a large number of people to get
something complex and multi-faceted done
over an extended time frame.”
  

Insurers that pursue digital transformation
will meet challenges. IT projects fall behind
schedule, channel conflicts arise, and
unexpected regulatory concerns emerge.
Typically, companies also struggle with
cultural issues and challenges in recruiting
new types of talent.

afford insight into decisions relating to
technology architecture, data architecture,
and platforms. Customer satisfaction is
likely to jump. Cycle times will be shorter
and costs will fall. New ways to accelerate
revenue growth will reveal themselves. This
is the time to double down on efforts.
A closing thought, and perhaps one that
reframes the challenge: the term digital
transformation puts the emphasis on
technological change. But it becomes
clear to anyone who understands digital
technology’s potential that what is afoot is
less of a digital transformation and more
of a fundamental rethink of the corporate
model, for which digital technology is the
catalyst. Sources of revenue, efficiency, and
the organization’s structure are all up for
scrutiny, as are talent models, which need
to offer more flexible, more empowering,
and more rewarding career paths. Some
executives might feel the reframing makes
the challenges more daunting still, others
that it makes the opportunities more
exciting. We are in the second camp.

Tanguy Catlin is a senior partner in
McKinsey’s Boston office, JohannesTobias Lorenz is a senior partner in the
Düssesldorf office, Bob Sternfels is a
senior partner in the San Francisco office,
and Paul Willmott is a senior partner in the
London office.

No rule book will solve all of this. A
transformation is not a science. The only
way forward for a company is to learn as it
goes and figure out how to apply lessons
as scale is built. Along the way there will be
important markers of success. IT strategy
will become clearer as early prototypes

Digital disruption in insurance: Cutting through the noise

105

Its application reveals that, relative to
sectors such as telecoms, travel, and
retail, the insurance industry remains in
the early stages of digital transformation.
Indeed, among the nine industries
measured, insurance ranked seventh,
scoring an average of 31 points out of 100
(Exhibit 1).

collaborate internally, and willingness
to collaborate externally—US P&C
insurers struggle most with the first
three (Exhibit 2).
The stark performance differential
matters. Top P&C insurers, those that
score 50 and higher, are increasing
revenue 1.5 times as fast as the rest of
the field and operating with a combined
ratio that is eight percentage points lower.
Our research examined what insurers are
doing differently in the four management
practice areas to outperform their peers
(Exhibit 3).

McKinsey research shows that this lag is
due largely to a weak digital culture.
Of the five attributes important to a digital
culture—an appetite for risk, a test-andlearn approach to product and service
development, agility, willingness to

Exhibit 1

Distribution of Digital Quotient®
score by industry, globally
49
42

Digital Quotient: Where does
your company stand?
To assess the digital maturity of businesses, and hence their ability to thrive in a digital
world, McKinsey has devised a simple metric, the Digital Quotient®. The DQ evaluates 18
management practices connected to four areas—digital strategy, capabilities, culture, and
organization—that correlate most strongly with growth and total returns to shareholders.

106

Digital Quotient: Where does your company stand?

35

Global average: 33

31
25

private equity

36

37

32

28

pharma/medical
products

Insurance1

banking

transport and
logistics

media/
entertainment

telecom

retail

travel/
hospitality

¹ Includes P&C and Life
Source: USDEC

Digital disruption in insurance: Cutting through the noise

107

Exhibit 2

Exhibit 3

P&C insurers’ performance scores in the five attributes important to a digital culture

Insurers’ digital maturity as measured by the Digital Quotient®
Points out of 100 for P&C insurers in each of four areas

P&C average

Top quartile P&C

1=Poor performace

5=Best practice

2.4

Risk appetite

strategy

Test-and-learn

Average among insurers
in the top quartile¹

2.3

3.1

Internal
collaboration

Average for all
insurers

3.2

External
collaboration

2.7
1

2

Strategy. Top-performing P&C insurers
scored on average 73 for the effectiveness
of their digital strategy, compared to an
average of 40 across all companies. This
strong performance was driven by three
enablers: a bold long-term vision based
on a clear and shared articulation of
customer priorities, strong support from
senior leaders, and a firm set of targets
for growth, market share, customer
satisfaction, and return on equity.
Capabilities. The best performers
were particularly strong on connectivity
between channels and digital content
creation, earning an average score of 43
for their digital capabilities compared to an

108

culture

organization

capabilities

2.7 2.9

Speed/agility
CULTURE

2.8

Digital Quotient: Where does your company stand?

2.5

4.0

Average among all
companies, excluding
insurers²

3.2
3

3.5

4

average cross-industry score of 29. They
generate 47 percent of all sales over digital
channels, compared to 11 percent for the
average insurer. They also make it easy for
customers to file first notice of loss claims
online, receiving 15 percent more such
notifications over digital channels than the
average insurer.
Culture. A handful of cultural attributes
separate outperformers from the rest of
the pack. They have a greater risk appetite
for digital initiatives, embrace a test-andlearn mind-set, enforce cross-disciplinary
collaboration, and look outward for
inspiration.

5

73
28

26

38

19

40

22

35

43
24

37

29

¹ Sample of some 30 insurance companies worldwide.
² Sample of 200+ companies drawn from range of US-based, non-insurance industries.

Organization. High-quality governance
and employee practices, and the effective
alignment of roles and responsibilities, are
especially correlated with market success.
But even top-quartile companies that
institute dynamic measurement and
talent development practices can
struggle to adapt the way they work.
Insurers on average record poor to
middling performance in fostering a digital
organization, with an average score of
22 compared to an average of 37 for all
industries.
For further details of the survey, see see
Tanguy Catlin, Ido Segev and Holger Wilms,
“The Hallmarks of Digital Leadership in P&C
Insurance,” McKinsey & Company, August 2016.

Digital disruption in insurance: Cutting through the noise

109

The following McKinsey consultants and
experts contributed to this compendium:
Elizabeth Abraham
Mila Adamova
Ramnath Balasubramanian
Simon Behm
Rohit Bhapkhar
Henk Broeders
Joao Bueno
Jacques Bughin
Rae Chen
Michael Chui
Peeyush Dalmia
Julie Goran
Matt Higginson
Khushpreet Kaur
Alex Kazaks
Somesh Khanna
Chandresh Kothari
Krish Krishnakanthan
Laura LaBerge
Jens Lansing
Xavier Lhuer
Ari Libarikian
Markus Löffler
Christopher Mokwa
Christopher Morrison
Björn Münstermann
Peter Braad Olesen

Pradip Patiath
Anand Rao
Jay Scanlan
Thomas Schumacher
Ido Segev
Parker Shi
Kate Smaje
Rohit Sood
Bob Sternfels
Kurt Strovink
Ashley Thomas
Shanon Varney
Amy Vickers
Paul Willmott
Holger Wilms
Shuang Wu
Olga Yurchenko

For more information, contact:
Tanguy Catlin
Senior Partner, Boston
tanguy_catlin@mckinsey.com

Johannes-Tobias Lorenz
Senior Partner, Düsseldorf
johannes-tobias_lorenz@mckinsey.com

March 2017
Copyright © McKinsey & Company
mckinsey.com
@digitalmckinsey

WHITE PAPER

Streamlining the Automotive Claims Process
via Integrated Intelligent Automation
MANUBHAV JAIN

STAS CHULSKY

ASTGHIK MKHITARYAN

PRINCIPAL,

SENIOR MANAGER,

SENIOR MANAGER,

INSURANCE

INTELLIGENT

INTELLIGENT

C O N S U LT I N G

AUTOMATION

AUTOMATION

C O N S U LT I N G

C O N S U LT I N G

NOVEMBER 2019

Contents

INTRODUCTION........................................................................................................................................ 3

FIRST NOTICE OF LOSS (FNOL) & CLAIM REGISTRATIONS. . .............................................................. 4

CLAIMS TRIAGE, ADJUDICATION & RESERVE ESTIMATION . . ............................................................ 5

REPAIRS & QUALITY CONTROL ............................................................................................................. 6

SUBROGATION BETWEEN INSURANCE COMPANIES & PAYMENTS TO VENDORS ..........................7

TOTAL LOSS SETTLEMENT & SALVAGES/RECOVERY . . ........................................................................ 8

SPECIAL INVESTIGATION UNIT & LEGAL ............................................................................................. 9

CONCLUSION ......................................................................................................................................... 10

Introduction

As intelligent automation (IA) emerges as a best practice across numerous industries, insurers and technology
service providers alike are racing to build out use cases for IA that streamline the various stages of an
automotive insurance claim. While automation cannot be viewed as an isolated lever or standalone solution
to bring efficiency and cost savings, when IA integrates with other channels, technologies and platforms, it
unleashes its full potential. This white paper provides insight on how an integrated IA approach can improve
the auto claims process and deliver a more favorable customer experience.

FIRST NOTICE OF LOSS (FNOL)
& CLAIM REGISTRATIONS

CLAIMS TRIAGE, ADJUDICATION
& RESERVE ESTIMATION

REPAIRS & QUALITY
CONTROL

SUBROGATION BETWEEN
INSURANCE COMPANIES &
PAYMENTS TO VENDORS

TOTAL LOSS SETTLEMENT &
SALVAGES/RECOVERY

SPECIAL INVESTIGATION
UNIT & LEGAL

TY P I C A L VA LU E C H A I N O F A N A U TO C LA I M

W H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation

2019 • 3

First Notice of Loss (FNOL) & Claim Registrations

STAGE OVERVIEW
The FNOL & claim registration stage is where call center agents collect data on the claim as well as the insured
and ultimately register a claim. Call center agents may alleviate any customer doubts and provide support
related to the registration of a claim. FNOL is an intake process which potentially impacts all downstream
stages of claim processing.
COMMON PROBLEM AREAS
Unstructured data stemming from voice interaction and decentralized information is kept in various systems.
Both voice interaction and decentralization of information result in swaths of discrete and unstructured data
deposits in the organization that require extensive operational effort to convert into actionable structured
data to fulfill a business task. The results of unstructured data at this stage not only affect claim registration
but have a substantial impact on all downstream claims processes, which ultimately leads to both increased
operational expenses for the insurer and a poor customer experience for the insured.
INTEGRATED IA APPROACH
•

Apply real-time natural language processing (NLP) and speech-to-text to authenticate caller and assist call
agent with data collection

•

Supplement all interactions between the insured and the carrier via mobile app and/or portal as an
additional channel

•

Enable real-time request-response through this app to trigger any forms (i.e. disclaimers) or information
exchanges needed for the claim filing process while a phone conversation with the agent is taking place

•

Prefill a significant amount of client data for expedited processing and, in the long term, predict relevant
data to be captured via machine learning (ML)

•

Provide the means to request additional services such as notifying emergency contacts associated with
policy, requesting ambulances or tow trucks, and providing tracking of those services until fulfilled

•

Use AI to identify, then notify the most optimal (location, cost, service quality, etc.) service providers and
then track the status until the service has been fulfilled

•

Ensure the ability to launch a video conferencing (VC) call for interaction and guidance with a customer
service rep in cases where special handling or in-person assistance is required

W H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation

2019 • 4

Claims Triage, Adjudication & Reserve Estimation

STAGE OVERVIEW
The claims triaging stage is where the optimal claims adjudicator is assigned to a case based on their
skillset. Once an adjudicator is assigned, they provide the initial estimate of the claim amount and create a
damage appraisal report by collecting additional information/evidence.
COMMON PROBLEM AREAS
Manual triaging of claims may lead to non-optimal assignment of skilled adjusters while decentralized and
inconsistent data-gathering frameworks between adjudicators and other departments lead to increases in
turnaround time and operational expenses. Additionally, prevailing manual reserve booking and management
processes can delay both accounting and financial visibility and accuracy.
INTEGRATED IA APPROACH
•

A decision engine should be created to effectively assign an adjudicator to a case based on their personal
profile, availability, skillset and type of claim

•

The adjudicator should be presented with information provided by the claimant, offered relevant forms
automatically, and have the ability to request additional information (if needed) via a common channel

•

The claimant should be able to receive all documents and forms electronically and have the option to sign
forms digitally

•

Business process management (BPM) tools can be used to orchestrate processes and transmit data and
forms from various departments and platforms

•

Claim data can be integrated with actuarial and/or financial systems for automatic reserve management

•

Artificial intelligence (AI) can be used to determine the initial estimate of loss and allow reserves to be
triggered for booking automatically

•

Initial estimates can be produced using ML models and algorithms which could be used for settling claim
variance at later stages of claims processing

W H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation

2019 • 5

Repairs & Quality Control

STAGE OVERVIEW
The repair stage is where an approved body shop repairs damages, procures parts and provides customer
service directly to the insured. Once the body shop determines which parts are required, they are procured
based on an agreed upon parts and services rate.
In conjunction with the repair stage, a quality control audit is performed by the insurer to make sure that the
body shop does not overprice repair items and provides quality customer service.
COMMON PROBLEM AREAS
Inconsistent and unstructured data flow between the external vendor and insurer may cause important data
loss, and the extra time taken to decide on a repair shop may impact the quality of the experience for the
customer, who is already dealing with the stress of an accident. A lack of real-time status-of-repair results in
uninformed and agitated customers while time spent between FNOL and sending the vehicle to a body shop
(customer service, tow truck and body shop selection) results in high cost to the insurer. All these components
threaten to reduce the quality of service, incur indirect expenses for the insurer and exacerbate a tense situation
for the customer.
INTEGRATED IA APPROACH
•

Depending on the captured damage type, incident location, customer address and customer reviews, IA can
suggest a list of appropriate repair shops and create initial estimates based on FNOL data and experience
with each body shop for the insured to decide on a suitable option

•

Using ML models, the system should be able to flag any pricing inconsistency

•

A carrier portal or app can be extended to interact with body shops and allow them to see the insurer’s
original estimate — along with all underlying data and pictures gathered through the FNOL and valuation
process — and adjust it if needed. The same portal can enable the body shop to provide a status of repair
and feed that information back to the insured as well as the carrier

•

A random audit can be automatically triggered to validate estimation samples

W H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation

2019 • 6

Subrogation Between Insurance Companies &
Payments to Vendors

STAGE OVERVIEW
Subrogation is the substitution of one person or group by another in respect to a debt or insurance claim,
accompanied by the transfer of any associated rights and duties. Put simply, the Subrogation Principle in
insurance means that when an insurer pays full compensation for any insured loss (of insured property), the
insurer holds the legal right (claim) to collect the loss amount from the third party (insurance or person).
Payment to vendor occurs when the vendor (repair shop or supplier) has rendered the services.
COMMON PROBLEM AREAS
Tedious paperwork to settle/initiate subrogation and lengthy external party interaction increases turnaround
time and operational cost. Manual accounts payable (AP) or accounts receivable (AR) reconciliation often
causes human error and delays the recognition of funds. The paper trail builds a three-way barrier between
the customer, insurer and third party by making the process convoluted for everyone involved. It also puts
constraints on the insurer’s cash reserves, as months are spent gathering discrete information and tracking
down the reimbursement of funds.
INTEGRATED IA APPROACH
•

Based on pre-set rules, an IA-enabled platform can flag cases where subrogation is applicable

•

Integration with virtual payment providers can expedite the payment process to vendors

•

IA/ML should be utilized to extract information from invoices received from vendors and apply a consistent
approach for entering data into the platform

•

A reconciliation process can be enabled via a mix of business rules and ML

•

The rules engine can track service level agreements (SLAs) around AP and AR to avoid penalties for AP and
minimize risk profile for AR

•

Digital content management should be leveraged to enable communication between insurance companies,
customers and vendors

W H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation

2019 • 7

Total Loss Settlement & Salvages/Recovery

STAGE OVERVIEW
The total loss settlement process is aimed to make sure that total loss damages are paid to the insured
according to the contract.
The salvages and recovery process is aimed to make sure that all salvaged cars are sold.
COMMON PROBLEM AREAS
Manually tracking total loss cases results in material process inefficiencies and ultimately leads to an increase
in operational costs. A lack of controls and sufficient market data to evaluate the appropriate market value of
vehicles increases the risk of overestimating or underestimating that value, which could subsequently lead to
the loss of funds. Additionally, manually tracking salvage inventory increases the time required to recycle the
vehicle and incur the appropriate amount.
INTEGRATED IA APPROACH
•

In cases where the contract is defined in a document rather than the policy administration database, IA can
be utilized to extract relevant information (deductibles, contract exclusions, state thresholds, etc.) using
cognitive document processing (CDP)

•

A rules engine can flag potential total loss cases when damage estimates are higher than the market value
of the vehicle

•

Automated payments or electronic fund transfers (EFTs) can be scheduled based on confirmation of total loss

•

Digital content management should be leveraged to enable communication between insurance companies
and customers

•

Integration with third parties allows insurers to uncover the market value of a vehicle to be utilized in
determining total loss amount

W H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation

2019 • 8

Special Investigation Unit & Legal

STAGE OVERVIEW
The Special Investigation Unit (SIU) reviews and investigates claims that are flagged as potential fraud
automatically by the system or manually by an adjuster.
COMMON PROBLEM AREA
A lack of relevant rules to flag potential fraud increases the possibility of fraud cases going undetected, and
the inaccuracy of this data misleads the machine learning engine behind the predictive and/or adoptive
model. False positives and negatives in this process lead to the displacement of valuable talent capital when
employees spend time researching legitimate claims for fraud while financial loss occurs when material fraud
cases are not identified.
INTEGRATED IA APPROACH
•

A rules engine, combined with IA, increases the accuracy and likelihood of detecting potential fraud cases

•

Adjusters can mark cases as potential fraud within a portal and provide structured input and
granular categorization which is later evaluated through IA to minimize false positives or expediate
fraud investigations

•

A portal can provide visibility for legal departments when collecting data, relevant information and key
details for a case

•

Integration with third-party data providers allows for insight into customer profiles before proceeding with
any litigation case

W H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation

2019 • 9

Conclusion

From FNOL through legal operating procedures, it is paramount to integrate all communication channels and
systems to enable seamless automation. However, before automating these processes, insurance organizations
must have a centralized process team which focuses on reviewing and optimizing processes (macro- and microlevel) across departments and identifies opportunities to re-engineer processes in order for any automation
program to garner the expected results. Failure to do so could result in unintended process inefficiencies that
lead to millions – and even billions – in indirect hidden expenses that manifest through long customer service
calls, fees and penalties – all of which can diminish user experience and lead to loss of customers.
With customer satisfaction so closely tied to the bottom line, it is vital for insurers to use intelligent automation to
prioritize process transparency for consumers to enable the real-time status tracking of settlements, increase the
ease of reviewing relevant case details/documents, and improve the visibility of estimated execution timelines so
that customers know what to expect next without needing to create an extensive paper trail of correspondences.
Doing so offers a unique opportunity for insurers to step into a future where convoluted infrastructures designed
to support numerous discreet and siloed processes become a thing of the past as the industry reaches a whole
new level of disruption, competition and innovation.

W H I T E P A P E R • Streamlining the Automotive Claims Process via Integrated Intelligent Automation

2019 • 10

ABOUT EPAM SYSTEMS

Since 1993, EPAM Systems, Inc. (NYSE: EPAM) has leveraged
its software engineering expertise to become a leading global
product development, digital platform engineering, and top digital
and product design agency. Through its ‘Engineering DNA’ and
innovative strategy, consulting, and design capabilities, EPAM
works in collaboration with its customers to deliver next-gen
solutions that turn complex business challenges into real business
outcomes. EPAM’s global teams serve customers in over 25

GLOBAL
41 University Drive, Suite 202
Newtown, PA 18940, USA
sales@epam.com
P: +1-267-759-9000
F: +1-267-759-8989

countries across North America, Europe, Asia and Australia. EPAM
is a recognized market leader in multiple categories among top
global independent research agencies and was one of only four
technology companies to appear on Forbes 25 Fastest Growing
Public Tech Companies list every year of publication since 2013.
Learn more at http://www.epam.com/ and follow us on Twitter @
EPAMSYSTEMS and LinkedIn.

© 1993-2019 EPAM. All Rights Reserved.

EPAM_B352

Insurance Practice

Insurance 2030—
The impact of AI on the
future of insurance
The industry is on the verge of a seismic, tech-driven shift. A focus on
four areas can position carriers to embrace this change.
by Ramnath Balasubramanian, Ari Libarikian, and Doug McElhaney

© Imaginima/Getty Images

March 2021

Welcome to the future of insurance, as seen
through the eyes of Scott, a customer in the year
2030. His digital personal assistant orders him a
vehicle with self-driving capabilities for a meeting
across town. Upon hopping into the arriving car,
Scott decides he wants to drive today and moves
the car into “active” mode. Scott’s personal
assistant maps out a potential route and shares
it with his mobility insurer, which immediately
responds with an alternate route that has a much
lower likelihood of accidents and auto damage as
well as the calculated adjustment to his monthly
premium. Scott’s assistant notifies him that his
mobility insurance premium will increase by 4 to
8 percent based on the route he selects and the
volume and distribution of other cars on the road.
It also alerts him that his life insurance policy,
which is now priced on a “pay-as-you-live” basis,
will increase by 2 percent for this quarter. The
additional amounts are automatically debited from
his bank account.
When Scott pulls into his destination’s parking
lot, his car bumps into one of several parking
signs. As soon as the car stops moving, its internal
diagnostics determine the extent of the damage.
His personal assistant instructs him to take three
pictures of the front right bumper area and two of
the surroundings. By the time Scott gets back to
the driver’s seat, the screen on the dash informs
him of the damage, confirms the claim has been
approved, and reports that a mobile response drone
has been dispatched to the lot for inspection. If the
vehicle is drivable, it may be directed to the nearest
in-network garage for repair after a replacement
vehicle arrives.
While this scenario may seem beyond the horizon,
such integrated user stories will emerge across
all lines of insurance with increasing frequency
over the next decade. In fact, all the technologies
required above already exist, and many are available
to consumers. With the new wave of deep learning
techniques, such as convolutional neural networks,1
1

2

artificial intelligence (AI) has the potential to live
up to its promise of mimicking the perception,
reasoning, learning, and problem solving of the
human mind (Exhibit 1). In this evolution, insurance
will shift from its current state of “detect and repair”
to “predict and prevent,” transforming every aspect
of the industry in the process. The pace of change
will also accelerate as brokers, consumers, financial
intermediaries, insurers, and suppliers become
more adept at using advanced technologies to
enhance decision making and productivity, lower
costs, and optimize the customer experience.
As AI becomes more deeply integrated in the
industry, carriers must position themselves to
respond to the changing business landscape.
Insurance executives must understand the factors
that will contribute to this change and how AI will
reshape claims, distribution, and underwriting
and pricing. With this understanding, they can
start to build the skills and talent, embrace the
emerging technologies, and create the culture and
perspective needed to be successful players in the
insurance industry of the future.

Four AI-related trends
shaping insurance
AI’s underlying technologies are already being
deployed in our businesses, homes, and vehicles,
as well as on our person. The disruption from
COVID-19 changed the timelines for the adoption
of AI by significantly accelerating digitization for
insurers. Virtually overnight, organizations had
to adjust to accommodate remote workforces,
expand their digital capabilities to support
distribution, and upgrade their online channels.
While most organizations likely didn’t invest heavily
in AI during the pandemic, the increased emphasis
on digital technologies and a greater willingness to
embrace change will put them in a better position
to incorporate AI into their operations.

Convolutional neural networks contain millions of simulated “neurons” structured in layers.

Insurance 2030—The impact of AI on the future of insurance

AI Insurance
Exhibit 1 of 2

Exhibit 1

Artificial intelligence can deliver on industry expectations through machine learning and
deep learning.
Artificial intelligence
The science and engineering of making
intelligent machines

1950s

1960s

1970s

Machine learning
A major approach to realizing artificial
intelligence

1980s

1990s

2000s

Deep learning
A branch of machine learning

2010s

Artificial intelligence (AI)

Machine learning (ML)

Deep learning (DL)

Intelligence exhibited by machines, whereby
machines mimic cognitive functions
associated with human minds; cognitive
functions include all aspects of learning,
perceiving, problem solving, and reasoning.

Major approach to realizing AI by learning
from, and making data-driven predictions
based on, data and learned experiences. ML
comprises several categories, including
reinforcement learning, supervised learning,
and unsupervised learning.

Branch of ML in which algorithms attempt
to model high-level abstractions in data. DL
connects artificial, software-based
calculators that approximate the function of
brain neurons. Neural networks, formed
by these calculators, receive, analyze, and
determine inputs and are informed if
determination is correct.

Source: Nvidia; Rockwell Anyoha, "The history of artificial intelligence," Science in the News, August 28, 2017, sitn.hms.harvard.edu

Four core technology trends, tightly coupled with
(and sometimes enabled by) AI, will reshape the
insurance industry over the next decade.
Explosion of data from connected devices
In industrial settings, equipment with sensors have
been omnipresent for some time, but the coming
years will see a huge increase in the number of
connected consumer devices. The penetration
of existing devices (such as cars, fitness trackers,
home assistants, smartphones, and smart watches)
will continue to increase rapidly, joined by new,
growing categories such as clothing, eyewear, home
appliances, medical devices, and shoes. Experts
estimate there will be up to one trillion connected
devices by 2025.2 The resulting avalanche of new
data created by these devices will allow carriers
to understand their clients more deeply, resulting
in new product categories, more personalized
pricing, and increasingly real-time service delivery.
2

For example, a wearable that is connected to an
actuarial database could calculate a consumer’s
personal risk score based on daily activities as well
as the probability and severity of potential events.
Increased prevalence of physical robotics
The field of robotics has seen many exciting
achievements recently, and this innovation will
continue to change how humans interact with
the world around them. Additive manufacturing,
also known as 3-D printing, will radically reshape
manufacturing and the commercial insurance
products of the future. By 2025, 3-D-printed
buildings will be common, and carriers will
need to assess how this development changes
risk assessments. In addition, programmable,
autonomous drones; autonomous farming
equipment; and enhanced surgical robots will all be
commercially viable in the next decade. By 2030,
a much larger proportion of standard vehicles will

World Economic Forum, 2015.

Insurance 2030—The impact of AI on the future of insurance

3

Experts estimate there will be up to one
trillion connected devices by 2025.
have autonomous features, such as self-driving
capabilities. Carriers will need to understand how
the increasing presence of robotics in everyday life
and across industries will shift risk pools, change
customer expectations, and enable new products
and channels.
Open-source and data ecosystems
As data becomes ubiquitous, open-source
protocols will emerge to ensure data can be
shared and used across industries. Various public
and private entities will come together to create
ecosystems in order to share data for multiple
use cases under a common regulatory and
cybersecurity framework. For example, wearable
data could be ported directly to insurance carriers,
and connected-home and auto data could be made
available through Amazon, Apple, Google, and a
variety of consumer device manufacturers.
Advances in cognitive technologies
Convolutional neural networks and other deep
learning technologies currently used primarily for
image, voice, and unstructured text processing
will evolve to be applied in a wide variety of
applications. These cognitive technologies, which
are loosely based on the human brain’s ability to
learn through decomposition and inference, will
become the standard approach for processing the
incredibly large and complex data streams that
will be generated by “active” insurance products
tied to an individual’s behavior and activities.
With the increased commercialization of these
types of technologies, carriers will have access to
models that are constantly learning and adapting
to the world around them—enabling new product
categories and engagement techniques while
responding to shifts in underlying risks or behaviors
in real time.

3

4

The state of insurance in 2030
AI and its related technologies will have a seismic
impact on all aspects of the insurance industry,
from distribution to underwriting and pricing to
claims. Advanced technologies and data are already
affecting distribution and underwriting, with policies
being priced, purchased, and bound in near real
time. An in-depth examination at what insurance
may look like in 2030 highlights dramatic changes
across the insurance value chain.
Distribution
The experience of purchasing insurance is faster,
with less active involvement on the part of the
insurer and the customer.3 Enough information is
known about individual behavior, with AI algorithms
creating risk profiles, so that cycle times for
completing the purchase of an auto, commercial,
or life policy will be reduced to minutes or even
seconds. Auto and home carriers have enabled
instant quotes for some time but will continue to
refine their ability to issue policies immediately
to a wider range of customers as telematics and
in-home Internet of Things (IoT) devices proliferate
and pricing algorithms mature. Many life carriers
are experimenting with simplified issue products,
but most are restricted to only the healthiest
applicants and are priced higher than a comparable
fully underwritten product. As AI permeates life
underwriting and carriers are able to identify risk
in a much more granular and sophisticated way,
we will see a new wave of mass-market instant
issue products.
Smart contracts enabled by blockchain
instantaneously authorize payments from a
customer’s financial account. Meanwhile, contract
processing and payment verification are eliminated
or streamlined, reducing customer acquisition costs

 imon Kaesler, Matt Leo, Shannon Varney, and Kaitlyn Young, “How insurance can prepare for the next distribution model,” June 12, 2020,
S
McKinsey.com.

Insurance 2030—The impact of AI on the future of insurance

for insurers. The purchase of commercial insurance
is similarly expedited as the combination of drones,
IoT, and other available data provides sufficient
information for AI-based cognitive models to
proactively generate a bindable quote.
Highly dynamic, usage-based insurance (UBI)
products proliferate and are tailored to the
behavior of individual consumers. Insurance
transitions from a “purchase and annual
renewal” model to a continuous cycle, as product
offerings constantly adapt to an individual’s
behavioral patterns. Furthermore, products are
disaggregated substantially into microcoverage
elements (for example, phone battery insurance,
flight delay insurance, different coverage for a
washer and dryer within the home) that consumers
can customize to their particular needs, with the
ability to instantaneously compare prices from
various carriers for their individualized baskets of
insurance products. New products emerge to cover
the shifting nature of living arrangements and
travel. UBI becomes the norm as physical assets
are shared across multiple parties, with a pay-bymile or pay-by-ride model for car sharing and payby-stay insurance for home-sharing services, such
as Airbnb.4
The role of insurance agents has changed
dramatically by 2030. The number of agents is
reduced substantially as active agents retire and
remaining agents rely heavily on technology to
increase productivity. The role of agents transitions
to process facilitators and product educators.
The agent of the future can sell nearly all types of
coverage and adds value by helping clients manage
their portfolios of coverage across experiences,
health, life, mobility, personal property, and
residential. Agents use smart personal assistants
to optimize their tasks as well as AI-enabled bots
to find potential deals for clients. These tools help
agents to support a substantially larger client
base while making customer interactions (a mix
of in-person, virtual, and digital) shorter and more

meaningful, given that each interaction will be
tailored to the exact current and future needs of
each individual client.
Underwriting and pricing
In 2030, underwriting as we know it today ceases
to exist for most personal and small-business
products across life and property and casualty
insurance.5 The process of underwriting is reduced
to a few seconds as the majority of underwriting
is automated and supported by a combination of
machine and deep learning models built within
the technology stack. These models are powered
by internal data as well as a broad set of external
data accessed through application programming
interfaces and outside data and analytics providers.
Information collected from devices provided by
mainline carriers, reinsurers, product manufacturers,
and product distributors is aggregated in a variety
of data repositories and data streams. These
information sources enable insurers to make ex
ante decisions regarding underwriting and pricing,
enabling proactive outreach with a bindable quote
for a product bundle tailored to the buyer’s risk
profile and coverage needs.
Regulators review AI-enabled, machine learning–
based models, a task that requires a transparent
method for determining traceability of a score
(similar to the rating factor derivations used today
with regression-based coefficients). To verify
that data usage is appropriate for marketing and
underwriting, regulators assess a combination of
model inputs. They also develop test policies for
providers when determining rates in online plans
to ensure the algorithm results are within approved
bounds. Public policy considerations limit access
to certain sensitive and predictive data (such as
health and genetic information) that would decrease
underwriting and pricing flexibility and increase
antiselection risk in some segments.
Price remains central in consumer decision making,
but carriers innovate to diminish competition purely

 ome insurtech companies are already designing these types of products; Slice, for example, provides variable commercial insurance
S
specifically tailored for home sharing.
5
Ramnath Balasubramanian, Ari Chester, and Nick Milinkovich, “Rewriting the rules: Digital and AI-powered underwriting in life insurance,”
July 31, 2020, McKinsey.com.
4

Insurance 2030—The impact of AI on the future of insurance

5

on price. Sophisticated proprietary platforms
connect customers and insurers and offer
customers differentiated experiences, features,
and value. In some segments, price competition
intensifies, and razor-thin margins are the norm,
while in other segments, unique insurance offerings
enable margin expansion and differentiation. In
jurisdictions where change is embraced, the pace
of pricing innovation is rapid. Pricing is available in
real time based on usage and a dynamic, data-rich
assessment of risk, empowering consumers to
make decisions about how their actions influence
coverage, insurability, and pricing.
Claims
Claims processing in 2030 remains a primary
function of carriers, but head count associated with
claims is reduced by 70 to 90 percent compared
with 2018 levels.6 Advanced algorithms handle initial
claims routing, increasing efficiency and accuracy.
Claims for personal lines and small-business
insurance are largely automated, enabling carriers to
achieve straight-through processing rates of more
than 90 percent and dramatically reducing claims
processing times from days to hours or minutes.
IoT sensors and an array of data-capture
technologies, such as drones, largely replace
traditional, manual methods of first notice of
loss. Claims triage and repair services are often
triggered automatically upon loss. In the case of
an auto accident, for example, a policyholder takes
streaming video of the damage, which is translated
into loss descriptions and estimate amounts.
Vehicles with autonomous features that sustain
minor damage direct themselves to repair shops for
service while another car with autonomous features
is dispatched in the interim. In the home, IoT devices
will be increasingly used to proactively monitor
water levels, temperature, and other key risk factors
and will proactively alert both tenants and insurers
of issues before they arise.

6

6

Automated customer service apps handle most
policyholder interactions through voice and text,
directly following self-learning scripts that interface
with the claims, fraud, medical service, policy, and
repair systems. The turnaround time for resolution
of many claims is measured in minutes rather
than days or weeks. Human claims management
focuses on a few areas: complex and unusual claims,
contested claims where human interaction and
negotiation are empowered by analytics and datadriven insights, claims linked to systemic issues
and risks created by new technology (for example,
hackers infiltrate critical IoT systems), and random
manual reviews of claims to ensure sufficient
oversight of algorithmic decision making.
Claims organizations increase their focus on risk
monitoring, prevention, and mitigation. IoT and new
data sources are used to monitor risk and trigger
interventions when factors exceed AI-defined
thresholds. Customer interaction with insurance
claims organizations focuses on avoiding potential
loss. Individuals receive real-time alerts that
may be linked with automatic interventions for
inspection, maintenance, and repair. For large-scale
catastrophe claims, insurers monitor homes and
vehicles in real time using integrated IoT, telematics,
and mobile phone data, assuming mobile phone
service and power haven’t been disrupted in the
area. When power goes out, insurers can prefile
claims by using data aggregators, which consolidate
data from satellites, networked drones, weather
services, and policyholder data in real time. This
system is pretested by the largest carriers across
multiple catastrophe types, so highly accurate loss
estimations are reliably filed in a real emergency.
Detailed reports are automatically provided to
reinsurers for faster reinsurance capital flow.

 ndy Fong, Kristen Ganjani, Elixabete Larrea, and José Miguel Novo Sánchez, “Claims 2030: A talent strategy for the future of insurance
A
claims,” December 10, 2020, McKinsey.com.

Insurance 2030—The impact of AI on the future of insurance

Article Title
Exhibit X of X

How insurers can prepare for
accelerating changes
The rapid evolution of the industry will be fueled
by the extensive adoption and integration of
automation, deep learning, and external data
ecosystems. While no one can predict exactly what
insurance might look like in 2030, carriers can take
several steps now to prepare for change.
1. Get smart on AI-related technologies
and trends
Although the tectonic shifts in the industry will be
tech-focused, addressing them is not the domain
of the IT team. Instead, board members and
customer-experience teams should invest the time
and resources to build a deep understanding of
these AI-related technologies. Part of this effort
will require exploring hypothesis-driven scenarios
in order to understand and highlight where and
when disruption might occur—and what it means
for certain business lines. For example, insurers are
unlikely to gain much insights from limited-scale
IoT pilot projects in discrete parts of the business.
Instead, they must proceed with purpose and an
understanding of how their organization might
participate in the IoT ecosystem at scale. Pilots
and proof-of-concept (POC) projects should be
designed to test not just how a technology works
but also how successful the carrier might be
operating in a particular role within a data- or IoTbased ecosystem.
2. Develop and begin implementation of a
coherent strategic plan
Building on the insights from AI explorations,
carriers must decide how to use technology
to support their business strategy. The senior
leadership team’s long-term strategic plan will
require a multiyear transformation that touches
operations, talent, and technology. Some carriers
are already beginning to take innovative approaches
such as starting their own venture-capital arms,
acquiring promising insurtech companies, and
forging partnerships with leading academic

Insurance 2030—The impact of AI on the future of insurance

Exhibit 2

There are four core elements in
defining a successful AI strategy.
Analytics strategy

Data
capabilities

Organization
and talent

Culture

Models
and tools

Change
management

institutions. Insurers should develop a perspective
on areas they want to invest in to meet or beat the
market and what strategic approach—for example,
forming a new entity or building in-house strategic
capabilities—is best suited for their organization.
This plan should address all four dimensions involved
in any large-scale, analytics-based initiative—
everything from data to people to culture (Exhibit 2).
The plan should outline a road map of AI-based pilots
and POCs and detail which parts of the organization
will require investments in skill building or focused
change management. Most important, a detailed
schedule of milestones and checkpoints is essential
to allow the organization to determine, on a regular
basis, how the plan should be modified to address
any shifts in the evolution of AI technologies and
significant changes or disruptions within the industry.
In addition to being able to understand and
implement AI technologies, carriers also need to
develop strategic responses to coming macrolevel

7

changes. As many lines shift toward a “predict and
prevent” methodology, carriers will need to rethink
their customer engagement and branding, product
design, and core earnings. Auto accidents will be
reduced through use of vehicles with self-driving
capabilities, in-home flooding will be prevented by
IoT devices, buildings will be reprinted after a natural
disaster, and lives will be saved and extended
by improved healthcare. Likewise, vehicles will
still break down, natural disasters will continue
to devastate coastal regions, and individuals will
require effective medical care and support when a
loved one passes. As these changes take root, profit
pools will shift, new types and lines of products
will emerge, and how consumers interact with their
insurers will change substantially.

agile development of new analytics insights and
capabilities. With external data, carriers must
focus on securing access to data that enriches
and complements their internal data sets. The real
challenge will be gaining access in a cost-efficient
way. As the external data ecosystem continues
to expand, it will likely remain highly fragmented,
making it quite difficult to identify high-quality data
at a reasonable cost. Overall, data strategy will need
to include a variety of ways to obtain and secure
access to external data, as well as ways to combine
this data with internal sources. Carriers should
be prepared to have a multifaceted procurement
strategy that could include the direct acquisition of
data assets and providers, licensing of data sources,
use of data APIs, and partnerships with data brokers.

Winning carriers of the future will create and enact
strategic plans that position their brand, products,
customer interactions, and technology successfully
to take advantage of the new economic structure on
the horizon.

4. Create the right talent and technology
infrastructure
In augmented chess, average players enabled by AI
tend to do better than expert chess players enabled
by the same AI. The underlying reason for this
counterintuitive outcome depends on whether the
individual interacting with AI embraces, trusts, and
understands the supporting technology. To ensure
that every part of the organization views advanced
analytics as a must-have capability, carriers must
make measured but sustained investments in
people. The insurance organization of the future will
require talent with the right mindsets and skills.7 The
next generation of successful frontline insurance
workers will be in increasingly high demand and
must possess a unique mix of being technologically
adept, creative, and willing to work at something
that will not be a static process but rather a mix of
semiautomated and machine-supported tasks that
continually evolve. Generating value from the AI use
cases of the future will require carriers to integrate
skills, technology, and insights from around the
organization to deliver unique, holistic customer
experiences. Doing so will require a conscious
culture shift for most carriers that will rely on buy-in

All of these efforts can produce a coherent analytics
and technology strategy that addresses all aspects
of the business, with a keen eye on both value
creation and differentiation.
3. Create and execute a comprehensive
data strategy
Data is fast becoming one of the most—if not the
most—valuable asset for any organization. The
insurance industry is no different: how carriers
identify, quantify, place, and manage risk is all
predicated on the volume and quality of data
they acquire during a policy’s life cycle. Most AI
technologies will perform best when they have a
high volume of data from a variety of sources. As
such, carriers must develop a well-structured and
actionable strategy with regard to both internal
and external data. Internal data will need to be
organized in ways that enable and support the

7

8

J ulie Goran, Krish Krishnakanthan, Eleonora Sharef, and Kurt Strovink, “How US insurers can build a winning digital workforce for the future,”
September 9, 2020, McKinsey.com.

Insurance 2030—The impact of AI on the future of insurance

and leadership from the executive suite. Developing
an aggressive strategy to attract, cultivate, and
retain a variety of workers with critical skill sets will
be essential to keep pace. These roles will include
data engineers, data scientists, technologists, cloud
computing specialists, and experience designers. To
retain knowledge while also ensuring the business
has the new skills and capabilities necessary to
compete, many organizations will design and
implement reskilling programs. As a last component
of developing the new workforce, organizations
will identify external resources and partners to
augment in-house capabilities that will help carriers
secure the needed support for business evolution
and execution. The IT architecture of the future will
also be radically different from today’s. Carriers
should start making targeted investments to enable
the migration to a more future-forward technology
8
stack that can support a two-speed IT architecture.

8

Rapid advances in technologies in the next decade
will lead to disruptive changes in the insurance
industry. The winners in AI-based insurance will
be carriers that use new technologies to create
innovative products, harness cognitive learning
insights from new data sources, streamline
processes and lower costs, and exceed customer
expectations for individualization and dynamic
adaptation. Most important, carriers that adopt a
mindset focused on creating opportunities from
disruptive technologies—instead of viewing them as
a threat to their current business—will thrive in the
insurance industry in 2030.

Driek Desmet, Markus Löffler, and Allen Weinberg, “Modernizing IT for a digital era,” September 1, 2016, McKinsey.com.

Ramnath Balasubramanian and Ari Libarikian are senior partners in McKinsey’s New York office, and Doug McElhaney is a
partner in the Washington, DC, office.
The authors would like to acknowledge the contributions of Gijs Biermans, Bayard Gennert, Nick Milinkovich, and Erik Summers.

Copyright © 2021 McKinsey & Company. All rights reserved.

Insurance 2030—The impact of AI on the future of insurance

9

Contact
To connect with someone on this topic in your region, please contact:
Americas
Ramnath Balasubramanian
Senior partner, New York
Ramnath_Balasubramanian@McKinsey.com

Europe
Elena Pizzocaro
Partner, Milan
Elena_Pizzocaro@McKinsey.com

João Bueno
Senior partner, São Paulo
Joao_Bueno@McKinsey.com

Khaled Rifai
Partner, Berlin
Khaled_Rifai@McKinsey.com

Ari Libarikian
Senior partner, New York
Ari_Libarikian@McKinsey.com

Asia
Suho Kim
Partner, Seoul
Suho_Kim@McKinsey.com

Doug McElhaney
Partner, Washington DC
Doug_McElhaney@McKinsey.com
Salomon Spak
Partner, Lima
Salomon_Spak@McKinsey.com

Further insights
McKinsey’s Insurance Practice publishes on issues of interest to industry executives.
Our recent articles include:

10

Rewriting the rules: Digital and AI-powered
underwriting in life insurance

Future of insurance: Unleashing growth through
new business building

How US insurers can build a winning digital
workforce for the future

Claims 2030: A talent strategy for the future of
insurance claims

Insurance 2030—The impact of AI on the future of insurance

Issues Paper on Increasing Digitalisation in
Insurance and its Potential Impact on
Consumer Outcomes

November 2018

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 1 of 36

About the IAIS
The International Association of Insurance Supervisors (IAIS) is a voluntary membership
organisation of insurance supervisors and regulators from more than 200 jurisdictions. The
mission of the IAIS is to promote effective and globally consistent supervision of the insurance
industry in order to develop and maintain fair, safe and stable insurance markets for the benefit
and protection of policyholders and to contribute to global financial stability.
Established in 1994, the IAIS is the international standard setting body responsible for
developing principles, standards and other supporting material for the supervision of the
insurance sector and assisting in their implementation. The IAIS also provides a forum for
Members to share their experiences and understanding of insurance supervision and insurance
markets.
The IAIS coordinates its work with other international financial policymakers and associations
of supervisors or regulators, and assists in shaping financial systems globally. In particular, the
IAIS is a member of the Financial Stability Board (FSB), member of the Standards Advisory
Council of the International Accounting Standards Board (IASB), and partner in the Access to
Insurance Initiative (A2ii). In recognition of its collective expertise, the IAIS also is routinely
called upon by the G20 leaders and other international standard setting bodies for input on
insurance issues as well as on issues related to the regulation and supervision of the global
financial sector.
Issues Papers provide background on particular topics, describe current practices, actual
examples or case studies pertaining to a particular topic and/or identify related regulatory and
supervisory issues and challenges. Issues Papers are primarily descriptive and not meant to
create expectations on how supervisors should implement supervisory material. Issues Papers
often form part of the preparatory work for developing standards and may contain
recommendations for future work by the IAIS.
International Association of Insurance Supervisors c/o Bank for International Settlements
CH-4002 Basel
Switzerland
Tel: +41 61 280 8090
Fax: +41 61 280 9151
www.iaisweb.org
This document was prepared by the Market Conduct Working Group in consultation with IAIS
Members.
This document is available on the IAIS website (www.iaisweb.org).
© International Association of Insurance Supervisors (IAIS), 2018.
All rights reserved. Brief excerpts may be reproduced or translated provided the source is stated.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 2 of 36

Contents
Executive summary ............................................................................................................... 4
Acronyms .............................................................................................................................. 6
1

Introduction .................................................................................................................... 7

2

Product Design .............................................................................................................. 9

3

2.1

Digitalisation Impact on Product Design .................................................................. 9

2.2

Examples of digitalisation impact on product design ............................................. 10

2.2.1

Background ................................................................................................... 10

2.2.2

Shared economy............................................................................................ 10

2.2.3

Usage based insurance ................................................................................. 11

2.2.4

On-demand insurance ................................................................................... 12

Marketing, Sales & Distribution .................................................................................... 14
3.1

Marketing and promotions..................................................................................... 14

3.1.1

Benefits and opportunities.............................................................................. 15

3.1.2

Potential Risks ............................................................................................... 15

3.2

(Robo) Advice ....................................................................................................... 17

3.2.1

Types of advice.............................................................................................. 17

3.2.2

Benefits and opportunities of robo advice ...................................................... 18

3.2.3

Potential risks ................................................................................................ 18

3.3

Price Comparison Websites .................................................................................. 19

3.3.1

Benefits and opportunities.............................................................................. 20

3.3.2

Potential risks ................................................................................................ 20

3.4

Disclosure and informed decision-making ............................................................. 21

3.4.1

Benefits and opportunities.............................................................................. 21

3.4.2

Potential Risks ............................................................................................... 22

4

Supervisory issues ....................................................................................................... 24

5

Conclusion and recommendations ............................................................................... 34

Annex: Digital technologies and alternative business models affecting insurance business 35

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 3 of 36

Executive summary
1.
Digitalisation is transforming insurance business. Examples such as mobile devices, the
internet of things (IoT), Big Data, artificial intelligence (AI), chat-bots, distributed ledger
technology (DLT), and robo advisors have an impact throughout the insurance value chain: from
the design, underwriting and pricing of products, their marketing and distribution, through to
claims processing and the ongoing management of customers.
2.
The purpose of this paper is to consider the impact of the trend of increasing digitalisation
in insurance on consumer outcomes and insurance supervision in light of Insurance Core
Principle 19 on Conduct of Business. The focus is on product design and underwriting, along
with marketing, sales and distribution aspects of the insurance value chain. It is recognised that
the impact of digitalisation may differ between jurisdictions depending on the legal frameworks
in place.
3.
In respect of product design, digitalisation may affect the nature of insurance coverage
through, for example, on-demand insurance, usage-based insurance and insurance based on
consumer-generated data from vehicles, homes or wearable devices. This can potentially
service a broader clientele (including people that are currently un(der)-served) if insurers are
able to adapt to evolving demands from the market. The data available to insurers on use of,
for example motor vehicles, will inform the pricing of the product, while consumers need to be
aware of such use. Risk pricing can be more tailored to the use and risk profile of the customer,
which can affect both the price and required reserves of insurers.
4.
In terms of marketing and promotions, digitalisation will have an impact on the
information provided to consumers. Regardless of the use of digital technology, the information
provided needs to be timely, clear, accurate and not misleading.
5.
Greater availability of customer related data, increased analytics and enhanced digital
deployment tools enable insurers and intermediaries to identify opportunities across the
insurance value chain to reduce customer friction, increase efficiencies and improve the overall
customer experience through digital technology.
6.
The use of social media may enable insurers and intermediaries to better reach target
markets. This may reduce marketing costs. It can, for example, improve customers’ experience
by offering easier and quicker ways for the insurer and consumer to communicate. On the other
hand, social media applications may not be transparent to consumers. This can result in
consumers being “nudged” without being aware – such as when consumers are confronted with
unsolicited offerings based on their use of the internet. There is a risk that customers are
persuaded into buying products or add-ons that are not in their best interest.
7.
A specific emerging sales method is the use of robo advice. This may improve
accessibility of products to the customer. It will, however, require proper design of underlying
algorithms and adequate availability and use of customer data. Also, depending on the
sophistication of the algorithm and available datasources, not all benefits of face-to-face
interaction between salesperson and customer may succeed, for example, in identifying (nonverbal) hesitation. Furthermore, flaws in the design and operation of the algorithm can create a
risk of selling products that are not (entirely) in the interest of the particular customer.
8.
Another development for promotion and sales is the use of price comparison websites
(PCWs). These can provide automated suggestions or proposals on products, providers and
prices based on input by the consumer. Increased accessibility and comparability of information
Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 4 of 36

on insurers and products as well as easy use of the on-line systems are benefits to the customer.
There may, however, be issues around transparency with respect to the identity and
independence of the owner/operator of the comparison website. Consumers may also be at risk
of selecting products that are less suitable for their needs if they solely focus on price and not
other elements of the product, such as its coverage.
9.
More generally, innovations influence the presentation and disclosure of information.
They offer the potential to provide relevant information to consumers in a useable manner at the
relevant time. Large volumes of information may, however, be difficult to read and understand,
for instance when using smart phones. Insurers should be mindful of the risk of misconceptions
by consumers and, therefore, flaws in consent given by digital means.
10.
As digitalisation changes the way insurance products are designed and distributed,
supervisors should monitor these developments, engage stakeholders both within and outside
the insurance industry and consider new supervisory responses to protect consumers’ interests.
One of the key challenges to supervisors will be to consider a balanced approach to facilitate
innovations while maintaining the level of consumer protection stipulated in laws and
regulations. Supervisors are likely to be confronted with new insurance market participants, like
start-ups and “Big Tech” firms. These entities may have different perspectives on consumer
interest and compliance culture than traditional incumbent insurers. Supervisors should be
cognisant of this and may need to take a proactive approach including by “educating” these new
market participants. Other challenges supervisors face are developing new tools and skills for
supervision of increasingly digitalised firms, enhancing cooperation with financial and other
authorities, safeguarding the supervisory parameters to prevent regulatory arbitrage and
enhancing information security.
11.
Supervisors should consider taking appropriate steps, such as issuing guidelines, to help
promote responsible use of new technologies by insurers and intermediaries and safeguard the
fair treatment of customers.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 5 of 36

Acronyms
ACPR

Autorité de contrôle prudentiel et de résolution (France)

AI

Artificial Intelligence

AFM

Autoriteit Financiele Markten (Netherlands)

AMF

Autorité des marchés financiers (Québec)

ASIC

Australian Securities and Investments Commission

BaFin

Bundesanstalt für Finanzdienstleistungsaufsicht (Germany)

BoE

Bank of England

DLT

Distributed Ledger Technology

FCA

Financial Conduct Authority

FINMA

Financial Market Supervisory Authority (Switzerland)

FinTech

Financial technology

FSB

Financial Stability Board

IAIS

International Association of Insurance Supervisors

ICP

Insurance Core Principle

Insurtech

Insurance technology

IoT

Internet of Things

IT

Information Technology

MAS

Monetary Authority of Singapore

ML

Machine Learning

NAIC

National Association of Insurance Commissioners (USA)

PCW

Price Comparison Website

Regtech

Regulatory Technology

Suptech

Supervisory Technology

UBI

Usage based insurance

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 6 of 36

1

Introduction

12.
Described by some observers as the “fourth industrial revolution” 1, digitalisation is
rapidly transforming societies and their economies. The velocity and scope of change are
significant. Digitalisation has the potential to fundamentally change almost every industry in
every country. 2 One of those industries is insurance. There has been and continues to be a
revolution in insurance pricing, marketing, product design and claims settlement resulting from
insurers' use of new technologies and available data.
13.
As diagram 1 illustrates, rapid change is evident throughout the insurance value chain:
from the design, underwriting and pricing of products, their marketing and distribution, through
to claims processing and the ongoing management of customer relationships. The examples of
digitalisation technologies – machine learning and artificial intelligence, distributed ledger
technology (eg blockchain) – and applications – telematics, robo-advisers, peer-to-peer and
platford business models – as set out in diagram one, are varied. These are described in the
Annex.
DIAGRAM 1: Digitalisation and the insurance value chain
•
•
•
•

Customer-specific “targeted”
marketing
Robo-advice, AI and chatbots
Internet sales and Price
Comparison websites
Social media and SMART
phone/device channels for
direct distribution

Marketing,
Sales and
Distribution

•

•
•
•
•
•

Pricing and
Underwriting

•

•

•

Automated (including nonhuman) product service
centers using robo-advice,
chat-bots and AI
Big Data enables ability to
predict what customers want
and need before they ask for
it
Continuous real-time
customer communication and
U/W

Product
Management

Telemetrics – customers and insurers
understand risk much better (wearables, IoT,
SMART phones, apps)
Big Data enabling more granular and
accurate pricing and faster U/W
Blockchain technologies to seamlessly
manage and instantly verify data sources
Peer-to-peer insurance models
Granular, customer-specific product
offerings including usage based insurance
Genetic data – potential impact on pricing
and availability

•
•
•

•

Claims
Handling

•
•
•
•

•

Platform business models
360 degree view of customer for
consultants
Continuous real time data
enabling focus on high value
customers
Unstructured data (eg voice)
analysis and learning

Customer
Interactions

Fraud detection using Big Data and
Blockchain
Blockchain facilitating trust-worthy and
timely claims information
AI and drones in assessing processes
Claims cost efficiencies re online/SMART
device claims lodgement, AI/automated
assessing, optimised pay-outs, reduced
labour costs
Supply chain management efficiencies,
vertically integrated claims processes

See for example, Schwab, K., “The Fourth Industrial Revolution: what it means, how to respond”,
https://www.weforum.org/agenda/2016/01/the-fourth-industrial-revolution-what-it-means-and-how-torespond/, 14 January 2016.
2 Schwab.
1

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 7 of 36

14.
The availability of data from sources such as telematics and wearable devices enable
insurers to design and price products on the basis of more information about the consumer.
Developments in artificial intelligence and machine learning enable the provision of automated
advice and facilitate fraud detection. Comparison websites can provide consumers with more
information about the range of products available. New technologies are able to speed up
processes, such as claims handling, and can lead to efficiencies that drive down costs.
15.
Whilst digitalisation has the potential to benefit consumers, it does give rise to risks that
could impact fair consumer outcomes, which should be considered by supervisors in light of the
requirements on Conduct of Business in Insurance Core Principle (ICP) 19. These include
potential impacts from reduced face-to-face contact, insufficient consumer understanding of the
product or service and its provider, risks in the security and potential misuse of increasing
amounts of consumer data, and potential exclusion for some consumers. The collection of data
on policyholders may enable a more granular risk categorisation that could potentially affect risk
pooling principles and may lead to issues around affordability of certain insurance products,
possibly even leading to exclusion. 3
16.
Supervisors should, therefore, (possibly in new ways) monitor consumer outcomes
carefully to ensure that supervisory regimes continue to facilitate the benefits to consumers from
technology and innovation, whilst safeguarding policyholder protection. This is vital if the intent
of ICP 19 is to be met.
17.
The purpose of this paper is to consider the impact of the increasing use of digital
technology in insurance. It will consider consumer outcomes and discuss what digitalisation
means for insurance supervision. It is recognised that the impact of digitalisation may differ
between jurisdictions depending on the legal frameworks in place. A distinction should be made
between applications and the data/information these applications use or generate. While this
paper cover both innovative applications and – more generally – the use of data, a separate
paper will discuss in more details the use of personal and other data from a conduct of business
perspective.
18.
This paper provides these considerations in the context of other IAIS work on FinTech
and Insurtech. Accordingly, this paper focuses on the product design and underwriting along
with marketing, sales and distribution aspects of the insurance value chain. The impact of the
increasing use of digital technology on other aspects of the insurance value chain will be
addressed in other IAIS material.
19.

3

The paper has three sections:
•

Section 2 considers the impacts of digitalisation on product design and underwriting.
This sections also gives examples of digitalisation impact on product design;

•

In section 3, robo advice and price comparison websites are considered to illustrate
digitalisation’s impact on the marketing, sale and distribution of those products; and

•

Finally, in section 4, the paper describes the challenges faced by supervisors in
responding to these developments.

Paragraph 10 of the IAIS report “FinTech Developments in the Insurance Industry”, March 2017.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 8 of 36

2

Product Design

2.1

Digitalisation impact on product design

20.
Digitalisation is changing the risk landscape and creating the need for, and enabling, the
development of new types and lines of products. Consumer needs are evolving in the digital
age with a growing expectation for accessibility of services and solutions at any time, in any
place, and in a variety of ways. Consumer trends and habits supported by new technologies,
such as greater connectivity through wearables, smart phones and smart homes, and greater
optionality through the sharing economy, are impacting the way insurers design products for
their policyholders.
21.
The changes driven by new technology create new opportunities but also new
challenges and risks for insurers and intermediaries. The need for new insurance coverages
and new products is likely to grow. Digitalisation brings opportunities to better serve customers
and their changing needs, and may also serve as a means to better reach underserved markets.
22.
Digitalisation may also lead to a shift from distribution focused product design (supplydriven) to consumer focused product design (demand-driven). While this may provide great
opportunity, it may be a challenge for insurers to meet consumers’ growing need of covering
new risks, or covering them in a different way, in the future.
23.
Digitalisation is impacting how insurers develop, design and underwrite their products.
Advancement in technology may enable the development of more adaptable or tailored products
and the creation of new insurance products:
•

•
•

•

•

Big Data means more data for risk assessment, which can enable underwriting to be
based on more granular data, which may, in turn, increase accuracy and allow for faster
and more risk-specific underwriting. This needs to be balanced against the privacy
concerns of the individual;
AI may create new possibilities for risk assessment and underwriting. For example,
insurers can use algorithms in combination with AI that uses the customer’s insurance
history and lifestyle information to suggest insurance products and for onboarding;
The IoT may create new products focusing on prevention or situational insurance, for
example, a sensor will be able to monitor a household's water consumptions patterns,
detecting potential leaks and interrupting the flow before the basement is flooded, thus
preventing major damage and costly claims 4. Such tools can improve the interaction with
and provide value to the customer, though they can raise concerns if data from devices
(eg alerts) are used for premium increases or changes to existing coverage;
Telematics In the context of IoT, telematics involve telecommunications, sensors and
computer science to allow sending, receiving, storing and processing data via
telecommunication devices, with or without interfering with or steering of remote objects;
and
DLT may be able to seamlessly manage and instantly verify data sources. Smart
contracts (ie programmes that automatically execute the claim payment under predefined conditions stored in the blockchain) have the potential to be fully digital and fully
automated products, as could be the case for agricultural parametric/index-based
insurance. 5 If this technology proves to be a viable tool, it could transform the insurance

Bain&Company, "Digitalization in Insurance: The Multibillion Dollar Opportunity", Henrik Naujoks,
Florian Mueller and Nikos Kotalakidis, March 20, 2017.
5 The Geneva Papers 2017: "The Impact of Digitalization on the Insurance Value Chain and the
Insurability of Risks", Martin Eling and Martin Lehman, Institute of Economics, University of St. Gallen.
4

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 9 of 36

industry through a shared, transparent record of contract-related information, enabling
all parties to have an immutable audit trail underpinning end-to-end underwriting and
claims governance without the need for an intermediary. 6
24.
In the following section, we consider these benefits and risks in the context of examples
of the impacts of digitalisation on product design.
2.2

Examples of digitalisation impact on product design

2.2.1

Background

25.
There are numerous examples of digitalisation changing the nature of insurance
products. The following section provides examples of three of the most widespread and
significant examples namely:
•

Shared economy;

•

Usage-based-insurance; and

•

On-demand insurance.

26.
These examples involve a fundamental change in the design of product. But there are
also examples of changes where digitalisation has facilitated small specific changes to product
features.
United Kingdom
UK-based FinTech start-up Cuvva was set up to address the gap of providing hourly car
insurance to infrequent drivers who wanted to borrow other people’s cars. Cuvva allows
customers to arrange cover via an app in seconds. Cuvva manages the sale, service and
first notification of loss process through a mobile app.
Since launching the initial car sharing product, Cuvva has since launched a second
proposition designed for those who own a car, but seldom drive it. Customers pay a small
amount to insure their car whilst not driving, and pay an additional amount via the app for
the hours that they drive it.
Netherlands
Clixx (a product of Dutch insurer OHRA) offers the opportunity to insure also a borrowed
car. The product is bought per day. Clixx’ premium is lower if the borrowed car is already
fully insured, as compared to when the borrowed car only has the legally obligated liability
insurance.
2.2.2

Shared economy

27.
New sharing models are creating a unique challenge as traditional insurance protection
and coverage may not align with the needs and approaches taken in the shared economy. Many
insurance products currently offered are based on exclusive legal or economic ownership of a
good. The shared economy is based on the shared use of goods. Additionally, traditional
insurance products are generally intended to cover personal or commercial use of a good; they
are not designed to cover part-time business use, whether compensated or not.

Strategic RISK Europe: "How digitalisation will transform the risk and insurance industry": Dieter
Goebbels, country manager Germany and regional manager Central Europe at XL Catlin.

6

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 10 of 36

28.
The availability of insurance coverage adapted to the needs of the participants of the
shared economy is important for the further development of the sharing industry, and for
acceptance by consumers. To grow the shared economy and adequately mitigate potential risk,
participants – providers and users – need appropriate insurance coverage.
29.
Currently, participants in the shared economy who try to obtain insurance coverage
through traditional means may be faced with the impossibility of taking out coverage that fully
meets their particular needs. For example, drivers working for ride sharing businesses (Uber)
and homeowners participating as a host in shared hosting services (Airbnb) have not always
been able to find adequate insurance coverage. Traditional protections covering vehicles and
homes generally did not extend to new businesses in the sharing industry where personal
property is used in part-time business. The insurance industry has already developed new
products to meet the need for adapted coverage.
30.
It is important for consumers to understand the differences and limitations of their
insurance coverage when acting as either a provider or user in the shared economy. 7 When
offering products to consumers taking part in the shared economy, it is equally important for
insurers to be clear on such limitations. There is the risk of disruption and damage to the
reputation of the insurance industry if products designed to meet the shared economy do not
deliver the same level of consumer protection as traditional insurance products.
2.2.3

Usage based insurance

31.
Digitalisation is already used in automobile insurance. In terms of product design, a
traditional motor vehicle insurance policy is joined with a data collection and analytics tool to
capture data generated by the vehicle. In some cases, UBI product design includes various
forms of real-time and after-the-fact feedback data transmission between the insurer and the
consumers. In terms of pricing, insurers’ model prices are based on vehicle-generated data,
which in turn is based on the use of a vehicle by the insured – including where, how, when and
by whom the vehicle is driven. UBI also produces data used in claim settlements to discover or
corroborate the damage event.
32.
In order to obtain data on the use of a vehicle, insurers mostly use telematics through
which they may identify granular driving habits (eg distance travelled, hard braking, number of
trips, destinations). This data allows insurers to establish a rate more personalised to the
individual customer.
33.
Telematics can be app-based relying on a smartphone’s sensors and GPS signal,
making this functionality dependent on the underlying smartphone’s capabilities. However, this
personalisation may have limits as the operation of the vehicle by a person other than the
policyholder will affect their data and the calculation of their insurance premiums. Thus it is
important that consumers have the information they need to be properly informed and make
sound decisions about insurance products that use a UBI programme. In addition, consumers
should be aware of whether participation in such programmes is on a voluntary basis or not.
Information that may help inform consumers about the features of the UBI programme may
include things such as:
•

programme eligibility criteria;

See NAIC White Paper, Insurance Implications of Home-Sharing: Regulator Insights and Consumer
Awareness (http://www.naic.org/prod_serv/IHS-OP-16.pdf).

7

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 11 of 36

•

type of data collected;

•

use of data (eg as part of an investigation for the settlement of a claim; third parties to
whom access and use of data has been granted);

•

insurer employees who could have access to collected data;

•

impact of data on insurance premium; and

•

period used for insurance premium reviews.

Québec
In 2015, the AMF published a notice about its expectations regarding UBI programmes. This
initiative was intended to highlight, for insurers, firms and representatives offering non-life
insurance, the importance of effectively managing the risks associated with data sent via UBI
programmes used for automobile insurance underwriting. It also underlined the need to act
fairly in their dealings with consumers who participate in such programmes. 8
Netherlands
In the Netherlands, the sharing economy marks a trend of offering services that already
include an additional insurance. For example, the Dutch initiative Swapfiets (meaning “Swap
bike”) offers the opportunity to lease a bicycle for a fixed amount of money per month.
Repairments are included, as well as getting a new bike if the original one gets stolen. The
customer does not have to arrange an additional insurance product.
2.2.4

On-demand insurance

34.
The emergence of the shared economy, underpinned by a changing attitude and
behaviour of new consumer groups such as millennials, is causing a shift in the product lines of
insurers that are trying to respond to the need for self-directed, tech-savvy and hyperpersonalised products and services. Historically, most insurance has been purchased for a fixed
period of coverage – typically six months or a year at a time. Insurers' systems and processes
have been developed around this type of product and coverage, with a few exceptions, such as
travel insurance. In response to both changing ownership models through the sharing economy
in which a consumer may use a product or good for a limited period of time but not own the
product, and changing consumer desires for coverage limited to more precise time frames (such
as insuring a bicycle only when being used), new market entrants and incumbent insurers are
responding by developing new products and by adapting existing product lines, pricing and
customer service experience to create on-demand insurance.
Trov
Trov is a mobile app that allows users to collect and store information about their
possessions including the value. It partners with insurers to enable users to insure specific
possessions for specified durations. Users can literally turn insurance coverage on and off
by sliding the appropriate option on their mobile phone. For example, they could choose to
insure their mobile phone only when they are out of their house.
35.
The key to on-demand insurance is that it is temporal in nature. It provides insurance
coverage for specific periods of time that can be turned off and on. Users identify when they
https://lautorite.qc.ca/fileadmin/lautorite/reglementation/assurances-instdepot/notice_automobile_usage-based.pdf
8

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 12 of 36

need insurance and get coverage for a specific period to meet that need. The ability to insure
“moments” enables consumers to tailor coverage so that they only pay for coverage that they
need and to quickly alter insurance coverage to meet changing personal circumstances.
36.
However, users need to be constantly engaged by actively turning their coverage on or
off to obtain the benefits of on-demand insurance. Failure to constantly engage may result in
being under or over insured. Insurers should be cognisant of this and build in controls to mitigate
the risks they pose, which could include:
•

proactive messages to remind consumers that their coverage is still active or, perhaps
more importantly, inactive. AI and learning from behavioural economics could be used
to optimise this messaging;

•

systems that enable customers to turn coverage on and off for set periods on a
reoccurring basis. For example they could have insurance for a mobile activated when
they are out of their house and use location tracking to verify this; and

•

inbuilt terms and conditions that provide back-up coverage in those circumstances when
customers inadvertently fail to turn on coverage.

37.
Issues that have been discussed in this section, notably the issues around the use of
data, will be the subject of a seprate paper.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 13 of 36

3
3.1

Marketing, Sales & Distribution
Marketing and promotions

38.
Consistent with ICP 19 (Conduct of Business) insurance products must be marketed and
sold in a manner that pays due regard to the interests and needs of customers.
39.
Insurers and intermediaries should be required to provide timely, clear and adequate
pre-contractual and contractual information to customers. 9 Supervisors should apply to digital
insurance activities requirements on transparency and disclosure that provide an equivalent
level of protection to customers as those applied to insurance business conducted through nondigital means. 10 Marketing and advertising through digital means offer new opportunities to
inform and empower consumers but may pose certain additional challenges to the insurance
industry and supervisors alike and necessitate further consideration in terms of specific
regulatory requirements or industry responses.
South Africa
The Insurance Policyholder Protection Rules were recently amended to ensure that the rules
relating to advertising and marketing would apply similarly irrespective of the medium used
for such advertising. The definitions of “advertisement” and “direct marketing” were clarified
and widened in scope as follows:
“advertisement” means any communication published through any medium and in any form,
by itself or together with any other communication, which is intended to create public interest
in the business, policies or related services of an insurer, or to persuade the public (or a part
thereof) to transact in relation to a policy or related service of the insurer in any manner, but
which does not purport to provide detailed information to or for a specific policyholder
regarding a specific policy or related service
“direct marketing” means the marketing of a policy by or on behalf of an insurer by way of
telephone, internet, digital application platform, media insert, direct or electronic mail in a
manner which entails the completion or submission of an application, proposal, order,
instruction or other contractual information required by the insurer in relation to the entering
into of a policy or other transaction in relation to a policy or related services, but excludes the
publication of an advertisement
Australia
ASIC's Good Practice Guide on Advertising 11 covers digital advertising, including online
advertisements, video streaming, social media and microblogging. Some of the points
highlighted are:
•

the particular impact of advertising in a 'high trust' environment and the need to distinguish
clearly between advertisement and other content (ie on blogs); and

•

that, while online advertising can be beneficial if it provides links to additional information
for customers, this cannot make up for any misleading impressions created by the initial
ad, and the need for balance in the promotion.

Standard 19.7
Guidance 19.7.23
11 See http://download.asic.gov.au/media/1246974/rg234.pdf
9

10

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 14 of 36

ASIC has taken action against a number of potentially misleading social media
advertisements relating to self-managed super (pension) funds. 12
3.1.1

Benefits and opportunities

40.
Insurers and intermediaries are increasingly focusing on ways to improve marketing,
sales and distribution and to increase their ability to reach customers by the same digitalisation
technologies as seen in product design, including telematics, AI and Big Data. One example is
"targeted marketing"; the ability to develop specific marketing messages for individual
customers or potential customers.
41.
Digital marketing may reduce the marketing costs of the insurer or the intermediary,
creating savings that may be passed on to the customer. The use of Big Data may result in a
better understanding of customers, which can inform personalised marketing and appropriate
levels of disclosures.
42.
Greater availability of customer related data, increased analytics and enhanced digital
deployment tools enable insurers and intermediaries to identify opportunities across the
insurance value chain to reduce customer friction, increase efficiencies and improve the overall
customer experience through digital technology. Insurers and intermediaries can use enhanced
customer experience as product differentiation in marketing campaigns. For example, a Québec
start-up, Covera, 13 based its marketing strategy on its digital solution that promises to break out
of the standard insurance renewal process, identified as a common painpoint for customers.
43.
The use of targeted social media campaigns to relay promotional material is a common
way of targeting particular customers, who are most active on social media platforms. Insurers
are tapping into this market by using social media to make marketing seem less like “cold
advertising” and more like information sharing, entertainment or “infotainment”. Examples in the
US include Gecko, Allstate’s Mayhem, and Progressive’s Flo, whose promotional mascots are
instantly recognisable to insurance customers and who all have their own social media
presence. 14
44.
To overcome fragmented communication with the policyholder, insurers and
intermediaries can use digital devices and the Internet to connect with consumers throughout
the life of the policy, not only at underwriting or claim. For example, some insurers have started
to provide customers with prevention tools, such as a free water and humidity detector that
sends an alert by notification, text message or email if it senses a problem. Such initiatives are
part of the new digital brand marketing strategy. These tools are not designed to provide data
for determining premium or coverage, but rather to attract and retain customers.
3.1.2

Potential risks

45.
The use of social media platforms and other digital marketing campaigns as well as the
increased collection and use of data may increasingly lead to customers being “nudged” or
directed, including by advertising, without them being aware. For example,
insurers,intermediaries and third party marketers may “target the customer through specific
For example, see: http://asic.gov.au/about-asic/media-centre/find-a-media-release/2016-releases/16041mr-asic-stops-potentially-misleading-smsf-social-media-advertising/
13 https://covera.ai/
14 http://www.digitalistmag.com/customer-experience/2017/04/13/social-media-in-insurance-marketingtoday-05030403
12

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 15 of 36

search engines or click on sponsored links, or may channel customers by highlighting or limiting
particular information to produce certain actions from the customer. There is often a lack of
transparency in the existence and purpose of these practices.
46.
Targeted marketing through social media may, in some cases, become confusing for
customers who may struggle with distinguishing neutral opinions on social media from
promotional material sponsored by insurers.
47.
Digital marketing and mobile based applications could also be used to respond in real
time to individual circumstances, including consumers’ emotions, such as when an individual’s
insecurity or want is heightened, or during key life events. In the context of insurance, where an
intangible product is intended to mitigate personal fears, this type of emotional framing may
pose a concern.
Singapore
An example of such small specific changes in Singapore is PolicyPal, a start-up that helps
consumers organise, understand and purchase insurance policies digitally through a mobile
app. PolicyPal graduated from the MAS sandbox and is now an insurance broker registered
with MAS. 15
Australia
An example is the revelation in Australian media outlets that in May 2017 Facebook disclosed
to a major Australian bank that it could exploit the moods and insecurities of users for the
potential benefit of advertisers. 16 This followed media reports in 2012 that Facebook contributed
to a published study with the Proceedings of the National Academy of Sciences of the USA
where it showed via an experiment on over 689,000 users that it could make people more
positive or negative through a process of what it described as "emotional contagion" – when
positive expressions were reduced, people produced fewer positive posts and more negative
posts; when negative expressions were reduced, the opposite pattern occurred. 17
France
French 2016-R-01 Recommendation on the use of social media for business purposes
To remind professionals of supervisory expectations and to explain how rules apply to the use
of social media, the French Autorité de contrôle prudentiel et de résolution (ACPR) issued a
recommendation in November 2016, that applies to the banking and insurance sectors from 1
October 2017.
Firstly, advertising material issued through social media has to fulfil the applicable rules
regarding the information disclosed and the presentation of this information.
Secondly, professionals should refrain from having unfair commercial practices when using
social media. For instance, misleading opinions (good or bad) issued by professionals on social
media should be avoided, as well as the practice of buying “likes” or “followers”.

https://www.theguardian.com/technology/2017/may/01/facebook-advertising-data-insecure-teens
See: http://www.pnas.org/content/111/24/8788.full
https://www.mckinsey.com/industries/financial-services/our-insights/insurtech-the-threat-that-inspires
16
17

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 16 of 36

Moreover, according to the ACPR recommendation, professionals should set up procedures on
the disclosure of content on social media. 18
48.
The promotion of “add on” insurance products during the sales of another product has
been on supervisors’ radar for a while. The use of digital means to market and sell insurance
products can nevertheless facilitate these practices. A common example is offering travel
insurance during the online sales process for airline tickets. In this example, whilst it could be in
the customer's interest to be informed of travel insurance options, the timing of the promotion at
the end of the sales process when the customer has already bought the airline ticket(s), and the
way in which the message is delivered, could result in customers believing that the purchase of
the add on insurance product is required before the primary purchase of the airline ticket(s) can
be completed. This creates the risk of passive purchases and of the purchase of a cover that is
not needed.
United Kingdom
In 2014, the FCA conducted a market study on the General Insurance Add-Ons. The market
study found that the add-on distribution method has a real impact on consumer behaviour
and affects consumer decision-making. Consumers often focus on the sale of the primary
product, leading many to purchase add-on products that they do not need or understand. The
FCA also found that consumers had poor awareness of what products they had bought – with
19% being unaware that they owned the add-ons considered in the market study. The findings
indicate that consumers’ ability to make choices is often hindered by insufficient information
being available about the quality and price of the add-ons, and by this information being
presented too late in the buying process. Following these findings, the FCA has implemented
two remedies to address these specific issues:
• A ban on opt-out selling; and
• Improved information provision for add-on buyers.
3.2

(Robo) advice

3.2.1

Types of advice

49.
Robo advice is essentially financial advice that is automated. In practice, a distinction
can be made between the following types of advice:
•

•
•
•

Full robo advice: the robo adviser completely takes over the work of the traditional
financial adviser. The “customer journey” is fully digitalised and the advice is fully
automated. The only human role is to develop and maintain the robo advice system and
to prevent malfunctions of the algorithm. There is no face-to-face contact;
Partial robo advice: the advice is fully automated, but the traditional adviser is still
available to answer questions;
Hybrid advice: the robo adviser and human beings interact with each other. For example,
the “customer journey” is fully digitalised, but the advice is still provided by a human,
possibly face-to-face; and
Traditional face-to-face advice: technology is only used as an additional tool, for example
to show graphs or animations.

For more information, see (in French): https://acpr.banquefrance.fr/fileadmin/user_upload/acp/publications/registre-officiel/20161116Annexe_Reco_2013_R_01.pdf
18

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 17 of 36

3.2.2

Benefits and opportunities of robo advice

50.
Robo advice has the potential to improve both the accessibility and consistency of
financial advice. Accessibility means that financial advice is easily accessible for the majority of
consumers. This includes the continuous availability of advice from one’s home, which may also
reduce the costs for the consumer. Furthermore, the consistency of advice can be improved
through use of technology. When new financial products become available or when product
conditions change, the algorithm can instantly take these changes into account. When
programmed correctly and using sufficient and accurate data, robo advice will consistently be
of the same quality. The robo adviser may be helpful in overcoming cognitive bias or insufficient
competence on the part of the human adviser. Another potential benefit of robo advice is that
customers may find disclosing pre-exiting conditions, such as mental health, easier to do to a
machine than a human adviser. This could encourage more disclosures, leading to customers
receiving more appropriate coverage.
51.
Robo advice can be considered as another form of distribution in addition to internet or
telephone-based sales, potentially without providing advice to the customer. Robo advice can
also be part of enhanced consumer communications throughout the life of the product.
52.
As with traditional advice, robo advice should have a solid audit trail.By its very nature
robo advice will produce an audit trail through the technology used, rather than requiring this to
be completed manually. Provided that the technology delivers an audit trail that is reliable and
of good quality, robo advice could therefore make it easier to deliver traceability and auditability
of advice . For all advice given, the provider of robo advice should be able to provide insight into
the data used, the algorithms used and the information presented to the customer. This would
make the robo advice traceable and reproducible allowing the customer, any other subsequent
adviser and the supervisor to check how the advice was established.
Robo advice in the Netherlands
In the Netherlands, robo advice has been available for a couple of years for different types
of financial products. However, until last year, robo advice was only available for noncomplex products, such as car insurance. Since 2017, robo advice is also being developed
for complex financial products, such as disability insurance. One of the challenges, according
to the developers, is making sure that customers completely understand what is meant by
specific questions posed by the robo advisor, since there is no human advisor present to
answer questions.
3.2.3

Potential risks

53.
According to ICP 19 advice provided to consumers should take into account the
customer’s disclosed circumstances. All advice should be communicated in a clear and accurate
manner, comprehensible to the customer. Where advice is provided, this should be
communicated to the customer in written format, on paper or in a durable and accessible
medium, and a record kept in a “client file”. 19
54.
However, there may be specific issues that need to be addressed to safeguard the fair
treatment of customers who use robo advice. Robo advice cannot solve every limitation of
traditional face-to-face advice. Robo advice does not, for example, overcome problems flowing
19

Guidance 19.8.6

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 18 of 36

from limited selection of available products. Nor will it overcome all the problems caused by the
complexity of products.
55.
When the advice is fully automated, the customer might not have the opportunity to ask
questions, unless programmed in a robo chat. The risk of misunderstanding is therefore more
present in robo advice than in face-to-face advice. The lack of interaction between humans
might also lead to a reduced detection of contradicting answers by customers.
56.
A human adviser can recognise when the customer is in doubt, which a robo adviser
may not be able to do. However, robo advice tools could be programmed for that purpose – or
example, the algorithm could detect when a customer continuously clicks back and forth
between pages and prompt a pop-up, asking the customer whether he needs additional help or
explanations. Detecting doubt is, however, one of the more challenging aspects of robo advice.
57.
As in conventional advice, in a fully automated advice process the customer is
responsible for its own subsequent decisions. However, in the case of face-to-face advice, the
adviser can discuss the customer’s hesitation to follow the advice with that customer and
possibly remove any doubts or concerns, while a fully automated concept cannot. Therefore,
there may be merit in limiting the possible deviations from the advice in a fully automated
process. This could prevent consumers making suboptimal choices in exchange for a lower
insurance premium.
58.
An incorrectly programmed algorithm can have far-reaching consequences. It is
therefore important that an algorithm is carefully developed and tested before it is used in
practice, and that it is subsequently subject to adequate maintenance. The design of the
algorithm of the robo advice needs to be such that the output treats the customer fairly. A faulty
algorithm or defective AI tool can lead to inconsistent advice or consistently bad or improper
advice. Such tools and the data they rely on can also potentially further reinforce or perpetuate
existing biases.
3.3

Price comparison websites (PCW)

59.
Price Comparison Websites (also known as Digital Comparison Tools) in insurance are
websites that present multiple insurance products and providers of insurance, thereby enabling,
in principle, consumers to compare and select products from an array of insurers and/or
intermediaries. After selecting a product to purchase, the website could direct the customer to
the website of the insurer or intermediary to complete the transaction.
60.
PCWs are currently well established in many jurisdictions for many products and
services such as electricity utility and air tickets. They are now also a key distribution channel
in some insurance markets.
61.
Whilst various methods of remuneration exist, most PCWs are remunerated by the
insurer or the intermediary for any successful transaction usually via a fixed amount per policy.
The PCW will usually not own the customer relationship, which is a significant difference from
other types of intermediation.
62.
The supervision of PCWs varies across jurisdictions depending on the activities
performed and the PCW’s business model and they may be considered as intermediaries. In
some jurisdictions PCWs need to comply with insurance intermediary requirements.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 19 of 36

3.3.1

Benefits and opportunities

63.
By bringing together product and price information in a single place, a potential benefit
of PCWs is that they empower a consumer to quickly compare, assess and select among
products available in the insurance market. If this potential is accomplished – reliable
information is objectively presented with related educational tools – PCWs promote competitive
markets by empowering consumers. PCWs further facilitate consumer shopping by being
accessible at any time, from anywhere, although most insurers offer websites for their products
with similar access. Another benefit of PCWs for consumers is the ability to enter their personal
information once to receive personally-relevant products and prices from multiple vendors – a
clear advantage to entering personal information every time when shopping across different
insurers' websites. In this respect PCWs can promote competition as well as reduce marketing
and underwrting costs which could result in lower insurance premiums.
3.3.2

Potential risks

64.
Some of the risks – such as consumers having to self-direct and inform themselves
without further assistance – are common to those of digital sales via the website of an insurer
or an intermediary. However, there are a number of particular risks. For example, a risk with
PCWs is non-disclosed conflicts of interest due to compensation arrangements or ownership
structures with particular providers shown on the web site. Such conflicts of interest can cause
the website to present only some products and/or guide consumers to products that are in the
interest of the website and not the interest of the consumer.
65.
PCWs that are not subject to specific disclosure requirements may lack of transparency.
The lack of transparency may relate, for example, to potential conflicts of interest, to
compensation arrangements with providers or to the true identity of the PCW owner/operator or
providers. This could affect adversely the ability of consumers to make informed decisions.
Consumers are generally not aware of the number of suppliers consulted during a given product
comparision and the criteria used to establish a recommendation (if the PCW is permitted to
make recommendations). Consumers may believe the PCW is providing objective and complete
information when, in fact, the PCW is providing limited and biased information that channels the
consumer to a specific product.
66.
Another major risk is that consumers focus only on the price to select a product and, as
a result, are not adequately covered because they bought a product that does not meet their
needs. There is also a risk that consumers buy unsuitable products based on the results
provided by a PCW because they believed they were receiving some form of advice, and/or that
all results shown met their cover needs.
67.
Specific risks have been identified in the use of PCWs that can create harm for
consumers including unreliable performance or disorderly failure (for example, caused by a
technology and/or data failure). Due to the volume of transactions generated through a relative
small number of PCWs, an issue with one PCW such a failure in the algorithm to present the
correct results or a display of inaccurate information about a product can have a far-reaching
impact and, in markets where PCWs play a big role, can even create a systemic issue across a
specific product line.
68.
It could be expected that if one PCW fails, others will pick up the market segment.
However, in some markets, concentration in certain segments/product lines may create harm
to consumers if there is a lack of availability of other PCWs.
Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 20 of 36

Netherlands
In 2014, the AFM issued a press release on the quality of PCWs.
The main findings were:
• Overall, the services of PCWs were found to be in the interest of the customer, based on
research on the five main PCWs in the Netherlands. Usually, comparisons are ranked on
both price and quality, based on the preferences of the customer;
• There were no signals that the overall comparison was based on payments of insurers.
However, often an insurer would only end up in the top 3, if the consumer was able to close
the product via the PCWs – which in turn was only possible if commissions are paid; and
• The main points of improvement were the provision of information, the way the top 3 is
constructed, the inclusion of one-off discounts in the premium and default preferences.
In 2018, the AFM issued another press release on the services of PCWs.
• The main finding was that PCWs sometimes provide financial advice, while advertising
their services as execution only. Their customer onboarding, however, is not suitable for
financial advice, as PCWs are based on execution only and therefore contains a limited
set of questions. The consumer, however, might get the impression that financial advice
was given;
• An example of a PCW giving advice, is presenting “the top 3 best suitable mortgages for
you”. This can qualify as financial advice, but it would not be compliant with the advice
rules, as there is no adequate customer onboarding; and
• In 5 Q&As, the AFM explained when the services of PCWs would qualify as financial
advice. Some market players will move from execution only towards robo advice in the
next couple of years.
3.4

Disclosure and informed decision-making

69.
Standard 19.7 requires insurers and intermediaries to provide timely, clear and adequate
pre-contractual and contractual information to customers. Product disclosure is a key
requirement that needs to adapt to new digital channels and habits.
3.4.1

Benefits and opportunities

70.
One of the advantages of online services is that providers can use visual information to
disclose features of a product. For example, the course of the premium over time can be
presented in a manner that is easily understandable and easily adjustable when the customer
enters new information, for example using graphs. The same goes for other product features.
Providers can experiment with the best way to disclose information to their customers, to
maximise the intelligibility thereof.
71.
Chatbots 20 may also assist when a customer takes too long to scroll past a certain
section or moves too quickly across a material term of the policy. This could indicate that the
customer is looking for additional information or further explanation either by the bot or by an
adviser depending on the complexity of the policy.

A computer programme designed to simulate conversation with human users, especially over the
Internet.

20

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 21 of 36

72.
Technologies may utilise customer data to introduce relevant disclosures based on the
information obtained about the customer from different data sources. Examples of such “virtual
or cognitive customer service representatives” or chatbots include UK based Spixii, which
“speaks” six languages or Flamingo’s “Rosie” in Australia which “learns from your business” in
order to respond to customers. 21
United States
The US based insurer Lemonade used two types of artificial intelligence or “cognitive”
systems to interface with customers. One is called “Maya” which signs up customers via
mobile devices and the other is called “Jim” which finalises claims without any assistance.
Insurify is another example which uses Evia (“Expert Virtual Insurance Agent”) and uses
natural language and image recognition to collect auto insurance quotes. Customers can
also engage with Evia when it comes to clarification of terms.
73.
“Comprehension testing” through technology may assist with obtaining certainty that the
disclosed information is adequate and that the consequences thereof are properly conveyed to
the insured. Technology, particularly machine learning and chatbots, can be used as an enabler
for customer comprehension. Online filter and quick test questions may also assist with gauging
the customer’s understanding. There may be a need to educate online users to dedicate
sufficient time to an adequate understanding of the contents of the agreement.
74.
The means of presentation (for instance through dedicated popup windows) can play an
important role in ensuring proper understanding of the information by customers and obtaining
explicit consents when appropriate. In addition, gamified product sales information – whereby
information is disclosed as part of game or challenge – can keep a consumer interested and
engaged so that critical information that might otherwise be overlooked is seen, understood and
retained by the consumer.
3.4.2

Potential risks

75.
The time efficiencies and instant gratification associated with digitised transacting mean
that customers expect a relatively quick transactional experience, particularly in instances where
smartphone applications are being used. This poses significant challenges for insurers in
maintaining a balance between convenient, seamless contracting versus the risk of inadequate
disclosure of material policy terms and conditions.
Netherlands
In the Netherlands, supervision of the intelligibility of products is part of the product approval
process. Also, in online services, customers are often obliged to read information and to
confirm the information is understood. It nevertheless remains a risk that customers do not
fully understand the details of a product before providing this confirmation. The supervisor
therefore encourages parties to write their product conditions in such a way that these are
complete, but as easy to read and understand as possible.

21

https://www.digitalpulse.pwc.com.au/how-insurtech-will-make-you-love-your-insurer/

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 22 of 36

76.
In contrast to face-to-face interaction, digital interaction can make it difficult to flag
misunderstandings and the need for more explanations based on non-verbal communication.
77.
In a digital context, customers are faced with a plethora of information from different
sources. It can be difficult to identify reliable product disclosures in a manner that is appropriately
presented, and differentiate between product disclosure and marketing. In addition, digital tools
can be used to highlight information that is relevant but can equally be used to downplay
information in a way that creates a risk of consumers choosing an inappropriate product.
78.
As with non-digital information and disclosure, consumers face the risks of information
overload from too many sources with no means to ascertain the accuracy or legitimacy of the
information.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 23 of 36

4

Supervisory issues

79.
Digitalisation is not only transforming the insurance industry but society itself. For
supervisors it presents a “moving target in a moving environment”. As digitalisation changes the
way insurance products are designed, marketed and distributed, supervisors should monitor
these new developments and engage stakeholders both within and outside the insurance
industry to protect consumers’ interests. This includes non-traditional stakeholders such as
cloud service providers and data vendors. In short, new developments / the shift in risks will
require new supervisory responses that are delivered in an adequate way. Some of the key
challenges are described below.
80.
Consumer outcomes: For supervisors in a digitalised world it is crucial to understand
how incumbent insurers and intermediaries as well as newer insurance market participants,
including Insurtech start-ups and Big Techs, are behaving with impact on outcomes for
consumers. Digitalisation and use of data have the potential to benefit consumers, but also
create risks of unfair treatment, discrimination, or concerns over access to, or exclusion from,
insurance services. Measuring and assessing these outcomes is challenging. Supervisors
should consider monitoring behaviour and outcomes by examining information from multiple
sources.
Australia
In September 2017, ASIC launched its 2017-18 Data Strategy. With the tag-line:
“Connecting the dots to achieve better regulatory outcomes, “its purpose is to describe
ASIC’s vision for data, its objectives and an approach to improving how it captures, shares
and uses data. 22
Germany
BaFin launched an internal project in 2015 to learn more about the business model of
technological start-ups (FinTech s) and their appearance on the market. Drawing on
expertise from the areas of banking, insurance and securities supervision, the objective of
the project group was to observe the latest developments in the FinTech market, and to
review whether BaFin needed to adjust its processes in view of new developments in the
area of digitalisation. As a result of this project, BaFin established an Innovation Hub. This
Innovation Hub analyses and evaluates upcoming technological solutions and new business
models based on those solutions.
Additionally, the Innovation Hub coordinates a network of experts from various areas of
responsibility within BaFin, which rates innovative business models with regard to regulatory
requirements. Experts from banking, insurance and securities regulators are represented in
the network, as well as from the licensing and the pursuit of unauthorised business
departments. The combination of experience and expertise from ongoing oversight and
review of licensing requirements allows rapid assessment of innovative business models
and processes that may not be unique to one department.

http://download.asic.gov.au/media/4479255/asic-data-strategy-2017-20-published-19-september2017.pdf
22

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 24 of 36

Québec
AMF has created a Fintech Lab to deepen the AMF’s knowledge of new business models
and underlying technologies, explore the current and potential applications of these
technologies and explore how the AMF itself can use them.
France
In 2017, the French central Bank launched The Lab. As an open space for discussions and
collaborative work, the Lab links up the Banque de France with various initiators of
innovative projects – start-ups and FinTechs, institutional players, universities – in order to
experiment with new concepts and technologies in connection with the activities of the
institution. The Lab is working on technologies such as blockchain (MADRE project), IoT,
IA etc.
The ACPR is developing a new tool for supervising business practices:
•
•

A database of the innovations of the insurance sector, which enables monitoring of
technological innovations, new services as well as guarantees offered; and
A web listening platform with an internal analysing tool to capture online messages from
consumers concerning bad market practices.

In March 2018, the ACPR launched a Task Force to tackle the opportunities and challenges
raised by AI in the financial sector. This Task Force (TF) is composed of banks, insurance
companies and FinTechs. It also includes other authorities such as the Data Protection
Authority. The primary goal of this TF will consist of issuing a Discussion Paper before end
of 2018, aiming at summarising the implications of using AI technologies in the financial
sector.
United Kingdom
In 2014, the FCA launched its Innovate department which is committed to encouraging
innovation in the interests of consumers. Innovate provides assistance to firms which are
using innovation to improve consumer outcomes, and helps firms better understand the
FCA’s rules, processes and guidance. Innovate helps the FCA keep ahead of developing
trends and potential harms in the market.
United States
In 2018, the NAIC and US state insurance supervisors launched a three year strategic plan,
State Ahead, to drive their efforts, resources and attention to meet ongoing challenges,
including the rapidly evolving marketplace fuelled by seismic shifts in consumer behaviour
and huge technological advances. As part of the goal to ensure consumer protection keeps
pace with changes in the marketplace, one objective is to optimise use of market data and
regulatory processes to enhance consumer protections, including:
• rebuilding the NAIC's Market Conduct Annual Statement (MCAS) application, as well as
those applications utilising MCAS data, as a cloud-based solution;
• implementing a business intelligence tool with self-service capabilities;
• creating an enterprise market data strategy and analytics data warehouse; and
• rebuilding the NAIC's Consumer Information Source (CIS) application.
Additionally, US state insurance regulators are trying to gain a good understanding of new,
innovative insurance products and services, including the manner in which they impact
consumers and other stakeholders in the insurance marketplace. Accordingly, the NAIC
Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 25 of 36

established the Innovation and Technology Task Force in 2017 to help insurance regulators
stay informed on key developments, including new products and services from start-up
companies, as well as established insurance industry players. Under this Task Force, the
NAIC's Big Data Working Group is charged in part with assessing data needs and required
tools for state insurance regulators to appropriately monitor the marketplace and evaluate
underwriting, rating, claims and marketing practices. This assessment includes gaining a
better understanding of currently available data and tools, as well as recommendations for
additional data and tools, as appropriate. Based on this assessment, the Working Group
will propose a means to collect, house and analyse needed data. 23
81.
Need to balance innovation and conduct concerns: Digitalisation and innovation
have enormous potential to help insurers and intermediaries build cultures of compliance,
identify potential consumer harms and improve outcomes for consumers. However, it could also
pose significant risks that could lead to consumer harms if not properly managed. These could
include technological exclusion, discrimination, and accessibility and affordability issues. 24 One
of the key challenges to supervisors will be to consider a balanced approach to facilitate
innovations by insurers while maintaining the level of consumer protection as stipulated in its
laws and regulations.
Australia
ASIC’s Innovation Hub drives much of the Australian conduct regulator’s support for
digitalisation and engagement with FinTech and Insurtech companies.
Through the Innovation Hub, ASIC provides informal assistance to Insurtechs on their
regulatory obligations, the overarching regulatory framework and, as appropriate, options
relating to ASIC’s exemption powers.
Germany
BaFin’s Innovation Hub serves – besides other responsibilities – as a communication platform
for incumbents and start-ups. One of its main aims is to gather and spread knowledge. For
example: There is a special contact form on BaFin’s webpage through which company
founders and FinTechs can submit preliminary inquiries or concrete questions about eg
business models. The term "contact form" may seem a bit old fashioned, but contributes to
the efficiency of the communication: It serves to quickly determine the responsible section for
the respective business model body within BaFin – for a public authority with about 2.700
employees, this is a decisive factor.
France
In 2016, the ACPR launched a FinTech Innovation Unit. It is the point of entry of financial
start-ups for their licensing process. The Unit evaluates the opportunities as well as the risks
related to innovations in the financial industry and gives recommendations on where
adjustments need to be made in the current regulation and in supervision practices. The
ACPR is coordinating its actions with the Securities & Markets Authority (SMF). They have
both launched the Fintech Forum in 2016 which is leading a dialogue with FinTech
professionals regarding regulation and supervision.

https://www.naic.org/state_ahead.htm
Many of these issues are addressed in the Application Paper on the Use of Digital Technology in
Inclusive Insurance (November 2018).
23
24

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 26 of 36

Québec
The AMF has created in 2016 an external advisory committee, the Technological Innovation
Advisory Committee that has the mandate to assist the AMF in identifying and analysing
trends and issues and help ensure a balance between consumer protection and market
efficiency.
Switzerland
FINMA has been working on the challenges presented by FinTech regarding authorisation,
supervision and regulation. Innovative trends and ideas require a solid framework within
which to operate, while clients and the financial system as a whole need protection during
this shift in direction.
FINMA regards innovation as key to the competitiveness of Switzerland’s financial centre, but
adopts an essentially neutral approach to certain business models and technologies. It
therefore reviewed whether specific provisions in its ordinances and circulars disadvantaged
some technologies and concluded that very few such obstacles existed.
An increasing number of financial intermediaries interact with their clients via internet and
mobile devices. FINMA has therefore been enhancing the regulatory framework to facilitate
client onboarding via digital channels. In its new circular, the anti-money laundering due
diligence requirements are explained in the context of digitalisation of financial services and
the need for technology-neutral regulation, particularly with respect to video identification. The
circular came into force on 18 March 2016.
Before launching operations, FinTech companies must establish whether they are subject to
anti-money laundering and authorisation requirements.
In general, authorisation for insurance is required if risks and dangers for clients are insured.
If services are rendered voluntarily and without any contractual obligation, authorisation might
not be required.
United Kingdom
The FCA launched its regulatory sandbox in 2016. The regulatory sandbox is a space in which
firms can apply to test innovative propositions in a live environment, with oversight and input
from the FCA. Firms testing in the sandbox are required to meet all relevant regulatory
requirements, and bespoke safeguards and mitigants are built into each testing plan. The
sandbox aims to enable firms to get to market to test their propositions at a faster speed and
reduced cost, and also gives the FCA an understanding of the opportunities and potential
harms that innovation can create in the market. Almost 90 firms have been supported across
the first four cohorts of the FCA’s sandbox, including a significant number from the insurance
sector.
82.
Supervisory skills and tools: Supervisors should become “data driven” and “digitalintelligence-led”. Supervising market conduct in a digital world requires different skill sets, in
addition to those of lawyers, economists, actuaries/mathematicians and data scientists.
Interdisciplinary supervisory teams will be vital in a digitalised world. Supervisors should be
technologically and numerically literate and understand the risks associated with data.
Supervisors will need new skills to identify, monitor and assess new applications of
technologies, for understanding market structures and the activities of new participants, and for
understanding consumer outcomes. They can arrange to have this knowledge and skill wihin
their own staff or – if this is more efficient – use third party providers. Lawyers, economists,
actuaries/mathematicians and data scientists will need to work together to supervise insurance
Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 27 of 36

markets. In this respect supervisory authorities will need to reconsider what qualifications and
skill sets they need to become “fit for the future”. They will continue to compete for talent within
the industry.
Germany
IT specialists have been part of supervising teams for a while. But in order to better prepare
itself for the challenges posed by inter alia IT and cyber risks, BaFin set up a separate
organisational unit for IT supervision in the financial sector as of 1 January 2018.
Québec
AMF has created a dedicated internal working group of experts on FinTech, involving more
than 60 employees working in cross-functional teams.
France
Alongside the creation of dedicated teams/hubs, the Central Bank has appointed a Chief
Digital Officer (CDO) in charge of the digital transformation of the institution, who is also
Chairing the innovation Lab.
United States
As part of the NAIC and US state insurance supervisors' State Ahead strategic plan, the theme
of consumer protection and education recognises the need to stay abreast of new
developments in the area of innovation and emerging technology and the need to become
more engaged in the areas of InsurTech and Regtech. Opportunities being explored to further
this objective include providing forums and programmes for state insurance regulator
education and discussion regarding changes in the insurance marketplace, including
innovation and technology; convening an Autonomous Vehicle Insurance Forum with
stakeholders to discuss insurance regulatory issues related to autonomous vehicles; and
considering the creation of a cybersecurity insurance institute.
Additionally, the NAIC's Innovation and Technology Task Force is charged, in part, with
discussing innovation and technology developments in the insurance sector, including the
collection and use of data by insurers and state insurance regulators – as well as new
products, services and distribution platforms – in order to educate state insurance regulators
on how these developments impact consumer protection, privacy, insurer and producer
oversight, marketplace dynamics and the state-based insurance regulatory framework.
83.
In particular, supervisors will need the skills to understand how digitalisation can result
in consumer harm. For example, supervisors may encounter challenges in supervising the (selflearning) algorithms that underlie the automated decisions made in a digitalised world, which is
not only problematic from a consumer protection perspective but also from a risk management
perspective.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 28 of 36

Robo advice
Robo advice provides a useful case-study on many of the issues pertinent to supervisors. As
a result, many supervisors have recently published guidelines on how they are approaching
robo advice including:
•

Germany:https://www.bafin.de/EN/Aufsicht/FinTech/Anlageberatung/anlageberatung_no
de_en.html;

•

Australia: https://asic.gov.au/regulatory-resources/find-a-document/regulatory-guides/rg255-providing-digital-financial-product-advice-to-retail-clients/; and

•

Netherlands:
https://www.afm.nl/~/profmedia/files/onderwerpen/roboadvies-sav/viewrobo-advice.pdf.

•

United Kingdom: The UK’s FCA launched its Advice Unit in May 2016 to provide
regulatory feedback to firms developing automated advice or guidance models across a
range of sectors, including insurance. Feedback focuses on helping the firm understand
the regulatory implications of their model. The FCA also publishes tools and resources for
all firms developing automated advice or guidance propositions, based on its experiences
with individual firms.

In developing guidelines, supervisors have needed to consider how the quality of the advice
provided is measured and verified. Should supervisors directly supervise the algorithm?
Should supervisors supervise and monitor the outputs – ie the advice itself? Should
supervisors require insurers and intermediaries to self-audit and provide it with annual
assurances that the advice its robo advisers are providing is appropriate? Should it require
insurers and intermediaries to engage external experts to conduct those audits?
Depending on how they address these questions, supervisors may need to establish
dedicated teams to address such technical matters involving IT specialists with the required
knowledge.
As with non-digital advice, supervisors need to mandate that insurers and intermediaries
adopt appropriate document management strategies. This includes retaining all versions of
the algorithm themselves. The robo adviser needs to save the algorithm, the used data and
the information and advice that has been provided to the customer.
84.
Supervisory authorities should consider how to embrace new technologies to help carry
out supervision, also referred to as Suptech solutions. 25
Germany
In the first half of 2018, BaFin has published a report on Big Data and AI together with
Partnerschaft Deutschland, the Fraunhofer Institute for Intelligent Analysis and Information
Systems and the Boston Consulting Group.
https://www.bafin.de/SharedDocs/Downloads/EN/dl_bdai_studie_en.html
With advancements in key technologies, increasing data availability and decreasing entry
barriers into the usage of Big Data and AI solutions, both incumbent institutions and new
Suptech is the use of technological innovations (or FinTech) by supervisory authorities. Regtech: the
use of technological innovations (or FinTech) for compliance purposes and reporting by regulated
financial institutions.
25

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 29 of 36

market players are able to structure their work processes more efficiently and develop new
business models. Supervisors, however, are facing new challenges as the use of Big Data
and AI impacts the financial markets. The aim of the report was to gain a better
understanding of these challenges. The report highlights the implications of technologydriven market developments from various regulatory and supervisory perspectives.
The six main observations and their implications can be summarised as follows:
1. The financial data innovation race is already underway. Systematic dependencies on
Big Data and AI companies could develop outside the regulatory framework in the
financial system.
2. Black-box modelling is spreading. But black-box modelling must never stand in the way
of a proper business organisation.
3. Big Data and AI accelerate the further automation of the process. But the responsibility
will always remain with the senior management.
4. The “transparent customer” is more than just a phrase in the age of Big Data and AI. Big
Data and AI must be used for the customers, not against them.
5. Consumer confidence is a catalyst for Big Data and AI innovations and an anchor of
stability for the integrity of the financial system.
6. The speed of innovation increasingly tests the limits of the regulatory framework’s
adaptability. A level playing field in the age of Big Data and AI means that the speed of
innovation has to be met with agile oversight and technology-neutral regulation.
United Kingdom
In 2017/18 the FCA worked with ING, the Commonwealth Bank of Australia and Pinsent
Masons to test the possibilities of using Natural Processing Language and AI technologies
to interpret Markets in Financial Instruments Directive II regulations and automatically build
and manage a compliance programme.
85.
Different entities: Supervisors will also need to deal with non-incumbent firms with
different entity structures and approaches to consumer related risk than incumbents.
Supervisors will need to engage with new entrants into insurance and financial services who
may not have experience or knowledge of financial services regulation. These new entrants
may have different entity structures and approaches to consumer related risk than incumbents
historically monitored by supervisors. Unlike incumbents, the general compliance awareness,
risk culture and ability to comply with regulatory requirements may differ significantly for these
non-traditional firms. This may require a proactive strategy for outreach and engagement with
these new entrants to inform and “educate” them on relevant supervisory matters and the proper
compliance attitude.
86.
Well capitalised “BigTech” platform businesses may move into distribution markets.
Small, nimble but lowly capitalised Insurtech start-ups may also look to enter insurance markets.
Supervisors will need to understand the different challenges posed by these new entities that
may lack knowledge of regulatory reuqirements, and may not have had experience in engaging
with supervisors.
87.
Supervisory cooperation: Cooperation between financial supervisory authorities is
crucial in a digitalised world. As digital innovations and risks do not stop at the border,
jurisdictional supervisors should coordinate with authorities in other jurisdictions. To meet these
challenges supervisors should proactively work together across jurisdictional and subject-matter
boundaries to identify emerging trends and to develop and implement solutions. This includes
collaboration between market conduct regulators and prudential, privacy and competition
Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 30 of 36

regulators given the implications of digital technology on consumer outcomes and the significant
number of solutions that are focused on the marketing, sales and distribution end of the value
chain. Regular and on-going interaction between supervisors will be crucial. The IAIS continues
to support and facilitate such discussions, including through its FinTech Forum.
France
In 2018, the ACPR signed an agreement with the authority supervising the security of
Information Systems (ANSSI). The authority is responsible for responding to threats targeting
public authorities and the private sector, in particular vital information systems, and
coordinates government action in the area of defence of those systems.
88.
Regulatory arbitrage: Supervisors also need to be cognisant of the emergence of
product types that have the effect of insurance but are structured in a way that falls outside the
legal definition of a regulated insurance product. This would enable product providers to avoid
regulatory requirements. To consumers, it means that they would not be able to access
compensation or policyholder protection schemes if product manufacturers are unable to meet
their claim costs.
89.

The arbitrage can take two forms:

•

Jurisdictional: where the product falls outside the jurisdictional power of a regulator despite
being available to customers in that jurisdiction; and

•

Definitional: where the product does not have the legal characteristics of an insurance
product although it has the effects of one.

90.
In addition, digitalisation and new technologies may increase the potential for regulatory
arbitrage created by products that bundle insurance and non-insurance products or services
with choice by the consumer limited to purchasing or not purchasing the package.
91.
Information security: Storage, protection and third party use of customer data (and the
insights gathered from it) is also an issue. As cyber risks and data protection questions become
of vital importance, with the steady rise of digitalisation, working together with the competent
authorities on these issues is of utmost importance.
World-wide
The "WannaCry" and “Petya” ransomware outbreaks in 2016 highlight that cyber risks are
on the rise. The data and customer specific behavioural insights that insurers hold would
have a high value in this context. This is particularly relevant given that in order to drive
efficiencies and reduce costs, many insurers now store data and insights on the "cloud" and
share data and insights with third parties, many of whom are off-shore. Cloud services are
probably more in the focus of hackers but are in most cases much better protected than onpremise installations run by incumbents. On the other hand if a cloud service is successfully
attacked, the outcomes can be worse or even systemic.
92.
Customers need to know that their data and insights specific to them that are derived
from their data are secure, not corrupted or tainted, and clearly who has access to such data.
This is a challenge not just to privacy regulators but also to financial services regulators.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 31 of 36

EU
The EU has reformed its data protection rules to simplify the use of Big Data for businesses 26
and to set high standards of data protection. As of May 2018, with the entry into application
of the General Data Protection Regulation 27, there is one set of data protection rules for all
companies operating in the EU, wherever they are based. 28
Also, the EU aims to strengthen its cybersecurity regulation to cope with the growing threat
of cyberattacks and take advantage of the opportunities offered by the new digital era. In
October 2017, the European Council called for a common approach to cybersecurity in line
with the European Commission's September reform package. 29
United States
The NAIC's Big Data Working Group is charged, in part, with reviewing current regulatory
frameworks used to oversee insurers' use of consumer and non-insurance data and, if
appropriate, recommend modifications to model laws and/or regulations regarding marketing,
rating, underwriting and claims, regulation of data vendors and brokers, regulatory reporting
requirements, and consumer disclosure requirements.
93.
Use of cloud computing is increasing in the insurance industry, and with it the need for
supervisors to enhance their regulatory frameworks and supervisory practices to effectively
capture the risks. Most supervisors typically apply existing frameworks on outsourcing,
governance, risk management, internal control, and information security to insurers’ cloud
computing activities, while others have issued cloud-specific recommendations and
expectations. Meanwhile, most authorities have put in place formal and informal communication
arrangements and are reinforcing their supervisory processes to better monitor and assess the
risks of cloud computing. 30
94.

Cloud services give rise to specific questions. For example:
•

In which country will the data be stored and how can this be verified?

•

Who has access to the data?

•

Which key controls will be provided?

•

Is there a danger of risk concentration as there are not so many cloud service providers?

•

Is there a (possible) conflict of interest if data of different insurers are stored on the same
server?

Information Commissioner's Office: "Big data, artificial intelligence, machine learning and data
protection", 20170904, Version: 2.2.
27 Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the
protection of natural persons with regard to the processing of personal data and on the free movement of
such data, and repealing Directive 95/46/EC (General Data Protection Regulation) (OJ L 119, 4.5.2016,
p. 1).
28European
Commission:
https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-doesgeneral-data-protection-regulation-gdpr-govern_en#references.
29 http://www.consilium.europa.eu/en/policies
30 FSI Insights, Emerging prudential approaches on outsourcing to the cloud: the case of insurance
companies, forthcoming.
26

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 32 of 36

•

Is there a (possible) conflict of interest if the cloud service provider decides to involve
itself in the insurance business?

95.
It is also vital that supervisors have the same direct and immediate access to data stored
on the cloud as they have to data stored on an insurer or intermediaries own servers.
Germany
BaFin published in its Journal 04/2018 an article on “Cloud Computing: Compliance with the
supervisory requirements regarding rights of information and audit and ability to monitor”. 31
With regard to outsourcing to cloud service providers, BaFin also holds discussions with the
respective cloud service providers and insurers about the content of outsourcing contracts.
A circular to clarify supervisory requirements for IT in insurers (VAIT) has been published
(https://www.bafin.de/SharedDocs/Downloads/DE/Rundschreiben/dl_rs_1810_vait_va.html).

31

https://www.bafin.de/SharedDocs/Veroeffentlichungen/EN/Fachartikel/2018/fa_bj_1804_Cloud_Comput
ing_en.html.
Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 33 of 36

5

Conclusion and recommendations

96.
Digital innovations can change and potentially improve the customer experience and
reduce insurers’ and intermediaries’ operating costs. However, digitalisation may have an
impact on consumer protection and the extent to which customers are treated fairly; from the
design, underwriting and pricing of products, their marketing and distribution, through to claims
processing and the ongoing management of customers. Therefore, in respect of product design,
marketing and sales, due attention needs to be given to achieving fair customer outcomes in
terms of products that meet consumers’ needs, the design and use of algorithms and the use
of customer data.
97.
To adjust to the digital age and foster innovation, supervisors should consider how to
ensure that new innovation does not come at the expense of protections for policyholders and
the integrity of the insurance sector as a whole.
98.
One of the key challenges to supervisors will be to consider a balanced approach to
facilitate innovations by insurers while maintaining the level of consumer protection as stipulated
in its laws and regulations. To promote innovation consistent with consumer benefit and
protection, it is recommended that supervisors develop a thorough understanding of how
innovations work and are applied to ensure a proper assessment of new products and business
models, and the design and functioning of the IT architecture, infrastructures and processes,
and how this is catered for in the insurers’ risk management framework.
99.
Supervisors may also need to develop new tools and skills for supervision of digitalised
insurers, enhancing cooperation with financial and other authorities, safeguarding the
supervisory perimeter to prevent regulatory arbitrage and enhancing information security.
100. Supported by further material to be developed by the IAIS, supervisors should consider
establishing guidance for appropriate and responsible use of new technologies to safeguard the
fair treatment of customers and – for example in terms of the use of AI and robo advice
mechanisms – promote advice and services that are suitable and affordable for the customers.

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 34 of 36

Annex: Digital technologies and alternative business models affecting
insurance business
General overview of significant innovations within the insurance industry as mentioned in the
IAIS report “FinTech Innovations in the Insurance Industry”. 32
Digital devices and the internet:
1. The changes addressed in this paper are facilitated by the proliferation of digital devices
(devices that contain a computer or microcontroller) such as smartphones, tablets and
"wearables". These devices are connected by the internet: a global network of computers
using standardised communication protocols.
Internet of Things (IoT): 33
2. IoT involves the internetworking of physical devices, vehicles, buildings and other items
(also referred to as "connected devices" and "smart devices"), embedded with electronics,
software, sensors, actuators, and network connectivity that enable these objects to collect
and exchange data.
Telematics / Telemetry:
3. In the context of IoT, telematics involves telecommunications, sensors and computer
science to allow sending, receiving, storing and processing data via telecommunication
devices, affecting or not control on remote objects. Telemetry involves the transmission of
measurements from the location of origin to the location of computing and consumption,
especially without affecting control on the remote objects. In the context of insurance, its
main applications are Connected Cars, Advanced Driver Assistance Systems (ADAS),
Health monitoring and Home monitoring.
Big Data 34 and Data Analytics: 35
4. In the insurance market, Big Data and Data Analytics could be used in various processes,
such as product offerings, risk selection, pricing, cross selling, claims prediction and fraud
detection, for example to offer customised products.
Comparators and Robo advisers:
5. Online services that provide automated, algorithm-based product comparison and advice
without human intervention.

32

21 February 2017; https://www.iaisweb.org/page/news/other-papers-and-reports/file/65625/report-on-fintechdevelopments-in-the-insurance-industry
33 The term IoT has been defined as a global infrastructure for the information society, enabling advanced services
by interconnecting (physical and virtual) things based on existing and evolving interoperable information and
communication technologies (source http://www.itu.int/ITU-T/recommendations/rec.aspx?rec=y.2060)
34 Big Data is the term used for the storage of data from different sources, in large volume and speed; IAIS, FinTech
Developments in the Insurance Industry, 21 February 2017.
35 Data Analytics is the process of inspecting, cleaning, transforming, and modelling data with the goal of discovering
useful information, suggesting conclusions, and supporting decision-making; IAIS, FinTech Developments in the
Insurance Industry, 21 February 2017.
Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 35 of 36

Machine Learning (ML) and Artificial Intelligence (AI):
6. The use of ML and AI enables several insurance industry processes to use data in real time
and, especially, use events prediction (eg vehicles thefts, health problems and weather
events). There is a vast scope for AI, not only in a better pricing of risks, but also in fraud
prevention, automated underwriting, claims handling or in preventive counselling.
Distributed Ledger Technology (DLT):
7. A distributed ledger is essentially an asset database that can be shared across a network
of multiple sites, geographies or institutions. The security and accuracy of the assets stored
in the ledger are maintained cryptographically through the use of “keys” and signatures to
control who did what within the shared ledger.
•
•

Blockchain:
This is a type of decentralised distributed ledger, comprised of unchangeable, digitally
recorded data in packages called “blocks” which are stored in a linear chain; and
Smart Contracts:
The novelty of DLT is that it is more than just a database – it can also set rules about a
transaction (business logic) that are tied to the transaction itself. Smart contract is a term
used to describe computer programme code that is capable of facilitating, executing,
and enforcing the negotiation or performance of an agreement using DLT.

Platform business models, Peer-to-peer, Usage Based, On-demand Insurance:
8. Emerging digital technologies are facilitating alternative business models, such as:
•

•

•

•

36

Platform business models: a "platform" is a business model that creates value by
facilitating exchanges between two or more independent groups, usually consumers and
producers. To make these exchanges happen, platforms harness and create large,
rapidly scalable networks of users and resources. Platforms don't own the means of
production – instead they create the means of connection. 36 Google, Apple, Facebook,
Amazon, Uber and Alibaba are all examples of platform business models;
Peer-to-Peer: a business model that allows the insured to pool their capital, self-organise
and self-administer their own insurance. Although it is not an innovative concept,
emerging technologies (like DLT) offer substantial benefits for implementing this model
in a broader scale;
Usage based insurance: a new business model introduced by insurers and
intermediaries that more closely aligns behaviours with premium rates. For example, in
auto insurance there are usage based insurance products where the customer only pays
for the actual distance driven and driver behaviours also impact price; and
On-demand insurance: a new business model that specialises in covering only those
risks faced at a certain moment.

https://www.applicoinc.com/blog/what-is-a-platform-business-model/ (accessed 2 January 2018)

Issues Paper on Increasing Digitalisation in Insurance and its Potential Impact on Consumer Outcomes
Approved by the IAIS Executive Committee on 7 November 2018

Page 36 of 36

State of Machine
Learning and Data
Science 2020


Enterprise Executive Summary Report

Table of Contents
Overview

02

Key Results

03

Data Scientist Profile

04

Education

07

Data Science & Machine Learning Experience

09

Employment

11

Technology

18

Conclusion

28

2020 Enterprise Executive Summary Report

Table of Contents

1

Overview
For the fourth year, Kaggle surveyed its community of data
enthusiasts to share trends within a quickly growing field.
Based on responses from 20,036 Kaggle members,
we’ve created this report focused on the 13% (2,675
respondents) who are currently employed as data
scientists. 


We can see a clear picture of what is common in the
community but also the diverse attributes of its members.

Report
Methodology
The content of this report focuses on respondents who are
currently employed and chose their current job title as
“data scientist”. There are many other job titles that
support data science and machine learning workflows and
you can find their responses in the complete 2020 survey
dataset on Kaggle.


Many survey questions were multiple choice with the
ability for respondents to select all options that applied to
them. For that reason, you may see visualizations where
the total percentage is more than 100%. All monetary
amounts captured in the report are in USD.


2020 Enterprise Executive Summary Report

Overview

2

Key Results
Profile
Data science continues to have a heavy gender
imbalance, with most identifying as male
The vast majority of data scientists are under 35 years
old

Over half of data scientists have graduate degrees


Education and Employment
Most data scientists continue to learn outside of
formal education

Most data scientists have been coding for less than a
decade

More than half of data scientists have less than three
years of experience with machine learning

Data scientists in the United States make substantially
more money than their international counterparts



Technology
More data scientists use cloud computing compared to
2019 results
Scikit-learn is the most popular machine learning tool
in 2020, with over four in five data scientists using it
Tableau and PowerBI are the most popular business
intelligence tools

2020 Enterprise Executive Summary Report

Key Results

3

Data Scientist Profile
Gender
Data science is still suffering from a large gender gap in the
workplace, as 82% of users identify as men. This is only a
slight change from last year’s results, where 84% of users
identified as males. This is the first year we’ve
differentiated between “Nonbinary” and “Prefer to
self-describe,” with each answer coming in around a third
of a percent.


G e n d e r i d e n t i t y o f d ata s c i e n t i s t s

Man

81.9%

Woman

Nonbinary

16.4%

0.3%

Prefer not
to say

1.1%

Prefer to
self-describe

0.4%

0%

10%

2020 Enterprise Executive Summary Report

20%

30%

40%

Data Scientist Profile

50%

60%

70%

80%

90%

100%

4

Age
Similar to 2019 results, data scientists tend to be in their
late 20s or early 30s, with about 60% between 22 and 34.
Only one in five professional data scientists are 40 or older.
There are signs of the numbers skewing even younger, as
generation Z gets more involved. Nearly 7% of data
scientists are aged 18-21, an increase from last year’s 5%.


Though not included in this chart, responses from students
have also increased each year (26.8% in 2020, 21% in 2019,
22.9% in 2018). As these students graduate into the
workforce, we may see future surveys with even younger
data scientists.

A g e r a n g e s o f d ata s c i e n t i s t s

0-17
18-21

6.9%

22-24

13.7%

25-29

25.2%

30-34

20.1%

35-39

13.4%

40-44

8.7%

45-49

5%

50-54

3.1%

55-59

1.5%

60-69

1.8%

79+

0.6%
0%

2020 Enterprise Executive Summary Report

10%

20%

Data Scientist Profile

30%

40%

5

Country
Two countries have far more representation in the Kaggle
community. India makes up almost 22% of Kaggle data
scientists, while 14.5% reside in the United States. Brazil is
a distant third, at under 5%.
M o s t c o m m o n n at i o n a l i t i e s

21.8%
20%
14.5%

15%
6.7%

10%

2.8%

ia

Ind

A.

r

he

.
U.S

Ot

dK

zil

ite

Bra

Un

ia

3.3%

ce

an

Jap

m

do

ing

y
an

rm

Ge

a

eri

3%

ss
Ru

2.6%

Nig

Sp

da

ain

2.4%

na

2.1%

Ca

1.8%

y
rke
Tu

alia
str

Au

nd

1.5%

la
Po

0%

1.4%

2.8%

n
Fra

5%

4.6%

4.2%

Responses per country

# of respondents
300+
150
100
50
0

2020 Enterprise Executive Summary Report

Data Scientist Profile

6

Education
Higher Education
Graduate degrees continue to be the norm for data
scientists, with over 68% having obtained either a Master’s
or doctoral degree. Fewer than 5% of data scientists have
no degree beyond a high school diploma.
E d u c at i o n l e v e l o f K a g g l e d ata s c i e n t i s t s

No formal
education past
high school

0.6%

Some
college/university
study without earning
a bachelor’s degree

2.4%

Bachelor’s
degree

24.2%

Master’s
degree

51.1%

Doctoral
degree

17.2%

Professional degree

3.2%

I prefer
not to
answer

1.3%

0%

2020 Enterprise Executive Summary Report

10%

20%

Education

30%

40%

50%

60%

7

Ongoing Learning
Data science and machine learning are quickly changing,

Coursera, Udemy, and Kaggle Learn top the most common

so it’s no surprise over 90% of Kaggle data scientists

mediums in our survey. Unsurprisingly, many Kaggle data

maintain ongoing education. While about 30% take

scientists chose multiple resources in the survey, with an

traditional higher education courses, many more learn

average of 2.8 mediums selected.

through online materials.
Popular ongoing learning resources

62.9%

Coursera

Udemy

34.7%

University
Courses(resulting in
a university degree

30.8%

Kaggle Learn
Courses

30.1%

DataCamp

29.6%

edX

22.5%

Udacity

19.2%

LinkedIn Learning

12.3%

Fast.ai

11.8%

Other

9.9%

Cloud-certification
programs (direct
from AWS, Azure,
GCP, or similar)

9.1%

None

7.4%
0%

2020 Enterprise Executive Summary Report

10%

20%

Education

30%

40%

50%

60%

70%

8

Data Science & Machine
Learning Experience
Programming Experience
Most Kaggle data scientists have at least a few years of

Compared to the global audience, United States data

experience under their belt. Just over 8% of data scientists

scientists have significantly greater programming

have been programming since the 20th century! That’s not

experience. In the US, 37% have been programming 10 or

to say there aren’t newcomers, however. Over 9% have

more years, versus 22% worldwide.



taken up programming in the last year. Just under 2% of
data scientists claim to have never written code at all.
Global

P r o g r a m m i n g b a c k g r o u n d o f d ata s c i e n t i s t s

USA

8.5%

20+ years

7.6%
13.3%

10-20 years

19.6%
21.9%

5-10 years

29.2%
27.9%

3-5 years
25.3%
17.3%

1-2 years
7%
9.3%

< 1 years
0.8%
I have never
written code

1.8%
0.5%
0%

2020 Enterprise Executive Summary Report

10%
Education

20%

30%
9

Machine Learning Experience
Most Kaggle data scientists are newer to machine learning
than programming. Slightly more than 55% of data
scientists have less than three years experience. Less than
6% of professional data scientists have been using
machine learning for a decade or more.

As with programming, US data scientists have more
machine learning experience than the global respondents.

Global

M a c h i n e l e a r n i n g b a c k g r o u n d o f K a g g l e d ata s c i e n t i s t s

20 or more
years

USA

2.1%
5.1%
3.9%

10-15 years

8.6%
13%

5-10 years

19.6%
10.9%

4-5 years

15.3%
12.3%

3-4 years

17.2%
15.9%

2-3 years

15.8%
21.4%

1-2 years

12.1%
17.9%

Under 1 year
5.6%
I do not use
machine
learning
methods

2.7%
0.8%
0%

2020 Enterprise Executive Summary Report

10%
Education

20%

30%
10

Employment
Pay
Companies in the United States are most likely to pay in

There are trends regionally, such as India where nearly

the six figures, based on these survey results. Global

90% make less than $50,000 USD per year.

companies have lower salary ranges that are more evenly
distributed.

G l o b a l s a l a r y d i s t r i b u t i o n f o r d a t a s c i e n t i s t s


> $500,000

0.5%

300,000-500,000

0.7%

250,000-299,999

0.3%

200,000-249,999

1.6%
4.7%

150,000-199,999

4.5%

125,000-149,999

6.8%

100,000-124,999
3.5%

90,000-99,999

3.2%

80,000-89,999

4.2%

70,000-79,999

3.7%

60,000-69,999

4.3%

50,000-59,999

5.5%

40,000-49,999
5%

30,000-39,999
3.5%

25,000-29,999

3.6%

20,000-24,999

4%

15,000-19,999

5.7%

10,000-14,999
7,500-9,999

2.7%

5,000-7,499

3.1%

4,000-4,999

1.8%

3,000-3,999

1.8%

2,000-2,999

2%

1,000-1,999

4.3%

$0-999

18.6%
0%

2020 Enterprise Executive Summary Report

5%

10%

Employment

15%

11

S a l a r y d i s t r i b u t i o n f o r U S - b a s e d d ata s c i e n t i s t s

> $500,000

0.8%

300,000-500,000

3.9%

250,000-299,999

1.4%

200,000-249,999

8.9%
21.3%

150,000-199,999
18%

125,000-149,999

18.6%

100,000-124,999
6.9%

90,000-99,999
5.3%

80,000-89,999
4.7%

70,000-79,999
0.8%

60,000-69,999

0.6%

50,000-59,999

1.1%

40,000-49,999
30,000-39,999

0.3%

20,000-24,999

0.3%

15,000-19,999

0.3%
0.8%

10,000-14,999
5,000-7,499

0.3%

4,000-4,999

0.3%

3,000-3,999

0.3%

1,000-1,999

0.3%

$0-999

5%
0%

2020 Enterprise Executive Summary Report

5%

10%

Employment

15%

20%

12

S a l a r y d i s t r i b u t i o n f o r I n d i a - b a s e d d ata s c i e n t i s t s

> $500,000

0.6%

300,000-500,000

0.2%

200,000-249,999

0.4%

150,000-199,999

0.6%

125,000-149,999

1%

100,000-124,999

1.2%

90,000-99,999

0.8%
1%

80,000-89,999
70,000-79,999

1.6%

60,000-69,999

0.8%
2.6%

50,000-59,999

3.4%

40,000-49,999

4.5%

30,000-39,999

4.7%

25,000-29,999

6.7%

20,000-24,999

7.3%

15,000-19,999

9.7%

10,000-14,999
7,500-9,999

5.7%

5,000-7,499

4.9%

4,000-4,999

3%

3,000-3,999

1.6%

2,000-2,999

1.6%

1,000-1,999

4%

$0-999

32%
0%

2020 Enterprise Executive Summary Report

5%

10%

Employment

15%

20%

25%

30%

13

Looking at the most common salaries by country, we see
that US companies are more likely to pay higher salaries.
Companies in Germany and Japan follow, with significantly
higher salaries than the other included regions.
M e d i a n s a l a r y f o r d ata s c i e n t i s t s b y c o u n t r y

125,000149,999

USA

70,000-79,999

Germany

Japan

40,000-49,999

Russia

10,000-14,999

Brazil

10,000-14,999

7,500-9,999

India

$0-999

1,0001,999

2020 Enterprise Executive Summary Report

5,0007,499

10,00014,999

15,00019,999

Employment

20,00024,999

50,00059,999

100,000124,999

125,000149,999

150,000199,999

14

Companies Employing Data Science
The most notable change from last year is that more

Large enterprises and small startups are the most common

Kaggle data scientists are working at the very smallest

choices of data scientists in this survey. Over half of

businesses, at over 37% (up from 30% in 2019).

employers have less than 250 employees. Yet, one in five
work at companies with over 10,000 employees.

C o m pa n y s i z e ( # o f e m p l oy e e s )

0-49
employees

37.3%

50-249
employees

13.7%

250-999
employees

10%

1000-9,999
employees

17%

10,000 +
employees

22%
0%

10%

20%

30%

40%

50%

Data Science Teams
With small companies being most common, it reasons that

Over half of data scientists work at companies with five or

the same is true for data science teams, most of which

fewer people on the data science team. Teams of one or

could be fed with two pizzas.

two are most common (23.25%), but large teams of 20+
come next at 22.93%.

D ata s c i e n c e t e a m s ( # o f e m p l oy e e s )

9.2%

0

23.3%

1-2
18.8%

3-4
15.1%

5-9
7.4%

10-14
3.4%

15-19

22.9%

20+
0%

2020 Enterprise Executive Summary Report

10%

20%

Employment

30%

40%

50%

15

Enterprise Machine Learning Adoption
Machine learning has become more rooted in the

Those exploring (or using it to generate insights) remain

companies where Kaggle scientists work. Nearly 31% of

about the same. Kaggle data scientists who said they’ve

data scientists claim well-established ML methods, up from

recently adopted ML decreased, likely due to more

28% in 2019 and 25% in 2018.

entrenched usage.

Machine learning adoption in the enterprise over time

2020

2019

2018

7.8%

I do not know

3.8%
4.9%

6.9%
No (we do not use
ML methods)

5.5%
4.2%

30.8%

We have well
established ML
methods (ie., models in
production for more
than 2 years)

28.9%
26%

23.9%

We recently started
using ML methods (ie.,
models in production
for less than 2 years)

30.7%
32.9%

We use ML methods
for generating insights
(but do not put
working models into
production)

13.1%
14.5%
13.2%

17.6%

We are exploring ML
methods (and maybe
one day put a model
into production)

16.7%
19%
0%

2020 Enterprise Executive Summary Report

10%

20%

Employment

30%

40%

50%

16

Spending
There’s plenty of money being spent on machine learning

Data scientists from the US spend more money in the cloud

and cloud computing products, but not by all data

than their global counterparts. There are more than two

scientists. There’s quite a range, with over a quarter of data

times the responses for the highest spending level in the

scientists claiming to have spent no money at all, while one

US compared to other countries.


in 10 has spent over $100,000 USD in the last five years.
GLOBAL

U S v s g l o b a l e n t e r p r i s e s p e n d i n g i n t h e pa s t 5 y e a r s ( $ U S D )

USA

11.6%

$100K+

25.6%

15.1%

$10K-

$99,999

20.8%

21.3%
$1K-$9,999

18.9%

16.3%
$100-$999

11.5%

10.2%
$1-$99

4.2%

25.6%
$0

18.9%
0%

2020 Enterprise Executive Summary Report

10%

Employment

20%

30%

17

Technology
Interactive Development
Environments

Jupyter-based IDEs continue to be the go-to tool for data

This is the first year it has been separated out from Visual

scientists, with around three-quarters of Kaggle data

Studio. The two combined for over 43% this year, versus

scientists using it. However, this has decreased from last

under 30% in 2019.

year’s 83%. Visual Studio Code is in the second spot with
just over 33%.
Popular IDE usage

JupyterLab

74.1%

Visual Studio Code

33.2%
31.9%

PyCharm
RStudio

31.5%

Spyder

21.8%

Notepad ++

19.4%

Sublime Text

15.2%

Vim, Emacs, or
similar

11%

Visual Studio

10.1%

MATLAB

5.8%
5.6%

Other
None

0.7%
0%

2020 Enterprise Executive Summary Report

20%
Technology

40%

60%
18

Methods & Algorithms
The most commonly used algorithms were linear and
logistic regression, followed closely by decision trees and
random forests. Of more complex methods, gradient
boosting machines and convolutional neural networks were
the most popular approaches.

Methods and algorithms usage

Linear or Logistic
Regression

83.7%

Decision Trees or
Random Forests

78.1%

Gradient Boosting
Machines (xgboost,
lightgbm, etc.)

61.4%
43.2%

Convolutional Neural
Networks
31.4%

ayesian Approaches
Recurrent Neural
Networks

30.2%

Neural Networks
(MLPs, etc.)

28.2%

Transformer Networks
(BERT, gpt-3, etc.)

14.8%

Generative Adversial
Networks

7.3%

Evolutionary
Approaches

6.5%

Other

4.5%

None

1.7%
0%

2020 Enterprise Executive Summary Report

10%

20%

30%

Technology

40%

50%

60%

70%

80%

90%

100%

19

Python-based tools continue to dominate the machine
learning frameworks. Scikit-learn, a swiss army knife
applicable to most projects, is the top with four in five data
scientists using it. TensorFlow and Keras, notably used in
combination for deep learning, were each selected on
about half of the data scientist surveys. Gradient boosting
library xgboost is fourth, with about the same usage as
2019.

The fifth place tool, PyTorch, climbed above 30%, up from
about 26% in 2019.


The most popular of the tools added to the survey this year
is R-based Tidymodels, reaching over 7 percent.

Machine learning framework usage

Scikit-learn

82.8%

TensorFlow

50.5%

Keras

50.5%

Xgboost

48.4%

PyTorch

30.9%

LightGBM

26.1%

Caret

14.1%

Catboost

13.7%

Prophet

10%

Fast.ai

7.5%

Tidymodels

7.2%

H20 3

6%

MXNet

2.1%

Other

3.7%

None

3.2%

JAX

0.7%
0%

10%

2020 Enterprise Executive Summary Report

20%

30%

40%
Technology

50%

60%

70%

80%

90%

100%
20

Enterprise Cloud Computing
There are clearly three big players in cloud computing, and
it’s no surprise who: Amazon Web Services, Google Cloud
Platform, and Microsoft Azure. Notably, more data
scientists are using the cloud overall. In 2019, about 25%
had not adopted cloud computing, which decreased to 17%
in this year’s survey.

Enterprise cloud usage

Amazon Web

48.2%

Services (AWS)

Google Cloud

35.3%

Platform (GCP)

Microsoft Azure

29.4%

None

17.1%

IBM Cloud / Red Hat

5.6%

Other

4.1%

3%

Oracle Cloud

VMware Cloud

2.9%

Salesforce Cloud

1.9%

SAP Cloud

1.8%

Alibaba Cloud

0.9%

Tencent Cloud

0.7%

0%

2020 Enterprise Executive Summary Report

10%

20%

Technology

30%

40%

50%

21

Those who use cloud services were asked about specific
products. Compute servers are the most common
products, followed by serverless technologies. One in five
did not name a cloud product.

Enterprise cloud product usage

Amazon EC2

40.6%

Google Cloud
Compute Engine

21.7%

21.1%

AWS Lambda
No/None

20.3%

Azure Cloud Services

19.8%

Amazon Elastic
Container Service

14.4%

Microsoft Azure
Container Instances

12.5%

Google Cloud Functions

12.1%

Google Cloud App
Engine

10.6%

9.3%

Azure Functions
Google Cloud Run

Other

6.1%

3.4%
0%

2020 Enterprise Executive Summary Report

10%

20%

Technology

30%

40%

50%

22

Enterprise Machine Learning Tools
Those who use AWS, Google Cloud Platform, or Microsoft

Of those with ML usage, Amazon SageMaker was the most

Azure were asked about machine learning (ML) tools in

popular answer, followed closely by Google Cloud AI and

particular. Over half of these data scientists do not use ML

ML.

in the cloud.
Enterprise machine learning product usage

55.2%

No/None

16.5%

Amazon SageMaker
Google Cloud AI
Platform/Google Cloud
ML Engine

14.8%

Azure Machine
Learning Studio

12.9%

Google Cloud Vision AI

8%

Google Cloud Natural
Language

7.8%

Azure Cognitive
Services

6.4%

Amazon Rekognition

4.3%

4.3%

Google CLoud Video AI

Amazon Forecast

3.7%

Other

2.9%
0%

2020 Enterprise Executive Summary Report

10%

20%

Technology

30%

40%

50%

60%

23

Enterprise Big Data
Business Intelligence tools help data scientists visualize
their data, but four in 10 do not use one. The majority do
employ BI, with Tableau as the most popular tool. Microsoft
Power BI and Google Data Studio round out the top three.
D ata s c i e n t i s t u s a g e o f b u s i n e s s i n t e l l i g e n c e t o o l s

None

38.8%

Tableau

33.3%

Microsoft Power BI

27%

Google Data Studio

9.1%

Other

6.4%

Qlik

5%

Amazon QuickSight

2.9%

Salesforce

2.8%

Looker

2.5%

Alteryx

2.1%

SAP Analytics Cloud

2%

TIBCO Spotfire

1.4%

Sisense

1.2%

Einstein Analytics

0.9%

Domo

0.7%
0%

2020 Enterprise Executive Summary Report

10%

Technology

20%

30%

40%

24

Regarding databases, there isn't a clear favorite among
data scientists. MySQL was mentioned most often (35.6%),
followed by PostgreSQL (28.86%) SQL Server (24.93%).
D ata b a s e u s a g e b y d ata s c i e n t i s t s

35.6%

MySQL
28.9%

PostgreSQL
24.9%

Microsoft SQL Server
18.7%

MongoDB

16.5%

SQLite
None

15.4%

Google Cloud BigQuery

13.5%

Oracle Database

12.9%

Amazon Redshift

9.3%

Microsoft Azure
Data Lake Storage

9.1%
7.9%

Other

6.7%

Amazon Athena

5.9%

Google Cloud SQL

5.6%

Snowflake

5.1%

Amazon DynamoDB
4.2%

Microsoft Access
IBM Db2

3.5%

Google Cloud Firestore

2.8%
0%

2020 Enterprise Executive Summary Report

10%

Technology

20%

30%

40%

25

Automated Machine Learning
As with machine learning overall, many data scientists
(33%) do not use auto ML tools. Google Cloud AutoML saw
gains from last year’s survey, nearly 14% versus 6% in 2019.
A u t o m at e d m a c h i n e l e a r n i n g f r a m e w o r k u s a g e

Google Cloud AutoML

13.9%

H20 Driverless AI

9.5%

DataRobot AutoML

8.4%

Databricks AutoML

6.5%
0%

2020 Enterprise Executive Summary Report

10%

Technology

20%

30%

40%

26

Machine Learning Experiments
Among data scientists who use tools to manage machine
learning experiments, TensorBoard is a clear favorite (over
21%). The closest competitor is Weights & Biases, with 6%.
However, the vast majority (68%) of data scientists do not
use special tools to keep track of and manage their ML
experiments.
Usage of machine learning experiment tools

No/None

68.1%

TensorBoard

21.6%

Weights & Biases

6%

Other

Trains

Neptune

Domino Model Monitor

Polyaxon

Guild.ai

Comet.ml

Sacred+Omniboard

5.4%

3.1%

2.3%

1%

0.9%

0.8%

0.7%

0.6%
0%

2020 Enterprise Executive Summary Report

10%

20%

30%

Technology

40%

50%

60%

70%

80%

90%

100%

27

Conclusion
This 2020 edition of the State of Machine Learning and
Data Science includes insights gathered from a survey of
20,036 Kaggle members. Their answers covered
demographic, education, employment, and technology
usage.


The charts and results are culled from professional data
scientists (covering 13% of respondents). There’s even
more to uncover in the most comprehensive dataset
available on the state of machine learning and data science
today.


Kaggle has published the complete dataset of responses
for the community to review, and we’ll run a competition
from November 18, 2020 to January 6, 2021 to learn even
more about data science practitioners in 2020.


2020 Enterprise Executive Summary Report

Conclusion

28

Application of AI, Insurtech and Real Estate
Technology
Introduction to Insurtech
Professor Christopher Geczy, PhD

Introduction to Insurtech
• The insurance industry is global and diversified across applications and
subsegments
• Gross Written Premium (GWP) was $4.8 trillion in 2017*
• The amount of insurance written, not including commissions and costs
• Net income worldwide grew 23% last year, vs. actual growth of 14% in
2017*

* Ernst & Young, “Global Insurance Trends Analysis 2018,” June 2018.
5

Introduction to Insurtech
• The insurance industry needs to respond to technological change & disruption
• Wearables
• Driverless vehicles
• Internet of Things
• “Big Data”
• Natural language processing
• Blockchain
• Distributed ledger technologies
• Climate change
• Etc.
• New technologies offer opportunities to increase efficiency in the industry and
serve new markets
* Ernst & Young, “Global Insurance Trends Analysis 2018,” June 2018.
16

Introduction to Insurtech
• There is no standardized definition of insurtech
• It is said to be revolutionizing the insurance industry and changing the way
insurers do business

* Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.
18

“ InsurTech can be described as “an insurance company,

intermediary, or insurance value chain segment specialist that
utilizes technology to either compete or provide valued-added
benefits to the insurance industry.”
- SIA PARTNERS, 2016

Key Emerging Technologies Leveraged by Insurtech Companies

Source: Capgemini World InsurTech Report 2018
20

Application of AI, Insurtech and Real Estate
Technology
Emerging Technologies: AI & Machine Learning
Professor Christopher Geczy, PhD

Key Emerging Technologies Leveraged by Insurtech Companies
• Artificial Intelligence
• A descriptor for software which can perform functions ordinarily
associated with human reasoning
• Iterative learning
• Self-awareness & emotions
• Insurers hope to exploit AI for chatbots
• Allstate’s Allstate Business Insurance
Expert (ABIE)
• Provides answers in real time to
customer owners' questions
• Other insurers have followed
Source: “What Is Insurtech and How Are Insurers Using It?”
https://www.thebalancesmb.com/what-is-insurtech-4584490.
Graphic: Accenture presentation: “Accenture’s 2017 Technology Vision for Insurance.”

37

Key Emerging Technologies Leveraged by Insurtech Companies
Machine Learning
• Enables computers to "learn" over time
• Using algorithms & mathematical models to simulate neural networks in the
human brain
• Allows computers to extract patterns from raw data rather than following
specific instructions
• Gives the appearance of being closer to the activities of the human brain
• Some insurance companies amass large amounts of data
• Yet, according to the National Association of Insurance Commissioners,
most insurers use only 10 to 15% of the data they collect
Source: “What Is Insurtech and How Are Insurers Using It?”
https://www.thebalancesmb.com/what-is-insurtech-4584490.
44

Key Emerging Technologies Leveraged by Insurtech Companies
• Machine learning could allow insurers to mine their data more effectively
and extract valuable information
• Risk modeling: Analyze claims data to predict the risk of future losses
• Demand modeling: Predict demand for their products in the future and
to estimate premiums
• Detecting fraud: Identify patterns of behavior that aren't obvious to
human adjusters
• Processing claims: Automate claim reporting and processing
• Underwriting: Help underwriters analyze data collected from applicants.
• Computers can aid in the decision making process, flag risks or
inconsistencies in data that underwriters might not be able to see
• Can also check external sources such as social media to verify the
Source: “What Is Insurtech and How Are Insurers Using It?”
accuracy of the data
https://www.thebalancesmb.com/what-is-insurtech-4584490.
52

Application of AI, Insurtech and Real Estate
Technology
Redefining the Insurance Industry
Professor Christopher Geczy, PhD

Redefining the Insurance Industry
1. Product Design
2. Selling & Marketing / Front Office
3. Underwriting
4. Policy Administration
5. Claims Management

58

Insurtech Are Redefining the Insurance Industry
Product Design

Front Office

Underwriting

Policy
Administration

Claims
Management

Actuarial Models and
Product Design

Marketing,
Distribution and
Channel Management

Underwriting
New Policies

Policy Acquisition
and Servicing

Claims Servicing
and Payout

• Real-time
information
capturing
• Advanced risk
analytics enabling
risk-based pricing
• Automated workflow
management & rules
engines
• Customer value-led
promotions &
discounts
• Digitized systems
with less reliance on
data that a customer
needs to provide

• Segment the market
based on servicing
desires
• Automated systems
with Straight
Through Processing
(STP) capabilities
• Automated ,
premium reminders
and renewal notice
• Anytime access to
policy details/view

• Streamlined claims
process with low
waiting time
• Instant notification of
the claim &
proactive status
updates
• Real-time claims
status monitoring
• Advanced analyticsbased fraud
detection

• 360° view of
customer’ needs
• More personalized
product designs
• Design new
products
• Adjust products in
real time
• Disaggregate
product mix
seamlessly
• Design and deliver
products to the end
customer they want

• Extended multidevice & mobility
offering
• Integrated
omnichannel
offerings & touch
points
• Real-time updates &
interactions with
clients
• Quick identification
of cross-selling &
up-selling
opportunities
• Detecting client
satisfaction
• Information
availability & price
transparency

Source: Capgemini World InsurTech Report 2018
63

Application of AI, Insurtech and Real Estate
Technology
Classification of Insurtech Companies
Professor Christopher Geczy, PhD

Segmentation of Insurtech Firms
• There are different classification methods for InsurTech firms and
initiatives, but they rotate to similar concepts:
• A “traditional” view: Full-stack / Agents / Brokers
• A nuanced view of sub-segments: Carriers / Enablers / Distributors

67

Segmentation of Insurtech Firms
• Milken Institute has three main classifications:
• Full-stack Insurers: Platforms that underwrite policies, assume the risk,
and, in most cases, manage the process from beginning to end
• Agents: Platforms that act on behalf of a carrier, essentially acting as an
extension of an incumbent carrier
• Brokers: Platforms that provide customers with a variety of policies
offered by both incumbent carriers and insurgent InsurTech platforms
• May or may not be paid commission based on the policies sold
through their platform
• May require customers to scroll through policies offered or
automatically connect customers to a preferred policy through
algorithms employed & based on a user’s response to a set of
questions
Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.
68

Segmentation of Insurtech Firms
• To make the classification:

Yes: It is an insurer
Is it a full insurance
company?

One: It is an agent
No: Is it partnered with
one or multiple
insurance companies?
Multiple: It is a broker

Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.
69

Classification of Insurtech Firms: Capgemnini
• Second classification system, used by Capgemini
• It categorizes InsurTech providers by their role in the distribution chain:
• Enablers
• Distributors
• Full Carriers

Source: Capgemini World InsurTech Report 2018
70

Classification of Insurtech Firms: Capgemnini

Source: Capgemini World InsurTech Report 2018
71

Segmentation of Insurtech Firms by Model
InsurTech Platform Models
Number of InsurTechs

35
30

31

25
20

18

15
10

12

5
0
FUL L STACK

AGENT

BROKER

InsurTech Platform Models
Milken Institute, “InsurTech Rising: A Profile of the InsurTech Landscape,” December 2018.
72

73

Product offering categories
E

S

pe

ci

al

ty

(i

(I

l.

nc

nc

et

nc

R

(I

l.

cy

p

cl

m

e;

ile

nc

ob

ra

rn

…

…

…

y…

on

tu

lit

si

re

en

su

e,

in

P

ng

bi

e)

…

;…

)

ed

nc

nk

ra

Li

lL
ia

t,

pi

en

ip

ri

bi

-t

em

in

ir

sh

t

th

ke

al

bi

he

su

ni

in

U

ily

na

le

,

am

so

tit

rm

/f

er

l.

/P

l.

te

nc

,

ty

(I

al

ili

t,

s

en

es

el

m

in

av

ow

Tr

nd

us

ab

t

rs

iv

l.

1

2

2
2

2

2

2

3
4

4

4

4

4

5

5

6
6

6

8

8

9

10
8

B

is

en

ve

nd

nc

13

16

18
16

D

R

ni

(I

(I
4
2

e/

(u

lth

o

No. of product offerings
6

om

fe

ea

ut
14

H

Li

H

A

Segmentation of Insurtech Firms by Model and Product
Type of Insurtech Platform, by Product Offering
Full Stack

Agent

12

Broker

0

Milken Institute, “InsurTech Rising: A Profile of the
InsurTech Landscape,” December 2018.

Segmentation of Insurtech Firms
• A significant portion (approximately 40%) of Insurtech companies could
better be described as technology solution providers
• Human Resources and Earned Benefits Solution Providers
• Data Solution Providers
• Infrastructure Solution Providers

Milken Institute, “InsurTech Rising: A Profile of the
InsurTech Landscape,” December 2018.
81

Segmentation of Insurtech Firms
• Human Resources and Earned Benefits Solution Providers
• Platforms using or deploying technology to help firms manage their
human capital more efficiently and cost effectively
• Data Solution Providers
• Platforms that specialize in collecting, aggregating, and/or analyzing
vast quantities of data to support (re)insurance carriers, startups, and
other stakeholders
• Infrastructure Solution Providers
• Platforms that focus on making back-end processes more efficient
through the use of application programming interfaces (APIs) or that
provide the means by which platforms can integrate and/or build
Milken Institute, “InsurTech Rising: A Profile of the
customizable insurance products and services
InsurTech Landscape,” December 2018.
87

Number of Technology Solution Providers in Insurtech Market
Insurtech Platform Models
15.5

Number of Providers

15

15

15

14.5
14
13.5
13

13

12.5
12

Human Resources and
Earned Benefits Solution
Provider - Small, Mild,
Large Businesses

Data Solution Provider

Type of Tech-Solution Providers
88

Infrastructure Solution
Provider

Milken Institute, “InsurTech Rising: A Profile of the
InsurTech Landscape,” December 2018.

Examples of Full-Carrier Insurtech Firms

Full Carriers

Insurtech Firm
Type

Products/Services Offered

Example

Digital Carrier

Traditional insurance model
conducted online or on mobile

ZhongAn is a Chinese property insurer that uses
the online channel to sell its products and handle
claims

P2P Insurer

A risk-sharing network where a
group of associated individuals pools
their premiums to insure against risk
and generally stands to benefit
regarding premium returns

insPeer, a France-based community insurance
platform, allows members to pool in money within
a group for covering a group member’s deductible

Micro Insurer

Smaller insurance packages with
lower premiums and typically lower
coverage

Leveraging mobile technology, BIMA offers
affordable insurance products to low-income
populations in emerging markets

On-Demand Insurer

On-demand insurance coverage that
can be purchased online as well as
via mobile apps

New York-based Sure offers on-demand personal
or episodic policies that a user can buy either via
website or an app

Usage-Based
Insurer

Premiums prices per usage or risky
behavior displayed by the customer

US-based Metromile offers auto insurance with
fees based on the number of miles the insured’s
car logs
Source: Capgemini World InsurTech Report 2018.

99

Distributors

Examples of Full-Distributor Insurtech Firms
Marketplace

Online site enables individuals to
compare plans from different insurers

PolicyBazaar specializes in comparative analysis of
products from various insurers based on price,
quality and key benefits

Personal Financial
Assistant

One-stop app(s) allows customers to
manage all their policies, obtain
coverage recommendations and
compare and purchase plans

Artificially intelligent insurance advisory application
Brolly delivers contextually relevant insights
through web and mobile applications, so customers
can manage policies in one place and know where
coverage may be duplicated or missing.

Digital Broker

Online platform allows customers to
compare and purchase policies

Licensed broker Coverfox offers insurance products
for vehicles, home, health services and travel.

B2B Digital
Distributor

Online site enables commercial
customers to compare plans from
different insurers

CoverHound, a US-based InsurTech firm that offers
a comparison platform for personal and small
commercial insurance products.

Customized or flexible front-office
solutions via partnership with an
insurer/reinsurer for risk management

London-based Bought By Many uses social media
data to connect people with similar insurance needs
and then uses the group’s collective buying power to
negotiate with insurers for deals that aren’t available
for individuals.

Value-Adding
Intermediary

Source: Capgemini World InsurTech Report 2018.
109

Examples of Full-Enabled Insurtech Firms
Process-improvement solutions for the
front office

PremFina’s white label solution for brokers allows
them to extend premium financing options to their
customers and manage insurance policies.

Policy/Plan
Management
Solution Provider

Process-improvement solutions for
underwriting and policy/plan
administration

RiskGenius applies artificial intelligence to streamline
the work of insurance professionals by retrieving
details on a specific coverage or exclusion, analyzing
policies and extracting relevant information such as a
premium limit or deductible.

Claims Management
Solution Provider

Process-improvement solutions
specifically for claims management

RightIndem has a white-label self-service insurance
claims platform for insurers that allows customers to
interact with their claim in their own time.

Data Specialist

Data capture or analytics solutions for
use cases or across the value chain

Carpe Data leverages fata from various channels
such as online content, social media, connected
devices and offers predictive scoring and data
products for the insurers, enabling them to predict risk
better.

Technology
Specialist

Solutions based on a specific
technology such as blockchain or
drones

Betterview,
a drone-technology specialist, allows
Source: Capgemini World InsurTech Report 2018.
drone-based inspection or property assessment.

Enablers

Front-Office Solution
Providers

Source: Capgemini World InsurTech Report 2018.
110

Application of AI, Insurtech and Real Estate
Technology
Investment & Market Size of the Insurtech Industry
Professor Christopher Geczy, PhD

Insurtech: Market Size
• What’s clear is that it’s large and growing
• Global Insurtech market revenue $532.7MM as of 2018*
• Market revenue expected to reach $1.2 billion by 2023 (+16% CAGR, 20182023) *
• AsiaPacific will have highest regional CAGR, growing in financial hubs in
Hong Kong/Singapore/India
• Health insurance segment is expected to have the higher segment CAGR*
• Total Insurtech investments, 2017: $3.2 billion **
• Total deals, 2017: 202 deals for $2.2 billion
• 83% involved an insurer/reinsurer as investor **
• Estimated 5-year CAGR, 2012-2017: +45% **
*Orbis Research, “Global (Insurance Technology) InsurTech
Market Size 2019,” Dec. 2018. **Ernst & Young, “Global
Insurance Trends Analysis 2018,” June 2018.
120

Size of Insurtech Market
Global private investment
(VC, PE and M&A)
in insurtech
2013-2018

$12.3

258
242

$10.3
164

165

114

$5.7

89

$4.0
$1.6
2013

$2.4
2014

2015

Capital invested ($B)

2016

2017

2018

Deal count
Source: KPMG International: Pulse of Fintech 2018,
Global Analysis of Investment in Fintech, January 4, 2019

121

Insurtech: Startup Count
Number of Insurtech Startups Exploded Over Just 2 Years
(Source: SMA – Strategy Meets Action, 2017)

122

Insurtech: Disruptors
The Top Insurtech Disruptors

123

Insurtech: Key Techs
• More and more insurers are moving key business functions to the cloud*
• Tech research firm Ovum’s annual survey of penetration of Software as a
Service (SaaS) grew from 13% in 2016 to 26% last year

* Source: Ovum survey data cited in Deloitte, “2019 Insurance Industry Outlook”
125

Insurtech: Key Techs

* Source: Ovum survey data cited in Deloitte, “2019 Insurance Industry Outlook”
126

Insurtech: Key Techs
• AIA Hong Kong has launched a blockchain app to share life policy data
with its bank distributors
• AXA Europe is offering flight delay insurance on a blockchain platform
featuring smart contracts
• Ovum’s annual survey of penetration of Software as a Service (SaaS) grew
from 13% in 2016 to 26% last year.
• Carriers and consortiums are expected to launch more impactful
blockchain initiatives due to concerns around data technology

130

Profile of Insurtech Market
• In the U.S., 63 Insurtech deals, with a total value of $1.59 billion, were
announced in Q4 2018
• Compared with Q4 of 2017, deal count in Q4 increased by 24%, while
funding volume also increased by 155%
• 63 transactions in Q4 2018 – higher than Q3, but lower than Q1 & Q2
• Globally
• UK investment down 9% from last quarter
• China the second largest investor for Q4 after the U.S.
• UK has been responsible for 8% of total investment since 2012
• Investment from international markets remains strong; transactions
outside of the U.S. account for 43% of total transactions since 2012 and
57% in the 4th quarter of 2018

*CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018

129

Profile of Insurtech Market
• While early-stage investments remain strong
• Seed and Series A account for 64% of total transactions since 2012 and
62% this quarter (up 4% from last quarter)
• Insurtech funding is maturing to mid – and later-stages – 45% of financings
in 2018 took place at the Series A, B, or C stages
• Could lead to consolidation further up

*CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018
133

Profile of Insurtech Market
• Property and Casualty (P&C) funding volume increased by 57% from Q3
2018 and increased by 89% from Q4 2017
• 41 P&C transactions in the quarter was only marginally higher than the
40 transactions in Q3 but marks a 52%
• Life and Health (L&H) funding volume increased by 1% from Q3 2018 but
marked a 362% increase from Q4 2017
• 2018 hits record level of Insurtech investment, driven by large
investments; deal count increased by 10% from Q3 2018 and funding
volume increased by 26%

*CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018
137

Size of Insurtech Market
Quarterly Insurtech funding volume – all stages
($ in millions)
$2,000

Property &
Casualty

$1,800
$1,600

Life & Health

$1,400
$1,200
$1,000
$800
$600
$400
$200
$0
Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4 Q1 Q2 Q3 Q4
2012
Deal Count

2013

2014

2015

2016

2017

2018

P&C

5

3

4

4

5

4

12

9

10

7

16

8

16

14

16

20

44

18

33

29

23

33

27

42

43

44

40

41

L&H

8

6

7

9

15

8

9

4

9

15

14

15

10

19

14

20

15

16

6

13

16

32

21

9

23

27

17

22

*CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018
138

Size of Insurtech Market
Quarterly Insurtech transaction by target country

2%
4%
5%

57%

6%
8%

United States
United Kingdom
China
Germany
India
France
Other

24%

Q4 2018

2012 – Q4 2018

18%

43%
5%
6%

United States
China
United Kingdom
Germany
South Africa
Other

9%
13%

2013-Q4 2018 Transactions: 972

Q4 2018 Transactions: 63

Quarterly InsurTech transaction by investments stage

3%
2%
5%

41%

13%

Seed/Angel
Series A
Series B
Series C
Series D
Series E+
Other

23%

2013-Q4 2018 Transactions: 972

139

16%

Q4 2018

2012 – Q4 2018

13%

32%

0%3%
5%
14%

Seed/Angel
Series A
Series B
Series C
Series D
Series E+
Other

30%

Q4 2018 Transactions: 63
*CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018

Size of Private Technology Investment by Insurers/Reinsurers
Private technology investments by (re)insurers
140

118

118

33

31

28

26

31

34

24

26

27

2016

2017

2018

105

120
100

66

80

28

60

22

29

40
20

26

1

27

19

4

14
11

0
2012

2013

2014

Q1

2015

Q2

Q3

Q4
CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018

144

Application of AI, Insurtech and Real Estate
Technology
Insurtech, Fintech, & Financial Inclusion
Professor Christopher Geczy, PhD

Microinsurance
• Investment firm: Omidyar Network
• Started by eBay founder Pierre Omidyar in 2004
• Model: Combines for-profit LLC with grantmaking
501(c)(3)
• $1.5B committed since inception:
• $676MM for-profit investments, $782MM grants

Pierre & Pam Omidyar

Total Commitments by
Year, 2004-YTD 2019

https://www.omidyar.com/financials
150

Microinsurance
• Common feature of microinsurance products is the low-income, low-networth population served which otherwise has very limited access to
insurance
• Microinsurance products by type (can be group or individual coverage)
• Term life
• Health / accident / disability
• Casualty (crop insurance, livestock, theft, fire, natural disaster)
• Certain forms of retirement savings plans
• Microinsurance by underwriter / delivery channel type
• Large multinational insurance companies
• Credit unions or mutual associations
• Government or NGOs
• Small community organizations
161

Microinsurance
• Benefits of microinsurance
• Financial protection through risk pooling
• Insureds can assume more risk
• Crop insurance vs. drought enables small farmers to plant crops which
have higher yields in “good” years & poorer yields in drought years
• Safeguard vs. families falling back into poverty due to illness, death of
breadwinner, housing destroyed, etc.
• Indian ministry of health found one-quarter of all hospitalizations
pushed individual or family into poverty due to cost of treatment*

* Tina Rosenberg, “The Microinsurance Revolution,” New York Times Opinionator blog, June 6, 2012 at
http://opinionator.blogs.nytimes.com/2012/06/06/the-microinsurance-revolution/
167

Microinsurance
• Benefits of microinsurance
• Can target specific at-risk populations
• HIV-positive, living in flood zone, microentrepreneurs
• Can complement social welfare programs, bolster other microfinance
initiatives
• Can assign term life policy to secure business, education or mortgage
loan

* Tina Rosenberg, “The Microinsurance Revolution,” New York Times Opinionator blog, June 6, 2012 at
http://opinionator.blogs.nytimes.com/2012/06/06/the-microinsurance-revolution/
172

Microinsurance
• BIMA – insurance business disruptor,
“Insurtech” leader
• Swedish-founded mobile insurance & health
company that provides accident, life,
& health insurance products to 26 million low-income consumers in 15
countries (Africa, Asia, LatAm)
• Largest markets: Ghana, Sri Lanka, Bangladesh, Pakistan.
• Mobile technology lowered prices and brought affordable insurance to
the world’s poorest
TechCrunch https://techcrunch.com/2017/12/19/bima-raises-97m-from-allianz-for-microinsurance-aimed-at-emerging-markets/
BIMA website: http://www.bimamobile.com/about-bima/about-us-new/
176

Microinsurance
• Business model: BIMA provides microinsurance subscription via basic
mobile phone service for as little as 60c/month on a “pay-as-you-go” rolling
monthly cover
• Offers payouts of up to $1,000 to the family if the insured person dies
• Sign-up takes only 3 minutes and payments are collected through basic
mobile phone service
• Customer growth: over 550 thousand new customers/month (BIMA, 2018)
• 93% of customers live on less than $10/day (BIMA, 2018)
• 75% of subscribers never had insurance before
• Educating customers is the top priority
• $300MM valuation at Dec-2017 sale of LeapFrog Investments’ stake to
Allianz for $97MM

TechCrunch https://techcrunch.com/2017/12/19/bima-raises-97m-from-allianz-for-microinsurance-aimed-at-emerging-markets/
BIMA website: http://www.bimamobile.com/about-bima/about-us-new/

184

Size of Private Technology Investment by Insurers/Reinsurers
Private technology investments by (re)insurers by target country

3%
5%
54%

6%
8%

United States
France
China
United Kingdom
Germany
Canada
Other

20%

Q4 2018

2012 – Q4 2018

16%

33%

United Kingdom
Other

13%

13%

8%

United States
China
Germany

2013-Q4 2018 Transactions: 972

21%

Q4 2018 Transactions: 63

Private technology investments by (re)insurers by investment stage

3%
4%

16%

13%
29%
24%

Seed/Angel
Series A
Series B
Series C
Series D
Series E+
Other

2013-Q4 2018 Transactions: 972

185

12%

11%

12%

Q4 2018

2012 – Q4 2018

11%

19%

11%

Seed
Series A
Series B
Series C
Series E+
Other

35%

Q4 2018 Transactions: 63
*CBINSIGHTS: Quarterly InsurTech Briefing Q4 2018

Technology and
innovation in the
insurance sector

TECHNOLOGY AND INNOVATION
IN THE INSURANCE SECTOR

Please cite this publication as:
OECD (2017), Technology and innovation in the insurance sector

This work is published under the responsibility of the Secretary-General of the OECD. The opinions
expressed and arguments employed herein do not necessarily reflect the official views of the OECD or
of the governments of its member countries or those of the European Union.
This document and any map included herein are without prejudice to the status or sovereignty over
any territory, to the delimitation of international frontiers and boundaries and to the name of any
territory, city, or area.

© OECD 2017

FOREWORD

Foreword
“Insurtech” is the term being used to describe the new technologies with the potential to
bring innovation to the insurance sector and impact the regulatory practices of insurance
markets. This report catalogues these technologies and examines how InsurTech is being
funded and how insurers are engaging with the start-ups entering the market.

This report was prepared as part of the programme of work of the OECD Insurance and
Private Pensions Committee, the international forum for addressing policy and regulatory
issues in insurance and private pensions for governments, international organisations and
industry. It has benefited from input from the insurance market and industry, including
from a number of insurance start-ups. This report contributes to the OECD’s Going
Digital project which is examining from a wide range of perspectives how technology and
innovation is affecting the economy.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│3

TABLE OF CONTENTS

│5

Table of contents
Executive summary ............................................................................................................................... 7
1. Introduction ....................................................................................................................................... 8
2. Funding of InsurTech ..................................................................................................................... 11
3. Insurance intermediation and distribution models ...................................................................... 16
4. The sharing economy and insurance ............................................................................................. 21
5. Robo-advice and AI ......................................................................................................................... 23
6. Data aggregation and analytics ...................................................................................................... 26
7. The role of policy and regulation in InsurTech ............................................................................ 29
Insurance regulation and innovation hubs ......................................................................................... 29
AI and the regulation of technology-based advice............................................................................. 34
Privacy and data protection issues ..................................................................................................... 34
RegTech ............................................................................................................................................. 36
Wider policy considerations .............................................................................................................. 38
8. Conclusions ...................................................................................................................................... 40
References ............................................................................................................................................ 42

Figures
1. InsurTech financing trend (2011-2016)............................................................................................. 13
2. InsurTech deal activity by focus area (US only, health vs non-health) ............................................. 13
3. Tech start-up investment by (re)insurers (2012-2016) ...................................................................... 14

Boxes
Box 1. OECD Work on FinTech ........................................................................................................... 10
Box 2. Technology relevant to InsurTech ............................................................................................. 11
Box 3. Funding of InsurTech by (re)insurers ........................................................................................ 14
Box 4. BIMA ......................................................................................................................................... 16
Box 5. Friendsurance, InsPeer and Guevara ......................................................................................... 17
Box 6. Lemonade .................................................................................................................................. 18
Box 7. The use of blockchains in insurance .......................................................................................... 19
Box 8. Robo-advice for insurance ......................................................................................................... 24
Box 9. PolicyGenius .............................................................................................................................. 24
Box 10. Estonian Insurance Association’s motor insurance database ................................................... 28
Box 11. Monetary Authority of Singapore's approach .......................................................................... 30
Box 12. The UK Financial Conduct Authority’s Project Innovate ....................................................... 31
TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

6│

TABLE OF CONTENTS

Box 13. Autonomous cars and insurance .............................................................................................. 33
Box 14. EU General Data Protection Regulation (GDPR) .................................................................... 36
Box 15. RegTech and algorithms .......................................................................................................... 37
Box 16. Estonia’s electronic ID card and digital signature services ..................................................... 39

EXECUTIVE SUMMARY

Executive summary
Innovation through new technologies is a key driver of change in the financial sector and this
has led to immeasurable efficiency gains, even though these changes can initially be
accompanied by uncertainty and doubt. The insurance sector is no exception to such
developments, with possibilities of new methods of service provision as well as greater
opportunities for data collection and fraud detection that can lead to better risk identification
and mitigation measures, which are being referred to as “InsurTech”.
This report catalogues the relevant technologies that are being viewed as having the potential
to bring innovation to the insurance sector. How InsurTech is being funded is examined, as
this indicates which markets are actively investing in start-ups and how insurers are engaging
with start-ups. Case studies are made of insurance start-ups, and how blockchain technology,
sharing economy, robo-advice and data aggregation are influencing the insurance sector is
discussed. The manner in which insurers engage technology to ensure better compliance with
regulation is also examined.
Innovation and new technologies have the potential to affect the franchise value of insurance
companies, with accompanying competition policy considerations. Policies which have
tailored coverage and simplified claims processes can improve coverage to segments of
society that hitherto were not able to access financial protection. Regulatory approaches, such
as the regulatory sandbox being developed by a number of jurisdictions, may bridge greater
competition and prudential requirements, although ensuring a level playing field as solutions
graduate into the full market require some consideration.
There are a number of areas in which greater regulatory discussion should take place, as the
transparency of the technology and the impact on policyholder’s choice and rights may not be
clear. Data protection is an area that will require closer examination by regulators, as the
volume of personal data handled by insurers increases, whether consensus was gained for the
intended use becomes blurred. Data aggregation brings forth the possibility of certain segment
of the population becoming uninsurable, so how data is harnessed should be closely
considered. The treatment of algorithms is also an area for further discussion to ensure that
the assumptions built in are appropriate and unintended consequences are avoided in so far as
possible, and regulators have a means of engaging in this assessment. These could have
implications on the ongoing monitoring of operational risk and internal control of insurers.
Ensuring that policyholders are fairly treated and appropriately protected when the
implications of certain innovations and technologies are uncertain will be important going
forward.
As emerging markets have less of an established distribution network of insurance, innovation
and technology may have the greatest impact in such markets. Nevertheless, whether
developed or emerging, appropriate regulatory monitoring should be carried out to ensure that
the welfare of policyholders is safeguarded.
There are number of ways in which regulatory approaches could be considered for InsurTech,
but as well in the wider FinTech realm. The OECD is engaging with FinTech issues in a
number of ways, and further areas of discussion are proposed. This report is part of a wider
project by the OECD on FinTech developments, as well as the OECD horizontal project on
Seizing the Benefits of Digitalisation for Growth and Well-Being.
TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│7

8│

1. INTRODUCTION

1. Introduction

Innovation through new technologies is a key driver of change in the financial sector and this
has led to immeasurable efficiency gains, even though these changes can initially be
accompanied by uncertainty and doubt. In recent years, such innovation has happened on
the back of new technological developments, with the phenomenon often being described
as “FinTech”. As financial services deal in intangible products, it is well suited for
technological innovation to lower transaction costs and expedite the delivery of services.
Although this has, in fact, been happening over the history of finance, the recent
proliferation of internet connections, home computing and mobile devices, and the
development of applications has led to the possibility of lowering the barrier for market
entry, paving the way for greater competition in or “disruption” of the financial industry.
However, slating technological and innovation as “disruptive” technology can be
misleading, and it is likely to be more a hindsight observation than the everyday trial and
error that accompanies innovation and technological advances.

The insurance sector is not an exception to this, with developments in technology leading
to possibilities of new methods of service provision as well as greater opportunities for
data collection that can lead to better risk identification and mitigation measures, which
are being referred to as “InsurTech”. InsurTech, as compared to FinTech, is more often
related to service improvements for individuals, as opposed to businesses.
Innovation is generally regarded as a positive development, delivering convenience and
efficiency. For example, the advent of cash points (ATMs) assisted people to gain access
to cash even out of business hours and lowered the costs for banks. Improvements in
communication networks and processing capacity have led to faster payment processes.
Insurance claims can be processed via online platforms, with less time for processing.
Comparative sites permit product comparison of various insurance products.
How the insurance sector responds to economical and society-wide technological
innovations, and provides insurance processes and policies that integrate such changes
would be an important development to consider. For example, the sharing economy has
made start-ups, such as Uber, making available ridesharing more conveniently and
widely. While commercial motor liability insurance would be a requirement for taxi
drivers, Uber drivers may not have the appropriate coverage as it is often their side
business or a part-time job. Insurance companies are already responding to this specific
case, but it presents a wider question of how insurance responds to new risks that do not
fit the traditional lifestyle and/or economic activity of individuals or businesses.
Given that underwriting is largely based on the analysis of historical data to carry out the
risk assessment of a policyholder, insurance, on first glance, appears particularly well
suited for “big data” analysis. Big data and blockchain have been major topics in many
insurance discourses of technology.
InsurTech has attracted large venture capital investments, and the trend of financing
indicates that many start-ups are considered by investors to be commercially viable on a

1. INTRODUCTION

mass-scaled basis. Insurers themselves are making strategic investments in insurance
start-ups, allowing them to have a stake in these developments while providing the capital
for such enterprises to develop their business.
A number of insurance start-ups such as Friendsurance, Lemonade and Policygenius have
attracted large investments. To comprehend how disruption may be happening in the
insurance sector, case studies of start-ups are presented throughout this report, to provide
context, and better understand how such businesses are being developed and how they are
different from traditional business models.
There are new forms of processes that may be improving the efficiency of intermediation
and claims management. Most insurance start-ups involved in distribution have sites with
well-developed contents, often accompanied by the application of artificial intelligence or
robo-advice. These are intended to give an improved customer experience and lower
commission/fees for when products are sold, although the initial fixed cost will likely be
higher. Some outlooks predict the number of insurance employees will drop as a result of
some of these evolutions (McKinsey, 2015).
This report examines the various innovations taking place in the insurance sector, and
what policy and regulatory impact they may have, as well as the benefits that could be
reaped from innovation in the insurance sector, especially for policyholders. There are
regulatory and competition considerations that need to be made as “disruption” to the
industry is often about new market entries as well as new modes of service provision
which may not fit the mode in which regulations was conceived upon. There are also
wider privacy and data protection issues which require close attention given that
InsurTech by nature usually involves a digital component to the technology.
This report is part of a wider project of the OECD to examine FinTech developments, as
well as the OECD horizontal project on Seizing the Benefits of Digitalisation for Growth
and Well-Being (see Box 1).

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│9

10 │

1. INTRODUCTION

Box 1. OECD Work on FinTech
As part of an OECD-wide project presented at the June 2016 Ministerial meeting on
Digitalisation of the Economy and Society, various OECD bodies have been, and will be,
covering this area from a wide range of perspectives, ranging from the impact on traditional
financial firms and payment systems to issues related to competition, and financial
consumer protection and literacy. The OECD’s Going Digital project (www.oecd.org/goingdigital/) was officially launched on 12 January 2017 at a conference organised jointly by the
OECD and the German Federal Ministry of Economic Affairs and Energy in Berlin. The
conference brought together a wide range of stakeholders to discuss some of the most
pressing policy challenges related to the digital transformation. This conference served to
inform the Going Digital project and to kick-off the German G20 Presidency’s digital
agenda.
In October 2015, OECD Competition Committee held a Hearing on Disruptive Innovation in
the Financial Sector. The hearing focused on the example of peer-to-peer lending, equity
crowd-funding, digital currencies, and payment mechanisms, and was based on the
premise that innovators are needed to contest markets, stimulate competition and enhance
productivity, especially in financial services where network effects can create natural
monopolies, concentrate rents and render financial services expensive and exclusive. The
hearing explored such issues, assessed the impact of selected financial innovations on
consumers, and discussed how existing regulatory frameworks can be changed to
encourage the introduction of new business models and technologies – and not stifle them
at too early a stage.
The OECD/International Network on Financial Education also undertook work on the
implications of digitalisation for financial literacy and relevant aspects of financial consumer
protection. The G20/OECD/INFE report “Ensuring Financial Education and Consumer
Protection for All in the Digital Age” was submitted to the G20 presidency in April 2017.
The OECD contributed to the development of the G20 High-level Principles on Digital
Financial Inclusion, endorsed by G20 Leaders in September 2016.
The OECD Insurance and Private Pensions Committee (IPPC) held a Roundtable on
Technology and Innovation in the Insurance Sector in June 2016. Participants approved a
project outlining the implications of technology and innovation for the insurance sector. As part
of its work on robo-advice, the IPPC organised a Roundtable on Robo-Advice in retirement
saving in June 2017 to discuss better understanding how these types of platforms operate from
a regulatory/supervisory perspective.

2. FUNDING OF INSURTECH

2. Funding of InsurTech1

Funding for new technology and innovation in the insurance sector are impacted by the
wider venture capital (VC) possibilities in the market. In the United States, InsurTechs
have benefited from a rich and competitive market place for VC funding, and many
insurance start-ups have successfully completed a number of funding rounds. On the
other hand, some markets do not have a strong VC culture, so the approach to raising
capital would be different, with public sources becoming more important. For example,
the French start-up, InsPeer, has funding from a number of public sources.

Box 2. Technology relevant to InsurTech
A number of wider technological developments and innovations underpin many of
InsurTech developments. Some of the technologies are inter-related and a brief review of
them is useful in establishing a common understanding of their nature.
Mobile technology and applications (apps)
The network effect of mobile phones and development of applications for these devices
(“apps”) has allowed many companies to reach a bigger audience than was previously
possible. Mobile technology may be working in different ways for InsurTech, depending
on the generation of mobile networks available, and the types of handsets that are most
widely used.
Smartphones and internet access enable innovations which are based on the use of
apps. For this, mobile networks that allow short messages and pre-paid mobile phones,
as well as large data transfers would be necessary. This is particularly relevant to
emerging markets which have low insurance penetration and do not have a wellestablished distribution network. As in the example of BIMA (Box 7), mobile phones have
the ability to notify individuals via SMS on anything from the insurance coverage to
reminding them of imminent withdrawal of airtime for premium payments.
Artificial intelligence (AI), algorithms and robo-advice
AI is intelligence exhibited by machines. A machine would be considered “intelligent”
when it takes into consideration its environment and takes action to maximise the
possibility of achieving its given goal. It is widely used when computer programmes are
developed to have cognitive functions such as learning and problem solving. AI research
is taking place in fields including reasoning, knowledge, planning, learning, natural
language processing, perception and moving/manipulating objects.

1

This section draws heavily on data from CB Insights which is the leading data and information
provider on private company investment.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 11

12 │

2. FUNDING OF INSURTECH

Box 2. Technology relevant to InsurTech (cont.)
Algorithms are part of AI, where there is a set of steps for a computer programme to
achieve a task under certain conditions. Well known algorithms include route navigation
systems or computer chess games. In the financial sector, algorithmic trading, such as
high frequency trading, is wide spread, with pre-programmed trading instructions to
execute large trading orders. The algorithm would follow a set of conditional instructions
for placing a trade order at a speed and frequency that is not possible for a human trader.
Robo-advice, or automated advice, is becoming prominent particularly for online
investment and savings platforms. It can cover a broad spectrum of services, but is
essentially an “on-line automated advice model that ha[s] the ability to deliver advice in a
more cost-efficient way” (HM Treasury and FCA, 2016). For the insurance sector, roboadvice is being developed for investment management and is now being increasingly
used for quotes with automated advice and offerings calculated through algorithms.
Instead of or combined with face-to-face advice, robo-advice can provide automated
guidance and execution on various financial decisions. Automated advice could assist
pockets of population that do not have access to financial advice to gain input in a more
cost efficient way than a human advisor. However, depending on how the algorithm to
provide advice is structured, it could also lead to inappropriate advice being made
inadvertently.
Smart contracts
“Smart contract” refers to any contract which is capable of executing or enforcing itself.
They are written as programming code which can be run on a computer or a network of
computers rather than in legal language on a printed document. This code can define
strict rules and consequences that emulate a traditional legal document, stating the
obligations, benefits and penalties due to either party being in various circumstances.
Smart contracts enable people to trade and do business with strangers, usually using the
internet, without the need for a large centralised authority site to act as an intermediary.
The limitation of a smart contract is that a programme may not know what is happening in
the physical world or react to unforeseen events, thus being unable to execute an action
that was the basis of the contract.
Smart contracts often run on blockchains or distributed ledger technology (DLT). Example
of a smart contract using DLT is a cryptocurrency, such as Bitcoin. Ethereum is one of the
largest platforms for smart contracts and blockchains.
Blockchain/distributed ledgers technology (DLT)
Blockchain or distributed ledger technology (DLT) is a protocol for the exchange of values
or data over the internet which does not require an intermediary. The protocol of
blockchain technology is to create a shared, encrypted database of transactions and
other information. Examples of ants and flocks of geese have been given to demonstrate
what a perfect blockchain society would be like; decentralised yet coordinated.
The technology is to establish an ever-lengthening chain of blocks of data. Each block
has compact record of validated transaction by participants in the blockchain, and the
premise of blockchain is that the information in the blocks is true. Once the transaction is
validated and recorded, the stored record is irreversible. Blockchain originally referred to
the database where all Bitcoin transactions are recorded and stored.

2. FUNDING OF INSURTECH

Figure 1. InsurTech financing trend (2011-2016)

Source: CB Insights (2017a) Insurance Tech Start-ups Raise $1.7B Across 173 Deals in 2016
www.cbinsights.com/blog/2016-insurance-tech-funding/ .

2015 saw record funding levels for InsurTech, with funding estimated to be
USD 2 669 billion in total. The 2016 Q3 saw funding levels of USD 1 401 billion, and the
number of deals in 2016 Q3 were 126, already exceeding the number of deals in 2015
(see Figure 1). It should be noted that in 2015, nearly 1/3 of funding went to Zhong An, a
Chinese internet-only insurer that was established in 2013 with backing from Alibaba
Group Holding, which raised USD 931 million in 2015, and is said to be planning a IPO.
Figure 2. InsurTech deal activity by focus area in the United States (health vs non-health)

Source: CB Insights (2017b), InsurTech Connect 2016 (October) https://www.cbinsights.com/reports/ITCinsurance-tech-deck.pdf.

In 2016, 59% of InsurTech deals went to US-based start-ups, followed by Germany (6%),
UK (5%), China (5%) and India (3%) (CB Insights, 2017a). This may not perfectly match
the population of InsurTechs, but is indicative of the VC possibilities in the market, in
particular for the US, although Asian InsurTech is much weaker compared to the wider
VC funding in the region. The number of VCs that are investing in InsurTech start-ups
has increased from 55 funds in 2012, to 141 in 2016 YTD (CB Insights, 2017b).
While the breakdown of the investment in the insurance sector is not available,
investment in health insurance is considered to be strong and growing, taking up 70% of
InsurTech investments in the United States (CB Insights, 2017b). At the same time,
investment in start-ups providing commercial distribution avenues has increased many
TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 13

14 │

2. FUNDING OF INSURTECH

folds, indicating the strong interest and the number of start-ups hoping to improve the
customer experience.
Life and annuity start-ups are attracting large investments, as are health and dental
insurance start-ups (CB Insights). Auto insurance distribution/comparison start-ups also
constitute a large cohort of the insurance start-ups.
In addition, insurers are providing funding structures that would allow them to have first
pick of successful new technology and innovation that could support their existing
operations and improve the customer experience. This has been via both general VC
funding opportunities and targeted InsurTech investments, as well as establishing
incubators that host InsurTech entrepreneurs and employees (see Box 3). A number of
insurers have provided investment to InsurTech start-ups, as well as Internet of Things
(IoT) start-ups.

Box 3. Funding of InsurTech by (re)insurers
The wider funding landscape for InsurTech is described above, but a more interesting development
has been how (re)insurers are funding InsurTech. Some of the larger insurers have set up specific
funds and VCs to invest in start-ups, including for InsurTech, indicating the likelihood of greater
investment into InsurTech, and the strategic investments existing insurers will make to ensure they
have a stake in a start-up that may be able to scale their business.
The number of deals made by (re)insurers in 2016 was 100 deals, compared to 67 in 2015 and 28
in 2014) (CB Insights, 2017c).
Figure 3. Tech start-up investment by (re)insurers (2012-2016)

Source: CB Insights (2017c), Where Insurers and Reinsurers Invested in Tech Start-ups in
2016 www.cbinsights.com/blog/2016-insurance-cvc-total/

Reflecting the wider InsurTech landscape but with certain specific differences, US (re)insurers are
making the majority of investments in InsurTech with 64% of deals being made (as opposed to the
actual funding level, for which data is not available)(CB Insight, 2017c). Most likely reflecting
investments that Ping An Insurance has made in Zhong An, and Taipang Insurance has made in
Alibaba Health, Chinese (re)insurance investments is 10% of deals made by (re)insurers. It may be
that given the lower penetration of insurance in China, it is being anticipated that the market may
develop based on the new intermediation models that are being introduced in China. France and
UK (re)insurers make respectively 11% and 6% of deals by (re)insurers (CB Insight, 2017c).
Many of the deals are made by (re)insurers’ strategic VC arm. Ping An Venture has been making some
of the largest investments in InsurTech with over 20 deals. Axa Strategic Ventures has also completed
20 deals and together with Ping An have been the most active in deal making of strategic investments.

2. FUNDING OF INSURTECH

│ 15

Box 3. Funding of InsurTech by (re)insurers (cont.)
US-based insurers MassMutual Venture, USAA, American Family Ventures, Transamerica and
New York Life follow with between five and ten deals each. After which, the European insurers
Allianz Ventures, MunichRE/HSB Ventures and Aviva Ventures continue.
More historically, Axa Strategic Ventures, Transamerica Ventures and American Family Ventures
have been the most active investors in private tech investing since the start of 2012. Axa provided
seed funding for five European start-ups under a fund set up in France in 2013, before launching
Axa Strategic Ventures in 2015. The €200 million (USD223.47 million) venture capital fund has a
mandate to invest in innovations in insurance, asset management, financial technology and
healthcare services. Axa created Kamet in 2015, which is a €100 million InsurTech incubator
working with both internal and external entrepreneurs. Axa recently invested €75m to take an 8%
stake in e-commerce company Africa Internet Group and has become the exclusive insurance
provider through Jumia and other platforms.
Allianz established Allianz Ventures as its centre for investments in and partnerships with start-ups
to target five key areas: InsurTech and wealth management; mobility and connected cars;
connected homes and properties; digital health; and cyber security and data intelligence. Recent
investments include a minority stake in digital wealth manager MoneyFarm. Allianz X is the group’s
“company builder” that identifies, builds and scales new business models in InsurTech and related
areas like blockchain and artificial intelligence.
Aviva launched a venture capital arm to invest in new digital businesses late 2015, based in Hoxton
Square, the hub of London’s digital entrepreneurs, with an annual fund of approximately £20 million
(USD24.8 million) to be invested over the next five years. Its first investment was in Cocoon, a
smart home security device that alerts householders to movement and sound within their property.
In May 2016, Aviva announced a partnership with Founders Factory, a digital accelerator and
incubator, becoming its exclusive financial services partner over the next five years, providing
capital and resources to support the growth of more than 200 technology businesses.
Munich Re has made investments through its HSB Ventures division in Slice Labs, a US provider
of on-demand insurance, which launched a product for hosts of homesharing using platforms like
Airbnb, HomeAway, OneFineStay and FlipKey. The insurance lasts specifically for the time the
owner is acting as a business so they insured can buy cover only when they need it. Munich Re
has secured the right to provide underwriting capital and insurance licensing for on-demand insurer
Trov in the US market. Trov’s app allows customers to insure individual items like electronics or
sports equipment from their smartphone and gives them the facility to switch cover on and off when
required.
In April 2016, smart sensor start-up Helium‘s USD20 million Series B was led by corporate venture
arm GV, but Munich Re/HSB Ventures also participated. Helium’s sensor technology lies in its
ability to use off-the-shelf sensor and connect it to the Helium Cloud which allow the OS to control
storage temperature. So thus the use of such technology can protect from liability arising, for
example, from leaving the refrigerator open in a restaurant or a hospital not managing its vaccine
stock.
Ping An Ventures has been actively investing in the healthcare sector.
Most of the (re)insurers on the have only been publicly investing in start-ups for the last two years
and areas in which (re)insurers consider there to be mass demand and practical application to their
businesses.
Axa Strategic Ventures, AIG, and American Family Ventures have investing in IoT start-ups in auto,
home, industrial, and other segments, and Axa, AmFam, USAA, and MunichRe/HSB have made
separate IoT investments as well.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

16 │

3. INSURANCE INTERMEDIATION AND DISTRIBUTION MODELS

3. Insurance intermediation and distribution models
Insurance intermediation has traditionally used either the agent/broker or bancassurance
model. While this remains the main intermediation channel for most developed insurance
markets, many InsurTech start-ups are taking on this model and proposing new
distribution models for insurance. These new modes of distribution are in particular
interesting for less developed insurance markets, where insurance penetration is low and
the conventional intermediation model of agent/brokers may not be efficient or effective.
Asia and Africa have witnessed large investments being made into start-ups and
technology based in their region.
BIMA, Friendsurance, InsPeer and Guevara are all distribution-based insurance start-ups,
providing new insurance services. While they do not intermediate policies in the more
traditional sense, they all have brokering licenses to triage the appropriate policy using
different business models. BIMA operates in less developed markets, and has had wide
success in intermediating health insurance products through their model of combining
agents with mobile platforms. BIMA has acquired a microinsurance license in some of
the markets it operates. Friendsurance, InsPeer and Guevara are all peer-to-peer (p2p)
insurance companies that rely on peer pressure for risk mitigation.

Box 4. BIMA
BIMA uses mobile technology to provide insurance services in developing and emerging
markets, which the technology permits with the lower entry costs. In many developing
countries, in Africa in particular, mobile phones are widely used for not only
telecommunications, but also for accessing banking and payment services. The proliferation of
mobile phones (penetration of 70% of population) and the acceptance of the technology for
financial services have enabled BIMA to expand its health services in 16 markets. The success
of this model has enabled BIMA to reach profitability in several markets already.
The main innovation of BIMA is the creation of a proprietary back-end tech platform which
creates a mechanism for both registration and payment. Policyholders register using their
handset to fill in some basic identification details which process takes approximately 2 minutes.
Premium payment is collected via automatic deduction of prepaid airtime credit; unlocking a
new payment channel that makes insurance affordable and accessible.
Distribution is carried out by a trained agent force. BIMA agents make the initial contact with
potential policyholders, providing product education about all aspects of the policy including
basics like the cost (just a few cents a day) and the coverage level. Post-sale, the customer will
receive a confirmation SMS plus a monthly reminder of their coverage status and amount to be
deducted.

BIMA sells a range of personal insurance products, including accident, life and hospitalisation
cover (this policy pays a fixed amount per night spent in hospital). To claim, policyholders call
customer support that will help them to file their claim which is paid in cash within 72 hours of
the claim being completed.

3. INSURANCE INTERMEDIATION AND DISTRIBUTION MODELS

│ 17

Box 4. BIMA (cont.)
BIMA is primarily licensed as an insurance intermediary and/or a licensed microinsurance
provider, where applicable, and not an underwriter. Data is stored in Sweden which data
protection regulation would apply.
In total, BIMA has raised USD75 million in capital so far. In 2015, BIMA closed its series C
funding round with USD38 million raised from existing investors, including Investment AB
Kinnevik, LeapFrog Investments and Millicom. This builds on a successful B series funding of
USD22 million and USD15 million of capital invested before these rounds.

Box 5. Friendsurance, InsPeer and Guevara
Friendsurance, InsPeer and Guevara are based on similar business models, although
operating in different markets (namely, France, Germany and the United Kingdom).
Friendsurance
Friendsurance was launched in 2010 and has funding from a number of internet venture
capitalists (Horizons Ventures, VantageFund, e.ventures, the German Start-ups Group as well
as the European Regional Development Fund). They are licensed as insurance brokers in
Germany. Friendsurance is considered a pioneer of “social” or “person-to-person” “peer-topeer” (p2p) insurance, offering household, personal liability and legal expenses and car
insurance. Policyholders with the same type of insurance form small groups, which could either
be with friends or find a group on their site. A part of the group’s premiums are paid into a
cashback pool. If no claims are submitted, the members of the group can get up to 40% of their
premiums paid from the cashback pool at the end of the year. Claims are settled using the
cashback pool, and thus the claims decrease the cash back amount at the end of the year.
Large claims are covered by normal insurers, with whom the firm has partnerships.
The benefit of p2p insurance is that the network effect discourages the group from making
claims for very small amounts and policyholders seek participation from friends to increase the
size of the pool.
InsPeer
InsPeer was launched in 2015 in France, and is a p2p insurance scheme that enables a group
of people to share their deductible when a claim is paid. InsPeer is backed by angel investors,
Bpifrance, the City of Paris, and Region Ile de France.
As the higher the deductible, the lower premiums become, a policyholder can raise their
deductibles and share the risk with the designated group of people. InsPeer provides services
for auto, motorcycle, and homeowners insurance policies. Other than increasing the deductible,
InsPeer does not require changes to insurance contracts.
Users form small groups which share the risk that one or all will file a claim. Users can
participate in as many insurance groups as they like but their exposure is limited to €100
pledged to any one participant and €1 500 across the platform. The service is completely free if
there are no claims. In the case of a claim, InsPeer keeps a 10% of the claim paid by the
insurer.
To assist policyholder assess who to share their risks, a risk indicator has been developed that
indicates the expected claims rate of one person. This risk indicator for consumers is
expressed in years, for example 8.5 years means that there is a chance of paying a claim once
every 8.5 years.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

18 │

3. INSURANCE INTERMEDIATION AND DISTRIBUTION MODELS

Box 5. Friendsurance, InsPeer and Guevara (cont.)
Guevara
Guevara is a UK based insurer, and offers a choice of groups to join for auto insurance
policyholders, to which their premium is split into one portion that is paid into the individual group
(protection pool) and the rest into the single collective pot (insurance fees) that supports all of the
individual groups. The split of premium is determined by the number of members in a group.
Claims are first paid from the protection pool until it is exhausted, after which claims are paid
from the insurance fees. When the insurance fee is also exhausted and the combined ratio
exceeds 100%, reinsurance is taken out. Any funds left in the protection pool remain in the pool
the following year, and requires being topped up only, in addition to the annual insurance fees.
By using peer pressure, the objective is to keep claims low. Guevara is authorised as a peer-topeer insurance provider by the FCA, and can operate as a broker.

The best known carrier model is Lemonade, which has acquired an insurance carrier
license, and attracted one of the largest seed funds for an InsurTech start-up in 2016.

Box 6. Lemonade
Lemonade is one of the InsurTechs to have raised the largest amounts of seed funds, with a
USD13 million seed round from venture capital Seqouia Capital and Aleph, as well as
investments from XL Innovate, XL Catlin’s venture capital arm. Lemonade Insurance
Company is a property and casualty insurance company based in New York and with a New
York state license as a full-stack insurance carrier.
Premiums are paid into a claims pool, and from that a fixed fee (20% of premiums) is taken
out monthly for reinsurance coverage and expenses, with the remaining being used to pay
claims. If the total of premiums paid is more than the fees and paid claims, moneys are
returned to policyholders in the form of an annual 'Giveback'. Giveback is donated to charities
of the policyholder’s choice, and for this purpose virtual groups of 'peers' are formed around
the charities of choice. Reinsurance is used to pay for claims that exceed the size of the pool.
Premiums are calculated individually for each policyholder and are based on a number of
different factors including credit history, recent claims and information about the property
including its age, size, and construction quality.
Lemonade has developed an AI app, Maya or Jim, to make the offer of an insurance policy.
Risk mitigation factors such as sensitivity of homes to windstorms, severe weather damage,
and fires are taken into account, and discounts made for protection equipment that may
have been installed, such as fire and burglar alarms.
It has hired a renown behavioural scientist, Dan Ariely, as its Chief Behavioural Officer.
Lemonade is a benefit corporation and a certified B-Corp, the only in the insurance industry,
which is certified by the non-profit B Lab and must meet high standards of social and
environmental performance, accountability and transparency. A benefit corporation is a forprofit corporation with a mission to achieve positive impact on society, workers, the
community and the environment in addition to profit as its legally defined goals.
Lemonade’s reinsurance partners are Everest Re, Hiscox-Lloyd’s of London, XL CatlinLloyd’s of London and Berkshire Hathaway’s National Indemnity.

3. INSURANCE INTERMEDIATION AND DISTRIBUTION MODELS

Finally, there is the self-governing model that often uses blockchains to auto execute the
contract. There are potential benefits that could be reaped for risk transfer tools, such as
cat bonds, which will be another area that blockchains are likely to further explore.
Blockchain is based on distributed computing, which results in a decentralised network. It
is by design meant to avoid centralised control and is characterised by free participation.
One of the advantages of blockchains in terms of financial transactions would be the
improved cyber security due to it decentralised nature. Another is the transparency of
transactions, which are all recorded in the node of the blockchain. Linked with this is that
when a smart contract is part of the blockchain, there will be no need to authenticate the
transaction, as it is effectively announced through its transparency and it is irreversible,
which is another feature of the blockchain.
Blockchain technology could be applied to insurance services in a number of ways.
InsurETH presents a case study (see Box 7), but if a blockchain can use external, thirdparty data sources, claims management could be automated potentially reducing the
transaction cost. Para-metric insurance could benefit from such a process, especially for
agriculture or disaster-related insurance for retail policyholders. Fraud detection could
also be improved if blockchains were able to access data on purchase records, police
reports, ownership etc.
The blockchain by nature does not permit amendments to transactions after the fact. This
means that while for standard policies the technology could be a useful tool, for complex
policies it may have limitations in its application. The legality of a blockchain-based
contract is unclear, and thus its enforceability could be compromised as a result. As the
policy would be written in the code of the blockchain, for regulatory and legal purposes
an administrative step could become necessary for it to be transformed into a legal
document, until the law recognises a blockchain as a legal document.

Box 7. The use of blockchains in insurance
Blockchains have the potential to change how transactions are processed, and this wave is
coming to the insurance sector as well. Allianz Risk Transfer and ILS fund manager
Nephilia Capital are piloting the use of blockchain smart contract technology for processing
a natural catastrophe swap. The technology would process the transaction and settlement
between insurers and investors.
The pilot demonstrates the technology has the possibility to simplify and accelerate contract
management. Each validated contract on the open shared infrastructure contains data and
self-executable codes inherent to that contract. When a triggering event occurs, meeting
the agreed conditions, the blockchain smart contract picks up the predefined data sources
of all participants, and then automatically activates and determines payout to or from
contract parties.
Another similar initiative taking place between insurers and reinsurers is to explore the
potential of distributed ledger to streamline paper work and reconciliations for (re-)
insurance contracts and accelerate information and money flows, while greatly improving
auditability. The B3i initiative is a cooperation between Aegon, Allianz, Munich Re, Swiss
Re and Zurich, which will pilot the feasibility of using anonymised transaction information
and anonymised quantitative data, in order to achieve a proof-of-concept for inter-group
retrocessions by the use of the blockchain technology.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 19

20 │

3. INSURANCE INTERMEDIATION AND DISTRIBUTION MODELS

Box 7. The use of blockchains in insurance (cont.)
"Cat bond" payments between insurers and investors can take weeks or even months after
the triggering event due to manual processing and authentication through intermediaries is
not required. As blockchains cannot be altered, their characteristic assists in ensuring that
ownership cannot be duplicated or forged.
A study forecasts that for the reinsurance industry, more efficient data processing and
reductions in claims leakage and fraud through blockchain solutions could remove 15% to
25% of reinsurance expense ratios which are typically 5%-10% of premiums (PwC, 2016).
One of the promising example of insurance using blockchain is the start-up InsurETH.
InsureETH uses one of the blockchain platforms, Ethereum. Ethereum is one of the most
popular blockchain platforms which is public and has a smart contract functionality.
InsurETH offers automated flight insurance which relies on Ethereum smart contracts, and
recording premium payment in the Ethereum blockchain. Travel insurance policies often
cover delay of flights or lost baggage, but policyholders are not often aware of this coverage
and often only make claims for higher expense claims such as delays due to a medical
emergency or to access medical benefits. This is in contrast to the flight delays and lost
baggage being a much more frequent incident and one which can be tracked using third
party data sources. InsurETH uses this advantage by selling flight insurance, and
automatically sources proof for claims using a public data feed Oraclise. This automates the
process, in that if a flight delay occurs claims are paid automatically based on the data feed
information.
The simplification of the contracting, which is done by inputting the flight number and
coverage amount, enables travellers to easily access the coverage. The payment is done
through the deposit of Ether, although denominated primarily in pound sterling, which is the
cryptocurrency of Ethereum. So the traveller would be required to create a Ether wallet
which is also a simplify process using applications.

4. THE SHARING ECONOMY AND INSURANCE

4. The sharing economy and insurance

The sharing or gig economy is becoming a larger part of the economy, as services such as
ridesharing (Uber, Lyft, BlaBla Car) and homesharing like AirBnB become common and
popular service platforms. As a commercial service, these services will be required to
have insurance coverage for certain aspects of their business.
As part of this, there is strong recognition that millennials2, which are one of the largest
age cohorts in the United States and are entering their highest consumption period, have a
preference for having digital solutions available for transactions (Goldman Sachs Global
Investment Research, 2016), and this is also prompting insurers to review how to
approach distribution and claim management. Millennials have a 10% less positive
customer experience of insurance transaction than other age cohorts, which is indicative
of the dissatisfaction felt by this generation to conventional insurance solutions
(Capgemini Consulting, 2015).
One of the key features of millennials and the sharing economy is that complete strangers
share their personal experience/review, car, house, quite freely, while confidence in
established business processes, such as insurance, is considered less positive. From the
distribution sites, robo-advice and data analytics discussion below, it could be that
insurers can expect greater willingness by policyholders to provide more personal data
and prefer computer generated advice. Insurer may have to adjust their business processes
in accordance with such consumer behaviour and take greater care of privacy.
While the provision of insurance coverage for ridesharing services is improving, the
nature of the service creates unique challenges to underwriting. Public transport and taxis
require insurance coverage as commercial service providers which are excluded from
standard auto insurance. Commercial coverage is based on the driver having certain
qualification and experience transporting the public, and the vehicle being maintained to a
certain standard on a periodic basis. Ridesharing typically uses drivers not authorised to
drive taxis and their personal vehicles, although in some cases they are licensed drivers
providing services in their spare time.3
Some insurers are addressing the unique nature of ridesharing. For example, Uber has
coverage by separating the coverage to the core policy of when a driver has picked up a
customer and dropped them off, lower coverage for when the driver is logged on to the
system and waiting for a pick up and a separate coverage for physical damage to the
driver’s vehicle while it is being used for the rideshare services. There is still a potential
gap of when a commercial coverage is in effect, and when the driver’s personal auto
insurance will be expected to cover any unplanned incidents.
2

Millennials are generally referred to as those born between 1980 and 2000.

3

This has resulted in the service being banned in a number cities as a result of opposition from
taxi unions.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 21

22 │

4. THE SHARING ECONOMY AND INSURANCE

Peer-to-peer homesharing, such as AirBnB, would likely require additional coverage as a
homeowner policy would not cover liabilities caused by a renter. When renting out a
home on a single occasion, it is likely that homeowner or renter’s insurance will cover
such an occasion, although it may require notification to the insurer in advance. However,
for repeated homesharing/renting, an add-on to the policy or commercial insurance may
be required, in particular to cover liability from guest damage. Monthly coverage is
becoming available for such additional coverage by a number of insurers.
Beginning in January 2015, Airbnb began including no-extra-cost USD1 million “Host
Protection Insurance” for hosts – and in some cases their landlords – designed to cover
the liability associated with a peer-to-peer rental. Intentional acts that aren’t the result of
an accident are not covered by the insurance, as well as what the website terms “property
issues,” such as mold, bedbugs and asbestos.

5. ROBO-ADVICE AND AI

5. Robo-advice and AI

While price comparison and distribution sites are becoming wide spread, much effort is
being made to develop sites that provide financial guidance which is tailored to the
policyholder’s income and needs with greater automation through algorithms for products
with investment and/or saving components. This could assist in narrowing the protection
gap of the lower income population as the cost of such services is lowered.
Robo-advice capabilities can be largely categorised into (Accenture, 2015):


Understanding client needs: gathering client information, understanding needs
and preference, assessing risk tolerance, considering outside accounts;



Proposing a policy: developing a financial plan, selecting asset allocation;



Implementing the policy: opening accounts, transferring assets; and



Monitoring and adjusting the policy: quarterly or annual performance reviews,
dashboards and status alerts, market updates and research.

In comparison to robo-advice, human interaction has benefits in that long-term relations
can nurture trust and understanding between a policyholder and financial
advisor/broker/agent, in particular in times of financial difficulty. Financial advisors may
be better at persuading policyholders to take certain actions. In addition, robo-advice has
not been challenged in poor market conditions where assets lose value. How robo-advice
might cope in such situations is unclear.
On the other hand, robo-advice has the ability of developing a financial plan addressing
multiple goals, including retirement, protection needs, estate planning and health/longterm care coverage. Robo-advice has the privacy which some may feel more comfortable
with given the sensitivity in discussing money matters.
What would be important for many policyholders is that the fee would be lower than
financial advisors. In the investment advisory sector in the US, for example, financial
advisers generally charge 1% of the assets under management as fees, this is opposed to
the between 15 to 35 b.p. of assets under management charged by investment roboadvisors (Investor Junkie, 2016).4 In comparison, in the United Kingdom for example,
Santander’s branch-based investment advice fees are 2.5% of assets invested, with a
minimum investment of £500 and a maximum of £150 000.

4

For example, Charles Schwab’s robo-advisor does not charge a fee.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 23

24 │

5. ROBO-ADVICE AND AI

Box 8. Robo-advice for insurance
Robo-advice is primarily being applied for investment advisory, and thus far its use in the
insurance sector is yet to be wide spread. However, some insurers are starting to provide
robo-advice on insurance in conjunction with other financial services they provide.
LV=, the UK-based mutual financial services company that provides insurance as well as
saving and investment products acquired a stake in Wealth Wizards in August 2015, to
provide advice on retirement saving products. The platform is now being offered as
Retirement Wizard in the LV= site, and is available to provide advice to those within three
months of wanting to access their pension and have a pot size between £13 500 and
£150 000. The advice would include annuities or income drawdown.
The Royal Bank of Scotland announced it was introducing robo advisory services for its
clients, with face-to-face financial advice becoming available only for customers with
£250 000 or more, but enabling clients with as little as £500 to access advice in March
2016. This has resulted in the cut of a large number of insurance product financial
advisers, as the RBS would be providing financial planning including insurance.
In both cases, robo-advice takes advantage of the service provider having alternative
investment products available not only in insurance. It is to be seen how the insurance
market will further incorporate robo-advice into its business process.

Box 9. PolicyGenius
PolicyGenius was founded 2014 to provide users with price comparison information on
life insurance, long-term disability insurance, renters insurance and pet insurance. As
opposed to most insurance comparison sites, it is not based on a lead generator model,
which interprets an inquiry as a request for a quote and sells the client inquiry to
insurance brokers/agents who would then try and sell the policy. Also the user experience
is considered a key factor of the business and contents is developed for an improved user
experience and provides advice on the offers being made. They are not affiliated with any
particular insurance company and their algorithms work to match the user with the best
policy to fit their needs. It is licensed as an independent broker in New York state.
Life insurance is the company’s most popular product, followed by disability insurance.
Through its “insurance checkup tool” it analyses and generates the advice that is suited
for the user.
It raised USD750 thousand in seed funding, and has raised USD5 million in a series A
and $15 million in a series B round of funding. Revolution Ventures led the Series B
round, with previous investors including Karlin Ventures, Susa Ventures, Axa Strategic
Ventures, Transamerica Ventures and MassMutual Ventures. PolicyGenius reached
800,000 users by the end of 2015 although whether the user inquiries led to actual
policies is undisclosed.

AI is being used in a number of sites such as through the algorithm used by Lemonade for
its policy offering and PolicyGenius (see Box 9). AI has the potential to simplify and
tailor policy offerings to match the needs and financial situation of the policyholder. This

5. ROBO-ADVICE AND AI

is different from robo-advice, where AI is specifically designed for personal advice,
primarily on investments.
On the other hand, the algorithms are a blackbox, which in some instances could be
leading to poor advice. A study indicates that for the majority of age groups, a
combination of robo-advice and personal advice was deemed to be optimal (E*Trade
Financial, 2016), which has generally been the way in which most insurer AI would be
developing their robo-advice (Acord & Surely, 2016).
The underlying algorithm of robo-advice and AI are not transparent in most cases, and
biases could be built in, both unintentionally and intentionally, leading to inappropriate
advice. The understanding of how this impacts policyholder behaviour and how
regulation should address this is unclear but an area that requires greater discussion (see
Box 14).

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 25

26 │

6. DATA AGGREGATION AND ANALYTICS

6. Data aggregation and analytics

Internet, the Internet of Things (IoT), hand held devices, and applications are all
contributing to the possibilities that technology can have in collecting more data from
businesses and individuals. Social media as well as devices such as Fitbit and Apple
watch permit device operators to collect individual activity data as well as health related
data. While insurance has traditionally relied on quantitative data to make risk
management decisions, data analytics goes beyond this remit and can be contentious in
some occasions. Underwriting and claim management are particularly data rich, and
insurers use data collected for fraud prevention, marketing, claims management and
pricing risk.
For example, personal auto insurance in the past relied on internal data sources such as
loss history. However, auto insurers have started to incorporate behaviour-based credit
scores from credit bureaus into their analysis, based on empirical evidence that people
who pay their bills on time are also safer drivers. There is an issue of risk awareness, as a
US Government Accountability Office report in 2005 reported that 53% of respondents to
a survey in the United States did not know this when they could request credit scores to
be excluded for premium considerations in times of hardship (GAO, 2005).
Some insurers could/are engaging such data by having an arrangement with the data
collector or purchasing the data from a data aggregator. As insurance depends on making
an actuarial assessment of the risk, having more relevant data would assist the analysis.
The release of previously unavailable or inaccessible public-sector data has greatly
expanded potential sources of third-party data. The US and UK governments and the
European Union have recently launched “open data” websites to make available massive
amounts of government statistics, including health, education, worker-safety, and energy
data, among others.
One example of such endeavour is the marriage of longevity data, face recognition
technology with underwriting for the provision of life insurance. Face recognition
technology is used to predict factors such as chronological age, gender, smoking habits
and body mass index (BMI). Based on this data, and accompanied by an activity sensor,
such a FitBit or physical activity tracker on a mobile phone, your expected life
expectancy is provided. A term life offer is made based on this, and the term period can
be selected by the policyholder.
Telematics and insurance is another avenue in which data analytics is being used to
monitor the behaviour of policyholders and mitigate risks in advance as well as
discounting premiums where applicable. Motor insurance related data has been
abundantly accumulated in insurance companies as it is one of the largest lines in most
countries. Telematics insurance is when a device is fitted into motor vehicles and used to
track driving. For example, the Italian Insurance Association estimates that blackboxes
have been installed in over 2 million cars in Italy, to support the provision of blackbox
insurance, “telematics car insurance” or Usage Based Insurance (UBI), and is one of the
large markets for telematics car insurance. Blackboxes devices track speed, braking,

6. DATA AGGREGATION AND ANALYTICS

acceleration, cornering and the time of the day a journey is made via satellite technology.
The data is transmitted to the insurer by GPS which enables the insurer to estimate the
likelihood of a claim being made. Such programmes benefit young drivers that do not
have a track record to influence their premiums, for example. While there is no research
that clearly indicates the link between telematics and accident rates (UK Transport
Research Laboratory, 2015), anecdotal evidence suggests telematics solutions can reduce
collisions by up to 20%, operating costs by up to 10%, and fuel consumption of between
8% and 11% (Zurich Fleet Intelligence, 2016). It is estimated that the number of
consumer subscribers to telematics insurance is expected to grow to 142 million globally
by 2023 (IHS Markit, 2016).
On a risk management level, there are a number of data analytic solutions that could
assist insurers. These include integrated geospatial analytic tools, geo-spatial analysis,
and data quality management tools and claims/exposure matching. In particular, claims
processes could benefit from the use of pictures taken and filed vie smartphones and
concierge services to smooth the process.
If data aggregation is being used for actuarial purposes, it could lead to potentially too
high premiums or uninsurability of certain segments of the society or individuals, or
ethically questionable outcomes. If premium are risk-based, granularity of the data could
have both a positive or negative impact. The negative impact would be when potential
policyholders are not able to purchase insurance at a reasonable premium level when it is
a risk-based premium (Keller & Hotte, 2015).
The Internet of Things (IoT) is when sensors and actuators embedded in physical
objects—from roadways to pacemakers—are linked through wired and wireless
networks, often using the same Internet Protocol (IP) that connects the Internet. The
connection permits large volumes of data to flow to computers for analysis (McKinsey,
2010). Telematics insurance is the best known example of insurance using the IoT. Other
examples of IoT devices being used for insurance are sensors in private homes, farms or
businesses to alert policyholders about risks such as bad weather conditions and security
surveillance, or to provide feedback about individual risks. Biometric data such as
electrocardiogram (EKG) and arrhythmia detection, pulse and variability, blood pressure,
respiration information, blood sugar level, muscle activity, sleep patterns, body
temperature, blood oxygen levels, skin conductance levels, brain activity, hydration
levels, posture, eye tracking data, ingestion and fertility information can also be generated
and applied in data analysis for insurance purposes.
Having extremely granular data may have a number of unintended consequences. The
most immediate would be the privacy of those who provide the data. While the data
protection of data relevant to the contracting of an insurance policy is clear, the treatment
of data collected additional or outside of this may not be. Tracking of data, whether by a
blackbox device or an activity sensor, provides much data beyond what the insurer may
require to determine the behaviour of the policyholder or the premium reductions.
Insurers would not only have data on the driver’s behaviour, but where they travel to and
visit, and the frequency of this. While activity sensors permit a better understanding of a
policyholder’s lifestyle, genetics also account for a large part of a policyholder’s health
and life expectancy. It would become important that a distinction is drawn by insurers for
when a poor lifestyle caused ill health, for example, and when a person is born with poor
health which have no way of being addressed by lifestyle choices.
The ownership of data generated through the IoT, as with many digital devices, is still
being discussed, and currently general privacy and data protection regulations would
TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 27

28 │

6. DATA AGGREGATION AND ANALYTICS

apply. The US Federal Trade Commission issued a report in 2013 (FTC, 2015),
examining some of the issues on the IoT and privacy. The manner in which IoT collects
data makes it difficult to gain consent every time data is collected, and is not necessary
either. However, it is important that a choice can be made by the individual before data is
collected, although not in instances when the context of collecting data is consistent with
the transaction the individual is entering with the company (FTC, 2012). Where the use of
data would be inconsistent with the context of the interaction, a clear and conspicuous
choice should be offered.
Box 10. Estonian Insurance Association’s motor insurance database
Estonia’s insurers developed a centralised online IT system for motor third-party
liability (MTPL) insurance in 1998, which eventually transitioned in 2001 to all policies
having to be concluded online and physical policies being prohibited. The database
has information on all issued MTPL policies and all MTPL claims handled. This
development was supported by the Law on MTPL which requires information to be
provided to the MTPL database prior to the start of the contract, as well the claims
information without delay (a 64 euro fine is applied for every erroneous entry). It thus
has a statutory status, and the database is legally recognized with third parties.
The MTPL database is cross-referenced with other state registrars which ensures
quality control of the database. As the database is open, the public can refer to it and
monitor the information that has been entered.
The MTPL database also has a claim mapping function, which enables the
aggregation of where incidents which has resulted in a claim have occurred. This
information has been used by road planners to fix cross-sections that have resulted in
many incidents.

Another issue is how cross-border data transfers should be treated. Data can be ubiquitous if
structured, and can be used to analyse behaviour in other countries. This is discussed in detail in
section 7.

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

7. The role of policy and regulation in InsurTech

Insurance regulation and innovation hubs
While innovations are generally a positive development, there are a number of potential
policy and regulatory ramifications which can create some uncertainty and certain
limitations in business developments. In terms of competition policy, the potential to have
new entrants to the market through the application of innovations and new technologies
could bring greater consumer utility. The rationale for competition law or policy is to
improve the consumers’ welfare and the efficiency in production and supply, which
would lead to lower prices and wider choice. The possibility of new entrants in the form
of start-ups and greater choice as a result of innovation and technology could bring a
number of positive developments to competition in the insurance market.
When start-ups want to become an insurer or an insurance agent/broker, there are
potentially prohibitive capital and/or fit and proper requirements that must be met to gain
authorisation to operate. Perhaps for this reason, there are very few InsurTech start-ups
that have gained insurance underwriting licenses, and most have broker licenses. While
for prudential purposes these requirements are an important cornerstone to ensure
policyholder protection, these could potentially be a barrier to new market entry, where
applicable. There is constant tension in the financial sector on the appropriate balance
between financial regulation and competition, and this is very much relevant in the
context of innovative technology.
To address this, some financial regulators have established platforms to enable FinTech
start-ups experiment with their technology and relaxing some of the regulatory
requirements within the platform. The UK Financial Conduct Authority (FCA)’s
Innovation Hub is one of the first applying the “regulatory sandbox” approach.
Singapore’s Monetary Authority of Singapore (MAS) has also adopted the regulatory
sandbox approach. Australia’s Securities and Investment Commission (ASIC) has
established an Innovation Hub to mitigate risks by engaging early with FinTech
innovators and helping new entrants understand the regulatory requirements. The Hong
Kong Monetary Authority and Canada’s Ontario Securities Commission have also
launched similar platforms in recent months. These platforms are all designed to assist
new market entries that would encourage greater competition and innovation in the
market, ultimately benefiting consumers.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 29

30 │

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

Box 11. Monetary Authority of Singapore's approach
The Monetary Authority of Singapore (MAS) has adopted the regulatory “sandbox”
approach to FinTech, which is applicable to the Singapore financial sector. The
approach that MAS has taken is to acknowledge that new technologies can increase
efficiency, manage risks better, create new opportunities and improve people’s lives.
A regulatory sandbox approach is being proposed to carve out a safe and conducive
space for Financial Institutions and FinTech players to experiment with FinTech
solutions, while containing any consequences of failure. However, Financial Institutions
are free to launch new solutions without MAS’ guidance if they are satisfied with their
own due diligence and there is no breach of legal and regulatory requirements.
MAS carried out a public consultation in June/July 2016 and issued the finalised FinTech
Regulatory Sandbox Guidelines in November 2016. The paragraphs below summarise
some of the key points of their approach.
The regulatory sandbox approach would involve MAS’ support by relaxing specific legal
and regulatory requirements prescribed by MAS for the duration of the sandbox. It would
generally not be available to FinTech solutions that are:


Similar to those already being offered in Singapore unless the applicant can
show that either a different technology is being applied or the same technology
is being applied differently, and



When the applicant has not demonstrated that it has done its due diligence,
including testing the proposed financial service in a laboratory environment and
knowing the legal and regulatory requirements for deploying the proposed
financial service.

There should also be an intention that the FinTech solution would eventually be
deployed in Singapore after exiting from the sandbox.
MAS has identified requirements that should continue to be applied even to sandbox
applicants, and those that could be relaxed. Requirements that will not be relaxed are
related to customer information confidentiality, fit and proper criteria particularly on
honesty and integrity, handling of customer’s moneys and assets by intermediaries, and
prevention of money laundering and countering terrorism financing.
Source: MAS, FinTech Regulatory Sandbox Guidelines (November 2016).

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

Box 12. The UK Financial Conduct Authority’s Project Innovate
The UK Financial Conduct Authority (FCA) launched Project Innovate in October 2014 to
encourage innovation that would benefit customers, and promote competition through
disruptive innovation. Project Innovate is led by the Innovation Hub which assists
innovative business gain access to fast, frank feedback on the regulatory implications of
their solutions, and identifies areas where the regulatory framework needs to adapt to
enable further innovation in the interests of consumers.
Support through the Innovation Hub is based on an eligibility criteria that includes
whether it is a genuine innovation, what would be the consumer benefit, has the
business invested appropriately to understand the relevant regulations, and does the
business have a genuine need for support from the Innovation Hub. The Innovation Hub
has had over 600 requests for support and offered direct support to over 300 firms.
The FCA has also developed a Regulatory Sandbox. The objective of the regulatory
sandbox is to create safe spaces in which businesses, both authorised and
unauthorised, small and large, can experiment with innovative products, services,
business models and delivery mechanisms without immediately incurring the normal
regulatory consequences of engaging in the activity in question. Unauthorised firms are
subject to a tailored authorisation process and must meet threshold requirements, but it
grants them restricted authorisation to test their ideas. The restricted authorisation
option is not available for a banking license. For authorised firms, the sandbox could
provide clarity to applicable rules that do not easily fit into existing guidelines. Consumer
benefits would be a prerequisite for applications to the sandbox.
The FCA has stated that it has accepted four applications from the insurance sector to
develop towards testing.

The regulatory sandbox approach intentionally creates a space for insurance technology
to be experimented in a different regulatory regime from the regular. Although it is early
stages of the approaches, it would be worthwhile to understand when technologies are
deemed successful and scalable, how they will be graduated into the regular regulatory
framework. Going forward, this will be important in ensuring that a level playing field is
applied at the appropriate stage.
A relevant development that is taking place between MAS, FCA and the Australian
Securities and Investment Commission are bilateral cooperation agreements between the
authorities that allow them to make referrals on innovative businesses seeking to enter
each other’s market. This would assist in enabling start-ups transfer their business models
on a cross-border basis, assisting with the businesses to scale when the opportunity arises.
Another relevant consideration, for developing countries in particular, is whether there is
merit in having a specific regulatory framework to allow new insurance products that
target specific limited risks, that are low in value and may benefit from greater
penetration of insurance policies while having a limited policyholder impact.5 To date, a
5

India’s Insurance Regulatory and Development Authority adopted a regulation on
microinsurance regulation in 2015.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 31

32 │

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

number of countries (Brazil, India, Mexico, Pakistan, Peru, the Philippines, South Africa,
and Taiwan and other African countries) have specific microinsurance regulation (Biener,
Eling & Schmit, 2013). Microinsurance can also be beneficial in OECD countries, as the
example of the start-up Trov demonstrates. Trov is on-demand insurance for your
possessions, which can be switched on and off through a mobile device. The mobile app
enables the value of the inventory of possessions to be tracked in real time and insurance
premiums as well. With the lower transaction costs that mobile technology can bring,
microinsurance may find a way to be more readily provided in developed insurance
markets as well.
In the age of new technology, insurance regulations which will likely be affected are
governance and market conduct related rules. The OECD Guidelines on Insurer
Governance recommends that board members and key executives should establish
internal controls that ensure compliance with applicable laws, regulation and standards, as
well as an incentive structure that promotes the fair conduct towards consumers and
policyholders. Controls functions are expected to assess the appropriateness of policies,
processes and procedures, and identify and follow up on any deficiencies.
The IAIS’ Insurance Core Principles (ICP) 19 states that requirements for conduct of
business be made to ensure the fair treatment of customers, whether before, during or
after the contract has been entered into. This should be based on insurers’ ethical
behaviour, acting in good faith and the prohibition of abusive practices. While new
technologies and innovations may enable insurers to provide products that are appropriate
to the needs of a customer, as stated in ICP 19.6, how to ensure the process is fair is
uncertain.
If an insurer does adopt new technologies or innovates processes/products, it should
consider whether the appropriate internal control considerations have been made, as well
as being appropriate in terms of market conduct.
A number of countries are engaged in a wider discourse on, for example, autonomous
cars, which will have on impact on auto insurance coverage. The recent fatality resulting
from a self-driving car in the United States (see Box 13) has brought to the attention the
reality of autonomous cars and how to ensure their safety. Together with this, how the
liability of such a car in an accident has yet to be fully resolved. The United Kingdom and
the United States have carried out consultations that touch upon this issue, and how this
proceeds will likely impact how other markets respond as well.

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

Box 13. Autonomous cars and insurance
The advent of mass autonomous or driverless cars is expected to be not in the far
future, and a number of countries (France, Singapore, Switzerland, United Kingdom
and United States) have started to permit the use of certain roads for driverless cars. In
1
the US, several states have enacted laws on the use of autonomous cars. Driverless
cars do have the potential to improve road safety by preventing human error, and
efficient traffic flows and fuel usage. As trials take place, the appropriate framework for
insurance coverage for such for vehicles will also become an important aspect for
governments to consider.
The United Kingdom carried out a consultation on product liability insurance that would
1
be required for autonomous driving or driverless cars in July 2016. The expectation is
for driverless cars that can be parked by remote control within the line of sight of the
driver, and/or cars that can be auto-piloted with human oversight at high speed will be
available for sale in the coming few years.
The US Department of Transportation and National Highway Traffic Safety
Administration have issued a paper that proposes updates to its Federal Automated
2
Vehicles Policy in September 2016. In the US, states have responsibility for mote
vehicle insurance and liability regimes. As part of this, states are asked to consider how
liability should be allocated among highly automated vehicle (HAV) owners, operators,
passengers, manufacturers, and others when a crash occurs.
Insurance coverage of a motor vehicle is for damages and third-party liability (TPL). The
difficulty for driverless cars is with who the liability is placed with: the driver or car
manufacturer. The manufacturer would be involved through product failure (liability) that
resulted in a collision while the driver would be liable when s/he did not take control of
the vehicle in certain circumstances resulting in a collision. There is much uncertainty
as to how the liability of a collision would be addressed when there is a mixture of
human input and autonomy involved in driving.
In May 2016, a Tesla self-driving car was involved in a collision when driving on
autopilot mode on the motor way which was fatal to the driver. TPL insurance would not
cover the driver unless supplemental insurance was acquired. Some motor
manufacturers are offering self-insurance to their automated vehicles.
The UK government inquires on whether there is a need for supplemental insurance
coverage such as product liability, and drivers and passengers. The industry has
responded that insurers could provide cover for all liabilities, and then take over
possible liability claims to the manufacturer for any potential product liability.
The Bank of England recently published projections that with the development and uptake
3
of autonomous cars, the UK motor insurance market may contract by 21% by 2040.
1.

UK Department for Transport and the Centre for Connected and Autonomous
Vehicles, Pathway to Driverless Cars: Proposals to support advanced driverless
assistance systems and automated vehicle technologies (July 2016).

2.

US Department of Transportation and National Highway Traffic Safety
Administration, Federal Automated Vehicles Policy: Accelerating the Next
Revolution in Roadway Safety (Sept. 2016).

3.

Bank of England, Quarterly Bulletin 2017 Q1.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 33

34 │

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

AI and the regulation of technology-based advice
As AI and robo-advice become more widely used, there could be uncertainty as to how
current regulation applies. For example, in New Zealand, current regulation requires that
advice be provided by a ‘natural person’. Planned changes in New Zealand seek to
broaden the definition of advice in order to accommodate technological innovations, and
require that entities providing robo-advice be licensed and held to the same requirements
as other types of advisors (New Zealand Ministry of Business, Innovation and
Employment, 2016). Fully automated services are not allowed to provide advice in
Canada, and any robo-advice service must provide some access to personalised advice
from an advisor (Lortie, 2016).
Regulators in several jurisdictions have been assessing how technology-based advice
should be regulated going forward. The Australian Securities & Investment Commission
(ASIC) issued a regulatory guide on robo-advice to retail clients in August 2016 (ASIC,
2016). The guide maintains that the qualification requirements for providers of roboadvice be the same as those for normal advisors, and lays out the requirements for testing
the algorithms used and the governance controls and processes in place.
In April 2016, the US Securities Exchange Commission (SEC) approved a rule proposed
by the Financial Industry Regulatory Authority (FINRA) that requires developers of
algorithmic trading to be registered as a securities trader, and be subject to the same
qualification requirements as securities traders to reduce market manipulation (SEC,
2016). The European Supervisory Authorities (European Banking Authority, European
Securities and Markets Authority and European Insurance and Occupational Pensions
Authority) have issued a joint discussion paper on the automation of financial advice
looking at the potential benefits and risks of such innovations in order to determine any
additional regulatory action needed to address automated financial advice (Joint
Committee of the European Supervisory Authorities, 2015).
The regulations and consultations taking place indicate the need for consistency with the
regulation of human financial advice and proper risk and governance controls of the roboadvice being provided. The type of advice being provided by the platform should clearly
indicate whether the advice being generated is general or has been personalised. If the
advice is determined to be personalised advice, clear processes would need to be in place
with respect to how suitability for the client is determined. The algorithms used for
automation should be extensively tested, and controls in place to ensure that procedures
are in place to ensure their proper functioning.
There is also the issue of whether algorithms may have biases that, whether intentional or
unintentional, may be leading to inappropriate advice. This could impact policyholders on
a wider base than advisors, as the bias would be built in and anyone who uses the
algorithm will be subject to it. Another issue that has been highlighted is that robo-advice
and risk management algorithms could lead to herding, increasing pro-cyclicality
(Carney, 2017).

Privacy and data protection issues
Technology that engages big data is complex, opaque and often uninterpretable. For this
reason, even those who develop the technology for usage of big data may not fully
comprehend the impact or appropriate usage of data. Firms should be able to demonstrate
that their use of data is appropriate and free of biases in so far as possible (see Box 14).

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

For big data and data analytics by insurers, privacy and data protection regulations should
be rigorously addressed, and ethically uncertain use of data should be fully assessed. In
this respect, the wider data protection regime will have a large impact on how this is
addressed. In addition, when notification requirements for data breaches are introduced,
insurers will need to ensure that databases have the capacity to support this requirement.
On the other hand, notification requirements are also likely to assist the development of
standalone cyber insurance markets.
Under the current EU regime, for example, cross-border data transfers are not permitted
unless made to an Adequate Jurisdiction or the data exporter has implemented a lawful
data transfer mechanism (according to EU Data Protection Directive (95/46/EC) and the
EU General Data Protection Regulation (GDPR) (see Box 15 below)). To be deemed an
Adequate Jurisdiction, the GDPR will be extending the requirements from the Directive
for the jurisdiction to have inter alia fundamental rule of law and legal protection of
human rights, access to transferred data by public authorities, and effective and
functioning data protection agencies (DPAs), international commitments and other
obligations in relation to the protection of personal data. For transfer of data within the
corporate group, GDPR requires corporate to have Binding Corporate Rules (BCRs) that
are legally binding and apply to and be enforced by every member of the group of
undertakings envisaged in joint economic activity, and have DPA approval of the BCR.
In the EU, outsourcing arrangements and distribution agreements must be agreed with
caution, in terms of who is controlling and processing data. Under the current EU
directive data protection regime, the processing of personal data cannot take place unless
there are legitimate grounds to do so, which under GDPR will require insurers (data
controllers) to carry out a “data protection impact assessment” before processing personal
data. Insurers are expected to implement sufficient consents and effective protocols for
collecting, handling and processing all data an insurer controls.
Further, under the GDPR, data controllers will be required to notify personal data
breaches to the competent supervisory authority, where feasible, no later than 72 hours
after becoming aware of the breach, unless the data controller is able to demonstrate that
the breach is unlikely to result in a risk to the rights and freedoms of the data subjects
concerned. Notifications must also be made to data subjects “without undue delay” if the
breach is likely to result in a high risk to their rights and freedoms. Businesses could be
fined up to €20 million or 4% of annual global turnover in the most recent financial year,
whichever is greater, for failure to comply with GDPR.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 35

36 │

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

Box 14. EU General Data Protection Regulation (GDPR)
The EU Parliament and the European Council agreed on the General Data Protection
Regulation (GDPR) in December 2015. It would be applicable to firms that process
personal data from those residing in the EU irrespective of whether their services are free
or fee-based, whether the firm is based in the EU or not. It is an update to the Data
Protection Directive which came into force in 1995. Under the GDPR, fines can be up to
€20 million or 4% of global annual turnover, whichever is the higher, if the action of the
firm leads to a loss of information or a data breach. It will take effect in member states
from 25 May 2018.
GDPR requires private information to be erased without undue delay when the data is no
longer required in relation to the purpose for which it was collected. The data used must
also restrict use of data when the data quality has been contested by the data subject.
The firm must maintain an accurate record of the data subject’s agreement for their data
to be used for primary and any secondary purposes, without which the firm may not have
the right or ability to use the data.
Depending on how and where insurers process their data, this could have implications on
how new technologies could be introduced. In addition, the market of cyber risk insurance
could expand depending on how GDPR is implemented. It could also impact data
analytics which may have relied on data collected for different purposes.

RegTech
RegTech is an emerging area in FinTech, that uses technologies to solve regulatory and
compliance requirements more effectively and efficiently (IIF, 2016). Given the various
regulatory reforms introduced after the financial crisis, RegTech has the potential to
ensure more effective compliance of complex regulations. Technologies that are deemed
to be applicable for RegTech include machine learning and artificial intelligence,
biometrics, the interpretation of unstructured data such as e-mails and Facebook posts,
and the use of application programming interfaces (APIs). Those tools can be brought to
bear on such areas as aggregating big data, modelling risk for stress-testing, monitoring of
capital-requirement compliance, updating compliance manuals, improving anti-money
laundering and know-your-customer (KYC) programs and preventing fraud and in-house
violations.
RegTech is an area where countries which have developed regulatory approaches to
FinTech have benefited more from start-ups, with 31% of RegTech start-ups
incorporating in the UK, as opposed to 20% in the United States (Mulder, 2016).
For insurance, for example, there are data analytics platforms that allow internal data of
financial institutions to be converted into regulatory reporting formats, and this could be
applied to the insurance sector. There are a number of know-your-customer (KYC)
platforms which may use external, open data to verify customer identity. As solvency
modernisation initiatives require asset managers of insurers to be able to report
investments on a look-through basis, RegTech solutions could provide a platform for
insurers to grasp their asset-under-management in a simple interface.

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

Box 15. RegTech and algorithms
While a number of solutions are being proposed for insurance in the area of RegTech, in
particular for fraud prevention and solvency compliance, an emerging area is how
insurance companies can ensure that their algorithms are compliant with market conduct
regulations. Some start-ups are working to address unintended consequences of
algorithms, to ensure that financial institutions, including insurance companies, can
integrate algorithms in their customer interface as well as enterprise risk management in
a manner that corresponds to the objective of efficiency and effective of business
processes while minimising potential risks of algorithms.
ORCAA is a NY-based technology start-up founded by a data scientist to carry out audits
of algorithms. The model being used is examined from four phases: data collection and
integrity, objective of algorithm, the basis in which the algorithm has been built, and
monitoring and updating of the algorithm. Algorithms have been known to use certain
proxies, such as post code, which could result in certain segments of the population being
unfairly treated depending on how the algorithm is modelled.
An area that financial regulation has been relatively exposed to algorithms is in highfrequency trading, where trading algorithms are used to execute high volume, high speed
automated trading in financial markets. Financial regulators such as the Federal Reserve
Board (2009) and the French Autorité des Marché Financiers (2009) had issued reports
on this issue; nevertheless, a mutual fund trade resulted in a mass withdrawal by high
frequency trading and the subsequent crash of the Dow Jones (“Flash Crash”) in May
2010. Germany adopted the High-Frequency Trading Act in 2013, which requires high
frequency trading firms, not previously supervised by BaFin, to be supervised by BaFin.
Firms are required to ensure that markets are not distorted or interrupted. The algorithmtagging rule mandates that exchanges have to implement rules requiring all exchange
members to flag all algorithmically generated orders with a unique key when sent to a
German exchange so as to allow the market surveillance system to allocate all orders to
the generating algorithm (Coombs, 2016). The EU Commission has issued a technical
standard to the Markets in Financial Instruments Directive (MiFID) II, which will be
implemented in 2018, on how to implement articles relevant to high frequency trading in
April 2016.
Monitoring of algorithms is complex, requiring special skills and expertise, and regulators
and supervisors are often not equipped to understand or assess algorithms and/or
whether big data is being appropriately used. In the insurance sector, the known uses of
algorithms are primarily related to the customer interface, although solvency initiatives are
likely pushing insurers to use algorithms for the measurement of solvency as well.
Regulators should consider how to approach the use of algorithms and big data by
insurers that would ensure that they are being appropriately developed and are avoiding,
in so far as possible, biases and unintended consequences. In particular, stress testing
might be carried out to determine how robo-advice would cope in certain extreme market
conditions.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 37

38 │

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

Wider policy considerations
Certain wider digital policies can assist insurance markets develop technological
solutions. For example, Estonia’s ID card and digital signature services have led to a
seamless incorporation of digital insurance solutions, as ID authentication can be easily
facilitated (Box 16).
Efforts to improve cyber security, and in particular standards, would assist in raising
awareness of risks associated with internet-based transactions as well as ensuring
sufficient development of security measures. For example, the UK developed the Cyber
Essential Scheme in 2014 to ensure that essential cyber controls were implemented by the
private sector. Such schemes would assist in individuals being aware of actions to take to
improve their cyber security when using internet-based insurance solutions and providers
of InsurTech to ensure that their network is implementing certain standards of security.
Financial institutions which directly access the payment system are routinely subject to
supervision/audit of their IT system, as part of operational risk management. 6 This can
include aspects of mobile banking and algorithmic trading as well.7 It may be worthwhile
to consider how insurance supervision and regulation can be carried out to appropriately
monitor what risks such innovations and technological advances can pose to insurers.
The IAIS’s Financial Crime Task Force has issued an Issues Paper on Cyber Risk to the
Insurance Sector (IAIS, 2016) which notes that the Insurance Core Principles do not
specifically address cyber risk and cyber resilience, although supervisors would have a
basis to address them. The regulatory sandbox approach may permit the introduction and
experimenting of the technology in a controlled environment, but ongoing monitoring
may be the next step that requires greater discussion.
How funding is available for start-up also appears to influence the number of FinTech
and InsurTech start-ups. Many countries have public sector funding schemes for equity
financing of enterprises, many established towards the development of venture capital.
Some target science and technology ventures (OECD, 2011).8

6

For example, the Federal Reserve Board has information technology examination guidance
(FFIEC Information Technology Examination Handbook – Information Security Booklet (SR-11614) and FFIEC Cybersecurity Assessment Tool for Chief Executive Officers and Boards of
Directors (SR 15-9).
The Basel Committee on Banking Supervision’s Core Principles of Effective Banking Supervision
principle 25 on operational risk has part of the essential criteria that supervisor determines that
banks have established appropriate information technology policies and processes to identify,
assess, monitor and manage technology risks.
7

BaFin has a supervisory process that clearly identifies the algorithmic trading as part of this.
BaFin (2013), IT Security: Expectations of banking supervision, www.bafin.de/SharedDocs/
Veroeffentlichungen/EN/Fachartikel/2013/fa_bj_2013_11_it_sicherheit_en.html.
8

As described in OECD (2012), governments have participated in funds of funds investing in local
high-technology companies (Mexico, Germany). In Germany, some Länder have set up equity
guarantee facilities for private investment in local SMEs. France, Finland, Germany and
Netherlands have been active in joining science and technology research with public funding.

7. POLICY AND REGULATION: ITS ROLE IN INSURTECH

Box 16. Estonia’s electronic ID card and digital signature services
As part of ‘’e-Estonia,” which is the term used to describe Estonia as one of the leading
countries in e-government solutions, the government developed the ID card services for
most of its residences which serves as an identity document and proof of ID for online
services. The ID card has a chip that not only holds information about the card's owner,
but also two certificates, one to authenticate identity and the second to render a digital
signature. In addition to the ID card, a mobile phone can be used to identify oneself for
online services, Mobiil-ID. A mobile phone can act as a card and a card reader at the
same time. For this purpose, Estonia adopted a legislation on digital signatures in 2000 to
enable a written signature be replaced by an electronic one.
Adapting the ID-card and digital signature services, an Estonian insurer and broker have
developed online services. Intermediation is carried out through a matching and quoting
system, and enables policies to become effective immediately as authentication of
identity and by linking this to car registration system assists to expedite the process.

“Gazelles”, young high-growth innovative firms, have been a focus of entrepreneurship policy and
have drawn policy makers' attention because of the number of jobs they are estimated to create.
Mexico, United States, Spain and the Netherlands have specific policies to support them.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 39

40 │

8. CONCLUSIONS

8. Conclusions

It appears that InsurTech businesses are developing business models that may, in fact,
better address the insurability of policyholders by using technology to simplify the
contracting process, and tailoring policies to better suit their needs. There is also scope
for insurance to adapt to wider changes in economic activity, such as the sharing
economy and the large millennial cohort. The other striking characteristic of many
InsurTechs are the social and environmental considerations that their business models
incorporate. Many of the InsurTechs try to improve the transparency of the contracting as
well as the claims management process, including fraud detection, providing greater
clarity to where the premiums paid go, which could have an impact on the wider
insurance industry.
There also seems to be a wider recognition that the fine print of an insurance quotation is
tedious to read, without giving much insight into the actual coverage of the policy for
retail clients. Sites are being developed that simplify the information on coverage of a
policy and try to clarify the level of premiums, while introducing peer pressure for risk
mitigation. There is often an algorithm to carry out the risk assessment using a few
questions which may also use external data sources to assist the assessment.
The scale of InsurTech investment is growing, and by (re)insurers in particular. As
InsurTechs start to attract a large number of users/policyholders, and provide an improved
customer experience, (re)insurers will likely hope to capitalise on the success of such
start-ups by having a stake in them. A number of (re)insurers have created strategic
venture capital arms for this purpose, and have been making strategic investments in a
number of start-ups.
Some countries are establishing regulatory platforms, such as the regulatory sandbox
approach, that allow innovative technologies to enter the market, and this will assist in
encouraging start-ups to develop their business model while becoming acclimatised with
regulatory requirements. Start-ups may opt to initiate their business in markets which
have such a ready platform.
More broadly, these technologies have the potential to bring better and more customised
insurance coverage to more people, including those in the lower income bracket, and
bring greater financial protection. In addition, the new distribution models can simplify
the insurance process, and bring insurance to less developed markets.
However, InsurTech will have to meet insurance regulations as well as wider data
protection and cyber security requirements as they try to scale their business. Ensuring
that not only is the customer experience positive when it is scaled up, but that consumer
protection and safety standards are met will remain a challenge for start-ups and
regulators alike.
The development of innovation hubs and regulatory sandbox approaches provides an
environment for new technologies and innovations to be nurtured, and has the potential to

8. CONCLUSIONS

enable a greater understanding of their impact on the markets. However, greater clarity on
the appropriate level of regulation in such platforms and how they graduate into full
regulation requires further discussion in order to balance the need for innovation as well
as the need for adequate protection of policyholders.
Market conduct and internal controls are the main area in which regulatory consideration
would apply for InsurTech, and while such rules are neutral to technologies, the practical
impact requires closer examination. In particular, internal controls that ensure compliance
with laws and regulation and fair conduct towards consumers and policyholders will be
important.
The impact of the use of big data and algorithms and how regulators could approach their
evaluation is unclear. The complexity involved has implications for how regulators
organise themselves as well as for how the spirit of regulation is applied. Firms should be
expected to demonstrate that their use of data is appropriate and free of bias in so far as
possible. RegTech may have a role to play in assisting that this is carried out going
forward.
Based on the analysis, the OECD can contribute to the efforts of governments to ensure
technology and innovation in the insurance sector, as well in the wider financial system,
could be address through additional research on some of the key areas raised above.
For example, the OECD could:


Analyse the trends of InsurTech in terms of technologies and innovations being
introduced and how it might impact the insurance sector in terms of business
models and processes, as well as regulation.



Consider the regulations relevant to technological and innovation in the insurance
sector, and discuss regulatory approaches which can be taken to facilitate this.



Discuss the development of best practices of regulatory sandbox approaches, in
particular for the insurance sector, but also including the wider financial sector in
cooperation with the OECD Committee on Financial Markets.



Carry out a stock-taking of how insurance solutions are being addressed by
regulation, for both start-ups as well as existing insurers. This could also be done
for the wider financial sector in cooperation with other OECD bodies and in
particular the OECD Committee on Financial Markets.

TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 41

42 │

REFERENCES

References

Accenture (2015), The Rise of Robo-Advice.
Acord & Surely (2016), AI—The Potential for Automated Advisory in the Insurance
Industry (February).
ASIC (2016), Regulatory guidance on providing digital financial product advice to retail
clients (August, Regulatory guide 255).
Autorité des Marchés Financiers (2009, “Key Issues Arising from the Emergence of Dark
Pools and Crossing Networks: Press Backgrounders”, 20 October.
BaFin (2013), IT Security: Expectations of banking supervision
www.bafin.de/SharedDocs/Veroeffentlichungen/EN/Fachartikel/2013/fa_bj_2013_
11_it_sicherheit_en.html.
Biener, C., Eling, M., & Schmit, J. (2013), Regulation in microinsurance markets:
principles, practice and directions for future development (Working papers on risk
management and insurance No. 127).
Capgemini Consulting (2015), Fixing the Insurance Industry.
Carney, M (2017) “The Promise of FinTech—Something New Under the Sun?” 25
January (Speech given to the Deutsche Bundesbank G20 conference on “Digitising
finance, financial inclusion and financial literacy”):
www.bankofengland.co.uk/publications/Documents/speeches/2017/speech956.pdf.
CB Insights (2017a), Insurance Tech Start-ups Raise $1.7B Across 173 Deals in 2016
(January) www.cbinsights.com/blog/2016-insurance-tech-funding/.
CB Insights (2017b), Where Insurers and Reinsurers Invested in Tech Start-ups in 2016
(January) www.cbinsights.com/blog/2016-insurance-cvc-total/.
CB Insights (2016a), Analyzing the Insurance tech Investment Landscape (July).
CB Insights (2016b), 6 Charts Breaking Down How Insurers Are Investing in Tech
(September) Start-upshttps://www.cbinsights.com/blog/insurance-corporateventuring-2016/.
CB Insights (2016c), InsurTech Connect 2016 (October)
https://www.cbinsights.com/reports/ITC-insurance-tech-deck.pdf.
CFTC & SEC. (2010), “Findings regarding the market events of May 6, 2010”
www.cftc.gov/ucm/groups/public/@otherif/documents/ifdocs/stafffindings050610.pdf.
Chaboud, A., B. Chiquoine, E. Hjalmarsson and C. Vega (2009) “Rise of the Machines:
Algorithmic Trading in the Foreign Exchange Market” (International Finance
Discussion Papers, No. 980, Board of Governors of the Federal Reserve System).

REFERENCES

Coombs, Nathan (2016) What is an algorithm? Financial regulation in the era of highfrequency trading, 45, Economy and Society.
E*Trade Financial (2016), E*TRADE Study Reveals Many Investors Prefer a Hybrid
Model When Considering RoboAdvisors (25 February)
https://about.etrade.com/releasedetail.cfm?ReleaseID=957103.
FTC (2015), Internet of Things: Privacy & Security in a Connected World (January, FTC
Staff Report).
FTC (2012), Protecting Consumer Privacy in an Era of Rapid Change: Recommendation
for Business and Policymakers (March).
GAO (2005), Credit Reporting Literacy (GA0-05-223).
Gartner (2016), Measuring the Strategic Value of the Internet of Things for Industries
(April 28).
Goldman Sachs Global Investment Research (2016), Millennials coming of age
www.goldmansachs.com/our-thinking/pages/millennials/index.html#intro.
HM Treasury and FCA (2016), Financial Advice Market Review: Final Report (March).
IHS Markit (2016), Usage-Based Insurance Expected to Grow to 142 Million Subscribers
Globally by 2023 (6 May) http://press.ihs.com/press-release/automotive/usagebased-insurance-expected-grow-142-million-subscribers-globally-2023-i.
IAIS (2016), Issues Paper on Cyber Risk to the Insurance Sector (August)
IIF (2016), Regtech in financial services: technology solutions for compliance and
reporting (March).
Investor Junkie (2016), The True Costs of Robo-Advisors – What Are the Annual Fees?
(9 October) https://investorjunkie.com/42668/true-costs-robo-advisors/.
Joint Committee of the European Supervisory Authorities (2015), Joint Committee
Discussion Paper on automation in financial advice.
Keller, B & Hott, C (2015), Big data, Insurance and the Expulsion from the Garden of
Eden (Geneva Association Insurance Economics Newsletter No.72).
KPMG & CB Insights (2016), Venture Pulse Q4 2015 (January).
Lortie, P. (2016), A Major Setback for Retirement Savings: Changing how financial
advisers are compensated could hurt less-than-wealthy investors most.
McKinsey (2015), Insurance on the threshold of digitization: Implications for the Life and
P&C workforce (December).
McKinsey (2010), The Internet of Things (March) www.mckinsey.com/industries/hightech/our-insights/the-internet-of-things.
Mulder, JM (2016), 100 RegTech start-ups to follow (17 June)
www.linkedin.com/pulse/100-regtech-start-ups-follow-jan-maarten-mulder.
New Zealand Ministry of Business, Innovation and Employment (2016), Final Report:
Review of the Financial Advisers Act 2008 and the Financial Service Providers
(Registration and Dispute Resolution) Act 2008.
OECD (2012), OECD Science, Technology and Industry Outlook 2012, OECD
Publishing, Paris. http://dx.doi.org/10.1787/sti_outlook-2012-en.
TECHNOLOGY AND INNOVATION IN THE INSURANCE SECTOR

│ 43

44 │

REFERENCES

OECD (2011), Science, Technology and Industry Outlook Policy Database.
http://qdd.oecd.org/DATA/STIOb_COUNTRY_ITEM_TOPIC_POLICY_SOURC
E/C6-1...STIO_2012?Page=1.
PwC (2016), Blockchain: the 5 billion dollar opportunity for reinsurers.
SEC (2016), Approving a Proposed Rule Change to Require Registration as Securities
Traders of Associated Persons Primarily Responsible for the Design, Development,
Significant Modification of Algorithmic Trading Strategies or Responsible for the
Day-to-Day Supervision of Such Activities (7 April, Release No. 34-77551; File
No. SR-FINRA-2016-007).
UK Transport Research Laboratory (2015), Provision of telematics research (2015).
Zurich Fleet Intelligence (2016), Reduced fleet operating costs
www.zurichfleetintelligence.com/us-en/benefits.php.

Technology and innovation in the
insurance sector
“Insurtech” is the term being used to describe the new
technologies with the potential to bring innovation to
the insurance sector and impact the regulatory
practices of insurance markets. This report
catalogues these technologies and examines how
InsurTech is being funded and how insurers are
engaging with the start-ups entering the market.

This report contributes to the OECD Going Digital project which
provides policy makers with tools to help economies and societies
prosper in an increasingly digital and data-driven world. For more
information, visit www.oecd.org/going-digital.

www.oecd.org/daf/fin/insurance

In collaboration with
Mitsubishi Chemical Holdings Corporation

Chatbots RESET
A Framework for
Governing Responsible
Use of Conversational AI
in Healthcare
DECEMBER 2020

Cover: Sean Anthony Eddy/Getty Images
Inside: Cyano66/Getty Images, Marco VDM/Getty Images, Filadendron/Getty Images,
Portra/Getty Images

Contents
3

Forewords

5

Executive Summary

6

1 Chatbots in Healthcare

7

2 The Chatbots RESET Project

8

3 Applications of Chatbots in Healthcare

10

Adoption of chatbots during COVID-19 and beyond

11

Governance gaps

13

4 The Chatbots RESET Framework

14

Types of stakeholders

14

Stages of the use of chatbots in healthcare

15

Types of chatbots

17

5 Chatbots RESET: Principles

20

6 Chatbots RESET: Operationalization

20

Operationalization actions that cut across principles

21

Safety/Non-maleficence

22

Efficacy

23

Data protection

24

Human agency

25

Accountability

26

Transparency

27

Fairness

28

Explainability

29

Integrity

30

Inclusiveness

31

Conclusion

32

Appendix: AI and Healthcare Ethics Principles

34

Contributors

36

Endnotes

© 2020 World Economic Forum. All rights reserved. No part of
this publication may be reproduced or transmitted in any form
or by any means, including photocopying and recording, or by
any information storage and retrieval system.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

2

December 2020

Chatbots RESET
A Framework for Governing Responsible Use of Conversational AI in Healthcare

Forewords
The global COVID-19 pandemic has brought into
sharp relief the importance of healthcare for the
sustainable well-being of people, society and our
planet. This coincides exactly with the corporate
philosophy, known as “KAITEKI”, of Mitsubishi
Chemical Holdings Corporation (MCHC). We are
deeply committed to these concepts for the long
term, not only in times of emergency.
Larry Meixner
Chief Innovation Officer and
CTO, Mitsubishi Chemical
Holdings Corporation;
Member of the Board,
Mitsubishi Tanabe
Pharma Corporation

Shobana Kamineni
Executive Vice-Chairperson,
Apollo Hospitals

Artificial intelligence (AI) is enabling transformative
advances in many domains, including healthcare.
Yet even as such Fourth Industrial Revolution
technologies show their power to solve social
problems, we must remain conscious of gaps that
appear when the technologies advance faster
than our ability to govern them. This recognition

Use of AI and chatbots in healthcare is a true
reflection of the current times, when the world had
to move in a flash to digital systems. Though the
technology advancements have been relentless in
this domain in recent past, the pandemic pushed
us to adopt and use it, rather abruptly. At Apollo
247 – Apollo Hospitals’ Digital Health platform
– we have been able to successfully use these
technologies and reach out to millions in a very
short period of time.
The Centre for the Fourth Industrial Revolution at
the World Economic Forum has taken up this timely
and impressive work to develop the frameworks

motivated MCHC to partner with the World
Economic Forum in launching and supporting the
Chatbots RESET project and to second a Fellow
from our company to co-lead the project.
The Chatbots RESET framework reflects months of
effort by the project community, with contributions
from numerous stakeholders bringing their diverse
perspectives to bear on key governance challenges.
The principles and recommended actions in this
framework are designed to be ethical “guardrails”
for the implementation of conversational AI systems
to address healthcare problems. We believe
this framework can help promote and generate
examples of the responsible use of technology
in healthcare.

and guiding principles for the development and
use of chatbots in healthcare. Healthcare chatbot
systems can improve and augment accessibility
(reaching to the last mile), enhance effective
interactions, deliver care faster and with higher
accuracy. However, it has to be safe, maintain
users’ privacy and integrity and be delivered in
a fair and inclusive manner. Hence, guiding the
ecosystem of developers, users and regulators
of this novel field remains as a paramount
objective. I strongly believe this framework is
developed at the right moment to provide the
ecosystem with a guiding light and will help the
ecosystem profoundly.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

3

The exponential march of digital technologies
like broadband connectivity, mobile devices,
cloud computing and AI have transformed every
aspect of human life. Digital technologies have
been used to deliver price-performance-experience
packages that are affordable and accessible to
not just the privileged few but to billions of people
across the globe.
Kiran Thomas
Chief Executive Officer,
Jio Platforms Ltd

But the solutions to many entrenched problems
of humanity have been constrained by the relative
scarcity of skilled manpower. A prominent example
is healthcare, which has always depended on
highly trained physicians. But modern AI services
combined with smart sensors are making rapid
strides in supplementing and even standing in

As the world battles an unseen enemy, the
change for humanity is incumbent on AI and its
augmentation of human knowledge. From
drug discovery to patient care – the world has
witnessed AI and its many principles in action
fighting this pandemic.

C.P. Gurnani
Managing Director and
Chief Executive Officer,
Tech Mahindra

While AI is the new electricity, and data the
new fuel, a realization has dawned upon us, the
realization of an augmented pollution (AP). This
realization reflects upon the maleficent usage of
this technology. Where AI could be useful for
myriads of cases, the world has also witnessed
numerous scenarios where usage of AI without
transparency, without empathy and without proper
controls causes havoc like deep fakes, maleficent
chatbots, etc.

for human doctors. This overcomes a critical
constraint that has prevented widespread
access to high-quality healthcare in developing
nations, rural populations and other difficult to
serve communities.
Even so, healthcare is a sensitive domain requiring
due consideration for safety, security and privacy.
Fortunately, there are robust standards, codes
of conduct and ethics in healthcare that have
evolved over centuries of human experience. This
white paper is a timely and welcome effort to
create a comprehensive framework that makes
it easy for technology regulators and developers
to conceptualize and incorporate these into AI
solutions of the future.

As we reflect on this change, I am pleased to
introduce a governance framework for responsible
use of conversational AI systems for healthcare,
an initiative led by the World Economic Forum
along with its partner companies. The framework
offers insights from leading AI experts on stages
of chatbots creation, adherence to principles like
safety, efficacy, explicability, data protection and,
above all, its operationalization.
I truly believe that we are at a precipice of a change
and the change requires a sustained effort from the
technology community to adhere to principles of
responsible and ethical usage of AI. I hope these
principles will provide you with the necessary
insights for a practical implementation of chatbots
in real-life healthcare scenarios.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

4

Executive Summary
Chatbots, or conversational artificial intelligence
(AI) systems, are used increasingly by
organizations to communicate with customers
in a natural and easy-to-use way by embedding
chatbots in websites, social network apps, smart
home devices, etc. The COVID-19 pandemic has
accelerated the adoption of chatbots in healthcare
applications. As examples, both the World Health
Organization and the Centers for Disease Control
deployed chatbots for coronavirus information
dissemination and symptom checking. So, too,
did many governments and healthcare providers.
Beyond the pandemic, the rate of adoption of
chatbots in healthcare applications is likely to be
sustained due to the access and cost benefits
they enable.
When a new technology is introduced in
healthcare, especially one based on AI, it
invites meticulous scrutiny and chatbots are
no exception. The exchange of sensitive health
information with chatbots is one of many
governance challenges that require careful
consideration in order to promote responsible use
of chatbots in healthcare. Other challenges include
performance assurance, patient considerations,
legality, privacy and security, in addition to classic
AI challenges such as fairness and explainability.
To address these governance challenges, earlier
this year the World Economic Forum assembled a
multistakeholder community, which has co-created

Chatbots RESET, a framework for governing the
responsible use of chatbots in healthcare.
The Chatbots RESET framework consists of two
parts: (1) A set of 10 principles carefully selected
from AI ethics and healthcare ethics principles and
interpreted within the context of the use of chatbots
in healthcare; and (2) Operationalization actions
for each principle in the form of recommendations
to implement in various stages of deployment
of chatbots in healthcare. The framework is an
actionable guide for three groups of stakeholders
to promote the responsible use of chatbots in
healthcare applications: technology developers,
healthcare providers and government regulators.
In the Chatbots RESET framework, chatbot
developers will find actions they can incorporate
within their project development process to create
more responsible implementations of chatbots;
healthcare providers can integrate actions from
the framework within their workflows to promote
responsible deployment; and regulators can choose
actions that resonate with their national strategy to
ensure responsible scale-up of chatbot technology.
Looking ahead, several partners of the Forum
will be piloting the Chatbots RESET framework.
Results of the piloting, including feedback on and
enhancements to the framework, will be shared in
a future publication.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

5

1

Chatbots in Healthcare
A chatbot is an AI programme designed to
converse in a natural manner with people via
voice interfaces or text messages. Chatbots are
typically found in websites, applications, or instant
messaging. Chatbots are mainly used for tech
support and lead generation.
In healthcare, chatbots can be used in many ways
to engage with patients, including to navigate
frequently asked questions, find a doctor or
service, schedule appointments, facilitate
symptom checking, conduct triage in emergency
care, help prepare for procedures and ensure
adherence to post-discharge instructions.
Chatbots can also act as virtual assistants to
physicians, lowering administrative burden on
physicians and giving them easy access to

patient health records (see Figure 1 for a more
comprehensive list of applications). These uses
of chatbots in healthcare can result in better care
management and customer engagement.
However, several issues could arise, such as
miscommunication between chatbots and
customers, customer perception of reduced
choices when interacting with chatbots, and
neglect of customer preferences in interacting
with chatbots vs humans. More serious issues
include incorrect/poor guidance, wrong diagnosis,
or failure to achieve timely interventions. It is
important to thoughtfully govern the deployment
of chatbots in healthcare to avoid these issues
and to ensure trust, transparency, reliability
and security.

What are chatbots and why should you care?
The AIML foundation defines a chatbot as a “computer program designed to respond to text or voice
inputs in natural language”. Chatbots are also referred to as conversational AI or conversational agents.
Typically, chatbots are preloaded with a set of rules or pre-trained using data in order to be able to have a
meaningful conversation with the user in real time and to provide useful services in the process.
You might have come across chatbots when looking for help or tech support on websites – they are
usually linked to a “chat” icon at the bottom-right corner of the site. You may also have smart home
devices at home that you talk to. When you talk to a chatbot, you are talking to an AI system. Though you
may find today’s chatbots somewhat limited in what they can do, chatbot technology is advancing fast
and soon you may not be able to distinguish between an automated system and a human. This can have
critical implications if the topic of your chat is healthcare.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

6

2

The Chatbots
RESET Project
The goal of the World Economic Forum’s Chatbots
RESET project is to design a governance
framework for the responsible use of chatbots in
healthcare by bringing together chatbot developers,
chatbot platforms, the medical community, civil
society, academia and healthcare regulators.
Through designing, piloting and scaling the
framework, we strive to achieve broad adoption of
chatbots in healthcare, maximizing their beneficial
uses while minimizing negative consequences.

start-ups, platform providers and governments1.
The discussion underscored the beneficial uses
of the technology and its potential to make a
positive impact on healthcare. In May 2020,
the Forum brought together global experts
in a design workshop to brainstorm the
potential uses, the issues and stakeholder
actions to maximize positive impacts and
minimize negative consequences of the use
of chatbots in healthcare.

The Chatbots RESET project was launched
in January 2020, with initial thoughts about
governance actions alluded to by the word
RESET (Reveal, Escalate, Substitute, Explain
and Track). Soon thereafter, the importance of
chatbots became apparent from their increased
use for disseminating curated information about
COVID-19 and the coronavirus. In March 2020, the
Forum hosted a panel discussion titled “Chatbots
for Coronavirus and Beyond”, which included an
in-depth discussion by a panel of experts from

Following the May workshop, the multistakeholder
project community began co-creating the
governance framework through a series of virtual
meetings. The result of this work, the Chatbots
RESET Governance Framework, containing 10
principles and 75 recommended actions, is the
primary focus of this paper. Piloting the framework
in actual use cases, working with partners drawn
from the diverse stakeholder community, is
ongoing. The results from the pilots will be shared
in a subsequent publication.

Conversational AI holds great promise in healthcare but there are also potential risks
and harms which may have direct impact on patient care. Accordingly, the creation of a
governance framework for chatbots is a matter of urgency. This work provides a global
guide to enable stakeholder organizations to ensure efficacy, safety, privacy and other
ethical considerations in their use of chatbots in healthcare and to build public trust in
the technology.
– Kay Firth-Butterfield, Head of Artificial Intelligence and Machine Learning,
World Economic Forum

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

7

Applications of
Chatbots in Healthcare

3

Chatbots are finding increasing uses in healthcare,
recently propelled by the COVID-19 pandemicrelated information dissemination needs. Many
global and national healthcare organizations have
adopted chatbots as a way of communicating with
their constituents about the virus and the disease.
As examples, the Centers for Disease Control uses a
chatbot on its website for coronavirus self-checking2,
and the World Health Organization has a WhatsApp
messenger chatbot for COVID-19 information. The

FIGURE 1:

Microsoft Healthcare Bot platform has been used
in over 1,000 chatbot implementations related
to the pandemic. Beyond COVID-19, there are
numerous applications of chatbots in the healthcare
ecosystem. See “Adoption of Chatbots during
COVID-19 and Beyond”.
There are many current and potential uses of
chatbots in healthcare scenarios. Figure 1 provides
a glimpse into the plethora of their possible uses,

Chatbots can be deployed in numerous instances in the healthcare journey of an
individual, some of which are shown here

Enrolment

Self-evaluation: Diagnosis,
pre-screening

Government benefits; health insurance
enrolment and support, subsidies,
medical cost advice, knowledge
transfer, population symptom checking,
surveys and assessments, programme
enrolment and onboarding

KEY
Patient-facing

Symptom checkers, disposition &
possible causes, health information
hotline, public health information,
medical terminology, employee
health check

Provider-facing

Check-in
Disease screening, gathering medical
history; question answering for common
symptoms, risks and prevalence

Provider search:
customer service
Provider lookup, office hours,
appointment scheduling

Treatment

Assessment

Treatment support, discussing test
results, mental health therapy,
behavioural therapy

Access medical
records; tracking patient
conditions/symptoms

Post-discharge
Patient compliance management, home-based
follow-ups, multi-party communications,
medication reminders and adherence, patient
education and FAQ

Discharge
Prescriptions,
lab and imaging
results follow-up

Wellness
Source: World Economic
Forum, with contributions
from the Chatbots RESET
project community

Generic information on diseases and symptoms,
health promotion, coaching, surveillance, companion,
reminders for preventive screening, health
assessment and risk scoring
Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

8

mapped on to a typical healthcare journey.
It is worth noting that not all chatbots are
patient-facing. Provider-facing chatbots, while
in early stages today, are likely to expand to
address many back-office functions in a virtual
assistant format.

Levels of automation in the use of AI in chatbots for healthcare, akin to levels of
automation used to classify autonomous vehicles

0

1

2

3

4

5

No
Automation

Basic
Assistance

Personalized
Assistance

Partial
Automation

Conditional
Automation

Full
Automation

No role

Pre-diagnostic
information
collection

Collect and
evaluate
patientspecific
information

Suggest
diagnosis and
treatment

Diagnosis
and treatment
of minor
illnesses with
no human
supervision

Entire process
of diagnosis
and treatment
planning

End-to-end

Analysis and
decisionmakig

Verification
and decisionmaking

Verification
and decision
approval

Intervention
in special or
difficult cases

Approval only,
if needed

Open

Open

some EHR*

EHR

EHR

EHR

Human

Human

Human

Hybrid

Hybrid

Telemedicine

Wellness

Symptom
checkers

Patient
guidance

Self-service

Chatbots

TA B L E 1 :

Chatbots are not always fully autonomous. A range
of automation is possible, as we suggest in Table 1,
which portrays increasing levels of automation with
escalating role and decision-making authority for
chatbots relative to human operators or providers.
This type of categorization is similar to that used for
autonomous vehicles[3].

Human

Description
of roles

Data sources

Decision-maker

Example

(human +
chatbot)

(human +
chatbot)

Chatbot

Cognitive
behavioural
therapy

(* EHR refers to electronic health records)
Source: World Economic Forum, based on a suggestion by Murali Doraiswamy, a member of the project community

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

9

Adoption of chatbots during COVID-19
and beyond
FIGURE 2:

COVID-19 is an accelerator for chatbot adoption in healthcare
COVID-19 is accelerating chatbot deployment
in healthcare

Many factors support
sustained chatbot
adoption rate

Significant benefits
Negative impacts

Source:
World Economic Forum

–

Healthcare systems
adopting chatbots

–

User acceptance of
chatbots for healthcare

–

Cost and efficiency
benefits

–

AI built in

–

Integration with hospital
IT system

–

HIPAA compliance

–

FDA approvals

–

Alliances with national
healthcare systems

In March 2020, we convened a panel of global
experts to explore how Chatbots are being used
during the COVID-19 pandemic, and on how they
might be used beyond the pandemic1. Here is what
we learned.
During the pandemic, healthcare systems ramped
up adoption of chatbots to provide curated
information about the virus and the disease.
Most chatbots included symptom checking and
guidance on next steps. User acceptance of
chatbots for healthcare information is increasing.
Significant cost and efficiency benefits are seen
as a result of reduced workload on call centres
and increased capacity to handle healthcare
inquiry volumes.
Telehealth consultations using audio/video
links have increased significantly during the
COVID-19 pandemic[4]. These are harbingers for
AI-based chatbot consultations in the future, as
patients are willing to forgo office visits in favour
of consulting with an expert – human or AI – on
their smartphones.

Beyond the pandemic, there are many factors
that will support sustained adoption rate for
chatbots in healthcare:
–

Most COVID chatbots already have AI built-in.
Though heavy use of AI might be limited today
to ensure consistent, curated information, the
use of AI can be easily ramped up later

–

Most chatbots are HIPAA-compliant, and some
have FDA approvals as medical devices

–

Many have been integrated with hospital IT
systems, and some even have alliances with
national healthcare systems (e.g., Babylon
Health, with the National Health Service in
the UK5)

With high adoption of chatbots for healthcare,
society can reap significant benefits (see box
‘Benefits’). At the same time, there are many
potentially negative impacts that create challenging
governance gaps. Addressing the governance gaps
is the focus of the Chatbots RESET framework.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

10

Benefits
Healthcare can reap significant benefits by using chatbots. Some are listed below:

24/7 access

Rapid
deployment

Low cost

Anytime, anywhere
access to healthcare
information

One chatbot can
service thousands
of customers

Can be deployed within
days to weeks (e.g.
COVID-19 chatbots)

Repurposing

Better
digital tools

Data for
future

Faster and more
intuitive than other
digital options

Automatically generates
large amount of data for
future use/training

Possibility to repurpose
for other uses
and public health
emergencies

Consistency

Provide consistent
replies, based on
current, curated
information

Customer
satisfaction

Results in improved
customer engagement

Source:
World Economic Forum

Governance gaps
The following is a list of key governance gap areas
we have identified, along with examples of typical
issues raised in each area. The questions in each
area are meant to be representative of the kind
of governance gaps issues, rather than being a
comprehensive list.

–

Is a second opinion needed every time a
diagnosis is made?

–

How will a chatbot escalate issues that are critical
or those that it is unable to understand/handle?

–

What are the expectations of the healthcare
system (providers, payers, etc.) on what
chatbots should/should not do? How will they
verify that the performance of a chatbot meets
these expectations?
What is the oversight body for addressing
deficiencies in chatbot performance?

Validation/accreditation
– What are the boundaries of chatbot operations?
In other words, what are “approved” uses of
chatbots in healthcare?
–

Are current standards for regulating chatbots
(e.g., as medical devices) adequate?

–

–

Do chatbots need to be qualified, in a manner
similar to qualifying medical doctors, assistants,
etc.? If so, who will do it and how?

Patient considerations
– Patient expectation management

Performance assurance
– How do we ensure that the limits of
performance of chatbots are well understood by
everyone? For example, how will they deal with
poor spelling, or speech in a noisy environment?
–

How will chatbots catch errors (e.g.,
misunderstanding user inputs)? What will be the
action/remedy if they don’t?

–

Will patients be misled into believing they are
talking to a human?

–

What if they want to opt out of talking to
a chatbot?

–

What if they want to switch to talking
to a human at any time of their choice
(similar to “dial 0 for operator” in automated
phone calls)?

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

11

–

Patient access
–

–

Will the system understand their (native)
language? How will chatbots operate in
countries with thousands of dialects?
What are the hardware/software
requirements? Is a smartphone needed?
How about connectivity/bandwidth needs?

Legality
– Who is responsible for wrong diagnosis or
misdirection or lack of timely response?
–

What are preventive and punitive measures?

–

How would consent work, for using the system,
and for allowing access to and storage of
personal data and chats?

Security
– Where will the chatbot and the AI reasoning
system reside?
–

Will conversations be recorded? If so, where
will they be stored? Who will have access to the
recordings, and when?

–

Will there be options to “host” and/or store data
on-premise?

“Classic” AI governance gaps
– How do we avoid “digital divide”, from
access, language and knowledgelevel perspectives?
–

What about transparency and explainability of
AI-powered systems?

–

How do we deal with bias and fairness and
also make the solutions are relevant to the
target population?

–

How can we address data privacy/data
rights issues?

Privacy
– Who will have access to electronic
health records?
–

What will be the rules governing the recording
of chats?

–

What if user accidentally reveals other private
information in a chat?

We strive to address these governance gaps in the
Chatbots RESET framework, which is presented in
the remainder of this document.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

12

4

The Chatbots
RESET Framework
The framework consists of two parts:
1. A set of principles selected by the
multistakeholder community to govern the
use of chatbots in healthcare. The principles
have been drawn from AI ethics principles and
healthcare ethics principles (see Appendix) and
interpreted specifically for the use of chatbots in
healthcare applications.

FIGURE 3:

2. Actions that stakeholders can take to
operationalize the principles in various
stages of the use of chatbots in healthcare.
For each principle, the framework
recommends a set of actions that
stakeholders can take to implement
the principles.

The Chatbots RESET framework consists of principles for responsible use of chatbots and
actions to operationalize the principles

AI Ethics

Healthcare Ethics

Chatbots
RESET
Principles

Developer

DEVELOP

Provider

DEPLOY

Regulator

SCALE

Source:
World Economic Forum

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

13

Rationale of the framework
–

–

By the process of starting from key AI and
healthcare ethics principles and interpreting
each principle for the specific use of chatbots
in healthcare, the framework provides two
benefits: (1) The users of the framework are
relieved from the guesswork of interpreting the
principles; and (2) The curated list of interpreted
principles serves as a uniform standard.
The operationalization actions provide
specific recommendations to the users of
the framework on how to implement the
principles during the development, deployment,
and scale-up stages of the use of chatbots
in healthcare. This allows the users of the
framework to focus on execution of the actions

rather than development of the actions.
(Caveat: The suggested actions are meant
to be starting points to implement the
principles, but they do not exhaustively
cover all possible scenarios.)
–

The framework has been co-created by a
multistakeholder project community, with
representation from start-ups, large companies,
academia, governments and civil society. The
diversity of the community and healthy debates
during the creation of the framework have
sharpened the focus on principles and actions
of broad appeal, which have been carefully
curated by the community.

Types of stakeholders
The framework has been developed with three
types of stakeholders in mind, shown in Figure 4.
Developers drive the creation of chatbots, providers
pilot and deploy chatbots in healthcare applications

FIGURE 4:

Source:
World Economic Forum

at local/small scale, and regulators monitor and
govern the widespread use of chatbots in society
for healthcare purposes. Some examples of each
type of stakeholder is shown in the figure below.

We consider three types of stakeholders – developers, providers and regulators – and give
examples under each

Developer

Provider

Regulator

–

Chatbot developers

–

Hospitals

–

Government (as a regulator)

–

Conversational AI platforms

–

Insurance companies

–

Consumer bodies

–

Chatbot hosting platforms

–

Technology providers
(direct to patient)

–

Medical bodies

–

Government (as a provider)

Stages of the use of chatbots in healthcare
The framework provides recommendations
for actions to be performed during three
operationalization stages (Figure 5):
1. Develop: Chatbots are designed and
developed by technology developers using
modern AI/ML techniques to meet potential
needs in healthcare, in consultation with
providers and in compliance with laws
developed by regulators

2. Deploy: Chatbots are piloted and employed in
small-scale by providers, with assistance from
technology developers and guidance
from regulators
3. Scale: As chatbots are broadly adopted
by society, they begin to impact large and
diverse populations, overseen by regulators,
with support and compliance from technology
developers and providers.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

14

All the stakeholders have roles to play in all the stages, but one stakeholder takes on a primary role in
each stage.

FIGURE 5:

Source:
World Economic Forum

The Chatbots RESET framework addresses three stages of operationalization. Develop,
deploy and scale

DEVELOP

DEPLOY

SCALE

Conversational
AI systems
and platforms

Conversational AI
among patients
and doctors

Monitor/Regulate the
absorption of conversational
AI for healthcare in society

Developer*

Developer

Developer

Provider

Provider*

Provider

Regulator

Regulator

Regulator*

Primary*

Types of chatbots
Not all chatbots are created equal; they constitute a
spectrum as they address a vast array of applications
within healthcare. At one extreme, they address
processes such as scheduling appointments and
follow-ups or providing information on diseases
and drugs. At the other extreme, they diagnose
and suggest treatment plans for severe illnesses
or provide therapeutic guidance for mental health.
Because of the different types of risk levels involved
in the use of different types of chatbots, the

FIGURE 6:

operationalization actions of the framework are not
equally applicable across the spectrum of chatbots.
To address this diversity of risk levels, the
framework includes a preliminary classification of
Chatbots into four types (Types I, II, III, or IV) based
on the severity of the healthcare condition and
the significance of the information provided by the
chatbots to healthcare decisions, as outlined in
Figure 6, which is directly inspired by the approach

The four “types” of chatbots

State of healthcare
situation or condition

Significance of information provided by chatbots to healthcare decisions
Inform clinical management

Drive clinical management

Treat or diagnose

Non-serious

I

I

II

Serious

I

II

III

Critical

II

III

IV

Source: Based on International Medical Device Regulators Forum Final Document “Software as a Medical Device: Possible
Framework for Risk Categorization and Corresponding Considerations”6

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

15

of the International Medical Device Regulators
Forum to software as a medical device6. In the
framework, we recommend operationalization
actions as they pertain to the four “types” of
chatbots on a three-level scale: optional, suggested
and required.

TA B L E 2 :

Examples and risk-level labelling for the four types of chatbots
Type

Source:
World Economic Forum

To facilitate the interpretation of the chatbot
type classification in Figure 6, we provide
examples in Table 2 that may be helpful to
the users of the framework to more easily
identify chatbot types based on their
application scope.

Risk level

Example

I

Low

Information only: addresses, office hours, find doctor, community health,
pandemic information, medicine dosage, drug interactions; scheduling:
appointments; post-visit follow-up

II

Moderate

Symptom checking without diagnosis; generic next step recommendations

III

High

Diagnosis; specific next step recommendations

IV

Very high

Treatment plan

The following pages of the document focus on the two parts of the Chatbots RESET framework shown
in Figure 3.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

16

5

Chatbots RESET:
Principles
Chatbots offer an unprecedented opportunity to enhance healthcare in a responsible
and evidence-based manner.
– Murali Doraiswamy, Digital Health Innovator, Duke University School of Medicine

Safety/
Non-maleficence

Inclusiveness

Integrity

Efficacy

Chatbots RESET Principles

Explainability

Data protection

Human Agency

Fairness

Accountability

Transparency

Source:
World Economic Forum

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

17

The principles for the framework have been derived
from both AI and healthcare ethics principles,
interpreted for the use of chatbots in healthcare.
The 10 Chatbots RESET principles derived,

interpreted and curated by the Chatbots RESET
project’s multistakeholder community are presented
here. In this section, the word “users” refers to
those who directly interact with chatbots.

Safety/Non-maleficence
–

The actions of chatbots shall not result in avoidable harm to humans or other unintended
consequences, including deception, addiction and lack of respect for diversity

Efficacy
–

Chatbots shall be fully verified for the efficacy of their purported service, in compliance with accepted
international standards

–

Chatbot outputs shall be tailored to their intended users, while keeping in mind the medical nature of
the information that is being communicated

Data protection
–

All data and history of interactions, including intended and unintended revelations of private data and
those collected with consent, shall be safeguarded and disposed of properly, respecting applicable
privacy and data protection regulations/laws

–

If any data is recorded during a session and/or used across sessions, the chatbot user consent and/
or any applicable ethics body approvals for research and data collection purposes shall be required

–

Chatbot users shall have the right and access to take ownership of personally identifiable information

–

Data collected by chatbots shall not be used for surveillance or punitive purposes, or to unfairly and
opaquely deny healthcare coverage to users

Human agency
–

Chatbots shall support the user’s agency, foster fundamental rights and allow for human oversight

–

Chatbots shall respect the ability of patients to make their own decisions about
healthcare interventions

–

Chatbots whose operating model includes real-time human oversight shall yield to the desire of the
user to interact with a human agent at any time the user wishes to do so

Accountability
–

An entity (person or group) in the organization shall be accountable for the governance of chatbots

–

Conclusions and recommendations of chatbots shall be auditable

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

18

Transparency
–

Chatbot users shall at all times be made aware of whether they are interacting with an AI or a human
or a combination of the two

–

Chatbots shall clearly inform users about the limits of performance of the system, except in situations
where not informing is required for the intended purpose of the chatbot

–

Chatbot users shall be immediately informed if the chatbot is unable to understand the user or is
unable to respond with certainty, except in situations where such communication interferes with the
intended purpose of the chatbot

Fairness
–

Chatbots shall not act in a systematically prejudiced manner with respect to ethnicity, geography,
language, age, gender, religion, etc.

–

If a chatbot “learns” from data, the training dataset should be representative of the target population

Explainability
–

Decisions and recommendations made by chatbots shall be explainable in a way that can be
understood by their intended users

Integrity
–

Chatbots shall limit their reasoning and responses to those that are based on reliable, high-quality
evidence/data, ethically sourced data and data collected for clearly defined purpose

Inclusiveness
–

Every effort shall be undertaken to make chatbots accessible to all intended users, with special
consideration given to identifying and enabling access for potentially excluded or vulnerable groups

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

19

6

Chatbots RESET:
Operationalization
Principles require actions to implement them. In this
section, we share the collection of actions developed
by the Chatbots RESET project community. For
each principle in the previous section, we present the
following, one principle per page:
Principle: The name of the principle.
Interpretation: The principle is interpreted for the
use of chatbots in healthcare (reproduced from the
previous section).
Table of operationalization actions: The actions
that can be taken by developers, providers and

regulators to implement the principle. The table
lists, by stage of implementation (as described
in Figure 5:), the actions and the responsible
stakeholder for each action. The last column
of the table shows a code that corresponds
to the applicability of each action to the four
types of chatbots (outlined in Figure 6 and
further elaborated in Table 2), using the
following mnemonic: green, optional; yellow,
suggested; red, required.
Here is an example of the code for an action that
is optional for Type I, suggested for Type II, and
required for Types III and IV:

Type I

Type II

Type III

Type IV

Optional

Suggested

Required

Required

For easy reference, we reproduce the icons for the stakeholders:

Developer

Provider

Regulator

Operationalization actions that
cut across principles
While developing the operationalization actions
for specific principles, we identified the following
actions that have broader applicability beyond a
single principle and often cut across all principles:
1. (Developers, providers, regulators) Create a
redressal mechanism for users, including, but
not limited to, the ability for users to provide
feedback and seek recourse
2. (Regulators) Create a mechanism for providers
to openly share causes, analyses and
conclusions reached in situations that resulted
in unexpected harm

3. (All) In all stages, follow well-established
standards of inclusiveness. Some examples
are provided here:
–

United Nations Convention on the Rights of
Persons with Disabilities7

–

Know Your Rights: Three Important Federal
Laws that Protect People with Disabilities8

–

ISO/IEC 30071-1:2019 Information technology
– Development of user interface accessibility9

–

Digital Inclusion, Identity, Trust and Agency10

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

20

Safety/Non-maleficence
The actions of chatbots shall not result in avoidable harm to humans or other unintended consequences,
including deception, addiction and lack of respect for diversity

The focus on patient / user safety remains paramount in clinical chatbots with the
principled aim – do no harm. However, as technology develops, we would need to
address how AI and clinicians together aim for better accuracy and foster a culture
of safe and effective communication between humans and machines.
– Sujoy Kar, Chief Medical Information Officer, Apollo Hospitals Group
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Build on existing guidelines to allow for
determination of critical/serious/non-serious cases

DEVELOP

Design a robust hand-off system for situations
when AI fails

DEVELOP

Install safeguards to identify abnormal behaviour
and prevent manipulation

DEPLOY

Develop mechanism to govern consent for use/
treat/diagnose options of chatbots

DEPLOY

Track and document mistakes attributable to
chatbot; share with developers and regulators for
continual improvement

DEPLOY

While suggesting diagnostic/treatment options,
consider issues related to patient safety

DEPLOY

Train all personnel on when and how to intervene

SCALE

Provide online user education
(“How chatbots work”)

SCALE

Perform planned and unplanned audits of
developer documentation

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

21

Efficacy
Chatbots shall be fully verified for the efficacy of their purported service, in compliance with accepted
international standards.
Chatbot outputs shall be tailored to their intended users, while keeping in mind the medical nature of the
information that is being communicated.

When users see a technology as effective, they begin to trust and use it.
Health domain is all about trust, and efficacy principles will be key to getting
chatbots adopted.
– Biplav Srivastava, Professor of Computer Science, AI Institute, University of South Carolina
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Identify intended users and understand their needs
at the beginning of the development process

DEVELOP

Use verified and tested clinical protocols that have a
regular cadence of updates

DEVELOP

Define bot-only actions, human-only actions and
hybrid actions

DEPLOY

Define a functional test along with chatbot dialog
flow to validate behaviour and test for regressions

DEPLOY

Include efficacy metrics as a central aspect in
procurement, and request evidence of efficacy
from developers

SCALE

Create a regionally relevant common testing
framework (e.g., ITU/WHO FG-AI4H project11,
TRIPOD12 for clinical AI) and a standard validation
dataset to validate chatbot behaviour (minimum bar)
and correctness of diagnosis

SCALE

Create patient education guidelines to ensure that
adequate educational resources are available to lay
users to interpret complex medical information

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

22

Data protection
All data and history of interactions, including intended and unintended revelations of private data and those
collected with consent, shall be safeguarded and disposed of properly, respecting applicable privacy and
data protection regulations/laws
If any data is recorded during a session and/or used across sessions, the chatbot user consent and/or any
applicable ethics body approvals for research and data collection purposes shall be required
Chatbot users shall have the right and access to take ownership of personally identifiable information
Data collected by chatbots shall not be used for surveillance or punitive purposes, or to unfairly and
opaquely deny healthcare coverage to users

Health and healthcare systems are undergoing a “Great Reset”, using technology
to enable greater and more cost-effective access to patients around the world. The
protection of patient data remains paramount in the development of AI-supported
applications. These governance principles are a strong step in the right direction to
ensuring ethical and responsible collection of increasing amounts of such data.
– Genya Dana, Head of Shaping the Future of Health and Healthcare, World Economic Forum
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Require security review before launch

DEVELOP

Implement role-based access controls for
conversation data

DEPLOY

Develop retention policy on conversation data

DEPLOY

Create accountability for stored data and penalties
for leaks/loss

DEPLOY

Require provision of opt-in for users to manage
stored data and their uses

DEPLOY

Train staff on consent mechanisms and data
hygiene practices

SCALE

Facilitate private-public data sharing, especially
when public funds are used

SCALE

Enhance existing health data protection regulation
to include chatbot conversations

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

23

Human agency
Chatbots shall support the user’s agency, foster fundamental rights and allow for human oversight
Chatbots shall respect the ability of patients to make their own decisions about healthcare interventions
Chatbots whose operating model includes real-time human oversight shall yield to the desire of the user to
interact with a human agent at any time the user wishes to do so

Chatbots in healthcare increase human efficiency and productivity. These chatbots
function at their best with the human agent in the loop. Humans and chatbots
strengthen each other to deliver optimal care. That is why this governance framework is
so important, because the patient deserves the best healthcare possible.
– Nupur Ruchika Kohli, Medical Doctor and Medical Adviser, Specialized Hospital Care
and Expensive Medication; Curator, Amsterdam Hub, Netherlands, Global Shapers, World
Economic Forum
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Include an option for seamless human real-time
communication

DEVELOP

Have a diverse test group (and ideally, a diverse
development team) representative of the target
population group

DEVELOP

Provide tooling/adaptations for patients with
visual impairments

DEVELOP

Provide for the human agent to view the chatbot
conversation history

DEVELOP

Include a “signpost” to where user can get help
in person

DEVELOP

Include a feature to generate a full transcript for
user review

DEVELOP

Be explicit about how “human in the loop” is
defined for the chatbot

DEPLOY

Verify safety of guidance/information provided when
a human is not available for real-time oversight

SCALE

Conduct detailed qualitative assessment with a
random user set and publish results

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

24

Accountability
An entity (person or group) in the organization shall be accountable for the governance of chatbots
Conclusions and recommendations of chatbots shall be auditable

With the growing demand of conversational AI, especially for healthcare,
accountability of a deployed conversational agent needs to be owned by a person,
entity or an organization. This accountability would form the quintessential core of the
chatbot design principle. ‘AI is the new electricity and data is the new fuel’, it is said.
We have to ensure we are accountable and responsible that when these two forces
meet, we create more power for humankind and less pollution.
– Nikhil Malhotra, Chief Innovation Officer, TechMahindra
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Ensure that chatbot workflows are human audited
at least every year internally to maintain current
status and accuracy

DEVELOP

Within the audit, be transparent about whether the
chatbot is dynamically learning or is static

DEVELOP

Seek clinical inputs in the decision to implement
a chatbot

DEPLOY

Liability insurance should cover chatbot malpractice
the way they cover healthcare providers

DEPLOY

In case of “accidents”, provide full explanation of
why the chatbot did what it did

DEPLOY

Proactively provide a framework and certification for
the safety of chatbots

DEPLOY

Create a mechanism for accountability, especially if
there is an adverse outcome

DEPLOY

Keep a comprehensive record of data governance

DEPLOY

Require evidence for engineering best practices
(e.g., IEC62304, ISO14971, ISO27001)

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

25

Transparency
Chatbot users shall at all times be made aware of whether they are interacting with an AI or a human, or a
combination of the two
Chatbots shall clearly inform users about the limits of performance of the system, except in situations where
not informing is required for the intended purpose of the chatbot
Chatbot users shall be immediately informed if the chatbot is unable to understand the user or is unable to
respond with certainty, except in situations where such communication interferes with the intended purpose
of the chatbot

The clear communication of limitations is critical because users and regulators need
to develop an accurate theory of function for the chatbot that enables them to consider
its recommendations and interactions rationally and critically. Because chatbots are
engineered systems, users do not have priors to help them intuitively build a model of
functionality and limitation as they do for human experts.
– Illah Nourbaksh, K&L Gates Professor of Ethics and Computational Technologies, Carnegie
Mellon University
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Use a chatbot persona clearly distinct from that of
a human

DEVELOP

Provide an explanation of decision or inference,
in plain language, infographic or video

DEPLOY

Publish limitations of the chatbot (possibility
of errors and consequences) and reliability of
the chatbot

DEPLOY

Require developers to inform users when chatbot
fails to understand the user

DEPLOY

Do testing in realistic conditions (pilot) to verify that
the chatbot can inform the user if there are issues
with understanding

DEPLOY

Set out guidelines on exceptions to transparency

DEPLOY

Inform users whether the matter is non-serious,
serious or critical

DEPLOY

Develop policy to inform users when AI is involved

DEPLOY

Share limitations of the use of algorithm and data

DEPLOY

Be explicit in distinguishing between
recommendation and information

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

26

Fairness
Chatbots shall not act in a systematically prejudiced manner with respect to ethnicity, geography, language,
age, gender, religion, etc.
If a chatbot “learns” from data, the training dataset should be representative of the target population

In order to reach the maximum potential of these new technologies, it is critical that
we take meaningful steps to ensure fairness and representation at every stage of their
design, development and deployment. This framework emphasizes the importance of
representative datasets, diverse development teams and collaboration at every level.
– Matthew Fenech, Medical Safety Lead, Ada Health
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Ensure that data used in development includes
underrepresented groups

DEVELOP

Create and publish representative population
statistics (including demographic and other) in a
format appropriate to leverage for ML training

DEVELOP

Provide details on models, data source and
collection methodology

DEPLOY

Define the distinction between unfair advice and
personalized advice

DEPLOY

Allow for review by medical ethics commission

DEPLOY

Conduct evidence-based studies to prevent bias

SCALE

Set specific accountability when building solutions
for sensitive groups

SCALE

Require open APIs to allow third-party testing

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

27

Explainability
Decisions and recommendations made by chatbots shall be explainable in a way that can be understood by
their intended users

Trust between patients and healthcare providers is fundamental in this sector.
When using AI in healthcare, understanding how AI systems make recommendations
and decisions is the first step to building that trust. This should be achieved through
communication that can be easily understood by the intended audience.
– Zee Kin Yeong, Assistant Chief Executive (Data Innovation and Protection Group), Infocommunications Media Development Authority of Singapore; Deputy Commissioner, Personal
Data Protection Commission (Singapore)
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Perform stakeholder analysis to determine the level
of explainability expected by users and providers

DEVELOP

Provide and integrate explanations into the
conversation user interface

DEVELOP

State assumptions made, including
user-knowledge level

DEVELOP

Use chatbot unit-testing tools for constant
verification and transparency of chatbot behaviour

DEPLOY

Promote rating of systems based on
testing performed

DEPLOY

Use lay-person terms in patient-facing chatbot
interactions (and provide links to clinical concepts)

SCALE

Create and maintain a list of use cases where an
“unexplainable” black box is unacceptable

SCALE

Determine acceptable range of
performance of chatbot (with deviations
triggering further investigation)

SCALE

Develop classification of levels of explainability (e.g.,
none; technical/clinical; partial/full, etc.)

SCALE

Create standardized queries to interrogate
chatbots, and standardized metrics of confidence

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

28

Integrity
Chatbots shall limit their reasoning and responses to those that are based on reliable, high-quality evidence/
data, ethically sourced data and data collected for clearly defined purpose

In the near future we will have virtual assistants acting as digital twins – a single
point of contact to our digital life. Constantly learning and improving to serve us in our
business and private spheres. Like in most relationships, these intelligent, life-improving
assistants won’t be trusted and adopted if they don’t come with a high level of integrity
– especially in healthcare.
– Jascha Stein, Co-Founder and Chief Executive Officer, OmniBot.ai
Operationalization actions
Stage

Action

Responsible

Code
I

DEVELOP

Implement functional tests to frequently validate the
integrity of the chatbot and that the conversation
behaves as expected

DEVELOP

Be open and transparent about article/data sources

DEPLOY

Build and communicate processes that protect and
handle data, with stakeholders that may interact
with the chatbot

DEPLOY

State intended use of chat logs for training/research

SCALE

Create a curated list of valid sources of data and
reliable medical knowledge repository

II

III

IV

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

29

Inclusiveness
Every effort shall be undertaken to make chatbots accessible to all intended system users, with special
consideration given to identifying and enabling access for potentially excluded or vulnerable groups

Service to every life matters, with all its diversity; [this] is an unreachable aim today
as constraints exist and the effort versus efficacy-funnel effect objectively limits service
to some lives. Comprehensive primary healthcare provider chatbots of tomorrow should
aim to reach the unreachable, speak to the unspeakable of today and not add another
layer of constraint.
– Suresh Munuswamy, Head of Technology Innovations & Health Informatics, Public Health
Foundation of India
Inclusiveness in a national context: India
The value of data should be measured in its
diversity and dimensionality and not by its
database size. India has 22 official scheduled
languages and about 19,500 dialects, some of
them spoken by fewer than 10,000 people with
no written script. Medicine is primarily taught in
English and practised in the regional language.
Primary healthcare is delivered through trained
healthcare workers who extend the service to a
few more dialects. This graded hierarchical model
favours simplicity and structural uniformity over
diversity of the determinants of disease, leading
to a direct treatment-centric approach versus a
diverse prevention-centric approach.
A simple healthcare symptom data point like fever
can possibly be expressed or sourced through
hundreds of words. If one were to attempt
to source and document the cause or actual
determinant of fever as data points – across
languages or dialects – it would be an impossible
challenge in the current ecosystem in India.

Conversational AI systems, or chatbots,
have the potential to address these
limitations by bridging the gap, directly
connecting healthcare professionals to
patients and hopefully preserving or
improving the diversity of conversation
content if, and only if, they are programmed
to value diversity and dimensionality.
Data and resultant service should be
continuously evaluated for its population scale
representativeness and diversity in inputs,
outputs and outcomes. Extra effort should be
directed to expand the reach in terms of diverse
socio demographics and economics. Reaching
comprehensively as in every person on the
planet should be the aim rather than reaching to
the constraint-less creamy layer.
– Suresh Munuswamy, Head of Technology
Innovations & Health Informatics, Public Health
Foundation of India

Operationalization: See the section on general actions

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

30

Conclusion
In this paper, we presented Chatbots RESET,
a framework to govern the responsible use of
chatbots in healthcare applications. The framework
was co-created by the World Economic Forum’s
multistakeholder project community, with
representation from start-ups, large businesses,
academia, governments and civil society. The
diversity of the community and healthy debates
during the creation of the framework have
sharpened the focus of the framework on principles
and actions of broad appeal that have been
carefully curated.
This has led to strong interest from Forum
partners to pilot the framework. The pilots

are designed to test the usability of the
framework, gather feedback for further
improvement and demonstrate its usefulness
with a broad range of organizations and
geographies. As we learn from the pilots,
we will update the framework to capture the
lessons and keep it relevant. We anticipate
sharing updates to this ongoing project in future
publications of the Forum.
We encourage government officials, industry
players, civil society representatives and
academics to join us on this journey to
strengthen our frameworks and ensure their
greater impact.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

31

Appendix: AI and
Healthcare Ethics
Principles
AI ethics principles
Principle
Accountability​

Human agency ​

Transparency​

Explainability​

Reliability,
robustness
and security​

Safety​

Privacy and
data governance

Fairness​

Definition

Criterion

It necessitates that mechanisms be put in place to ensure
responsibility and accountability for AI systems and their
outcomes, both before and after their development,
deployment and use​.

–

Auditability

–

Minimizing and reporting
negative impact

–

Documenting trade-offs

–

Ability to redress

–

Liability

AI systems should support human autonomy and decisionmaking, as prescribed by the principle of respect for human
autonomy. This requires that AI systems should both act as
enablers to a democratic, flourishing and equitable society by
supporting the user’s agency and foster fundamental rights,
and allow for human oversight. ​

–

Fundamental rights

–

Human agency

–

Human oversight

This requirement is closely linked with the principle
of explicability and encompasses transparency of
elements relevant to an AI system: the data, the
system and the business models. ​

–

Traceability

–

Communication

Technical robustness requires that AI systems be developed
with a preventative approach to risks and in a manner
such that they reliably behave as intended while minimizing
unintentional and unexpected harm, and preventing
unacceptable harm. ​

–

Resilience to attack
and security

–

Accuracy

–

Reliability and reproducibility

It must be ensured that the system will do
what it is supposed to do without harming
living beings or the environment. This includes the
minimization of unintended consequences and errors.​

–

Fall-back plan and
general safety

Prevention of harm to privacy also necessitates adequate
data governance that covers the quality and integrity of the
data used, its relevance in light of the domain in which the
AI systems will be deployed, its access protocols and the
capability to process data in a manner that protects privacy.​

–

Respect for privacy and data
protection:

–

Quality and integrity of data

–

Access to data

Bias affects the fairness of an AI solution and refers to a
breach in the performance of AI solutions such that the results
are systematically prejudiced; this is typically introduced
into the system through three forms: Bias in data, Bias in
algorithms, Bias in people.

–

Unfair bias avoidance​

Ability to explain both the technical processes of an AI system
and the related human decisions (e.g. application areas of
a system)​.

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

32

Principle
Diversity

Beneficial AI:
Societal and
environmental
well-being​

Definition

Criterion

We must enable inclusion and diversity throughout the
entire AI system’s life cycle. Besides the consideration and
involvement of all affected stakeholders throughout the
process, this also entails ensuring equal
access through inclusive design processes
as well as equal treatment. ​

–

Accessibility and
universal design

–

Stakeholder participation

The broader society, other sentient beings and the
environment should be also considered as stakeholders
throughout the AI system’s life cycle. Sustainability and
ecological responsibility of AI systems should be encouraged,
and research should be fostered into AI solutions addressing
areas of global concern, such as for instance the Sustainable
Development Goals. Ideally, AI systems should be used to
benefit all human beings, including future generations.​

–

Sustainable and
environmentally
friendly AI

–

Social impact

–

Society and democracy

Source: Derived from the ETHICS GUIDELINES FOR TRUSTWORTHY AI, High-Level Expert Group on AI , European Commission https://ec.europa.eu/digitalsingle-market/en/news/ethics-guidelines-trustworthy-ai

Healthcare ethics principles
Principle

Definition

Criterion

Non-harming or inflicting the least harm possible
to reach a beneficial outcome.

“Do no harm”, safety

Beneficence

An act of charity, mercy and kindness with a
strong connotation of doing good to others
including moral obligation.

Fidelity, cultural understanding, empathy

Health
maximization

Obligation to maximize health in populations.

Cost-effectiveness or cost-utility analyses

Non-maleficence

Efficiency

The use of evidence base and the performance
of cost-benefit analyses to decide what should
be done and how to do it.

Respect for
autonomy

Allowing or enabling patients to make their own
decisions about which healthcare interventions
they will or will not receive.

Informed consent, confidentiality, privacy

Justice

There should be an element of fairness in all
medical decisions: fairness in decisions that
burden and benefit, as well as equal distribution
of scarce resources and new treatments, and
for medical practitioners to uphold applicable
laws and legislation when making choices.

Security, equity

Proportionality

It demands that in weighing and balancing
individual freedom against wider social
goods, considerations will be made in a
proportionate way.

Source: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4196023/

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

33

Contributors
The World Economic Forum’s Chatbot RESET
project is a global, multistakeholder and crossdisciplinary initiative intended to help shape the
adoption of chatbots in healthcare applications.
The project has engaged leaders from
private companies, governments, civil society
organizations and academia to understand

the potential uses of chatbots in healthcare,
identify challenges and define principles
and actions to guide the responsible use
of this technology. The opinions expressed
herein may not correspond with the opinions
of all members and organizations involved in
the project.

Lead authors
Venkataraman Sundareswaran
Mitsubishi Chemical Holdings Project Fellow,
Artificial Intelligence and Machine Learning, World
Economic Forum
Arunima Sarkar
Project Lead, Artificial Intelligence and Machine
Learning, World Economic Forum

Acknowledgements
The World Economic Forum would like to thank
Mitsubishi Chemical Holdings Corporation for its
collaboration in the development of this report.
The framework presented in this white paper was cocreated by many experts and diverse stakeholders
in the World Economic Forum project community
who shared insights and lessons learned, through
interviews and design workshop sessions. The
authors would like to thank the following individuals
for their insights and contributions:

Kristi Ebong, Senior Vice-President, Corporate
Strategy, Orbita
Kay Firth-Butterfield, Head of Artificial
Intelligence and Machine Learning,
World Economic Forum
Kai-Fu Lee, Chairman and Chief Executive Officer,
Sinovation Ventures
Mark Minevich, Executive Chairman, Digital
Pioneers Network

Alan Winfield, Professor of Robot Ethics, University
of the West of England, United Kingdom

Matthew Fenech, Medical Safety Lead, Ada Health

Alexis Smirnov, Co-founder and CTO,
Dialogue, Canada

Nikhil Malhotra, Global Head of Innovation,
Tech Mahindra

Biplav Srivastava, Professor, Computer
Science and Engineering, AI Institute, University
of South Carolina

Nupur Ruchika Kohli, Medical Doctor, Adviser
Specialized Hospital Care, Curator Global Shapers
Amsterdam, The Netherlands

Hadas Bitran, Group Manager, Microsoft
Healthcare Israel

Murali Doraiswamy, Professor of Psychiatry
and Behavioral Sciences, Duke University School
of Medicine

Illah Nourbaksh, K&L Gates Professor of
Ethics and Computational Technologies,
Carnegie Mellon University

Sangeeta Agarawal, Founder and CEO,
Helpsy health

Jascha Stein, Co-Founder and Chief Executive
Officer, OmniBot.ai

Shailesh Kumar, Chief Data Scientist ,
CoE AI/ML, Jio

Jonathon Carr-Brown, Chief Operating Officer,
Your.MD

Sujoy Kar, Chief Medical Information Officer, Apollo
Hospitals Group

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

34

Suresh Munuswamy, Head of Technology
Innovations & Health Informatics, Public Health
Foundation of India

Wen Rui Tan, Manager (AI Governance),
Info-communications Media Development Authority
of Singapore

Tushaar Bhatt, Founder and CEO, Convaise

Zaki Hakim, Adjunct Lecturer, University of Toronto

The authors would like to thank the following panellists for their insights and experiences shared in the
March 2020 panel discussion on “Chatbots for Coronavirus and Beyond”:
Alexis Smirnov, Co-Founder and CTO, Dialogue
Andreea Bodnari, Product Manager Cloud AI
Healthcare & Life Sciences, Google

Jascha Stein, Co-Founder and CEO,
OmniBot.ai
Kristi Ebong, Senior Vice-President, Corporate
Strategy, Orbita

Andrew Le, CEO, Bouy Health
Caroline Hargrove, CTO, Babylon Health

Nikhil Malhotra, Global Head of Innovation,
Tech Mahindra

Hadas Bitran, Group Manager, Microsoft
Healthcare Israel

Varun Jhaveri, Officer on Special Duty to CEO,
National Health Authority, India

Editing and graphics
Elizabeth Mills, Editor
Ann Brady, Editor
Floris Landi, Lead, Publications and
Graphic Design
Jean-Philippe Stanway, Designer
Contact
For questions about this paper or the World
Economic Forum’s work in Artificial Intelligence and
Machine Learning, please contact ai@weforum.org

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

35

Endnotes
1.

“Chatbots for Coronavirus and Beyond” Panel Discussion summary, https://www.linkedin.com/posts/tosundar_chatbotscovid19-ai-activity-6658781912812916736-rMBr (link as of 23/11/20)

2.

Coronavirus Self-Checker https://www.cdc.gov/coronavirus/2019-ncov/symptoms-testing/testing.html (link as of
23/11/20)

3.

Automated Vehicles for Safety https://www.nhtsa.gov/technology-innovation/automated-vehicles (link as of 23/11/20)

4.

Trends in the Use of Telehealth During the Emergence of the COVID-19 Pandemic – United States, January-March 2020,
https://www.cdc.gov/mmwr/volumes/69/wr/mm6943a3.htm (link as of 23/11/20)

5.

https://www.babylonhealth.com/nhs (link as of 23/11/20)

6.

International Medical Device Regulators Forum Final Document “Software as a Medical Device: Possible Framework for
Risk Categorization and Corresponding Considerations” Authoring Group: IMDRF Software as a Medical Device (SaMD)
Working Group

7.

United Nations Convention on the Rights of Persons with Disabilities https://ec.europa.eu/social/main.
jsp?catId=1138&langId=en (link as of 23/11/20)

8.

Know Your Rights: Three Important Federal Laws that Protect People with Disabilities https://hiehelpcenter.org/resources/
know-rights-three-important-federal-laws-protect-people-disabilities/ (link as of 23/11/20)

9.

ISO/IEC 30071-1:2019 Information technology – Development of user interface accessibility https://www.iso.org/obp/
ui/#iso:std:iso-iec:30071:-1:ed-1:v1:en (link as of 23/11/20)

10.

Digital Inclusion, Identity, Trust, and Agency https://standards.ieee.org/industry-connections/diita/index.html (link as of
23/11/20)

11.

The ITU/WHO Focus Group of Artificial Intelligence for Health (FG-AI4H) https://www.itu.int/en/ITU-T/focusgroups/ai4h/
Pages/default.aspx (link as of 23/11/20)

12.

Moons KGM, Altman DG, Reitsma JB, et al. “Transparent reporting of a multivariable prediction model for individual
prognosis or diagnosis (TRIPOD): explanation and elaboration,” Ann Intern Med 2015;162:W1-73. 10.7326/M14-0698

Chatbots RESET A Framework for Governing Responsible Use of Conversational AI in Healthcare

36

The World Economic Forum,
committed to improving
the state of the world, is the
International Organization for
Public-Private Cooperation.
The Forum engages the
foremost political, business
and other leaders of society
to shape global, regional
and industry agendas.

World Economic Forum
91–93 route de la Capite
CH-1223 Cologny/Geneva
Switzerland
Tel.: +41 (0) 22 869 1212
Fax: +41 (0) 22 786 2744
contact@weforum.org
www.weforum.org

From mystery to mastery:
Unlocking the business value
of Artificial Intelligence in the
insurance industry

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 3

1.

Introduction

4

2.

Turbulent times ahead

7

AI is the enabler for disruption

3.	There’s more to AI than robots

11

4.	Lost in translation

17

5.

The road to business value

23

6.

Seize the opportunity

29

Act now

39

AI technology is poised to make a difference

Apply our Use Case Formula to leverage AI in your business

7.

AI creates tangible business value in four distinct ways

Insurers have yet to unlock the full potential of AI

4 |

1.
Introduction
On January 25th, 2017, sixty-eight years after it was published, George Orwell’s book
Nineteen Eighty-Four became the number one best-selling book on Amazon. With its
eerily accurate predictions about advanced technology becoming an integral part of
everyday life, the book’s revived popularity only hints at the massive surge of excitement
surrounding one specific subject: Artificial Intelligence (AI). The numbers tell the same
story: deals to AI startups enabling computers to mimic human intelligence and decision
behavior increased 4.6x, from 150 in 2012 to 698 in 20161. With a combined funding of
USD 4.8B in 20162 and a projected overall spend of USD 47B by 20203, a world in which AI
is prevalent is no longer mere fiction.
However, not all the book’s predictions have come true. Nineteen Eighty-Four painted
a futuristic picture of a totalitarian Big Brother regime that controls every breath of its
inhabitants, but a look at the world today reveals the fundamentally positive impact of AI
on organizations, entire industries, and life itself. Tech giant Google, for instance, has cut
its data center energy usage by 15% by applying its DeepMind AI technology to predict
the incoming computational load and adjusting its power consumption patterns4. In
the financial industry, the world’s largest asset manager, BlackRock Inc., is now entrusting more of its USD 5.1T in assets to robot stock pickers to decide what to buy and sell
instead of human portfolio managers. And car manufacturer Tesla has achieved a 40%
drop in road accidents with a new AI feature deployed in its cars, compared to models
without the feature5. Apart from organizations, our own homes are also getting more intelligent. In 2017, 35.6M people in the US will have used a voice-activated assistant device
at least once a month. That’s a jump of almost 130% over 20166. And we expect numbers
in Europe and Asia to follow this trend as well.
With more than 85% of customer interactions predicted to be managed without a human by 20207, the evident business value of AI is attracting the attention of CEOs and
top managers regardless of industry. That said, while almost all industries have already
seen major success with AI or have started investing, some are lagging behind. With only
1.33% of insurance companies investing in AI compared to 32% in software and internet
technologies, the insurance industry is still lagging behind the world’s AI movement8.
Executives in insurance, however, are experiencing a particular sense of urgency to act: in
few other industries is AI’s underlying sustenance – data – as abundant and important as
in the insurance industry – making the mastery of this area of technology a key competitive differentiator for insurers moving forward into the digital future.
But how can executives guide their organizations to reap the benefits from AI and exploit
the benefits that beckon? To enable businesses to move from mystery to mastery, this
paper presents a set of tools and frameworks that provide key insights into the fundamentals of AI technology, including the inner workings of Machine Learning. We will
cover the art and science of defining concrete AI use cases and the models of AI value
creation to deliver top- and bottom-line impact. Moreover, we aim to give an insight into
the current status of and future outlook for AI use cases that will impact the 300-year-old
insurance industry as it rapidly approaches disruptive transformation.

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 5

4.6x

projected
overall spend of

increase of
AI deals from
2012 to 2016

$ 47B
by 2020

35.6M
people in the
US will have a
voice-activated
assistant in
2017

1.33%

of insurance companies
invest in AI in 2016

With a combined funding of USD
4.8B in 2016, a world in which AI is
prevalent is no longer mere fiction.

6 |

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 7

2.
AI is the enabler for
disruption
2.1 The future is now – technology-enabled
innovation in insurance
Deterioration of traditional profit pools, changing competitor landscapes, technological innovations and changing customer expectations are calling for a new perspective
on insurance. However, traditional insurance players are often slow to react to the new
demands and possibilities of the digital era. This opens up the playing field to InsurTech
startups and Tech incumbents who capitalize on new business models and improved customer experiences. The new players promise and deliver faster claim payments, greater
price transparency and on-demand policies, while decreasing the cost and the resources
required (Figure 1). Global VC funding of these new businesses totaled USD 1.7B across
173 deals in 2016 as compared to USD 0.14B among 28 deals in 2011 – clearly increasing
the seriousness of competition for traditional insurance players (Figure 1).9
At the heart of the rise of these new players is the improvement in existing digital technologies and the emergence of new ones. These technologies enable practitioners to
rethink the 300-year-old insurance industry at every level, from business model to value
chain and from customer interaction to automation. In reshaping the industry’s datahungry and mostly standardized processes as well as the resource-intensive handling
of their customer relationships, one technology stands out as particularly promising for
insurers: Artificial Intelligence. Ninety-five percent of insurance executives intend to start
or continue investing in AI capabilities in the future10, while investments in AI have already
increased by 69% between 2011 and 2014, totaling a staggering USD 5B in 2014.11

Ninety-eight percent of insurance
executives believe that cognitive
computing will play a disruptive
role in the insurance industry.

8 |

Figure 1: Value proposition and Venture Capital funding of selected InsurTech startups

Clover

Fabric

Funding: USD 285M

Funding: USD 2.5M

HQ: San Francisco, CA, USA

HQ: Brooklyn, NY, USA

A health insurance startup that leverages AI by

A life insurance startup that uses AI to generate

using data and software to build clinical profiles

quotes for accidental death claims. Simpli-

of people, and identify gaps in care. These gaps

fied processes enable a life insurance sign-up

in care are filled with visits and a free choice of

process in just two minutes. The browser-based

doctor to avoid costly hospital stays.

product is available on tablet, notebook, and
mobile.

GetSafe

Trov

Funding: Undisclosed

Funding: USD 87M

HQ: Heidelberg, Germany

HQ: San Francisco, CA, USA

An InsurTech startup that takes advantage of

An on-demand property insurance startup in

AI by advising customers on which insurance

which an AI chatbot handles claims. Insurance

policies to purchase and by collecting relevant

can be started immediately via an app to cover

information. GetSafe also provides customers

damage, loss, and theft. Like dating apps, cus-

with an app to manage all their insurance poli-

tomers can swipe insurance on their valuables

cies in one place.

on or off.

Lemonade
Funding: USD 60M
HQ: New York, NY, USA
A property and casualty peer to peer insurance
that uses AI-powered claims analysis. Eighteen
anti-fraud algorithms are run on image and
video claims information from the customer
and a response is given to within minutes.
At Lemonade, small insurance groups pay a
fixed fee into a claims pool. Surpluses from the
claims pool go to chosen common causes or
back to the peers.

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 9

2.2 Data is the new currency –
the importance of AI to win in a
“datafied” world
One key reason why AI will prove to be crucial is the everincreasing “datafication” of business interactions, private life,
and public life. In the age of digitalization, more and more
data is being collected – by organizations, governments,
households, and individuals. Already today, 2.5M terabytes of

The success of using data to one’s advantage therefore
depends on the appropriate tools to process it – and this is
where AI comes in. By intelligently and independently sifting
through huge amounts of data with powerful algorithms, AI
goes beyond “just” data analytics in generating novel insights
and automating repetitive tasks. AI thereby offers great opportunities for optimizing existing and enabling new procedures, giving a competitive edge to businesses today and in
the future.

data are created every day.13 That quantity is due to increase

However, understanding how AI and more specifically Ma-

exponentially on account of new sources like sensors on

chine Learning turns data into knowledge and action and how

property and machines, connected devices, mobile devices,

this relates to hyped topics, such as chatbots or robo-advi-

and digitalization of processes, together with customers’ in-

sors, can be challenging. To be able to leverage AI effectively,

creasing willingness to share personal information – provided

organizations need to combine a technological view with a

they receive a benefit in return (Figure 2).14

business view – in other words, an understanding of what is

An “arms race” has begun as to who can gather the most data
in an effort to achieve a competitive advantage. However, the

technologically feasible today with an understanding of how
AI can create value for an organization.

data must be meaningfully processed to access its value, as
illustrated by the following quote of a Chief Underwriting
Officer (EMEA): “We have a lot of data but the question
remains what to do with it.”15

Figure 2: Proportion of customers who would be willing to track their
behavior and share this data with insurers for a more accurate premium

49%

11%

45%

17%

38%

14%

No

Don‘t know

40%

38%

48%

HEALTH

HOME

MOTOR

Yes

10 |

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 11

3.
AI technology is poised
to make a difference
3.1 Don’t believe the sci-fi movies –
where AI stands today
Recent quotes like the one by Stephen Hawking below are often used as eye-catchers,
but tend to portray a negative picture of an almighty AI. Despite the speed of AI developments today, the current status of the field by no means resembles a villainous, freewilled AI as portrayed in science fiction movies. Instead, AI has slowly but steadily begun
to affect our lives by facilitating many of our everyday tasks and activities, from scheduling meetings, to selecting movies, to driving cars.
The type of AI often feared in the media is referred to as Artificial Superintelligence
(ASI) – an omnipotent AI that significantly outperforms its human counterparts in any
area or task. In contrast, an AI matching humans’ capabilities and ways of thinking is
referred to as Artificial General Intelligence (AGI). However, both ASI and AGI are still a
long way from today’s reality.

STEPHEN

HAWKING

The rise of powerful AI will
be either the best or the worst
thing ever to happen to humanity.
We do not know which.

12 |

Figure 3: How smart is Artificial Intelligence?17

INTELLIGENCE

Superintelligence

ASI

Narrow level

ANI

Today

General/human level

AGI

2030

2060

EMERGENCE

The state of AI feasible today is referred to as Artificial

the human champion of the Chinese board game Go. Yet, the

Narrow Intelligence (ANI) (Figure 3). ANI has seen a recent

same ANI would be unable to learn how to play chess, even

boom in applications – enabled predominantly by exponential

though chess is significantly less complex than the game of

increases in available processing power paired with advances

Go.16

in gathering, storing, and transferring data. However, each
ANI is still only able to complete very narrow, clearly defined
tasks within one application area and unable to build on its

Underlying the applications of ANI are a multitude of instruments rooted in data science – the most important one being
Machine Learning (ML). In contrast to traditional program-

capabilities to learn tasks outside this application area. In a
specific task, however, an ANI can become as good as or better than any human – as illustrated by Google’s ANI that beat

ming and data science, an ML algorithm learns the transformation rules to create a desired output based on a given
input by itself (Figure 4).

Figure 4: Difference between traditional programming and machine learning

Input

&

Transformation
rules

Output

Input

&

Output

Transformation
rules

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 13

Figure 5: Machine learning training types18+19

Supervised
Learning

Unsupervised
Learning

Reinforcement
Learning

is a method of training an algo-

is a method of training an algo-

is a method of training an

rithm on labeled data, meaning

rithm on unlabeled data, meaning

algorithm on unlabeled data

that parts of a document are

the algorithm identifies concepts

where the output is rated by the

“tagged”. For example, in a con-

and derives conclusions entirely

researcher as either correct or in-

tract, items like the contract par-

on its own. For example, in the

correct. Over time, the algorithm

ties’ street, postal code, etc. may

Google Brain project an algorithm

can produce the correct output

be tagged to enable the algorithm

learned to detect faces and cats

based on learning from reinforce-

to identify addresses in contracts

in videos by clustering common

ment of correct answers and pun-

by knowing which elements con-

structures (without knowing the

ishment for incorrect ones. For

stitute an address and where to

definition of a cat or face prior to

example, an algorithm learned

look for them. Supervised learn-

analysis).

to play Super Mario by reinforce-

ing is the most common method

ment, being punished for “dying”

for training ML algorithms today.

in the game and reinforced for
“surviving” and collecting points.

The ML algorithm learns the transformation rules by itera-

A human would require no more than a handful of pictures

tively reducing the deviation between the created and the

to learn the same skill. However, unlike humans, ML is un-

desired output – a process referred to as training. While

able to make abstractions and inferences from only a few

different forms of training exist, today’s most prominent form

data points. Over time, as the amount of data processed by

of training is the so-called supervised learning – a method

the algorithm increases, predictions become more accurate.

requiring large amounts of labeled data to train the algorithm

Facebook’s “Deep Face” can identify a specific face with 97%

(Figure 5).

accuracy, which equals human ability.

Facebook’s facial recognition for example, an image recognition ML algorithm20 independently learned how to identify
and detect human faces. This nine-layer neural network was
trained with four million images of faces of 4,000 individuals, meaning the algorithm was exposed to 1,000 different
pictures of each person in order to enable the algorithm to
be able to reliably identify the specific person among others.

14 |

3.2 Machine Learning is not the
full picture – understanding AI
applications
Typically, an AI can either be trained from scratch using a
Machine Learning framework, such as Google’s TensorFlow
or Apache Spark’s MLlib, or be bought as a pre-trained model.
These pre-trained models are typically specialized in a certain
area such as voice or image and video recognition, text analytics, biometrics, sentiment detection or decision management (Figure 6).

a wearable such as headphones. It will soon be possible to
integrate Alexa into any external app or website to enhance
it with a natural language layer. Capital One has built a skill
for Alexa, which can be activated in the Alexa app, to enable transactions and account balance checks using voice
commands. A customer can ask Alexa about their account
balance (speech input); Alexa then analyzes the intent of the
question, connects to the customer’s Capital One account
(database) to retrieve the account balance, and presents the
answer in spoken natural language to the customer (speech
output) (Figure 7). An AI application includes not just the
underlying ML algorithm but also retrieving relevant informa-

For example, Amazon’s Alexa is already trained to understand

tion from a database and presenting it through an interface

several languages. Companies can build certain “Alexa skills”

to the user.

that are then integrated in the Alexa app, Amazon Echo, or

Figure 6: Exemplary list of pre-trained AI models

1.
Speech/voice
recognition

2.
Sentiment
detection

3.
Recommendation
engine

Understand and interpret the

Detect and analyze emotions in

Interpret results and recommend

spoken word. For example, Pop

the written or spoken word. For

appropriate actions. For example,

Up Archive automatically tags, in-

example, Beyond Verbal e
 xtracts

Zendrive uses sensor data on

dexes and transcribes a
 udio files

emotions from raw voice clips in

smart­phones to analyze driving b
 e-

with speaker d
 etection.

real time.

havior and re-commend behavior
changes.

4.
Text analytics
and NLP

5.
Pattern/
anomaly detection

6.
Automatic decision
management

Understand and interpret written

Detect patterns and anomalies to

Derive and automatically apply

text and whole documents. For

derive conclusions. For example,

rules and logic in AI systems. For

example, x.ai’s personal assistant

Cyence models cyber riskbased

example, Boomtrain enables auto-

Amy“reads” emails to find sched-

on human and m
 achine data

matic,personalized marketing b
y

uling related information to b
 ook

cluster analysis.

targeting only s pecific customers.

meetings in t he user’s c alendar.

4.
Natural Language
Generation (NLG)

5.
Object detection

6.
Biometrics

Interpret and analyze objects

Uniquely identify a person or

Generate and deliver information

shown on images or in video. For

measure and interpret human

in natural language. For example,

example, Nauto uses video analy-

physical states (such as emotion or

Narrative Science offers easy-to-

sis to offer guidanceand insights

intent) from biological character-

understand reports in n
 atural

to driversin real time.

istics, such as f acial structures or

language b
 ased on c ompany d
 ata.

heart rate. Forexample, Bionym
enables authentication through
cardiac rhythm measured by a
wristband.

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 15

Figure 7: Simplified demonstration of communication transfer between AI and users

User

Text
Images
Speech
etc.

Text
Images
Speech
etc.

App
Website
Robot
etc.

AI application

Cloud solution,
Company server
etc.

Machine Learning Models

Speech/voice recognition

Sentiment detection

Text Analytics/
Natural Language Processing

Pattern/anomaly
detection

Natural Language
Generation

Recommendation engine

Object recognition

etc.

16 |

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 17

4.
Apply our Use Case Formula to
leverage AI in your business
4.1 Know what you want – the importance of
defining a specific use case
The wide array of possible AI applications is appealing for businesses, yet the extremely
narrow nature of today’s AI applications is severely complicating the identification of a
relevant starting point. As a result, almost 40% of practitioners who have not yet invested
in AI don’t know what AI can be used for in their business.22 In order to exploit relevant
opportunities, businesses must face the challenge of identifying concrete use cases for AI.
In contrast to the purely technological view on AI applications, which focuses on the ML
model and interface (see Figure 7), an AI use case describes the underlying technology
and observable functionality from the perspective of the user and the resulting value. A
use case hence represents the business view of an AI application.

Almost 40% of practitioners
who have not yet invested in
AI don’t know what AI can be
used for in their business.

18 |

4.2 Apply the magic Formula –
the three dimensions of a Use
Case Formula

Using these three dimensions, a use case can be described

We reviewed publicly available information on nearly 200 AI

First and foremost, a use case must serve a functionality.

vendors of ML models and AI applications and categorized

The spectrum of possible functionalities includes an inform-

them according to their predominantly deployed ML models

ing, recommending or deciding AI solution in a business

(Figure 8). By determining the fundamental characteristics

process. Within each of these categories, many applications

that differentiate one use case from another, we have identi-

are already feasible today, but even more will emerge as AI

fied three distinctive dimensions. These dimensions encom-

technology continues to develop.

based on our Use Case Formula: Do x (functionality) based on
y (data) in order to address z (need). Figure 9 describes the
formula with the content of each part in detail.

pass the functionality, input data and addressed customer
need of an AI application.

Leading Vendors

Figure 8: Dataset of 187 leading AI vendors categorized according to predominantly deployed ML models

VENDORS OF SINGLE ML MODELS OR APPLICATIONS
BASED ON SINGLE ML MODEL
Speech/voice recognition
Sentiment detection

3
1

Recommendation engine

31

Text analytics and NLP

26
27

Pattern/anomaly detection
19

Automatic decision management
Natural Language Generation

4
19

Object detection
Biometrics

4

VENDORS OF COMBINED ML MODELS OR APPLICATIONS
BASED ON COMBINED ML MODELS
Two dominant ML Models
Three dominant ML Models

43
10

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 19

In performing any of these functionalities, the AI application

In order to derive tangible business value from a use case, a

can either serve an internal customer, such as an employee,

third ingredient must be included in the Formula: customer

or an external customer, such as an end user. For example,

and organizational needs. A use case must always address

the KLM chatbot on Facebook Messenger informs external

and resolve a user need in order to create value.

customers about their flight status or issues their boarding
pass. In contrast, KIA uses sentiment analysis of customer
feedback on social media sites to inform internal marketing employees of the success or failure of an advertising
campaign.

Organizational needs can include becoming more efficient or
keeping knowledge within the company. (External) customer
needs refer to, for example, more personalized communication with customer service agents or receiving an affordable
product price.

The second ingredient necessary to define a use case is data.
Without high quantity and quality of data, it is impossible
to get an AI application up and running. In fact, for modern
data-hungry ML models, performance is more dependent on
the quantity and quality of the data than the algorithm doing
the learning, partially because many ML models are available
as open source, such as Google’s TensorFlow or the popular
Scikit-learn.

To define concrete use cases for AI, all three ingredients need
to be combined into the Use Case Formula, thereby organizing all crucial characteristics and information about a use case
in a relevant structure. The categories of possible “input values” provided for each ingredient of the Formula outline the
comprehensive playing field for AI applications, whereas the
examples given in each category may need to be extended
and adjusted to a specific industry. Practitioners can analyze

There are four data types that can act as input for a ML

existing case studies or brainstorm novel use cases by refer-

model: company data, such as call logs or manuals; public

ring to the Use Case Formula’s three dimensions and possible

data, such as Wikipedia or other open source content free to

“input values” in each dimension.

use without permission; third-party data, such as Facebook
user profiles, which can’t be used without permission; and
lastly customer data, such as data gathered through sensors
at home or in a car.

For analysis purposes, the Use Case Formula can be applied
to abstract an underlying AI use case from e.g., a media report about a business successfully leveraging AI by synthesizing the most relevant information into a pointed use case

Depending on the data type used, time and effort must go
into preparing and cleaning appropriate training sets. AI applications are not limited to run on structured data but can
also process unstructured data, which accounts for about
80% of today’s data.24 For example, Facebook leverages a
combination of structured user profile data and unstructured
public data, including e.g., news sources, to help eliminate
fake news.25

definition.
For ideation purposes, practitioners can play through different meaningful combinations of the “input values”, keeping in
mind that not all combinations are feasible or valuable today
and interdisciplinary expert advice should be sought in the
process.
Once a valuable use case is identified, it can be effectively
communicated using the Use Case Formula.

PETER

NORVIG

|

RESEARCH

DIRECTOR

AT

GOOGLE

We don’t have better algorithms,
we just have more data.

20 |

Figure 9: The Use Case Formula and its three dimensions

“Do x (functionality) based on y (data)
For example, Spotify recommends new artists and songs (functionality)
based on previously played genres/artists (data) in order to make
relevant content easily and quickly accessible to users (need).

Functionality

Input data

Inform

Recommend

Decide

Company

Public

Provide insights based

Advice on appropriate

Perform actions based on

Product literature

Wikipedia

on one or a combination

actions based on insights

derived implications from

of several data sources

generated from pro-

processed data sources

Manuals/specifications

Industry definitions

(manuals, sales data, etc.)

cessed data sources (user

(user behavior, customer

Guidelines

Open source content

behavior, public historic

emails, voice commands,

data, etc.)

etc.)

Procedures/policies

Regulatory guidelines

cluster customer seg-

Common recommend-

Common deciding ap-

Journals

Images

ments, evaluate customer

ing applications are e.g.,

plications are e.g., adjust

Best practices

Type systems

behavior, predict disease

suggest new products

prices in real time, send

outbreak, etc.

to customers, prescribe

emails to customers,

FAQs

etc.

medical treatment plans

schedule meetings, trans-

for patients, guide cus-

fer money, etc.

Common informing applications are e.g.,

tomers through the product selection process, etc.

Certification tests
Brochures
Call logs
Images
Client profiles
Usage data
Sensor data
etc.

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 21

in order to address z (need).”

Customer and
organizational needs
Third-party

Customer

Product

Communi­cation Operations

Subject matter domain

Feedback data

Offering

Channel

Processes

Smart home data

Adequate to fulfill needs

Reachable through any

Efficient use of time and

of customers

channel

money

Personalized to individual

Continuous interaction

Accurate processes with-

customers

across channels

out mistakes

Available at any given

etc.

Engaging tasks for em-

content
News feeds
Market data
Business reports
Industry intelligence
Risk analysis
assessments
Industry definitions
Images
Taxonomy/ontology
Market data
etc.

Driving behavior
Fitness/health records
Sensor data
etc.

time
etc.

Touchpoints
Trustworthy customer

ployee
etc.

Cost

service

Resources

Affordable product price

Helpful problem-solving

Effective resolving of

and solution recommen-

problems and fulfilling

dations

tasks

Personal service and

Knowledgeable employ-

conversations

ees

Timely responses and

etc.

Transparent cost structure
etc.
Convenience
Intuitive interface and
use of product
Flexible use rates of
product
Private use of data
Fast product delivery
etc.

help
etc.

22 |

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 23

5.
AI creates tangible business
value in four distinct ways
5.1 Know where to start – four models
of value creation for AI
The Use Case Formula provides the required generic structure for defining and describing AI use cases. However, it does not specify where to begin looking for use cases or
the context in which to place a use case. Having reviewed close to 200 vendors and their
clients, we found that a comprehensive overview of the AI value landscape in a business
context can be achieved by mapping AI use cases in a 2x2 matrix according to their type
of result and type of impact. This overview clusters use cases based on the type of value
created and can serve to define a starting point and context (Figure 10) for AI use cases.

ANDREW

NG

|

FOUNDER

OF

GOOGLE

BRAIN

Even though a lot of the buzz in AI has
been around large tech companies,
if you look across an entire economy,
really any Fortune 500 company can
create a lot of value with AI as well.

24 |

Figure 10: The AI value map – four areas of value creation

TYPE OF RESULT
Primary output of the AI solution, e.g., answer or action

Unknown
Solution generates a
new result previously
not known to exist

Known
Solution replicates one
(or several) results from
a set of predetermined
results

Operations discovery

Customer discovery

Ability to discover novel answers
or actions that serve to improve
speed and accuracy of internal
processes/decisions

Ability to discover novel answers
or actions that serve to improve
customer conversion and loyalty

Examples: Process mining,
scenario planning

Examples: Market segmentation,
product development

Operations efficacy

Customer efficacy

Ability to produce intended
answers or actions that serve to
improve speed and accuracy of
internal processes/decisions

Ability to produce intended
answers or actions that serve to
improve customer conversion
and loyalty

Examples: Customer service
automation, fraud detection

Examples: Recommendation
systems, pricing automation

Bottom-line
Cost reductions

Top-line
Revenue Increases

TYPE OF IMPACT
Primary intended ﬁnancial contribution of the AI solution

Eﬃcacy = ability to produce a desired or intended result
Top-line = measure of revenue or gross sales
Bottom-line = measure of net income

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 25

The type of result axis distinguishes whether an AI applica-

and reliability improvements. Top-line improvements are real-

tion produces unknown or known results. The replication of

ized through increases in revenue, such as reduced customer

known results matches input with a predetermined set of

churn or improved customer lifetime value. Combining the

answers or actions, e.g., a chatbot tries to understand the

two dimensions, four areas for value creation with AI emerge.

intent of the customer and retrieves the most appropriate
of a set of existing answers from a database. On the other
hand, the generation of previously unknown results creates
new insights and answers. These generative algorithms are
capable of things such as identifying new criteria based on
which they can more effectively segment customer groups or

Notably, many real-world use cases can create value in more
than one of the four value areas – particularly if they evolve
over time. The categorization into the four areas, however,
helps to define a clear starting point as well as to sharpen the
focus for the primary intent of an AI application.

design completely new products based on only a few given
parameters.
The type of impact axis distinguishes whether an AI application delivers financial impact on the top or bottom line.
Bottom-line improvements are achieved through reduced
costs, e.g., reduced need for highly qualified staff or quality

JUERGEN

MUELLER

|

CIO

OF

SAP

Recruiters spend 60% of their
time reading CVs. Why should
a person read 300 resumes if a
machine can propose the top 10?

26 |

5.2 Operations efficacy
AI applications reproducing known results with the primary intention to create bottomline impact enable operations efficacy, e.g., by automating repetitive tasks, identifying
fraudulent behavior, or making knowledge accessible in real-time. For example, JP Morgan has been able to save 360,000 work hours by automating part of their lawyer work by
leveraging an operations efficacy application.

JP Morgan
Machine Learning System COIN autonomously reviews and interprets commercial loan agreements.
Value: Saved 360,000 hours of lawyers’ and loan officers’ work with
an AI that needed only seconds for the same work and decreases
susceptibility to error.

5.3 Customer efficacy
Top-line impact can be delivered through AI applications targeted at customer efficacy
by employing AI to perform actions that are known to increase customer satisfaction and
consequently conversion and loyalty. Such tasks include in particular sales, marketing,
and pricing automation, as illustrated by the case study of The North Face.

The North Face
The North Face leverages IBM’s Watson as a personal shopping
assistant in their online shop to recommend products in natural
language based on qualifying questions about location, temperature,
etc.
Value: The solution supports customers in finding the right product
in a fast and convenient manner and has yielded an increase in clickthrough rates to 60%.

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 27

5.4 Operations discovery
AI applications for the generation of unknown results with the primary intention to deliver bottom-line impact enable operations discovery. Such applications may e.g., discover
new insights about process inefficiencies or cost drivers and serve to increase quality or
time and cost effectiveness. By implementing such a maintenance expert system, Korean
Air has been able to reduce maintenance lead times by 90%.

Korean Air
An expert system that discovers previously hidden relationships and
interdependencies in maintenance issues to solve problems faster.
Value: Reduced the number of flight delays and cancellations
caused by maintenance issues and shortened its maintenance lead
times by 90%.

5.5 Customer discovery
Alternatively, top-line impact can be delivered through AI applications targeted at customer discovery, i.e., discovering new insights, actions or products that serve to strengthen the relationship with the existing customer or the acquisition of new customers. Such
tasks can include e.g., the discovery of novel criteria for successful customer segmentation or data-driven product development, as illustrated by the case study of Netflix.

Netflix
Netflix leverages AI to tag its movie content, allowing for a detailed
analysis of play-and-search data, giving Netflix an understanding of
what a user really likes about a movie. Netflix combines the usage
data with metadata and ecosystem data, such as what is trending
in the news, in order to identify success criteria for original content
production.
Value: Netflix’s original content rates 11% higher than its licensed
content, yielding increased customer engagement and reduced
churn.

28 |

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 29

6.
Insurers have yet to unlock
the full potential of AI
In view of the fact that businesses across industries are reaping real benefits from AI,
it is uncontested that AI will also be a key enabler for insurance businesses to equip
themselves for the challenges and opportunities of their digital future. High labor costs,
regulatory requirements, intensifying competition with InsurTech startups, and changing
customer expectations are just some of the issues that can be addressed by leveraging
AI. Yet, the insurance industry is lagging behind tapping into AI’s potential compared to
other industries such as life sciences, retail, and manufacturing.26 By looking at the insurance value chain (see Figure 11), it becomes clear that there are numerous entry points
and possibilities. In order to obtain an overview of the industry’s current status quo and
put one’s own efforts as well as future value options into perspective, we analyzed the
insurance industry based on our value framework in the upcoming chapter.

The insurance industry is lagging
behind tapping into AI’s potential
compared to other industries such as
life sciences, retail, and manufacturing.

30 |

Figure 11: Potential AI use cases for the insurance value chain
(illustrative selected examples, technology’s state of the art could be less advanced)

Insurance
Value Chain

Product
development

Marketing &
sales

Underwriting &
risk-rating

Text analytics & NLP

Scan and structure existing policies and product descriptions to
develop future products faster
and more efficiently

Analyze customer feedback based
on support calls & social media
posts to develop new marketing
campaigns

Scan for ambiguities and rate
risks in insurance applications
based on claims to detect fraud
faster

Pattern/Anomaly
detection

Derive patterns for new products
based on customer feedback
analysis in to create more targeted products

Identify customer segments for
personalization based on cluster
analysis of feedback to target
specific customer groups

Predict premiums based on past
risk assessments to make risk
assessment more precise

Recommendation
engine

Analyze customer buying behavior based on sales data following
product recommendations to
improve offerings

Provide new sales leads based
on social media and purchase
data in order to gain new customers

Suggest risk categories for customers based on previous claims
and events to prevent human
errors

Conversational
service solutions

Use feedback data based on customer conversations with virtual
service agent to improve future
products

Use virtual sales agent to consult
and accompany the sales process
in order to achieve higher sales
targets

Use a chatbot to lead risk related
conversations based on natural
language data base to obtain
specific facts from customers

Speech
recognition

Identify customer pain points
with products through speech
analytics of feedback to improve
future products

Analyze customer speech – emotion detection included - based
on lead calls in order to improve
personalization

Detect fraud based on voice
analysis of customer calls in order
to improve security measures

Image and video
analysis & facial
recognition

Analyze customer emotions and
reactions based on customer
interview videos to test new
product

Recommend marketing messages
for promotion videos based on
public video analysis to improve
marketing effectiveness

Predict risk based on image/vide
analysis to improve accuracy

Natural language
generation

Generate new policy descriptions
for new products based on past
documents to increase operations efficiency

Generate marketing messages in
various styles based on public advertisements to improve targeting

Generate reports and written
documents based on internal
reports to improve operations
efficiency

Potential use case: Lead management

Potential use case: Fraud analytics

AI can provide support in pointing out positive leads to both

Annual fraud-related costs in the insurance industry are

sales and marketing by pulling value from respective data and

estimated to add up to billions of euros and an estimated

indicating potential quality leads that might not have been

amount of 10% of their overall claims expenditure. AI can

considered before. Insurance firms can achieve competi-

be used to, e.g., assist insurers during the claims handling

tive advantage by additionally enriching internal data with

process by querying the asserted events of an incident: If a

a multitude of sources e.g., information from social media

driver involved in a car accident indicates it was raining at the

campaigns or weblog clickstreams. AI can also deliver ad-

time of the event, negatively impacting his braking distance,

ditional value in offering personalized content by predicting

AI can check weather reports to confirm whether this was the

e.g., potential spend, suggested campaigns, and personalized

case. If a policyholder’s assumed conditions are invalidated

products for cross- and upsell to maximize sales and new

by the AI’s searches, this indicates that the claim could be

revenue. The personalized lead interaction with AI can also

fraudulent; the system can thus appropriately flag the case

be applied in call centers.

for further investigation by a human insurance agent.

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 31

Customer
servicing

Claims
management

Financial
assets

Operations

Generate structured feedback
reports based on customer
support requests to save time
& costs

Generate structured data sets
based on claims reports to process claims faster

Detect opportunities and issues
early on based on analysis of news
and social media data to improve
timely reactions

Transform process descriptions
into structured data that can be
analyzed to track effectiveness of
processes

Provide automatic matching of answers to customer inquiries based
on past customers questions in
order to automate service

Validate claims by checking
external data sources e.g., weather
reports for validation to save costs
of wrong evaluations

Integrate robo advisory into portfolio decisions based on automatic
market analysis to detect market
anomalies faster

Predict process workload based
on historical data in order to
prevent bottlenecks

Provide issue solving recommendations to customers based
on clustered historical service
data to save time

Recommend template for incoming claims based oWn historical
similar claim reports to process
claims more effectively

Integrate robo advisory into portfolio decisions based on automatic
market analysis to recommend
financial actions and decisions

Recommend suitable candidates
based on social network data
to improve future recruiting
purposes

Integrate virtual agent onto
customer service platform based
on product and service data to
improve customer experience

Provide chatbot interface for
claims reporting based on natural
language base and historical claims
data in order to improve efficiency

Supply internal, digital personal assistant based on financial data for
employees to look up specifics

Supply internal, digital personal assistant for scheduling of meetings
to improve internal efficiency

Improve customer communication directly based on emotion
detection during customer calls to
improve service

Automatic text production of
speech claims including emotions
and behavior based on phone calls
in order to improve efficiency

Analyze important investor calls of
financial asset providers to detect
potential issues early on

Produce automatic protocols
and key results based on spoken
word during meetings in order to
improve efficiency

Analyze customer emotions during service calls based live audio
feeds to improve communication
with customers

Define damage severity based on
photos taken by customers during
claims reports to improve speed
and convenience

Inform market forecasting by
evaluation news reports based
on public sources to improve
forecasting accuracy

Warn employees of workplace incidents based on security camera
feeds to improve workplace safety

Generate customer emails as
responses based structured input
data to optimize workload for
employees

Generate automatic claim reports
based on structured data to optimize workload for employees

Generate investor financial
reports based on structured
input to optimize workload and
consistency

Generate internal company
information material and blog
entries based on structured input
to increase transparency

Potential use case: Automated input management

Potential use case: Intelligent virtual assistants

One use case to use the steadily growing amount of available

Companies have used chatbots for customer service for a

data for insurance companies is automated input manage-

number of years, typically to replace or assist live agents in

ment, including input recognition, clustering and routing.

call centers for first level support or as an alternative to point-

By using Artificial Intelligence, it is possible to design a more

and-click interfaces for customers visiting websites. With the

efficient input management that avoids subsequent manual

advances in Natural Language Understanding and Processing,

work and that is able to learn from previous events. The new

this technology is now capable of being used in more complex

kind of input management can be divided into three major

customer communication, providing more and more natural

steps: Data Analysis – Data Clustering – Routing. These enable

conversations which rarely differ from those with humans.

the customer interface to route each issue to the right inter-

However, as long as there is no Artificial General Intelligence,

nal contact or solution provider.

customers will be routed to real people when it comes to
more complex issues.

32 |

6.1 The current status – insurers
leverage AI for improved efficacy
Recent efforts by insurers to implement AI have been focused
on automating repetitive tasks by using AI applications to
replicate known answers, yielding both top- and bottom line
improvements. Consequently, leading insurers are already
achieving operations and customer efficacy through the application of AI.

EXHIBIT

1:

FUKOKU

LIFE

In February 2017, Japanese Life insurer Fukoku
Mutual Life announced that it has introduced an AI
application based on IBM’s Watson explorer to boost
its operations efficacy in medical claims processing.

Operations efficacy

The application is tasked with calculating accurate

Among the most mature applications of AI in insurance is the

pay-outs based on details of the administered

automation of claims handling processes (Figure 12).

procedure, period of hospitalization, medical history

Traditionally, the claims processing department is the most
labor intensive and therefore the largest cost center for insurers. Meanwhile, work in claims processing is highly standardized and repetitive – and therefore extremely eligible for
automation through AI. Consequently, there are both great
incentives and high feasibility for insurers to automate claims
processing. Typically, insurers’ legacy systems are already

and insurance conditions. The application thus accesses medical certificates, hospital bills, and internal
claims history files and scans the insurance contract
for special coverage clauses to prevent payment
oversights. The accurate pay-out is calculated and is
submitted to a member of staff who approves and
then releases the pay-out.

capable of (partially) automated processes in quotation, con-

The application increased productivity by 30% and

tract, and claims. However, the modern AI applications can

yielded improvements in accuracy of pay-outs.

improve content recognition, prioritize more intelligently, and

Fukoku Life expects to see a return on investment in

even increase customer satisfaction by significantly reducing

under two years, realized through annual savings of

response time.

JPY 140M. Additionally, the insurer aims to increase

The Case Study of Fukoku Mutual Life (Exhibit 1) serves to
illustrate how insurers can reap these benefits from the ap-

customer satisfaction scores through reduced lead
time of pay-outs.

plication of AI in claims processing.

Figure 12: Selected case studies of AI-enabled
claims automation

Basler Versicherungen
use AI to automate parts of their glass damage claims processing including payment transactions.

Zurich UK
have launched a pilot AI application to automate their injury
claims processing.

UNDERLYING

USE

CASE:

“Derive and calculate pay-outs based on company
data (historic claims files, contracts) and third party
data (medical certificates and hospital bills) in order
to achieve accurate and efficient claims processing
and fast pay-outs.”

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 33

Customer efficacy
Other currently popular AI applications for efficacy gains include
customer self-service solutions like biometrics authorizations
and even more prominently chatbots – as a variety of existing
insurance case studies demonstrate (Figure 13).

EXHIBIT

According to Gartner, by 2019, 40% of enterprises will be using

As early as 2010, American insurer Aetna introduced

chatbots to facilitate business processes using natural-language

their virtual assistant “Ann” to improve their customer

interactions. The advantages of chatbots include standardized

efficacy. Ann was developed by NextIT and primarily

and consistent quality of service, 24-hour service for customers

intended to act as a welcome guide on Aetna’s website,

as well as reduced costs. Beyond business-to-customer applica-

engaging customers and answering login and registra-

tions, chatbots are increasingly being implemented in the insur-

tion questions. Today, Ann helps customers find a

ance industry for internal usage to automate processes in HR,

doctor, estimate the cost of services, answer questions

IT, and operations, or to assist knowledge workers in accessing

about claims, ID cards and more. She answers around

expertise in real-time.

20,000 questions daily and can provide responses in

27

The case study of Aetna (Exhibit 2) gives detailed insights into
how a chatbot can be set up and trained in order to deliver such
efficacy gains.

2:

AETNA

written or spoken form, although questions can only
be asked via text. Aetna has installed a member of
staff who can check Ann’s answers and enrich them
whenever necessary. If Ann does not know the correct answer, she directs the customer to a dedicated

Figure 13: Selected case studies of AI-enabled
customer self-service solutions

customer service agent. The bot has started off with
achieving high quality responses only 35% of the time
but has quickly improved to 80% within a year. Ann

EXTERNAL

CUSTOMER-FACING

has enabled customers to resolve issues directly via

APPLICATIONS

the website, without having to call an agent and wait-

Axa

ing in line during specific hours.

have introduced a chatbot on their Health App “Xtra”, which
advises users on fitness and nutrition tracks, and answers
questions regarding health goals.

Manulife

UNDERLYING

have introduced a biometrics authorization application that is
able to identify users based on their unique voice profile.

PNB MetLife
have introduced a medical chatbot on Facebook Messenger
that informs users of cancer and heart disease risks and

USE

CASE:

“Answer customer questions based on company
data (user profiles, product info, doctor database)
and customer data (text-based customer question
sets) in order to provide timely, personal, and helpful
customer service.”

simplifies medical information on these. It can also calculate a
user’s health quotient by asking the customer questions.
A review of the current AI landscape reveals that the majority

INTERNAL

CUSTOMER-FACING

APPLICATIONS

Versicherungskammer Bayern
leverage AI in order to increase the effectiveness of
their customer service by letting Watson sort and classify
customer emails.

Anthem
use IBM Watson to assist doctors with creation of
treatment plans.

of prominent use cases realized by traditional insurers today,
are focused on achieving efficacy gains and that these efforts
are already yielding tangible benefits.
To keep up with the industry, insurers must continually strive
to implement and improve their AI efficacy endeavors. Applying the Use Case Formula to analyze the successful AI initiatives of their competitors enables insurers to abstract generic
AI use cases that can be replicated and serve as a starting
point for newcomers in the AI space.

34 |

6.2 The future of insurance – insurers have yet to leverage AI
for discovery
Large scale AI applications in the area of operations and customer discovery are still rare in the insurance industry today.

Case studies of consumer-led companies leading in the application of AI outside the insurance industry serve as an indicator of what is already technologically feasible today. Analyzing
their case studies to identify underlying AI use cases can
serve as valuable starting point for ideation by transferring
the identified use case into the insurance context.

Leveraging AI to generate new insights in the form of previ-

Operations discovery

ously unknown answers or actions is simply more difficult

In operations discovery, AI can unlock value through novel

to realize from a technological point of view and requires a

insights that lead to cost reductions. In this realm one capabil-

greater amount of creativity.

ity stands out as a key differentiating competitive advantage

However, a few cross-industry players such as Google, as
well as innovative InsurTech start-ups, such as Lemonade or
Oscar, have already started invading this area to seize value.
For example, Oscar, a health insurance and tech company,
is using claims data to make inferences about which doctor
performs which procedure at what frequency – enabling them
to discover narrow specialty areas and consequently refer
patients deliberately to the right specialist.

for insurance businesses in the future: fast and cost-effective
product innovation and design. In the era of rapidly evolving
customer needs and rising product personalization, insurance
businesses are struggling more than ever to keep pace with
customer demand in a cost-efficient manner. Reductions in
lead times and product costs will hence become even more
crucial in the digitalized future.
While the application of AI in this realm is still mostly un-

Oscar and other new market entrants are changing the
rules of the game with customer-centric design, exceptional
engagement, and cost-effective operations. Although revolutionary in the insurance industry, their business models are

tapped in the insurance industry, the case study of Stanley
Black & Decker (Exhibit 3) serves as an illustrative benchmark
for leading edge insurance companies who may begin to
leverage generative AI solutions for product design.

often inspired by consumer business innovations. As a result,
insurers who want to lead in their industry beyond the current status, must turn to consumer businesses as a source of
inspiration.

JEFF

KOWALSKI

|

CTO

AT

AUTODESK

Generative design is a departure
from the way that we have traditionally done design, but these technologies are not a threat, they’re more
like superpowers.

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 35

The case study of Stanley Black & Decker demonstrates that
AI technology is ready to unlock opportunities for shortened
product creation lead times as well as costs and thereby to
gain significant bottom and top-line line effects. Although the

EXHIBIT 3: STANLEY
BLACK & DECKER
Early in 2017, Stanley Black & Decker, a company
producing tools for industry and household purposes,
released a statement that they have been working
on opportunities to leverage AI in product design to
decrease time spent on development, thereby yielding
operations discovery. An AI-powered algorithm helped
Stanley Black & Decker’s employees to redesign a tool
for electricians for fixing electrical and telephone lines.

tools manufacturing industry has little in common with the
insurance industry, the application of the Use Case Formula
can help to transfer Stanley Black & Decker’s successful AI endeavor into the insurance context. Similarly to Stanley Black
& Decker, insurers can datafy their product development
process in order to automate product design. With access to
diverse data sources as well as a set of pre-defined product
requirements, AI can design the optimal product for any
target group or even individual customer – at no incremental
cost.

Previous tools had been almost 7 kg, which made
them too heavy for workers. The company used an AI
enhanced generative design software from Autodesk,
which designs new products via mimicking nature’s
evolutionary approach. The algorithm explores all possible product combinations, which can be thousands
or millions of design choices to find the best one,
matching a set of user requirements such as light
weight. The AI learns throughout the process which
combinations work best and eventually comes up with
the most effective and efficient design. The human
designer can then adapt the choice if needed. Generative design may even go beyond human capabilities of
design creation and therefore not only increase speed
and reduce the cost of product development but may
also have a positive impact on product quality and
desirability.

UNDERLYING

USE

CASE:

“Create new products based on company data (previous product components, product requirements,
employee feedback data) in order to deliver innovative products to the market faster and at lower cost.“

Potential insurance use case:
“Design new insurance policies based on company data (e.g.,
previous policies, customer feedback, claims data), public data
(e.g., forums, Google search data, demographics statistics)
and customer data (e.g., smart home, wearables, phone usage) in order to deliver more diverse and tailored insurance
products to the market faster and at lower cost.“

36 |

Customer discovery

While customer discovery applications of AI are still rare

Within the realm of customer discovery, AI can deliver value

in the insurance industry, the case study of Under Armour

by unlocking revenues through previously unknown insights.

(Exhibit 4) illustrates not only the technological feasibility of

This may be the most game-changing area for insurers to lev-

such applications, but also serves as a benchmark for insur-

erage AI in the future: The rise of self-driving cars, the sharing

ance companies who aim to progress to sustainable business

economy, social brokering, and comparison platforms are just

models.

some of the threats to their traditional profit models insurers
are facing. It is therefore undisputed that insurers will have to
identify and unlock new revenue streams and AI may be the
needed enabler for redefining the business.

SUNDAR

PICHAI

|

CEO

AT

GOOGLE

We’ve been building these incredible capabilities, be it search, the knowledge graph, our understanding of natural language, image recognition, voice recognition, translation. Particularly
over the last three years, we have felt that with
Machine Learning and Artificial Intelligence,
we can do these things better than ever before.

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 37

Under Armour’s case study powerfully demonstrates how
AI can open up opportunities for completely new, profitable
business models centered on data and user interactions –
even for a business with its origin in the sports fashion manu-

EXHIBIT

4:

UNDER

ARMOUR

In a transition from its traditional fashion manufacturing business model towards a data-driven
business model, Under Armour has partnered with
IBM to develop “UA Record” - a cognitive coach
that serves as a personal health consultant, fitness
trainer and assistant by providing users with timely,
evidence-based advice regarding nutrition, activity
and sleep. Under Armour has developed a range of
wearable devices, including a fitness tracker, heart
rate monitor chest strap, and smart scales that
gather user behavior and health data. The coaching
is based on a comparative model, clustering users
based on criteria such as age, gender, and activity

facturing industry. Applying the Use Case Formula to the case
study of Under Armour, the underlying use case can be used
to transfer this data-driven business model into the context of
insurance. Like Under Armour, insurers can leverage diverse
data pools to help customers reduce health risks. However,
insurers can go one step further by leveraging AI to prevent yet unknown risks in all areas of life: Health, transport,
household, finances and more. Insurers will then no longer be
confined to reimbursing materialized risk but instead prevent
loss events from occurring in the first place. To become an
effective and trusted assistant in preventing personal loss
events for the customer, the insurer needs to build a much
closer relationship – for which the use of AI for meaningful
analysis of big data will be the enabler.

level in order to provide relevant recommendations
for users. The model will, over time, be able to tap
into additional data pools to derive novel connections between e.g., weather and training behavior.
UA Record not only serves to strengthen customer
relationships and build emotional connections but is
also able to collect relevant data from customers to
cater to needs more efficiently in the future.

Potential insurance use case:
“Suggest risk prevention measures based on company
internal data (e.g., customer profiles, claims files), public data
(e.g., demographic statistics, geographical statistics, financial
data), customer data (e.g., location tracking, wearables data,
pictures) and third party data (e.g., Facebook, Instagram) in
order to increase customer safety.”

A review of applications of AI in insurance reveals that little

UNDERLYING

USE

CASE:

“Recommend sport activities and sleep behavior
based on company internal data (customer profile, product usage data), customer data (nutrition
info) and public data (weather) in order to increase
customer health and determine future needs more
effectively.”

tangible and meaningful impact is yet created with generative
solutions in the realm of operations or customer discovery.
However, selected case studies from consumer-led companies serve to demonstrate that the technological feasibility is
already given today. To innovate and lead in their industry,
insurers can turn to state of the art examples in other industries as a source of inspiration – the Use Case Formula can
help in this endeavor by distilling the most relevant use case
characteristics to ideate analog insurance use cases.

38 |

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 39

7.
Act now
Change is here. It is evident that AI has already begun to create a tangible impact in
insurance. Looking beyond the insurance industry, however, offers a glimpse into the real
magnitude of the AI-enabled disruption still to come. AI technology holds the potential to
fundamentally redefine the industry on all levels – challenging traditional cost structures,
enabling novel relationships with end-customers and much more.
For those who are yet to embark on their journey towards an artificially intelligent future,
the time to act is now. For leaders looking to steer their organizational ships into the
auspicious waters of AI, the following three principles provide guidance:
1. Data is the key: Before getting started with any AI use case, it is crucial to determine
the type, quantity, and quality of data at hand. This will provide you with an initial
idea for feasible and relevant use cases to pursue. But don’t throw in the towel if your
business doesn’t have access to the data required to follow through with the use cases
you envisioned. Consider accessing public data sources or partnering with a business
that has the data you are looking for. Alternatively, you can develop a strategy to start
gathering valuable data, e.g., by “datafying” previously analog processes or developing
a consumer-facing digital product that serves as a “vehicle” to gather user data.
2. Start small: There is no need to go overboard and start selecting the most complex
use cases in an attempt to gain as much value as possible. By starting small you can
release an application faster and then build on it over time. AI applications can be
iteratively extended to scale, so companies can expand their capabilities and capture
increasing value over time.
3. Don’t be afraid of failure: AI projects can be particularly challenging in that they need
to tackle the social stigmatization of the technology, require interdisciplinary collaboration and bear high cost and time investments. In order to succeed, it is crucial to accept
failure as an integral part of innovation that contributes to organizational learning.
Only by adopting a “fail fast, fail early” mindset, can time and cost investments be kept
in check.

What steam power has been to industrial manufacturing, Artificial Intelligence will be to insurance. Ask
yourself this question: What would have happened
to your manufacturing business after 1800 if you had
never used steam? True innovators are those who
look into the future and act before it arrives!

40 |

Our team

Nikolay Kolev

Jost Geimer

Marc Stiller

Partner, Deloitte Digital

Senior Manager, Deloitte Digital

Senior Consultant, Deloitte Digital

nkolev@deloitte.de

jgeimer@deloitte.de

mstiller@deloitte.de

+49 89 29036 7896

+49 89 29036 7672

+49 89 29036 7609

Esther Daweke

Luisa Butzmann

Consultant, Deloitte Digital

Business Analyst, Deloitte Digital

edaweke@deloitte.de

lbutzmann@deloitte.de

+49 89 29036 7875

+49 89 29036 6993

Contributors
Sascha Bauer

Fabrice Wegner

Thomas Viehmann

Senior Manager,

Manager,

Senior Manager,

Big Data and Machine Learning expert

Product Development and InsurTech expert

Machine Learning expert

sasbauer@deloitte.de

fwegner@deloitte.de

tviehmann@deloitte.de

Deloitte Insurance Tech Community
InsTechGermany@deloitte.de

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 41

Subject Matter Experts

Dr. Christian Schareck

Nicolai Andersen

Dr. Kurt Mitzner

Partner, Industry Leader Insurance

Partner, Head of Innovation EMEA

Partner, Strategy Insurance Sector

cschareck@deloitte.de

nicandersen@deloitte.de

kmitzner@deloitte.de

Jens Parthe

Dr. Björn Bringmann

Partner, Strategy Financial Services

Director, Head of Data Science

jparthe@deloitte.de

bbringmann@deloitte.de

42 |

Sources
18. Tim Urban, Wait but Why (2015): The AI Revolution: The Road to
Superintelligence; URL: https://waitbutwhy.com/2015/01/artificialintelligence-revolution-1.html

1.

CB Insights (2017): The State of Artificial Intelligence; URL: https://
www.cbinsights.com/research/report/artificial-intelligence-trends/

2.

CB Insights (2017): The State of Artificial Intelligence; URL: https://
www.cbinsights.com/research/report/artificial-intelligence-trends/

3.

IDC (2017): Worldwide Semiannual Cognitive/Artificial Intelligence
Systems Spending Guide

4.

DeepMind (2016): DeepMind AI Reduces Google Data Centre
Cooling Bill by 40%; URL: https://deepmind.com/blog/deepmind-aireduces-google-data-centre-cooling-bill-40/

5.

U.S. Department of Transportation (2016): ODI RESUME ; URL:
https://static.nhtsa.gov/odi/inv/2016/INCLA-PE16007-7876.PDF

21. Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, Lior Wolf (2014):
DeepFace: Closing the Gap to Human-Level Performance in Face
Verification

6.

eMarketer (2017): Alexa, Say What?! Voice-Enabled Speaker Usage
to Grow Nearly 130% This Year; URL: https://www.emarketer.com/
Article/Alexa-Say-What-Voice-Enabled-Speaker-Usage-Grow-Nearly130-This-Year/1015812?ecid=NL1001

22. Capital One (2016): Bank on Alexa with Capital One’s Expanding
Skill for Managing Your Money; URL: https://www.capitalone.com/
onefocus/bank-alexa/

7.

Gartner (2016): Gartner Predicts 2017

8.

O’Reilly (2017): The New Artificial Intelligence Market

9.

IBM Institute for Business Value (2016): Understanding customers
and risk

10. IBM Institute for Business Value (2016): Understanding customers
and risk
11. IBM Institute for Business Value (2016): Understanding customers
and risk
12. IBM (2017): 10 Key Marketing Trends for 2017
13. Deloitte UK (2015): Insurance disrupted
14. Deloitte (2017): EMEA Insurance data analytics study
15. Wired (2016): Google’s AI wins fifth and final game against Go genius
Lee Sedol; URL: https://www.wired.com/2016/03/googles-ai-winsfifth-final-game-go-genius-lee-sedol/
16. Tim Urban, Wait but Why (2015): The AI Revolution: The Road to
Superintelligence; URL: https://waitbutwhy.com/2015/01/artificialintelligence-revolution-1.html
17. Wired (2016): Google’s AI wins fifth and final game against Go genius
Lee Sedol; URL: https://www.wired.com/2016/03/googles-ai-winsfifth-final-game-go-genius-lee-sedol/

19. Google (2012): Using large-scale brain simulations for machine
learning and A.I.; URL: https://www.blog.google/topics/machinelearning/using-large-scale-brain-simulations-for/
20. Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell, Cornell
University (2017): Curiosity-driven Exploration by Self-supervised
Prediction

23. Forrester, TechRadar (2017): AI Technologies Q1
24. Forbes (2011): Google’s “Infrigenovation” Secrets; URL: https://www.
forbes.com/sites/scottcleland/2011/10/03/googles-infringenovationsecrets/
25. IBM (2016): The biggest data challenges that you might not even
know you have; URL: https://www.ibm.com/blogs/watson/2016/05/
biggest-data-challenges-might-not-even-know/
26. Techcrunch (2016): Facebook chose to fight fake news with AI,
not just user reports; URL: https://techcrunch.com/2016/11/14/
facebook-fake-news/
27. O’Reilly (2017): The new data-driven Artificial Intelligence Market
28. Gartner (2017): Four Use Cases for Chatbots in the Enterprise…Now

From mystery to mastery: Unlocking the business value of Artificial Intelligence in the insurance industry | 43

This communication contains general information only not
suitable for addressing the particular circumstances of any
individual case and is not intended to be used as a basis for
commercial decisions or decisions of any other kind. None of
Deloitte Digital GmbH or Deloitte Touche Tohmatsu Limited, its
member firms, or their related entities (collectively, the “Deloitte
network”) is, by means of this communication, rendering
professional advice or services. No entity in the Deloitte
network shall be responsible for any loss whatsoever sustained
by any person who relies on this communication.
Deloitte refers to one or more of Deloitte Touche Tohmatsu
Limited, a UK private company limited by guarantee (“DTTL”), its
network of member firms, and their related entities. DTTL and
each of its member firms are legally separate and independent
entities. DTTL (also referred to as “Deloitte Global”) does not
provide services to clients. Please see www.deloitte.com/de/
UeberUns for a more detailed description of DTTL and its
member firms.
Deloitte provides audit, risk advisory, tax, financial advisory
and consulting services to public and private clients spanning
multiple industries; legal advisory services in Germany are
provided by Deloitte Legal. With a globally connected network
of member firms in more than 150 countries, Deloitte brings
world-class capabilities and high-quality service to clients,
delivering the insights they need to address their most
complex business challenges. Deloitte’s approximately 263,900
professionals are committed to making an impact that matters.
Issue 11/2017

AI 360: insights from the
next frontier of business
From the corner office to the corner store,
mapping the progress and challenges
of AI adoption

TABLE OF

CONTENTS

2

AI adoption accelerates but is everyone on board?

03

Senior executives say performance eclipses cost
savings as the biggest payoff

05

Consumers more open to AI but have doubts that
businesses can’t afford to miss

08

Workers’ comfort and concerns

13

Embedding AI as your neural wiring: the view from
the visionaries

15

Cross-industry insights

16

About the research

19

AI adoption accelerates, but is
everyone on board?
While the media continues to debate artificial
intelligence (AI), businesses are clear. AI is here to
stay. Companies around the world are adopting it to
boost performance and insights, and are reaching the
point where it is no longer just on the fringes of an
organization but at its core – the neural wiring. The
second edition of Genpact’s research series of
C-suite and senior executives, consumers, and
workers shows remarkable progress in AI adoption
and awareness since the inaugural 2017 study.
Within large companies, a quarter of senior executives
say they plan to fundamentally reimagine their
businesses with AI by the end of 2021 (compared
to 14% in 2017), and 54% will use AI to transform
processes (up from 41%). But many people –
consumers and workers – still have doubts.
Consumers worry about bias and discrimination
from AI-based decisions, data protection, and the

job market for future generations. And even though
workers say they are willing to be retrained and more
businesses now offer reskilling opportunities, few
employees say that they have seen them, and even
fewer have taken advantage.
With media attention often focused on the negative
impact of AI on workers and consumers, organizations
must continue to change this narrative. To move
beyond the hype, advanced businesses are taking
practical steps to respond to lingering concerns,
aligning AI to the business strategy, and dedicating
time to deciding where and how to roll it out.
As businesses’ attitudes toward AI evolve, they are
more likely to see it as a means to a much larger end,
says Sanjay Srivastava, Genpact’s chief digital
officer. “After all, it isn’t just about AI. It’s about
transforming the business strategy, value proposition,
and operations. AI facilitates that,” he says.

It isn’t just about AI. It’s about
transforming the business strategy,
value proposition, and operations. AI
facilitates that.
Sanjay Srivastava, chief digital officer, Genpact

3

The future is instinctive
For companies that seek to stay relevant and thrive
in markets that face constant disruption, AI plays a
significant role. We believe the next generation of
advanced businesses will have AI embedded as their
neural wiring, allowing them to adapt instinctively
and make accurate, real-time decisions for the
benefit of their customers.
This organization of the future is an instinctive
enterprise and has three hallmarks. With a connected
ecosystem, interwoven with AI, it eliminates silos and
strengthens external partnerships. It uses AI to spot

patterns and generate predictive insights, while also
enhancing how its people work, enabling an adaptive
workforce with purpose-driven careers.
The 25% of respondents who say their companies
plan to reimagine their businesses with AI are
the visionaries who are taking bolder steps toward
becoming instinctive enterprises. For example, twothirds of these visionaries already apply AI in at least
four business functions, and 26% have it in seven or
more (figure 1). We will draw on the experiences from
these visionaries throughout the report.

Where organizations are adopting AI
In which of the following areas is your company using AI?
Visionaries

Others

% of senior executives

Finance/accounting
Information technology services

56%

28%

Operations/production

55%

23%

Strategy/general management

50%

25%

Customer service

31%

46%
43%

Research and development

20%

Compliance/risk

36%

20%

30%

Marketing

19%

Procurement

25%

17%

24%
25%

Supply chain

20%
20%

Human resources

17%
18%

Sales
Figure 1

4

72%

46%

Senior executives say performance
eclipses cost savings as the biggest payoff
With the growth of AI adoption, companies
are generating greater impact and have higher
expectations (figure 2). For example, when they were
asked about the biggest benefit from AI, improved
ability to leverage data and analytics rose to the top
(37%), compared to fifth place in a similar ranking of
AI benefits in 2017. This reflects how companies are
getting a better handle on their analytics strategies
and developing more sophisticated algorithms
that light up dark data.

The value of better data visibility also supports the
second biggest areas of impact: collaboration and
improved processes at 35% each. In 2017, senior
executives cited cost reduction as the top benefit,
followed closely by improved customer experience
and more efficient processes. But in the current study,
two purely financial indicators – reduced costs and
increased revenues – stand at the bottom of a list of
nine potential benefits.

Feeling the impact from AI
Where is AI generating benefits for your business?
Now

In three years

% of senior executives

33%
37%

33%
35%

Improved our ability
to leverage our
data and analytics

34%
35%

Improved processes
and greater
efficiency at our
company

29%
29%

30%
34%

Improved our ability
to collaborate
across departments
and functions

27%
28%

Improved our
ability to target
new markets

33%
28%

Reduced
costs

35%
34%

Improved our
ability to make
predictions or
forecasts

Improved
customer
experience
and service

30%
27%

Freed up more time
for employees
to focus on more
important tasks

Increased
revenues

Figure 2

5

A game changer: from process improvement to prediction

Robert Pompey, senior vice president,
commercial credit management at TD Bank, a
US financial institution with operations on the East
Coast, says these findings on impact coincide with his
own experience. He says AI is making a difference in
process functions, such as early-warning mechanisms
in the credit cycle.
“Process functions have become a primary focus for
us,” Pompey says. “In the past, we were touching every
[loan application] and doing a deep analysis of each
one to understand credit histories. Now we’re largely
just doing exception processing manually. Using AI,
we can be more predictive about client behaviors
that could lead to a potential default or credit risk. TD
Bank is also leveraging AI tools and machine-learning
capabilities with documents to deal with financial
statement spreading to make credit decisions faster.”
Pompey adds that while the firm recognizes these
functional improvements, the impact is also being
felt in the institution’s commercial credit workforce
and in the customer experience. “Anti-moneylaundering activities are another great example of

Using AI, we can be more predictive
about client behaviors that could lead to
a potential default or credit risk.
Robert Pompey, SVP, commercial credit management,
TD Bank

6

the transformational properties of AI. Investigative
transactional reviews that used to take days can
now be completed in hours and/or minutes using
forms of AI – a game changer. We’re also increasing
our ability to mitigate fraud – and that translates
into an improved customer experience, even if
customers aren’t aware that we’ve created a better
and safer environment for them.
“Then there’s speed to market. Customers are
expecting faster credit decisions and banks are driving
this change. And, finally, we can now enhance our
employees’ jobs by elevating their roles to higher
levels of activity beyond data entry or other skills
and requirements. Those are all developments made
possible by technology and AI.”
Executives expect the trend for enhancing functional
performance with AI to continue into 2021. They
also anticipate that better customer experience will
remain a top benefit. And they believe freeing up time
for employees to focus on more valuable tasks will
continue to increase in importance.

Pushback falls away and shifts from the corner office to
entry-level workers
Another key development is that resistance to AI
implementation has sharply diminished since the
2017 study. About 58% of senior executives now say
there is no individual or group resisting the adoption
of AI, compared with 21% previously.

pushback (41%) from these employees. As companies,
especially those most advanced with AI, extend
the technology across the organization, entry-level
workers – a group that typically knows less about AI or
how to implement it – are getting exposed to it more.

Notably, the proportion of executives who say that
they perceive the strongest resistance from the top
(C-suite, board, or upper-management) plummeted
to only 15% from 51% in 2017. Srivastava says this isn’t
surprising because executives are now more likely
to see and understand the benefits of AI, so they
are more likely to embrace it.

In addition, senior executives often have unrealistic
expectations from AI, expecting an Alexa-style
experience within weeks or months that employees
can’t deliver. For this reason, the next vital phase of
change management will occur among the entry and
mid-level ranks, where much of a company’s crucial
work gets done.

At the same time, though, respondents are reporting
greater resistance from entry-level workers, a number
that has jumped significantly to 19% from 5% in 2017.
Indeed, the AI visionaries are more likely to report

On the whole, however, consumers and workers
have become more accepting of AI, in part because
they are now more familiar with it. Yet pockets of
concern remain that AI proponents need to address.

The next vital phase of change
management will occur among the
entry and mid-level ranks.

7

Consumers more open to AI but
have doubts that businesses can’t
afford to miss
Consumers are generally more at ease with AI than they were in 2017. The proportion who say that AI has made
their lives at least a little better rose to more than half (53%), compared to about a third (34%) in the previous
study. Only 29% of people say AI makes no difference in their lives, falling from 41% in 2017.
But acceptance is still far from widespread. Only 15% of consumers say AI has made their lives much better. And
37% say either that AI does not personally benefit them or that they don’t know if it does. Broader acceptance is
held back by several specific concerns that businesses do not always fully understand.

Greater willingness to share data, but reservations remain
Consumer-facing AI applications rely on users’
data to personalize services and improve the
customer experience. But despite many wellknown businesses suffering high-profile data
breaches recently, consumers have become more
open to sharing their data. When asked if they are
comfortable with companies using AI to access
personal data to improve their customer experience,
54% say they are (figure 3), an increase from 30% in

When consumers were asked if they
are comfortable with companies using
AI to access personal data to improve
their customer experience, 54% say
they are.

8

2017. That said, 46% still have concerns, and more
than half (52%) think that the government should be
doing more to protect their data, although this has
dropped from 59%.
“There are two things companies have to provide
consumers with when they use their data,” says
Kristian Hammond of Northwestern University.
Hammond directs the school’s CS+X initiative, where

students combine a computer science curriculum with another subject that they are passionate
about. A computer sciences professor himself, he also heads Northwestern’s Master of Science in Artificial
Intelligence programs.
“One, companies have to be incredibly transparent about how they’re using the data and who gets to have access
to it,” he says. “And, two, they have to be more vigilant about protecting it. There are always going to be people
ahead of the curve who can hack into systems, but the reality is that more can be done to protect data so that if it
does slip out, it doesn’t slip out completely.”

AI and your personal data
How comfortable are you with companies using AI to access your personal data to
improve your customer experience?
Very comfortable

Fairly comfortable

Not very comfortable

Not at all comfortable

% of consumers

9%

12%

21%

7%

13%
22%

24%
35%
33%
42%
36%

47%

Millennials and younger
(18-37)

Generation X
(38-53)

Baby boomers and older
(54+)

Percentages may not add to 100% due to number rounding

Figure 3

9

Confronting AI bias is a major challenge for all
While there is greater openness among consumers
about AI accessing personal data, a large majority
(78%) expect companies to actively address bias and
discrimination from AI, fearing it can cause them harm.
For example, some systems that identify candidates
for recruitment or promotion have been shown to
be biased against women because the algorithms
are based on data from when men were prevalent in
senior roles.
“We can take the approach based on historical data
or we could diversify by using other AI techniques,”
says Srivastava. For example, when finding the right
team for a candidate, instead of using algorithms that
inadvertently favor a candidate’s face, race, religion,
or résumé, recruiters can expose them to a series of
photographs and, according to the choices they make,
use algorithms to indicate the best kind of team for
each person. “That’s a very different way to use AI for
recruitment,” says Srivastava.
Financial services firms, for example, have already
taken steps to prevent bias from entering their systems
and affecting their customers.

If we can’t explain how the technology
works, we’re not touching it.

Robert Pompey, senior vice president, commercial
credit management, TD Bank

10

“From the commercial credit management standpoint,
if we can’t explain how the technology works,
we’re not touching it,” says TD Bank’s Pompey. “It’s
critical that we understand the algorithms before we
implement them. We need a clear picture of how they
operate. Then we have to be able to communicate that
information to our employees to ensure they’re being
transparent with our customers, too.”
More than two-thirds (67%) of all consumers say
they are at least somewhat concerned about AI
discriminating against them in the way it makes
decisions. And there are only minor differences among
age groups, ranging from 68% for Gen-Z to 69% and
higher for baby boomers and older generations.
In other words, AI bias is a serious issue for nearly
everyone, which calls for urgent attention from
companies that deploy AI.
The good news is that nearly all companies say
they are addressing AI bias, but, for the most part,
their efforts have not been aggressive. Nearly all
executives say that they have taken at least one action
to combat AI bias, and about half have taken more

than one. But most firms have only begun to address
the bias issue. Just 34% of all companies say they
have established a comprehensive governance and
internal control framework to manage AI bias. Most
of the others have taken smaller steps, such as using
diverse teams to eliminate particular biases, discussing
the potential of AI bias with employees who use the
data, and modifying algorithms to eliminate particular
biases. Once again, the visionaries – the senior
executives from companies with deeper plans for AI –
have made the most progress here (figure 4).
Rob Laubacher, executive director at the MIT
Center for Collective Intelligence, shares an
example of bias that could have had disastrous effects.

“There was a research group using machine learning
to identify pneumonia patients needing special
attention,” he explains. Yet asthmatics, who are in
particular danger from pneumonia, were not identified
as needing extra attention within this group. “It turned
out that doctors addressed these cases right away,
as they knew asthmatics were at higher risk, so they
were already getting additional help. But because
the algorithm didn’t know that, it would have told
hospitals to ignore that class of patient. In this case, the
researcher told the hospital not to use the algorithm
because he did not understand the reasoning behind
it. Companies would do well to use similar caution.”

Addressing AI bias
Which of the following steps has your company taken to combat AI bias?
Visionaries

Others

% of senior executives
50%
31%

46%
33%

Discussed the potential of AI
bias with employees
who are using the data

Used diverse teams to select
the samples
and train the algorithms

46%
32%

38%
33%

Established a
comprehensive
governance and internal
control framework to
manage AI bias

Modified algorithms to eliminate
particular biases, for
example in hiring, sales, or
customer service

Figure 4

11

To bot or not to bot?
Another major disconnect: Senior executives may
be making decisions about the use of customerfacing bots without fully understanding consumer
preferences.
More than 86% of senior executives believe
customers will prefer to be served by a bot than a
call center agent by 2021, a significant increase from
38% in 2017. Moreover, 45% strongly believe this to
be the case. But the consumer perspective is much
different and has hardly changed from the previous
study: in 2017, only 12% said they would prefer
to be served by a chatbot in the next three years,
compared to 15% in this latest study.
Laubacher argues that this disconnect may be
because executives are responding to the cost-saving
properties of bots, while consumers are responding to
their perceptions or experiences with them.

12

“I wonder whether chatbots on their own are a flawed
customer-service model and that’s why only 15%
of consumers feel that they’re being well served by
them,” says Laubacher. “Chatbots were overhyped
for a while – not just for tech support but also for
marketing. But if companies can combine elements
of self-service with a good web experience, using
chatbots as intermediaries could work.”
Many businesses have been quick to implement
chatbots and AI, but not all have taken an end-toend view of the customer experience that requires
connecting often-siloed internal functions. To close
the perception gap, companies can use techniques
like service design and design thinking to identify
where a chatbot will be most effective and make the
customer interaction seamless.

Workers’ comfort and concerns
Workers are also becoming more at ease with AI. For
one thing, the expectation concerning job security
is becoming more positive, with those who say that
AI presents new career opportunities (36%) slightly
outnumbering those who say it threatens their jobs
(28%). But while they may not be worried about
their roles, 43% of workers are concerned about
jobs for their children and future generations, which
could indicate they are still unclear on the long-term
benefits of the technology.
And even though workers are not
overwhelmingly worried about losing their
current jobs, their enthusiasm for AI falls short of
senior executive expectations. The vast majority
(86%) of senior executives expect workers to be

comfortable working alongside AI by the end of 2021,
and 26% believe so strongly. But while more workers
say they expect to be at least fairly comfortable
working alongside robots than in 2017 (62% today vs.
40% in 2017), workers are less emphatic than senior
executives expect them to be. Indeed, only 17% say
they would be very comfortable.
The survey findings suggest that companies can
address this gap in expectations and continue
changing the narrative by communicating with
workers about the value AI will bring. A majority
(59%) of workers say they would be more comfortable
with AI if they understood it better, while only 11%
disagree, pointing to a practical way to get workers on
side with better communications and education.

A rise in reskilling but room to improve
Workers are very interested in receiving AI-related
training. Fully 80% say they are willing to learn
new skills to take advantage of AI in their current
job, and 31% say they are very willing. Notably, this
interest is strong across age groups. While Gen-Z and
millennial workers are most interested at 87%, 67%
of the boomer generation also say they are willing to
learn new skills. Also, workers who have invested in
training on their own time are more likely to say that
AI is having a positive impact on their lives.
This willingness to accept training should be good
news to companies, especially in an era of skills
shortages. While businesses are moving in the right
direction, there’s still room to improve: in 2017,
only 38% of senior executives said their companies
offered reskilling options like training, seminars, and
workshops, compared to 53% today. But few workers
have been exposed to those options, as only 35%
say their organizations offer AI-related training, and

fewer than one-quarter of workers say they have
received training. Even among the tech-friendly
Gen-Z and millennial groups, only 27% say they
have taken advantage of training at their companies.
Businesses risk missing a key opportunity to get
employees ready for AI.
TD Bank’s Robert Pompey says his team
equates training with engagement, which he
counts as a critical imperative. That’s why he says
the commercial credit division involves and trains
workers at every stage of an AI initiative.
“We welcome employees to participate in the design,
development, and testing of our AI deployments,” he
says. “We’re transparent, so they understand when
something is coming, know exactly what it is, and
are clear about how it’s going to fix their pain points.
Our technologies focus on helping them feel more
connected and being part of the transformation.
That’s how we get engagement.”

13

The rising demand for ‘bilingual’ workers
When it comes to the skills and experience that
businesses need to succeed with AI, Srivastava says
both technical and industry or process expertise are
crucial. If organizations have what he calls ‘bilingual’
talent – workers who are confident moving
between the fields of business and technology –
they are ready for the future workforce.
“The world will be at the intersection of those
technical and business skills,” he says. “And that’s great

14

because today’s workforce already has deep skills in
one or the other of those languages, so we are not
starting from scratch. You’re just broadening people
into the other language. When I evaluate AI maturity
at companies, I don’t look at how many data scientists
they have. I don’t look at how many process engineers
they have. I try to find how many bilinguals they have.
Because true success comes at that intersection.”

Embedding AI as your neural wiring:
the view from the visionaries
Having AI embedded throughout the organization is
a core trait of an instinctive enterprise. And the senior
executives who say their companies are planning to
fundamentally reimagine their businesses with AI are
ahead of the pack.
These companies – the visionaries – are on the road
to becoming instinctive enterprises and are starting
to exhibit the core hallmarks. They demonstrate
how they are developing a connected ecosystem,
as they strongly agree that they can easily share
data across all departments (63% vs. 41% of other
respondents) and find that AI helps them collaborate
across departments and functions (36% vs. 33%).
They also strongly agree that AI is improving their
company’s ability to make more effective business
decisions (70% vs. 46%) and that it is freeing up time
for employees to focus on more important tasks (34%
vs. 25%).
Furthermore, they are more likely to say that their
employees are willing to learn new skills to take
advantage of AI (59% vs. 46% strongly agree).
As such, these senior executives’ organizations

are much more likely to provide employees with
reskilling options, such as training, seminars, and
workshops (81% vs. 44%).
These organizations are reshaping their businesses
to thrive in the future. With AI as their neural wiring,
they will be better set to evolve into instinctive
enterprises, able to move markets, reinvent business
models, and amplify human potential.
But to get there, they must first be very clear on
which processes to apply AI to and ensure it will
impact their business’ strategy. They must also
address consumers’ doubts about privacy, bias, and
AI-enabled services such as chatbots. Enterprises that
undertake their own disruption must fully engage
their employees in the journey by offering training
and education on roles for future generations and be
transparent about how new technologies will affect
their customers. In a highly interconnected world,
consumers’ and workers’ trust must be earned and
renewed constantly, with each new platform, device,
or algorithm companies introduce.

Making decisions
AI is improving my company’s ability to
make more effective business decisions
% of senior executives

70%
46%

Visionaries

Others

Figure 5

15

Cross-industry insights
AI is becoming ubiquitous across processes and functions, but each industry has its own priorities, areas of
impact, and adoption challenges. See how your industry compares.

High tech
Reimagining their businesses

The technology sector ranks far ahead of other
industries in its AI adoption. With pressure from
companies such as Microsoft, Amazon, Alibaba, and
Google, it’s not surprising that more than 60% of
high-tech senior executives say they are extensively
implementing AI to reimagine their businesses,
compared with the all-industry average of 25%.
Technology companies are also more likely to have
adopted a greater number of AI technologies than
other sectors.
This industry has applied AI more extensively in nine
out of twelve functions listed in the survey when
compared to the cross-industry average (figure 1). The
three exceptions are supply chain, human resources,

16

and marketing. And at 61%, the industry’s use of AI
in finance and accounting is second only to banking/
financial services.
This extensive use of AI is paying off: 48% of hightech executives say they are achieving very
positive outcomes, compared with an average of
25%. They are more likely to expect AI to generate
revenue increases. And 48% also strongly agree that
employees will be comfortable working with robots
by the end of 2021 vs. a 26% average (figure 6). Yet at
the same time, technology executives point to greater
pushback against AI adoption, especially from entrylevel workers (39% vs. a 19% average).

Comfort with robots
Will workers be comfortable working with robots by the end of 2021

Industrial manufacturing

18%
21%

High tech

48%
26%

Insurance

19%
18%

Healthcare and life sciences

25%
13%

Consumer goods/retail

31%
15%

Banking/financial services

25%
38%

77%
50%

53%

45%
71%

44%
61%
44%
54%
50%
53%
40%

% senior executives who agree employees will be comfortable
working alongside robots in 2021
Strongly agree

Somewhat agree

% workers who say they will be comfortable working alongside
robots in 2021
Very comfortable

Fairly comfortable

Figure 6

Insurance
High expectations that customers will prefer bots

In the insurance sector, executives are much more
likely than their counterparts in other sectors to
strongly agree that AI is improving their ability to
make more effective strategic business decisions
(75% vs. 52%). Moreover, 54% strongly agree that they
can easily share data across all departments, second
only to the high-tech sector.
Freeing up time for employees to focus on more
important tasks is another key benefit where
insurance stands out. Nearly half (46%) of executives

cite this as one of the three top benefits of AI,
compared with an average of 28%.
Insurance also stands out for high expectations that
customers will prefer bots over agents in 2021, with
71% of executives agreeing strongly, compared with an
average of 45%. Improved customer experiences are
a strategic lever that many insurers have invested in
heavily. AI offers enhanced opportunities if they apply
it correctly.

17

Industrial manufacturing
Ahead on technology but behind on impact
Already well recognized for their use of technology
for production, industrial manufacturing companies
are also relatively intensive users of computer vision
(44% vs. an average of 35%), although less so than
the high-tech sector (48%). They are also second only
to the tech industry in their use of machine learning
(41% vs. 38% across industries) and natural language
processing (43% vs. 33%).
The sector has not, however, achieved strong
outcomes from AI adoption relative to most others.
Only 28% of senior executives strongly agree that

AI enables more effective business decisions,
compared with an average of 52%. The industry also
lags behind other sectors by a wide margin in
providing reskilling options, with only 37% saying
their company does so, compared to high tech (77%)
and the cross-industry average (53%).
When it’s well applied, manufacturing firms can reap
significant rewards from AI, including improved
product design, timely maintenance, supply chain
operations, and worker safety.

Consumer goods and retail
AI for sales and predictive insights
Not surprisingly, the consumer goods/retail sector has
applied AI to the sales function more than any other
sector (32% vs. 17% for all industries), and customer
service is second only to high tech (40% vs. 34%

overall). It is tied with the technology sector as the
biggest user of predictive analytics (45% vs. 38%
across industries), which reflects the sector’s need to
accurately predict supply and demand.

Banking and financial services
Big spend on AI but lower impact
The banking/financial services industry stands out
as the top spender on AI technology. Forty-two
percent of senior executives in banking say their
businesses invest $10 million or more annually
on AI, compared to an average of 26% in all other
industries. Even then, executives aren’t sure they’re
spending enough: only 38% strongly agree that their
organization allocates sufficient resources toward AIrelated technologies, and only consumer goods/retail
is lower.

18

But despite the big expenditure, banking/financial
services is only average in achieving very positive
outcomes from AI (26%) and ranks second-last behind
industrial manufacturing in strong agreement that AI
is improving the organization’s ability to make more
effective strategic business decisions.

About the research
Genpact worked with Wakefield Research to conduct a study between November 20 and December 3,
2018. The survey of C-suite and senior executives included 500 executives in the United States, United
Kingdom, Australia, and Japan, and was conducted via an email invitation and an online questionnaire.
Respondents were from the financial services, healthcare, life sciences, high tech, consumer packaged
goods, retail, and industrial manufacturing industries, and worked at companies with at least $1 billion in
annual revenue or at least $50 billion in annual revenue for financial institutions.
Wakefield Research also used an email invitation and online survey to poll 4,000 adults in the United
States, United Kingdom, Australia, and Japan, of which 2,103 were working at least eight hours a week.
In 2017, Genpact conducted similar research, working with research firm YouGov, to survey 5,179 people
in the United States, United Kingdom, and Australia. Of the total survey population, 2,795 were employed
at least eight hours per week. YouGov conducted the fieldwork online between August 15-30, 2017.
In a separate study conducted in June 2017, Genpact and Fortune Knowledge Group surveyed 300 global
senior executives. Respondents were from the financial services, healthcare, life sciences, high tech,
consumer packaged goods, retail, and industrial manufacturing industries, and worked at companies
with at least $1 billion in annual revenue or at least $50 billion in annual revenue for financial institutions.

19

About Genpact
We drive digital-led innovation and digitally-enabled intelligent operations for our clients. Guided by the experience that comes from running
thousands of processes for Fortune 500 companies.
We think with design. Dream in digital. Solve problems with data. Obsess over operations and sweat the small stuff. All 87,000 of us. From New York
to New Delhi (and 20 countries in between), we have the end-to-end expertise to connect every dot. Reimagine every process. And renew your ways
of working. Because we know that rethinking each step from start to finish will change your business for the better.
Wherever you want your business to go, we’ll focus on getting you there. So you can focus on your big picture, big returns or next big thing. Whatever
it is, we’ll be there with you – putting data and digital to work to create bold, lasting results.
Because transformation happens here.
Get to know us. Genpact.com.

Follow Genpact on LinkedIn, Twitter, Youtube, and Facebook.

Copyright © Genpact 2019. All Rights Reserved.

AI in Insurance
Top Use Cases, Challenges, and Trends

WHITE PAPER
www.dataiku.com

Introduction
Combining mathematics and data analysis in insurance for prediction is not new but has perhaps come into the spotlight with
recent worldwide buzz surrounding data science and machine learning. And in the past few years, the pace and scale of data science,
machine learning, and ultimately AI adoption has accelerated to enhance or reinvent the processes core to the insurance business.
From McKinsey’s Digital Insurance in 2018

“During the last 12 months, Gartner
has seen great interest in the use
cases and application of AI for
many tasks, including chatbots
for customer service, underwriting
assistance platforms, and AI for notouch claims processing.”
- Gartner 2019 CIO Agenda: Insurance
Industry Insights, Kimberly HarrisFerrante, 15 October 2018 (report
available to Gartner subscribers)

“During the first three
quarters of 2019, a total of
US$4.36 billion had been
deployed to InsurTech
companies across 239
transactions. That already
marked a 5 percent increase
from the total amount of
investment in all of 2018.”
- Source: Willis Towers Watson

This growth of data science, machine learning, and AI in insurance is driven by a variety
of factors, including:
• The breadth of use cases that can be developed using machine learning techniques,
particularly those that go well beyond traditional uses of data by actuaries.
• Disruption from fintech (insurtech) and the resulting need to retain and attract clients
through a better customer experience.
• Reinforced client price sensitivity.
• The potential business impact in using AI for more and larger use cases.
• Increased pools (and subsequent hiring) of machine learning and AI talent.
• The need to model evolving - and ever more complex - risks.
• The need for increased profit and loss management in a long-term, long-yield environment.
This white paper will explore some of the up-and-coming use cases, challenges
that traditional insurance companies face in implementation of those use cases,
and ways to address those challenges to be successful in the race to AI. It will also
explore trends in how successful companies are executing on AI initiatives.

1

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

AI in Insurance: High-Value
Use Cases
“As AI becomes more deeply integrated in the industry, carriers must position themselves
to respond to the changing business landscape. Insurance executives must understand the
factors that will contribute to this change and how AI will reshape claims, distribution, and
underwriting and pricing. With this understanding, they can start to build the skills and
talent, embrace the emerging technologies, and create the culture and perspective needed to
be successful players in the insurance industry of the future.”
- McKinsey, Insurance 2030: The Impact of AI on the Future of Insurance

Overall, use cases for data science, machine learning, and AI fall into one of five categories:

Increased
Revenue
(Marketing)

Mitigating
Risk

Decreased
Costs
(Operations)

Competitive
Edge
(Innovation)

Speet-to-Value & Team Efficiency
(Organization)
These categories are broad, which means there is no shortage of use cases when it comes to data science, machine learning, and AI in
the insurance industry. This section takes a (non-exhaustive) look at some of today’s most high-value use cases.

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

2

INCREASED REVENUE
(SALES AND
MARKETING)
Customer retention and churn prediction: A 2019 study by TechSee revealed that 50 percent of insurance customers actively
search for an alternate insurer at renewal. And even though more than half (54 percent) of insurance companies made efforts to keep
those customers, their efforts were largely unfruitful.
That leaves huge opportunities for AI not to simply predict possible churners, (as some customers will leave no matter what) but
to take things one step further with a technique called uplift modeling. Since marketing efforts will not change the mind of every
potential churner, uplift modeling is a second prediction after the initial prediction that identifies potential churners likely to respond
positively to marketing messages.

If Not Treated

+
+

If Treated

Sure Things

Do Not Di
stur
bs

Those that would have had a positive response
either way and thus represent wasted marketing
costs.

Persuadables

-

-

Those who would have had a positive response but
are then negatively impacted by marketing and thus
should not be targeted.

Lost Causes

The target group - those that would have had a
negative response but are then positively
persuaded by marketing.

Those that would have responded negatively with
or without marketing and thus represent wasted
marketing costs.

AI-powered customer acquisition: Businesses can develop machine learning-based systems that help sales prioritize their work
by assigning an individual probability of conversion to each prospect, whether that prospect is an individual or a group.
One insurance company working with Dataiku did this by first looking at data on existing clients (specifically, their cost of acquisition
and lifetime value). They then used this analysis to establish “look alikes” for each prospect - that is, an existing customer who has
similar characteristics and therefore will likely mirror the future actions of the prospect.
The end result of this system is a tool available for sales that allows them to more effectively prioritize their prospects by providing
two pieces of information to consider: likelihood of conversion and likelihood of recuperation of acquisition costs. The team also
created an interactive map containing this data so that any travel to visit prospects could be maximized by visiting other promising
prospects nearby.

3

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

Optimal pricing and conversion: Optimal pricing is a delicate balance between understanding what the customer is willing to
pay as well as how much risk they bring. The sheer number of factors and amount of data at play make it a great use case for AI;
however, accurate price prediction is also incredibly challenging.
For example, AXA used machine learning to predict if a driver may cause a large-loss case during the insurance period and using
Random Forest - a common machine learning algorithm that is very accurate for certain use cases - and achieved only 40 percent
accuracy. In order to get to nearly 80 percent accuracy, AXA ultimately developed a much more complex deep learning (neuralnetwork) model.
Improved customer service, driven by machine learning-powered customer segmentation: This category is broad, but
important, as increasingly, it’s the customer experience that provides the “stickiness” needed to retain clients. AI-powered
innovations that range from applications delivering personalized offerings to internal recommendation engines allowing
representatives to offer relevant services can help drive additional revenue.

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

4

FEATURE

A Glance at ADA:
Aviva’s Algorithmic Decision Agent
• 33k employees globally (16k in the United Kingdom)
• The UK’s largest multi-line insurer
• Global data science practice - called Quantum - has more than 700 data and analytics professionals
• The Customer Data Science Team is the company’s customer-first data center-of-excellence and is made up
entirely of data scientists

One of the Customer Data Science Team at Aviva’s most celebrated projects is ADA (Algorithmic Decision Agent),
Aviva’s personalization AI, which helps the company be more specific and relevant to its customers. The AI, built
using Dataiku, helps the company understand its customers better and delivers tailored marketing experiences
based on their needs.

“As a customer data science team, we’re always looking at how we can make things
better for customers. And happily, that also tends to drive profit.”
- Tom Spencer, Head of Customer Data Science | Aviva

“When we started building ADA … it was taking us quite a long time with the existing
legacy systems. But through using Dataiku and the API functionality, we reduced the
amount of time from beginning to end to build a model and push out the model into the
marketing channel.”
- Ayca Kandur, Data Scientist | Aviva

5

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

Enhanced client advisory on financial protection services: In some markets, insurers play a key role in providing long-term
financial protection and retirement schemes to customers. In this field, fueling deep understanding of clients in recommendation
engines capable of adjusting to a wide range of financial profiles help insurers to deliver much more tailored financial advisory. Such
recommendation engines, developed as proprietary tools by the insurance companies, enable them to position themselves as longterm advisors to their clients, offering appropriate recommendations for risk-adjusted solutions.

MITIGATING
RISK
Claims forecasting and prediction: In the age of AI and algorithms, older modeling techniques fail to incorporate the wide variety
of data sources needed to produce forecasts precise enough for the modern enterprise. Traditional claims reserve estimates don’t
look at the individual characteristics of policyholders, which effects predictability of future claims.
AI-based systems can use machine learning to take into account many more patterns in data than a human could, including those
individual characteristics for better accuracy. And because AI-based systems can easily scale with automation, they can predict
payments at an individual policy level, not just at the group level.
Regulatory reporting automation: Insurance companies worldwide have to deal with a slew of regulatory reporting standards
that are both time consuming and risky if done incorrectly. The very act of having a centralized AI platform in which all data projects
are built and stored is, in and of itself, a step in the right direction for smoother regulatory reporting.
With all actions logged and - in the case of Dataiku - a transparent view of all data pipelines, regulatory compliance can be monitored
more real-time to ensure that there are no surprises if an audit arises. In case of an audit, AI itself can help automate manual work
like validation of customer data, customer data security operations, and more.

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

6

Winning the Race for ESG in
the Investment Space:
A Data and Modeling Game

FEATURE

ESG, SRI, impact investment: all of these names and acronyms point to the same reality, which is that the demand
for responsible investment has moved from promise to fact. The need for long-term appreciation of risks beyond
pure financial factors - even more so in the face of today’s uncertain health crisis - is pushing for accelerated
demand from investors toward the full integration of Environment, Social, and Governance dimensions in the
investment strategies run by asset managers.
“Our ambition is to better take into account environmental, social, and governance-related risks in investment
decisions.” What could sound straightforward is turning into a true mind bender for all investment professionals
who are confronted with pressure from regulators, NGOs, and clients to act and demonstrate.
At the same time, the markets are still very much trying to get a full grasp on the topic, struggling to put frameworks
in place on subjects (not to mention public debate and regulations) that continuously evolve. On top of this
uncertainty are strong local specificities plus a general lack of stakeholder alignment on the appropriate KPIs to
track the different ESG components, and it’s easy to see why the topic is so complex.
But waiting for norms to emerge is not an option: asset managers who do not position themselves in a credible
manner will simply fail to survive in a market already marked by increased competition, challenges of traditional
active strategies by passives and alternatives, and continuous cost of regulation.
Out of the several ingredients needed to win the race for ESG, two stand out as essentials:
1. Strong convictions, embedded throughout processes and built on…
2. Distinctive ESG models capable of delivering strong ESG-adjusted performance and adapting to ongoing
evolutions of the ESG landscape.
It’s a Data Jungle Out There (& Getting Thicker by the Minute)
Asset managers who enter into the ESG space have to learn to navigate the world of external data providers with
their respective frameworks, coverage, naming conventions, and types of data. To which they need to add all the
material provided by brokers, research bodies, specialists, NGOs, and news feeds.
Asset managers who want to be serious about ESG more often than not end up juggling more than a dozen
providers on top of their traditional financial data sources. And this jungle only gets thicker by the minute: as some
large providers (such as MSCI and Sustainalytics) start emerging as industry standards and become commodities,
asset managers are encouraged to combine these new must-haves with data issued by smaller organizations
focusing on specific topics to have detailed impact tracking and preserve a differentiating edge.

7

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

FEATURE
To build their framework, asset managers will have to make choices, resting on the right selection of large
providers, niche organizations, and public data, combined with internal insights and proprietary signals built
from unstructured data. The capacity to blend, test, complete data coverage through machine learning, reverse
engineer, and quickly review over time will be paramount to win the ESG race. So will be the capacity to enrich the
set foundations with new data sources. For example, in an environment marked by new extreme weather hazards,
can satellite images step in to assess emerging risks of exposure to flooding?
Turning the Switch to ESG: The Need for Collaboration
For a traditional portfolio manager, entering into the field of ESG starts with data. ESG scores aim at giving
an overview of the ESG health of an issuer or asset, including controversy scores, CO2 emissions, or revenue
exposure to sensitive activities such as coal and weapons production, etc. But asset managers can’t expect all
their professionals to become ESG experts over the course of a few days (or even a few weeks or months).
Furthermore, ESG data should not be seen as just a simple addition to existing framework; it behaves
fundamentally differently than traditional financial data, and more often than not, it demands full end-to-end
rethinking of processes.
If the ambition is to manage portfolios with the objective of alignment with a 2° scenario, should this link to the
exclusion of full sectors - and is that manageable from a liquidity and diversification standpoint? What are the
appropriate indicators and how do they fit in with traditional financial indicators? Are their ways of developing
proprietary signals to give forward views on stock or yield-curve evolution based on environmental performance?
Putting such frameworks in place requires bringing together ESG experts along with traditional portfolio
managers, portfolio engineers, and operation process owners around the same environment to jointly build the
right models. Keeping this topic a specialist one is a tempting route, but it will not produce the internal buy-in nor
the models that can, in the long run, deliver true ESG risk-adjusted performance.
Sharing the Right ESG Impact View
The ultimate objective of responsible investment is to better hedge investors versus emerging ESG risks, aligning
their investments with their own beliefs. This might mean covering desire to disinvest from controversial activities
such as coal production, tar sands, weaponry, palm oil, or to actively tune exposure toward social wellbeing,
green activities development, gender balance, etc.
ESG topics don’t stop with asset classes. A growing demand is emerging for cross-asset class views with
consolidated perspectives on risks and exposures (beyond traditional ways of reporting and managing assets).
To develop this type of ESG fiduciary and advisory service, asset managers will need to build new models as well
as demonstrate agility to preserve their operational core foundations while building views and advice adapted to
the specific demands of their clients.

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

8

FEATURE
Working with traditional frameworks will not solve this complex equation. New data types will need to play a role,
along with new types of stress tests and models to help clients make the appropriate decisions. Fully preparing all
business lines to foster these models and demonstrating proactive advice to clients using resulting learnings will
be among key ingredients to emerge as true ESG leaders.
The Bottom Line
Most asset managers - not to mention investors - are only starting their journey on ESG. The topic remains a true
shape-shifter, supported by strong local specificities, which makes it all the more difficult to capture. That makes
it even more important to recognize the inherent complexity of the topic and the need to root answers in:
• Solid data and modeling foundations, combined with…
• Collaboration across all experts
Both of these will be paramount for asset managers, putting them on the right track to capture current demand
and answer future needs, including the need to leverage AI as the switch from reaction to proactivity becomes a
must-have.

ABOUT THE AUTHOR
Sophie Dionnet is VP Strategy at Dataiku where she drives strategic projects and helps financial players on their
path to Enterprise AI. She has 14 years of experience in the asset management industry and notably acted as
COO for a multi-asset portfolio management division, conducting large scale IT, regulatory, and transformation
projects, including active development of responsible investment.

9

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

DECREASED
COSTS
(OPERATIONS)
Claims processing: The potential for AI to improve the claims processes is massive because it not only promises reduced costs from
eliminating inefficiencies, but also increased customer satisfaction that has the opportunity to increase revenue.
Today’s cutting-edge insurtech and - more recently - traditional insurance businesses are increasingly looking to AI for faster triage
(e.g., larger claims with more uncertainty can be started more quickly by specialized teams, letting smaller claims be closed out even
faster). Plus, technologies like deep learning (specifically natural language processing, or NLP) and computer vision move automatic
processing into the realm of possibility, allowing businesses to move away from time-consuming, manual processing.
In fact, automation and advanced prioritization have the potential to touch nearly every part of the claims process, from intake to
assessment to settlement. As an added bonus to speed, automation also can ensure fewer human errors and easier auditing. This
is not to say that claims automation removes humans from the loop entirely; rather, much like fraud detection, it allows them to be
leveraged more smartly only in cases where a human is truly essential. For example, claims with missing data might be routed to a
human who can handle the case (and bonus: a bot could see how the human resolves the missing data and learn for future cases).
AI is allowing for such quick advancement and efficiency gain around claims handling that some companies - including New Yorkbased Lemonade - are able to pay out claims in less than a day, resulting in high satisfaction from its growing base of loyal customers.
Underwriting: Machine learning is well suited for underwriting, identifying patterns in diverse data sources (from imagery to credit
bureau data) to create a more tailored risk assessment.
For example, unstructured data can be incorporated to improve underwriting decisions. Public satellite or private imagery can be
quickly and automatically analyzed to confirm risks on a property or site, while data from IoT devices (combined with other publiclyavailable information on individuals) can be instantly parsed for much more accurate - and timely - assessments of coverage.
Of course, insurance rating laws require rates and rating factors that are not excessive, inadequate, or unfairly discriminatory. AI has
the potential to reinforce current underwriting approaches if developed with a strict white-box approach with full understanding
and traceability of factors and resulting outputs (read more about this in the section Responsible AI & the Insurance Industry).
Fraud detection and prevention: Insurance organizations are all exposed to fraud risks, whether dealing with false claims, false
billings, unnecessary procedures, staged incidents, withholding of information, and much more. This industry must be on the cutting
edge of technology to stay ahead of fraudsters and reduce losses.
With limited resources on fraud investigation teams, every investigation into a case ultimately identified as low risk is wasted time.
Hiring more staff to conduct these manual audits is an expensive and inefficient option - instead, the key is optimizing that team’s
work by using AI to detect fraudulent activity with a higher degree of accuracy. With detailed, specific small data from patients and
providers feeding into these large data sets for analysis, audit teams look only at the highest-risk cases and can therefore detect
more fraud.

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

10

FEATURE

Accurately Identifying Fraudulent Claims
Santéclair, a health network (part of Allianz), found fraudulent reimbursements stemmed both from opticians as
well as patients, but they didn’t have a system in place that allowed them to effectively analyze the right data and
that would adapt with increasingly sophisticated fraudsters. Instead, they relied on “if-then-else” business rules
to identify likely fraud cases, which resulted in the manual audit team spending their time on too many low-risk
cases. With the increase of reimbursement volume (more than $1.5 million a year), they needed to improve their
efficiency and productivity.
Leveraging Automation and Advanced Machine Learning
Santéclair identified these high-risk cases using Dataiku by:
• Outsmarting fraudsters with advanced machine learning algorithms that continually update and
automatically learn or retrain using the latest data so that any new fraud patterns are immediately
identified and audited. Dataiku handles the entire workflow, from raw data to exposing the predictive
model to the operational applications.
• Automatically combining hundreds of variables from different datasets, including patient/prescriber
history, interaction graphs, prescription characteristics, and other contextual data.
• Allowing teams to develop their data science skills through Dataiku’s collaborative, easy-to-use interface.
Saving Customers Money with 3x More Effective Fraud Detection
Due to the comprehensive solution developed with Dataiku, Santéclair and Eulidia have:
• Enabled fraud detection teams to target actual fraud cases three times more effectively.
• Reduced time-to-market for similar projects by making a POC in a few weeks and then industrializing
the project within a few months with a low impact on the IT team, thanks to the production-ready
component in Dataiku.
• Saved their customers a lot of money by decreasing fraudulent behaviors in the health network and
excluding the fraudsters from the network.
• Saved time with a model automatically updated and monitored along the way to prevent drifting of
performance with little human supervision

“In less than a year, Santéclair has developed an unprecedented fraud detection system
using Dataiku that allows our company to handle a growing volume of invoices and
control costs. By choosing Dataiku, Santéclair was able to internalize its data skills and
pursue additional analytics projects.”
- Jocelyn Philippe, Head of Partnerships and Development at Santéclair

11

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

Improvement and automation of processes: There are still plenty of manual business processes in the industry outside of claims
processing that can be improved through robotic process automation (RPA) and AI. One major example is policy management,
but even smaller customer-side improvements (like streamlining the application process) can save time and reduce the chance of
human error.

AI Meets Mail Processing:
AI for Insurance Admin Tasks

FEATURE

Even as we continue to reach new technological milestones and solve the world's most demanding problems,
many insurance companies are still confronted with the oldest of administrative nightmares: piles and piles of
physical mail.
Head of Dataiku AI Lab Léo Dreyfus-Schmidt offered a scalable solution to the eternal problem of mail processing
by using AI and deep learning techniques to solve the four major problems of mail processing, driven by a real use
case for an insurance company:
1. Distinguishing if a letter is handwritten or typed
2. Parsing the text from a typed letter
3. Detecting words in handwritten letters
4. Extracting meaning from the images of words
Their goal was ultimately to deliver a production-ready tool that could be used to automatically sort any letters
received and send them to their appropriate departments. Traditionally, this would have to be done by hand -- an
expensive and time-consuming task.
The first challenge that the data team had to overcome was a very heterogeneous data set. While initially expecting
to receive a pretty even mix of handwritten and typed letters, the actual training set contained a mix of letters,
envelopes, forms, leaflets, and other forms of written documents.
With the 200,000 unlabeled images that they received, they went through the long process of labeling every
document by type using a webapp they built. This allowed them to begin building their deep learning model on
a large training set of data.

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

12

FEATURE
The Model
In a process that involved constructing a vector representation of the document images using an autoencoder (a
process explained more thoroughly in the talk on the subject) and running a Random Forest machine learning
model on the dataset, the team was able to successfully distinguish hand-written documents from typed ones.
Extracting text from images now accurately identified as being typed was the first (and perhaps most straightforward)
step. They used an open source OCR (Optical Character Recognition) engine called Tesseract to do this.
Then came the hard part -- the handwritten letters. This process involved using computer vision techniques to
detect paragraphs on the page and then to detect words from those paragraphs. They then stacked two common
layers of deep learning techniques to learn and read the visual characteristics of those words.
Using some open-source datasets (and some augmented versions of those datasets) as the training set, they were
then able to create a deep learning model in Dataiku that was able to identify the meaning of those handwritten
words with fairly high confidence. Once this step was completed, the team had an operational method of extracting
meaning from all of the incoming documents.

BEHIND THE MODEL
Léo Dreyfus-Schmidt is a mathematician and holds a PhD in pure mathematics from University of Oxford and
University of Paris VII. After five years focusing on homological algebra and representation theory in Paris, Oxford,
and the University of California - Los Angeles, he joined Dataiku where he has been developing solutions for
predictive maintenance, personalized ranking systems, price elasticity, and natural language applications.

13

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

COMPETITIVE EDGE
(INNOVATION)

Seamless customer experience: The idea of a better customer experience by weaving AI throughout processes - from acquisition
all the way through claims processing, prevention actions and advisory - in order to retain clients has already been well-covered
throughout this section on use cases.
However, it’s worth mentioning again the competitive advantage that a truly seamless customer experience can provide. In addition,
as more processes become automated and as client information is leveraged in a refined manner, client service operators can
switch from being task managers to giving a more human, tailored experience to customers, providing client experiences capable of
competing with promises of new fintechs.
Product development: Machine learning and AI open up opportunities for new products that were previously impossible, perhaps
because of risk or cost. For example, private insurance coverage for some very risky exposures are unavailable today for lack of a
dynamic model that can actually accurately represent the possible losses. More sophisticated and complex models can incorporate
data from more sources than ever before to accurately estimate loss outcomes and probabilities.
Improved agility around data and AI also offers insurance companies the opportunity to better adapt to new client demands and
emerging risks. For example, the rise of environmental concerns and increase in climate change-related risks, which demands
adaptation both in pricing models and products with new types of data lacking maturity of traditional KPIs, is a key area where
mastery of data science-related techniqueswill come as an essential differentiator.

Trends in AI for Insurance
Understanding how AI can be valuable in different parts of the business is easy; but actually executing on any of the use cases described
in the previous section requires calculated coordination between people, processes, and technology. As more and more insurance
companies start their journey in building Enterprise AI, there are a few trends emerging as best practice:

People
Growingcollabo
ration et
bw een a
dta scienti
sts and
actuaries:
Though the two are currently largely
working on different projects, the crossover is growing.
Finding ways (likely via technology) to encourage and
facilitate their collaboration is paramount to the
success of insurance companies in AI initiatives.
Especially considering that actuaries largely outnumber
data scientists in most traditional insurance
organizations, getting the two to work toward a
common goal is critical.

GO FURTHER: UBS on How to Build a Data Science
Service Center of Excellence

Processes

GO FURTHER: Get the White Paper Enabling AI Services
through Operationalization and
Self-Service Analytics

GO FURTHER: The Guidebook to Going From Excel to
Dataiku

Technology

GO FURTHER: Get the White Paper: Why Enterprises Need
AI Platforms
GO FURTHER: Get the White Paper: Get Up to Speed With
NLP

15

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

FEATURED USE CASE

Bringing Insurance into the Age of AI:
On Growing Data Science Collaboration
Analysts and actuaries have been around practically for centuries to bring mathematical models to the world of
insurance. But data science, machine learning, and AI have the potential to take it one step further.
Even so, getting these initiatives off the ground has generally not been easy for most in the insurance business.
The industry is often characterized as traditional and slow-moving, or worse, one that is not as customer-centric
as it should be. However, there are some working to - and succeeding at - bucking these trends to bring data
science to the forefront of insurance, transforming the way the business works with data for the better.
Aviva’s Keys to Success
Aviva, the United Kingdom’s largest multi-line insurer, developed their Customer Data Science Team around two
years ago to compliment their already existing (and robust) global data practice.
We talked to Aviva’s Head of Customer Data Science Tom Spencer and Data Scientist Ayca Kandur about what
makes their team 5 times faster at going from raw data to production:
1. Good data. Upstream to downstream, one of the most important contributors to great data science is
great data. For Spencer and his team, that means not only high quality raw data, but having a staff that
knows what data is, what it means, and where it comes from.
2. Proper tooling. When Spencer started building the Customer Data Science Team at Aviva, his first priority
was getting great people, but a close second was getting the tools in place that would allow that team to
work together and to be its most productive. Today, the entire data science team uses Dataiku for every
step of the data pipeline, from connecting to data to data preparation, building models to deploying
them to production, and everything in between.
3. Staying grounded in results. The team at Aviva recognizes that data science is neither fun nor useful
if the business ultimately isn’t actually using what they’re producing, so they have a strong focus on
pushing to production (not just playing in a sandbox) for real impact.
4. Staying Agile. Delivering value fast is also important to Aviva, which means the Customer Data Science
Team strikes a balance between quick-and-dirty R&D and more structured push-to-production strategies.

“The answers often aren’t from insurance, they’re from other disciplines out in the world.
Our job, then, is to partner with our colleagues that have deep insurance expertise for a
data-driven outlook.”
- Tom Spencer, Head of Customer Data Science | Aviva
Read Aviva’s full story

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

16

Responsible AI & the Insurance
Industry
Going from producing one machine learning model a year
to thousands a day is well within the average insurance
organization’s reach, and operationalization has made it
possible for a single model to impact millions of decisions (as
well as people). Yet, despite the exponential increase in the

In a 2019 survey of more than 400 data
professionals, Dataiku asked: Does your
organization have processes in place to ensure
data science, machine learning, and AI are
leveraged responsibly and ethically?

amount of machine learning models in production, only a
few companies have dedicated the time and effort to ensure
these models are deployed responsibly.
Responsible AI is perhaps even more important to consider
for insurance organizations, as they must strike a delicate
balance between efficiency and profitability as well as

The overall responses vs. those of respondents in
the financial services industry were as follows:
Does Your Organization Have Processes in Place
to Ensure Data Science, ML, and AI
are Leveraged Responsibly and Ethically?

customer satisfaction, trust, and regulatory compliance.

13%

A responsible use of AI covers three main dimensions,

No

which should all be considered when developing an
organizational implementation strategy:
• Accountability: Ensuring that models are designed
and behave in ways aligned with their purpose. This
includes using white-box over black-box models when
it makes sense, which is more inherent in the insurance

9%

35%

I’m not
sure

No, but we're
working on it

43%
Yes

world due to regulations but still can be a challenge
when it comes to increasing model complexity (read

Respondents in the Finance Industry

more in White-Box vs. Black-Box Models: Balancing

5%

Interpretability and Accuracy) as well as taking the

I’m not
sure

right precautions when it comes to model bias (read
3 Steps Toward More Ethical AI).

16%
No

44%
Yes

35%

No, but we're
working on it

17

©2020 Dataiku,
Dataiku, Inc.
Inc. || www.dataiku.com
www.dataiku.com || contact@dataiku.com
contact@dataiku.com || @dataiku
@dataiku
©2020

• Sustainability: Establishing the continued reliability of AI-augmented processes in their operation as well as execution. This
means that no matter what changes in technology or architecture lie ahead, they don’t have adverse effects on existing models
in production.
• Governability: Centrally controlling, managing, and auditing the Enterprise AI effort. This means expanding the idea of governance
beyond the traditional IT sense and instead thinking about it more globally, from access and security all the way up to centralized
model management (so-called MLOps).
Given these dimensions (which are each complex in and of themselves), it’s clear that having a comprehensive strategy for
responsible AI is not easy. So where can organizations begin?
Of course, it’s important to empower people, hold them accountable, and develop concrete policies and processes around how
responsible AI should be executed. But underpinning all of this is having the right technology, which can enable people and
processes while at the same time delivering accountability, sustainability, and governability of data and AI projects. Dataiku, for
example, is technology agnostic and built for best-practice methodology and governance throughout a model’s entire lifecycle,
including concrete features for responsible AI efforts like advanced, granular model explainability.

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

18

Challenges & Solutions
to Get Started
Despite progress and trends moving the industry in the right direction, there are undoubtedly still challenges that can hinder progress
as organizations move forward on the path to Enterprise AI. This section will address just a few and propose solutions:
Finding and hiring data talent: Hiring people with skills in machine learning and AI is extremely difficult across industries due to a
shortage of talent and skyrocketing demand. But the insurance industry is better positioned than most to overcome this challenge via
upskilling. With tens (possibly hundreds) of statistics-minded actuaries already on staff, providing the right tools to nudge them into the
world of AI is a small step. Upskilling business staff is also a great way to fill talent gaps, and in many ways, it’s necessary for insurance
businesses that want to democratize AI. Ultimately, it’s much more difficult to teach a pure AI talent the ins-and-outs of the business
than to teach someone who knows the business like the back of their hand some basic skills for using data in their day-to-day work.
Legacy tools: While insurance is undoubtedly more advanced when it comes to technology than some of the financial services players
on the banking side (just look at cloud adoption), there is still a lingering issue of legacy tools and systems. It becomes infinitely more
difficult to upskill staff per the previous section if data tools are difficult to use, aimed exclusively at the coder population, or even if the
user experience is constantly changing with each new technology that gets introduced. AI platforms like Dataiku can help resolve this
challenge by being a unified visual abstraction layer for data projects, providing robust features no matter who the audience (coder or
non-coder) and a consistent experience no matter what the underlying changes in technology.
Breaking down silos: Today’s enterprises tend to be siloed in many ways - from individuals to entire teams and potentially also data,
it’s difficult to get everything (and everyone) working together across these lines. Again, tools can help when it comes to breaking down
the silos of data. But when it comes to people, the answer is collaboration. Insurance organizations should be looking for ways on AI
projects that actuaries can work with data scientists and also with excerpts from the business side to come up with the best possible
solutions to high-value use cases.

19

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

Conclusion
This white paper has covered at a very high level some of the use cases, challenges, and next steps for insurance, but obviously there
are many other types of insurance business out there that might have different needs or use cases when it comes to data science,
machine learning, and AI.
However, no matter the specific applications, the model for moving into this new era and for success remains the same:
People: It’s essential to start educating and upskilling staff on data science and machine learning technologies and
initiatives. It’s become increasingly clear that the only way to transform a business around data is for the initiative to be
democratized; that is, not only supported from the top down, but also the bottom up.
Processes: One of the biggest challenges in democratizing working with data across the business is having the systems and
processes to do so. Setting up self-service analytics systems that allow people to access and use data in a controlled way
is a good way to get started.
Technology: Of course, mobilizing people and creating processes are difficult to do without the right technology. Data
science, machine learning, and AI platforms can facilitate the journey; for example, Dataiku does this at scale by making
data accessible to a wider population within the enterprise, facilitating and accelerating the design of machine learning
models, and by providing a centralized, controlled, and governable environment.

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

20

Your Path to
Enterprise AI

300+

CUSTOMERS

Dataiku is one of the world’s leading AI and machine

30,000+

learning platforms, supporting agility in organizations’
data efforts via collaborative, elastic, and responsible AI,
Dataiku to underpin their essential business operations

ACTIVE USERS

and ensure they stay relevant in a changing world.

*data scientists, analysts, engineers, & more

all at enterprise scale. Hundreds of companies use

1. Clean & Wrangle

5. Monitor & Adjust

Network_dataset

Test

Test_Scored

2

Netezza
Teradata

Train

MLlib_Prediction

Oracle

HDFS_Avro

Joined_Data

Amazon_S3

HDFS_Parquet

2

Vertica

Cassandra

2. Build + Apply
Machine Learning

4. Deploy
to production

3. Mining
& Visualization

21

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

©2020 Dataiku, Inc. | www.dataiku.com | contact@dataiku.com | @dataiku

22

WHITE PAPER
www.dataiku.com

REPORT

GETTING THE
FUTURE RIGHT
―
ARTIFICIAL
INTELLIGENCE AND
FUNDAMENTAL
RIGHTS

© European Union Agency for Fundamental Rights, 2020
Reproduction is authorised provided the source is acknowledged.
For any use or reproduction of photos or other material that is not under the European Union Agency for Fundamental Rights
copyright, permission must be sought directly from the copyright holders.
Neither the European Union Agency for Fundamental Rights nor any person acting on behalf of the Agency is responsible
for the use that might be made of the following information.

Luxembourg: Publications Office of the European Union, 2020
Print

ISBN 978-92-9474-861-4

doi:10.2811/58563

TK-03-20-119-EN-C

PDF

ISBN 978-92-9474-860-7

doi:10.2811/774118

TK-03-20-119-EN-N

© Photo credits:
Cover: HQUALITY/Adobe Stock
Page 5: Mimi Potter/Adobe Stock
Page 8: monsitj/Adobe Stock
Page 14: Monsitj/Adobe Stock
Page 16: Mykola Mazuryk/Adobe Stock
Page 20: metamorworks/Adobe Stock
Page 25: Gorodenkoff/Adobe Stock
Page 28: Dimco/Adobe Stock
Page 32: VideoFlow/Adobe Stock
Page 37: zapp2photo/Adobe Stock
Page 41: bestforbest/Adobe Stock
Page 44: zapp2photo/Adobe Stock
Page 47: European Communities
Page 52: blacksalmon/Adobe Stock

Page 61: Copyright © 2020 CODED BIAS - All Rights Reserved
Page 63: Siberian Art/Adobe Stock
Page 68: Good Studio/Adobe Stock
Page 75: Sikov/Adobe Stock
Page 79: robsonphoto/Adobe Stock
Page 82: thodonal/Adobe Stock
Page 86: blackboard/Adobe Stock
Page 88: blackboard/Adobe Stock
Page 92: Monopoly919/Adobe Stock
Page 95: Gorodenkoff/Adobe Stock
Page 96: Freedomz/Adobe Stock
Page 100: Copyright © 2020 CODED BIAS - All Rights Reserved
Page 103: Copyright © 2020 CODED BIAS - All Rights Reserved

Foreword
Did you know that artificial intelligence already plays a role in deciding
what unemployment benefits someone gets, where a burglary is likely to
take place, whether someone is at risk of cancer, or who sees that catchy
advertisement for low mortgage rates?
We speak of artificial intelligence (AI) when machines do the kind of things
that only people used to be able to do. Today, AI is more present in our lives
than we realise – and its use keeps growing. The possibilities seem endless.
But how can we fully uphold fundamental rights standards when using AI?
This report presents concrete examples of how companies and public
administrations in the EU are using, or trying to use, AI. It discusses the
potential implications for fundamental rights and shows whether and how
those using AI are taking rights into account.
FRA interviewed just over a hundred public administration officials, private
company staff, as well as diverse experts – including from supervisory and
oversight authorities, non-governmental organisations and lawyers – who
variously work in the AI field.
Based on these interviews, the report analyses how fundamental rights are
taken into consideration when using or developing AI applications. It focuses
on four core areas – social benefits, predictive policing, health services and
targeted advertising. The AI uses differ in terms of how complex they are,
how much automation is involved, their potential impact on people, and how
widely they are being applied.
The findings underscore that a lot of work lies ahead – for everyone.
One way to foster rights protection is to ensure that people can seek remedies
when something goes awry. To do so, they need to know that AI is being
used. It also means that organisations using AI need to be able to explain
their AI systems and how they deliver decisions based on them.
Yet the systems at issue can be truly complex. Both those using AI systems,
and those responsible for regulating their use, acknowledge that they do not
always fully understand them. Hiring staff with technical expertise is key.
Awareness of potential rights implications is also lacking. Most know that
data protection can be a concern, and some refer to non-discrimination. They
are less aware that other rights – such as human dignity, access to justice and
consumer protection, among others – can also be at risk. Not surprisingly,
when developers review the potential impact of AI systems, they tend to
focus on technical aspects.
To tackle these challenges, let’s encourage those working on human rights
protection and those working on AI to cooperate and share much-needed
knowledge – about tech and about rights.

1

Those who develop and use AI also need to have the right tools to assess
comprehensively its fundamental rights implications, many of which may not
be immediately obvious. Accessible fundamental rights impact assessments
can encourage such reflection and help ensure that AI uses comply with
legal standards.
The interviews suggest that AI use in the EU, while growing, is still in its
infancy. But technology moves quicker than the law. We need to seize the
chance now to ensure that the future EU regulatory framework for AI is firmly
grounded in respect for human and fundamental rights.
We hope the empirical evidence and analysis presented in this report spurs
policymakers to embrace that challenge.
Michael O’Flaherty
Director

2

Contents

Foreword ��������������������������������������������������������������������������������������������������������������������������������������������������������������� 1
Key findings and FRA opinions ��������������������������������������������������������������������������������������������������������������������������� 5
1

AI AND FUNDAMENTAL RIGHTS – WHY IT IS RELEVANT FOR POLICYMAKING ������������������������������� 15
1.1.

WHY THIS REPORT? ����������������������������������������������������������������������������������������������������������������������� 17

1.2.

WHAT DO WE MEAN BY ARTIFICIAL INTELLIGENCE? ���������������������������������������������������������������� 19

1.3.	
AI AND FUNDAMENTAL RIGHTS IN THE EU POLICY FRAMEWORK: MOVING
TOWARDS REGULATION ��������������������������������������������������������������������������������������������������������������� 21
ENDNOTES ���������������������������������������������������������������������������������������������������������������������������������������������� 24
2

PUTTING FUNDAMENTAL RIGHTS IN CONTEXT – SELECTED USE CASES OF AI IN THE EU ������������� 25
2.1.

EXAMPLES OF AI USE IN PUBLIC ADMINISTRATION ���������������������������������������������������������������� 30

2.2. EXAMPLES OF AI USE IN THE PRIVATE SECTOR ������������������������������������������������������������������������ 37
ENDNOTES ���������������������������������������������������������������������������������������������������������������������������������������������� 45
3

FUNDAMENTAL RIGHTS FRAMEWORK APPLICABLE TO AI ������������������������������������������������������������ 47
3.1.

FUNDAMENTAL RIGHTS FRAMEWORK GOVERNING THE USE OF AI �������������������������������������� 47

3.2.

‘USE CASE’ EXAMPLES ���������������������������������������������������������������������������������������������������������������� 50

3.3.

REQUIREMENTS FOR JUSTIFIED INTERFERENCES WITH FUNDAMENTAL RIGHTS ������������������ 52

ENDNOTES ���������������������������������������������������������������������������������������������������������������������������������������������� 54
4

IMPACT OF CURRENT USE OF AI ON SELECTED FUNDAMENTAL RIGHTS ���������������������������������������� 57
4.1.

PERCEIVED RISKS �������������������������������������������������������������������������������������������������������������������������� 57

4.2.

GENERAL AWARENESS OF FUNDAMENTAL RIGHTS AND LEGAL FRAMEWORKS IN
THE AI CONTEXT �������������������������������������������������������������������������������������������������������������������������� 58

4.3.

HUMAN DIGNITY �������������������������������������������������������������������������������������������������������������������������� 60

4.4.

RIGHT TO PRIVACY AND DATA PROTECTION – SELECTED CHALLENGES ��������������������������������� 61

4.5.

EQUALITY AND NON-DISCRIMINATION �������������������������������������������������������������������������������������� 68

4.6.

ACCESS TO JUSTICE ���������������������������������������������������������������������������������������������������������������������� 75

4.7.

RIGHT TO SOCIAL SECURITY AND SOCIAL ASSISTANCE ������������������������������������������������������������ 79

4.8.

CONSUMER PROTECTION ������������������������������������������������������������������������������������������������������������ 79

4.9.

RIGHT TO GOOD ADMINISTRATION ��������������������������������������������������������������������������������������������� 81

ENDNOTES ���������������������������������������������������������������������������������������������������������������������������������������������� 83
5

FUNDAMENTAL RIGHTS IMPACT ASSESSMENT – A PRACTICAL TOOL FOR PROTECTING
FUNDAMENTAL RIGHTS �������������������������������������������������������������������������������������������������������������������� 87
5.1.

CALLING FOR A FUNDAMENTAL RIGHTS IMPACT ASSESSMENT – AVAILABLE
GUIDANCE AND TOOLS ���������������������������������������������������������������������������������������������������������������� 87

5.2.

IMPACT ASSESSMENTS AND TESTING IN PRACTICE ���������������������������������������������������������������� 91

5.3.

FUNDAMENTAL RIGHTS IMPACT ASSESSMENT IN PRACTICE �������������������������������������������������� 96

ENDNOTES ���������������������������������������������������������������������������������������������������������������������������������������������� 99
6

MOVING FORWARD: CHALLENGES AND OPPORTUNITIES ������������������������������������������������������������� 101
3

Figures

4

Figure 1:

Companies using AI in 2020, by Member State (%) ������������������������������������������������������������������������������������������������������������� 26

Figure 2:

Examples of different automation and complexity levels in use cases covered ��������������������������������������������������������������� 27

Figure 3:

Words interviewees most often used to describe the AI ‘use cases’ ��������������������������������������������������������������������������������� 29

Figure 4:

Awareness of GDPR right to opt out from direct marketing, in the EU and United Kingdom, by country and
region (%) ��������������������������������������������������������������������������������������������������������������������������������������������������������������������������������� 65

Figure 5:

Awareness of right to have a say when decisions are automated, by age, gender and difficulty in paying bills (%) ������� 67

Figure 6:

Awareness about the risks of discrimination when using AI, by country (%) ������������������������������������������������������������������� 73

Figure 7:

Correlations of words respondents most often mention when discussing future plans to use AI ������������������������������� 102

Key findings and FRA opinions

New technologies have profoundly changed how we organise and live
our lives. In particular, new data-driven technologies have spurred the
development of artificial intelligence (AI), including increased automation of
tasks usually carried out by humans. The COVID-19 health crisis has boosted
AI adoption and data sharing – creating new opportunities, but also challenges
and threats to human and fundamental rights.

Developments in AI have received wide attention by the media, civil
society, academia, human rights bodies and policymakers. Much of that
attention focuses on its potential to support economic growth. How different
technologies can affect fundamental rights has received less attention. To
date, we do not yet have a large body of empirical evidence about the wide
range of rights AI implicates, or about the safeguards needed to ensure that
the use of AI complies with fundamental rights in practice.
On 19 February 2020, the European Commission published a White Paper on
Artificial Intelligence – A European approach to excellence and trust. It outlines
the main principles of a future EU regulatory framework for AI in Europe.
The White Paper notes that it is vital that such a framework is grounded in
the EU’s fundamental values, including respect for human rights – Article 2
of the Treaty on European Union (TEU).
This report supports that goal by analysing fundamental rights implications
when using artificial intelligence. Based on concrete ‘use cases’ of AI
in selected areas, it focuses on the situation on the ground in terms of
fundamental rights challenges and opportunities when using AI.

5

Legal
framework

The overarching fundamental rights framework* that applies to the use of AI in the
EU consists of the Charter of Fundamental Rights of the EU (the Charter) as well as the
European Convention on Human Rights.
Multiple other Council of Europe and international human rights instruments are relevant.
These include the 1948 Universal Declaration of Human Rights and the major UN human
rights conventions.**
In addition, sector-specific secondary EU law, notably the EU data protection acquis and
EU non-discrimination legislation, helps safeguard fundamental rights in the context of AI.
Finally, the national laws of EU Member States also apply.
*

For more, see FRA (2012), Bringing rights to life: The fundamental rights landscape of the
European Union, Luxembourg, Publications Office of the European Union.

** These major conventions include: the 1966 International Covenant on Civil and Political Rights;
the 1966 International Covenant on Economic, Social and Cultural Rights; the 1965 International
Convention on the Elimination of All Forms of Racial Discrimination; the 1979 Convention on the
Elimination of All Forms of Discrimination against Women; the 1984 Convention against Torture;
the 1989 Convention on the Rights of the Child; the 2006 Convention on the Rights of Persons with
Disabilities; and the 2006 International Convention for the Protection of All Persons from Enforced
Disappearance.
For more on the universal international human rights law framework, including their enforcement
mechanisms, see e.g. De Schutter, O. (2015), International Human Rights Law: Cases, Materials,
Commentary, Cambridge, Cambridge University Press, 2nd edition.

The report is based on 91 interviews with officials in public administration
and staff in private companies, in selected EU Member States. They were
asked about their use of AI, their awareness of fundamental rights issues
involved, and practices in terms of assessing and mitigating risks linked to
the use of AI.
Moreover, 10 interviews were conducted with experts who deal, in various
ways, with the potential fundamental rights challenges of AI. This group
included public bodies (such as supervisory and oversight authorities), nongovernmental organisations and lawyers.

6

SAFEGUARDING FUNDAMENTAL RIGHTS – SCOPE,
IMPACT ASSESSMENTS AND ACCOUNTABILITY
Considering the full scope of fundamental rights
with respect to AI

Using AI systems engages a wide range of fundamental
rights, regardless of the field of application. These
include – but also go beyond – privacy, data protection,
non-discrimination and access to justice.
The EU Charter of Fundamental Rights (the Charter)
became legally binding in December 2009 and has the
same legal value as the EU treaties. It brings together
civil, political, economic and social rights in a single text.
Pursuant to Article 51 (1) of the Charter, the institutions,
bodies, offices and agencies of the Union have to respect
all the rights as embodied in the Charter. EU Member
States have to do so when they are implementing Union
law. This applies equally to AI as to any other field.
The fieldwork of this research shows that a large
variety of systems are used under the heading of AI.
The technologies analysed entail different levels of
automation and complexity. They also vary in terms of
the scale and potential impact on people.
FRA’s findings show that using AI systems implicate a
wide spectrum of fundamental rights, regardless of the
field of application. These include, but also go beyond,
privacy and data protection, non-discrimination and
access to justice. Yet, when addressing the impact of AI
with respect to fundamental rights, the interviews show,
the scope is often delimited to specific rights.
A wider range of rights need to be considered when
using AI, depending on the technology and area of use. In
addition to rights concerning privacy and data protection,
equality and non-discrimination, and access to justice,
other rights could be considered. These include, for
example, human dignity, the right to social security and
social assistance, the right to good administration (mostly
relevant for the public sector) and consumer protection
(particularly important for businesses). Depending on
the context of the AI use, any other right protected in
the Charter needs consideration.

FRA OPINION 1
When introducing new policies and
adopting new legislation on AI, the
EU legislator and the Member States,
acting within the scope of EU law,
must ensure that respect for the full
spectrum of fundamental rights, as
enshrined in the Charter and the EU
Treaties, is taken into account. Specific
fundamental rights safeguards need to
accompany relevant policies and laws.
In doing so, the EU and its Member
States should rely on robust evidence
concerning AI’s impact on fundamental
rights to ensure that any restrictions
of certain fundamental rights respect
the principles of necessity and
proportionality.
Relevant safeguards need to be
provided for by law to effectively
protect against arbitrary interference
with fundamental rights and to give
legal certainty to both AI developers
and users. Voluntary schemes
for observing and safeguarding
fundamental rights in the development
and use of AI can further help mitigate
rights violations. In line with the
minimum requirements of legal clarity
– as a basic principle of the rule of
law and a prerequisite for securing
fundamental rights – the legislator has
to take due care when defining the
scope of any such AI law.
Given the variety of technology
subsumed under the term AI and
the lack of knowledge about the full
scope of its potential fundamental
rights impact, the legal definition of
AI-related terms might need to be
assessed on a regular basis.

7

Using effective impact assessments to prevent
negative effects

FRA OPINION 2
The EU legislator should consider making
mandatory impact assessments that
cover the full spectrum of fundamental
rights. These should cover the private
and public sectors, and be applied
before any AI-system is used. The
impact assessments should take into
account the varying nature and scope
of AI technologies, including the level of
automation and complexity, as well as
the potential harm. They should include
basic screening requirements that can
also serve to raise awareness of potential
fundamental rights implications.
Impact assessments should draw on
established good practice from other
fields and be regularly repeated during
deployment, where appropriate. These
assessments should be conducted in a
transparent manner. Their outcomes
and recommendations should be in the
public domain, to the extent possible.
To aid the impact assessment process,
companies and public administration
should be required to collect the
information needed for thoroughly
assessing the potential fundamental
rights impact.
The EU and Member States should
consider targeted actions to support
those developing, using or planning
to use AI systems, to ensure effective
compliance with their fundamental
rights impact assessment obligations.
Such actions could include funding,
guidelines, training or awareness
raising. They should particularly – but
not exclusively – target the private
sector.
The EU and Member States should
consider using existing tools, such as
checklists or self-evaluation tools,
developed at European and international
level. These include those developed by
the EU High-Level Group on Artificial
Intelligence.

Prior impact assessments mainly focus on technical
issues. They rarely address potential effects on
fundamental rights. This is because knowledge on how
AI affects such rights is lacking.
Deploying AI systems engages a wide spectrum of
fundamental rights, regardless of the field of application.
Pursuant to Article 51 (1) of the Charter, EU Member States
must respect all rights embodied in the Charter when
they are implementing Union law. In line with existing
international standards – notably the United National
Guiding Principles on Business and Human Rights (UNGPs)
– businesses should have in place “a human rights due
diligence process to identify, prevent, mitigate and account
for how they address their impacts on human rights”
(Principles 15 and 17). This is irrespective of their size and
sector, and encompasses businesses working with AI.
While pursuing its commitments to the UNGPs, the EU
has adopted several legislative acts addressing sectorspecific instruments, in particular in the context of due
diligence-related obligations for human rights. Discussions
are currently underway on proposing new EU secondary
law. Such law would require businesses to carry out due
diligence of the potential human rights and environmental
impacts of their operations and supply chains. Such law
would likely be cross-sectoral and provide for sanctions
for non-compliance – which should encompass the use of
AI. See FRA’s recent report on Business and Human rights
– access to remedy, which calls for improved horizontal
human rights diligence rules for EU-based companies.
Impact assessments are an important tool for businesses
and public administration alike to mitigate the potential
negative impact of their activities on fundamental rights.
EU law in specific sectors requires some forms of impact
assessments, such as Data Protection Impact Assessments
under the General Data Protection Regulation (GDPR).
Many interviewees reported that a data protection impact
assessment, as required by law, was conducted. However,
these took different forms. Moreover, prior assessments,
when conducted, focus mainly on technical aspects. They
rarely address potential impacts on fundamental rights.
According to some interviewees, fundamental rights impact
assessments are not carried out when an AI system does
not, or appears not to, affect fundamental rights negatively.

The research shows that the interviewees’ knowledge on
fundamental rights – other than data protection and, to
some extent, non-discrimination – is limited. The majority
acknowledge, however, that the use of AI has an impact
on fundamental rights. Some interviewees indicate that their systems do not
affect fundamental rights, which is to some extent linked to the tasks the AI
systems are used for.
All respondents are aware of data protection issues. Most respondents also
realise that discrimination could – generally – be a problem when AI is used.

8

However, the exact meaning and applicability of rights related to data protection
and non-discrimination remains unclear to many respondents.
The research findings show differences between the private and public sector.
Interviewees from the private sector are often less aware of the wider range of
fundamental rights that could be affected. Data protection issues are known to
the private sector. However, other rights, such as non-discrimination or access
to justice-related rights, are less well known among business representatives
who work with AI. Some were fully aware of potential problems. But others
said that the responsibility for checking fundamental rights issues lies with
their clients.

Ensuring effective oversight and overall
accountability

Businesses and public administrations that are
developing and using AI are in contact with various
bodies that are responsible for overseeing AI-related
systems within their respective mandates and sectors.
These bodies include data protection authorities. But
those using AI are not always sure which bodies are
responsible for overseeing AI systems.
In line with well-established international human rights
standards – for example, Article 1 of the European
Convention on Human Rights (ECHR) and Article 51 of the
Charter – states are obliged to secure people’s rights and
freedoms. To effectively comply, states have to – among
others – put in place effective monitoring and enforcement
mechanisms. This applies equally with respect to AI.
At the level of monitoring, the findings point to the
important role of specialised bodies established in specific
sectors that are also responsible for AI oversight within
their mandates. These include, for example, oversight
in the area of banking, or data protection authorities.
A variety of such bodies are potentially relevant to the
oversight of AI from a fundamental rights perspective.
However, the responsibilities of bodies concerning
the oversight of AI remains unclear to many of those
interviewed from the private and the public sector.
Public administrations’ use of AI is sometimes audited,
as part of their regular audits. Private companies in
specific sectors also have specialised oversight bodies,
for example in the area of health or financial services.
These also check the use of AI and related technologies,
for example as part of their certification schemes. Private
sector interviewees expressed a wish for bodies that could
provide expert advice on the possibilities and legality of
potential AI uses.
In the EU, there is a well-developed set of independent
bodies with a mandate to protect and promote fundamental
rights. These include data protection authorities, equality
bodies, national human rights institutions and ombuds
institutions. The research shows that those using or
planning to use AI often contacted different bodies about
their use of AI, such as consumer protection bodies.

FRA OPINION 3
The EU and Member States should ensure that
effective accountability systems are in place
to monitor and, where needed, effectively
address any negative impact of AI systems
on fundamental rights. They should consider,
in addition to fundamental rights impact
assessments (see FRA opinion 2), introducing
specific safeguards to ensure that the
accountability regime is effective. This could
include a legal requirement to make available
enough information to allow for an assessment
of the fundamental rights impact of AI systems.
This would enable external monitoring and
human rights oversight by competent bodies.
The EU and Member States should also
make better use of existing oversight expert
structures to protect fundamental rights
when using AI. These include data protection
authorities, equality bodies, national human
rights institutions, ombuds institutions and
consumer protection bodies.
Additional resources should be earmarked to
establish effective accountability systems by
‘upskilling’ and diversifying staff working for
oversight bodies. This would allow them to
deal with complex issues linked to developing
and using AI.
Similarly, the appropriate bodies should be
equipped with sufficient resources, powers
and – importantly – expertise to prevent and
assess fundamental rights violations and
effectively support those whose fundamental
rights are affected by AI.
Facilitating cooperation between appropriate
bodies at national and European level can help
share expertise and experience. Engaging with
other actors with relevant expertise – such
as specialist civil society organisations – can
also help. When implementing such actions at
national level, Member States should consider
using available EU funding mechanisms.
9

Most often, users of AI contacted data protection authorities to seek guidance, input
or approval where personal data processing was involved. Interviewed experts
highlight the relevance of data protection authorities for overseeing AI systems with
respect to the use of personal data. However, they also note that data protection
authorities are under-resourced for this task and lack specific expertise on AI issues.
Experts, including those working for oversight bodies such as equality bodies and
data protection authorities, agree that the expertise of existing oversight bodies
needs to be strengthened to allow them to provide effective oversight of AI related
issues. According to the experts, this can be challenging given that these bodies’
resources are already stretched. They also highlighted the important role of relevant
civil society organisations specialised in the fields of technology, digital rights and
algorithms. They can enhance accountability in the use of AI systems.

NON-DISCRIMINATION, DATA PROTECTION AND ACCESS TO
JUSTICE: THREE HORIZONTAL THEMES
The research shows that the use of AI affects various fundamental rights.
Apart from context-related specific aspects that affect different rights to a
varying extent, the fundamental rights topics which emerged in the research to
repeatedly apply to most AI cases include: the need to ensure non-discriminatory
use of AI (right not to be discriminated); the requirement to process data legally
(right to personal data protection); and the possibility to complain about AI-based
decisions and seek redress (right to an effective remedy and to a fair trial).
The two main fundamental rights highlighted in the interviews are data
protection and non-discrimination. In addition, effective ways to complain
about the use of AI came up repeatedly, linked to the right to a fair trial
and effective remedy. The following three FRA opinions, which reflect these
findings, should be read alongside the other opinions, which call for a more
comprehensive recognition of, and response to, the full
range of fundamental rights affected by AI.

Specific safeguards to ensure non-discrimination
when using AI

FRA OPINION 4
EU Member States should consider
encouraging companies and public
administration to assess any potentially
discriminatory outcomes when using AI
systems.
The European Commission and Member
States should consider providing funding
for targeted research on potentially
discriminatory impacts of the use of AI
and algorithms. Such research would
benefit from the adaptation of established
research methodologies, from the social
sciences, that are employed to identify
potential discrimination in different areas
– ranging from recruitment to customer
profiling.
Building on the results of such research,
guidance and tools to support those
using AI to detect possible discriminatory
outcomes should be developed.
10

Interviewees rarely mentioned carrying out detailed
assessments of potential discrimination when using AI.
This suggests a lack of in-depth assessments of such
discrimination in automated decision making.
The obligation to respect the principle of nondiscrimination is enshrined in Article 2 of the TEU,
Article 10 of the TFEU (requiring the Union to combat
discrimination on a number of grounds), and Articles 20
and 21 of the Charter (equality before the law and nondiscrimination on a range of grounds). More specific and
detailed provisions in several EU directives also enshrine
this principle, with varying scopes of application.
Automation and the use of AI can greatly increase
the efficiency of services and can scale up tasks that
humans would not be able to undertake. However, it is
necessary to ensure that services and decisions based on
AI are not discriminatory. Recognising this, the European
Commission recently highlighted the need for additional

legislation to safeguard non-discrimination when using AI in the EU antiracism action plan 2020-2025.
Most interviewees are in principle aware that discrimination might happen.
Yet, they rarely raised this issue themselves. Only few believe their systems
could actually discriminate.
Interviewees also rarely mentioned detailed assessments of potential
discrimination, meaning that there is a lack of in-depth assessment of potential
discrimination.
A common perception is that omitting information about protected attributes,
such as gender, age or ethnic origin, can guarantee that an AI system does
not discriminate. This is not necessarily true, however. Information potentially
indicating protected characteristics (proxies), which can often be found in
datasets, could lead to discrimination.
In certain cases, AI systems can also be used to test for and detect discriminatory
behaviour, which can be encoded in datasets. However, very few interviewees
mentioned the possibility of collecting such information about disadvantaged
groups to detect potential discrimination. In the absence of in-depth analysis
of potential discrimination in the actual use of AI systems, there is also almost
no discussion and analysis of the potential positive effect of using algorithms
to make decisions fairer. Moreover, none of the interviewees working on AI
mentioned using AI to detect possible discrimination as a positive outcome, in
the sense that discrimination can be better detected when data are analysed
for potential bias.
Since detecting potential discrimination through the use of AI and algorithms
remains challenging, and interviewees only briefly addressed the issue, different
measures are needed to address this. These include the requirement to consider
issues linked to discrimination when assessing the use of AI, and investment
into further studies of potential discrimination that use a diverse range of
methodologies.
This could involve, for example, discrimination testing. This could build on similar
established methodologies for testing bias in everyday life, such as with respect
to job applications, where the applicant’s name is changed to (indirectly) identify
ethnicity. In relation to AI applications, such tests could involve the possible
creation of fake profiles for online tools, which only differ with respect to
protected attributes. In this way, the outcomes can be checked with respect to
potential discrimination. Research could also benefit from advanced statistical
analysis to detect differences in datasets concerning protected groups, and
therefore can be used as a basis for exploring potential discrimination.
Finally, some research interviews underscored that results from complex
machine learning algorithms are often very difficult to understand and explain.
Thus, further research to better understand and explain such results (so-called
‘explainable AI’) can also help to better detect discrimination when using AI.

11

More guidance on data protection

More clarity is needed on the scope and meaning of
legal provisions regarding automated decision making.

FRA OPINION 5
The European Data Protection Board
(EDPB) and the European Data
Protection Supervisor (EDPS) should
consider providing further guidance
and support to effectively implement
GDPR provisions that directly apply
to the use of AI for safeguarding
fundamental rights, in particular as
regards the meaning of personal data
and its use in AI, including in AI training
datasets.
There is a high level of uncertainty
concerning the meaning of automated
decision making and the right to
human review linked to the use of AI
and automated decision making. Thus,
the EDPB and the EDPS should also
consider further clarifying the concepts
of ‘automated decision making’ and
‘human review’, where they are
mentioned in EU law.
In addition, national data protection
bodies should provide practical
guidance on how data protection
provisions apply to the use of
AI. Such guidance could include
recommendations and checklists,
based on concrete use cases of AI,
to support compliance with data
protection provisions.

Data protection is critical in the development and use of
AI. Article 8 (1) of the Charter and Article 16 (1) of the TFEU
provide that everyone has the right to the protection of
their personal data. The GDPR and the Law Enforcement
Directive (Directive (EU) 2018/680) further elaborate
on this right, and include many provisions applicable to
the use of AI.
The interviewees indicated that most of the AI systems
they employ use personal data, meaning data protection
is affected in many different ways. However, a few
applications – according to the interviewees – do not
use personal data, or only use anonymised data, and
hence data protection law would not apply. If personal
data are used, all data protection related principles and
provisions apply.
This report highlights an important issue linked to data
protection, which is also relevant for other fundamental
rights with respect to automated decision making.
According to a Eurobarometer survey, only 40 % of
Europeans know that they can have a say when decisions
are automated. Knowledge about this right is considerably
higher among those working with AI – the majority of
interviewees raised this issue. However, many of the
interviewees, including experts, argued that more clarity
is needed on the scope and meaning of legal provisions
on automated decision making.
In the area of social benefits, interviewees mentioned only
one example of fully automated, rule-based decisions.
All other applications they mentioned are reviewed by
humans. Interviewees in public administration stressed
the importance of human review of any decisions.
However, they rarely described what such human review
actually involves and how other information was used
when reviewing output from AI systems.

While interviewees disagree as to whether or not the existing legislation is
sufficient, many called for more concrete interpretation of the existing data
protection rules with respect to automated decision making, as enshrined
in Article 22 of the GDPR.

12

Effective access to justice in cases involving
AI-based decisions

To effectively contest decisions based on the use of AI,
people need to know that AI is used, and how and where
to complain. Organisations using AI need to be able to
explain their AI system and decisions based on AI.
Access to justice is both a process and a goal, and is crucial
for individuals seeking to benefit from other procedural
and substantive rights. It encompasses a number of core
human rights. These include the right to a fair trial and to
an effective remedy under Article 6 and 13 of the ECHR
and Article 47 of the EU Charter of Fundamental Rights.
Accordingly, the notion of access to justice obliges states
to guarantee each individual’s right to go to court – or,
in some circumstances, an alternative dispute resolution
body – to obtain a remedy if it is found that the individual’s
rights have been violated.
In accordance with these standards, a victim of a human
rights violation arising from the development or use of an
AI system by a public or private entity has to be provided
with access to remedy before a national authority. In line
with relevant case law under Article 47 of the Charter and
Article 13 of the ECHR, the remedy must be “effective in
practice as well as in law”.
The research findings identify the following preconditions
for the remedy to be effective in practice in cases
involving AI systems and their impact on fundamental
rights: everyone needs to be aware when AI is used and
informed of how and where to complain. Organisations
using AI must ensure that the public is informed about
their AI system and the decisions based on them.

FRA OPINION 6
The EU legislator and Member States
should ensure effective access to
justice for individuals in cases involving
AI-based decisions.
To ensure that available remedies are
accessible in practice, the EU legislator
and Member States could consider
introducing a legal duty for public
administration and private companies
using AI systems to provide those
seeking redress information about
the operation of their AI systems.
This includes information on how
these AI systems arrive at automated
decisions. This obligation would help
achieve equality of arms in cases of
individuals seeking justice. It would
also support the effectiveness of
external monitoring and human
rights oversight of AI systems (see
FRA opinion 3).
In view of the difficulty of explaining
complex AI systems, the EU, jointly
with the Member States, should
consider developing guidelines to
support transparency efforts in this
area. In so doing, they should draw on
the expertise of national human rights
bodies and civil society organisations
active in this field.

The findings show that explaining AI systems and how
they make decisions in layman terms can be challenging.
Intellectual property rights can hamper the provision of detailed information
about how an algorithm works. In addition, certain AI systems are complex.
This makes it difficult to provide meaningful information about the way a
system works, and on related decisions.
To tackle this problem, some companies interviewed avoid using complex
methods for certain decision making altogether, because they would not be
able to explain the decisions. Alternatively, they use simpler data analysis
methods for the same problem to obtain some understanding about the main
factors influencing certain outcomes. Some of the private sector interviewees
pointed to efforts made to gradually improve their understanding of AI
technology.

13

14

1

AI AND FUNDAMENTAL RIGHTS – WHY
IT IS RELEVANT FOR POLICYMAKING
Artificial intelligence (AI) is increasingly used in the private and public sectors,
affecting daily life. Some see AI as the end of human control over machines.
Others view it as the technology that will help humanity address some of its
most pressing challenges. While neither portrayal may be accurate, concerns
about AI’s fundamental rights impact are clearly mounting, meriting scrutiny
of its use by human rights actors.
Examples of potential problems with using AI-related technologies in relation
to fundamental rights have increasingly emerged. These include:
――an algorithm used to recruit human resources was found to generally
prefer men over women;1
――an online chatbot2 became ‘racist’ within a couple of hours;3
――machine translations showed gender bias;4
――facial recognition systems detect gender well for white men, but not for
black women;5
――a public administration’s use of algorithms to categorise unemployed
people did not comply with the law;6
――and a court stopped an algorithmic system supporting social benefit
decisions for breaching data protection laws.7
These examples raise profound questions about whether modern AI systems
are fit for purpose and how fundamental rights standards can be upheld
when using or considering using AI systems.
This report addresses these questions by providing a snapshot of the current
use of AI-related technologies in the EU – based on selected use cases – and
its implications on fundamental rights.

15

FRA’s work
on AI, big
data and
fundamental
rights

This report is the main publication stemming from FRA’s project on Artificial intelligence,
big data and fundamental rights. The project aims to assess the positive and negative
fundamental rights implications of new technologies, including AI and big data.
The current report builds on the findings of a number of earlier papers:
•

Facial recognition technology: fundamental rights considerations in the context of law
enforcement (2019): this paper outlines and analyses fundamental rights challenges
triggered when public authorities deploy live FRT for law enforcement purposes. It also
briefly presents steps to take to help avoid rights violations.

•

Data quality and artificial intelligence – mitigating bias and error to protect
fundamental rights (2019): this paper highlights the importance of awareness and
avoidance of poor data quality.

•

#BigData: Discrimination in data-supported decision making (2018): this focus paper
discusses how such discrimination can occur and suggests possible solutions.

As part of the project, FRA is also exploring the feasibility of studying concrete examples of
fundamental rights challenges when using algorithms for decision making through either
online experiments or simulation studies.
Several other FRA publications address relevant issues:

16

•

The Guide on Preventing unlawful profiling today and in the future (2018) illustrates
what profiling is, the legal frameworks that regulate it, and why conducting profiling
lawfully is both necessary to comply with fundamental rights and crucial for effective
policing and border management.

•

The Handbook on European data protection law (2018 edition) is designed to
familiarise legal practitioners not specialised in data protection with this area of law.

•

Data from FRA’s Fundamental Rights Survey. It surveyed a random sample of 35,000
people across the EU, including findings on people’s opinions and experiences linked to
data protection and technology (2020) and security (2020).

•

FRA’s report on Business and human rights – access to remedy analyses obstacles and
promising practices in relation to access to remedies for victims of business-related
human rights abuses. By analysing complaints mechanisms in EU Member States, the
research maps what hinders and what facilitates access to remedies.

1.1.	 WHY THIS REPORT?
The growing attention to AI and its potential to drive economic growth has not
been matched by a body of evidence about how different technologies can
affect fundamental rights – positively or negatively. Only concrete examples
allow for a thorough examination of whether, and to what extent, applying
a technology interferes with various fundamental rights – and whether any
such interference can be justified, in line with the principles of necessity
and proportionality.
This report provides a fundamental rights-based analysis of concrete ‘use
cases’ – or case studies. ‘Use case’ is a term in software engineering. This
report loosely defines it as the specific application of a technology for a
certain goal used by a specified actor.
The report illustrates some of the ways that companies and the public sector
in the EU are looking to use AI to support their work, and whether – and
how – they are taking fundamental rights considerations into account. In
this way, it contributes empirical evidence, analysed from a fundamental
rights perspective, that can inform EU and national policymaking efforts to
regulate the use of AI tools.
What did the research cover?
FRA conducted fieldwork research in five EU Member States: Estonia, Finland,
France, the Netherlands and Spain. It collected information from those involved
in designing and using AI systems in key private and public sectors on how
they address relevant fundamental rights issues.
The research – based on 91 personal interviews – gathered information on:
――the purpose and practical application of AI technologies;
――the assessments conducted when using AI and the applicable legal
framework and oversight mechanisms;
――the awareness of fundamental rights issues and potential safeguards
in place; and
――future plans.
In addition, 10 experts involved in monitoring or observing potential
fundamental rights violations concerning the use of AI, including civil society,
lawyers and oversight bodies, were interviewed.
Presenting the main findings
This report presents the main findings of the fieldwork. In particular, the
report includes:
――An overview of the use of AI in the EU across a range of sectors, with a
focus on: (1) social benefits, (2) predictive policing, (3) healthcare, and
(4) targeted advertising.
――An analysis of the awareness of fundamental rights and further implications
on selected rights, with a focus on the four use cases.
――A discussion of measures to assess and mitigate the impact of AI-related
technologies on people’s fundamental rights.
Two annexes, available on FRA’s website, supplement the report:
――Annex 1 gives a detailed description of the research methodology and
the questions asked in the interviews.
――Annex 2 provides examples of potential errors when using AI in selected
areas.
17

In addition, country-specific information on each of the five Member States
covered complements the fieldwork. This research, delivered by the contractor,
is also available on FRA’s website. It maps policy developments on AI and
the legal framework governing its use in different sectors.
Supporting rights-compliant policymaking
This report provides evidence on the extent to which fundamental rights
considerations are brought into discussions and activities to develop, test, employ
and monitor AI systems in the EU. It also highlights how different technologies
can affect some of the rights set out in the Charter, and reflects on how to protect
these rights as AI becomes both more widespread and more sophisticated.
The analysis of selected fundamental rights challenges can help the EU and
its Member States, as well as other stakeholders, assess the fundamental
rights compatibility of AI systems in different contexts. The findings in the
report about current views and practices among those using AI supports
policymakers in identifying where further actions are needed.
The report does not aim to provide a comprehensive mapping of the use of
different AI systems in the five EU Member States covered by the research,
or to provide in-depth technical information about how the different systems
mentioned by the interviewees work.

Conducting
the
interviews

Who?
This report is based on 91 semi-structured interviews with representatives from public
administration and private companies who are involved in the use of AI for their services and
businesses. FRA intentionally provided a very general definition of AI to those interviewed as
part of the research, based on existing definitions.
The organisations interviewed were active in public administration in general, with some
working in law enforcement.
The private companies include those working in health, retail, pricing and marketing,
financial services, insurance, employment, transport and energy. Importantly, except for two
interviewees, the research did not include companies that sell AI to other companies. Instead,
the entities use AI to support their own operations.
In addition, ten interviews were conducted with experts dealing with potential challenges of
AI in public administration (e.g. supervisory authorities), in non-governmental organisations
or as lawyers working in this field.
Where?
Interviews were carried out in five EU Member States (Estonia, Finland, France, the
Netherlands and Spain). These countries were selected based on their different levels
of uptake of AI technology and of policy development in the area of AI, as well as to
incorporate experience from across different parts of the EU.
How?
FRA outsourced the fieldwork to Ecorys. FRA staff supervised the work, and developed
the research questions and methodology. Interviewers received dedicated training before
conducting the fieldwork.
Interviews were carried out anonymously. As a consequence, no information identifying the
organisation concerned is provided in the report. In addition, certain details of the applications
described – most notably the country – are omitted to protect respondents’ anonymity. This
was communicated to interviewees, increasing their level of trust and allowing them to speak
more freely about their work. It also proved useful for recruiting respondents.

18

1.2.	 WHAT DO WE MEAN BY ARTIFICIAL INTELLIGENCE?
There is no universally accepted definition of AI. Rather than referring to
concrete applications, it reflects recent technological developments that
encompass a variety of technologies. Although AI is usually defined very
widely, a survey conducted in 2020 on behalf of the European Commission
among companies in the EU showed that eight in ten people working at
companies in the EU say they know what AI is. Slightly more than two in
10 respondents from companies in the EU-27 do not know (7 %) or are not
sure about (14 %) what AI is.8
FRA’s research did not apply a strict definition of AI on the use
cases it presents. For the interviews, AI was defined broadly,
with reference to the definition provided by the High-Level
Expert Group on Artificial Intelligence (AI HLEG ).

High-level
expert group
on artificial
intelligence
“Artificial intelligence (AI) refers to systems that
display intelligent behaviour by analysing their
environment and taking actions – with some degree
of autonomy – to achieve specific goals. AI-based
systems can be purely software-based, acting
in the virtual world (e.g. voice assistants, image
analysis software, search engines, speech and
face recognition systems) or AI can be embedded
in hardware devices (e.g. advanced robots,
autonomous cars, drones or Internet of Things
applications).”
This initial definition of AI HLEG was subject to
further discussion in the groups. See AI HLEG (2019), A
definition of AI: Main capabilities and disciplines.

The interviewees also expressed a variety of ways to think
about AI. When identifying use cases to explore in the research,
the project focused on applications that support decision
making based on data and machine learning, and applications
and systems that contribute to automating tasks that are usually
undertaken by humans or which cannot be undertaken by
humans due to their large scale. As such, the use cases in this
report provide insight into the different technologies that are
used and discussed in selected areas under the broad heading
of AI. As there may be some contention concerning whether
certain use cases constitute AI at the current level of use, the
report often refers to ‘AI and related technologies’.
The past years have seen an enormous increase in computing
power, increased availability of data and the development of
new technologies for analysing data. The increased amount and
variety of data, sometimes available almost in real time over
the internet, is often referred to as big data. Machine learning
technologies and related algorithms, including deep learning,
benefit enormously from this increased computing power and
data availability, and their development and use is flourishing.

The use of these terms is, however, of limited use. It can even prove
counterproductive, as it triggers ideas linked to science fiction rather than
any real application of AI. A variety of myths exist about what AI is and
can do,9 often spread via (social) media. For example, some claim that AI
can act on its own, being some form of entity. This distracts from the fact
that all AI systems are made by humans and that computers only follow
instructions made and given by humans. For a human-centric approach to
AI, it is important to note that AI can never do anything on its own – it is
human beings who use technology to achieve certain goals. However, the
human work and decision making behind the AI systems is often not visible
or the centre of attention.
“Currently, there is no lawyer
who can tell the definition of AI
and we’ve asked around pretty
thoroughly. No one can tell.”
(Public administration, Netherlands)

Entire studies and many discussions have explored possible AI definitions.
The European Commission’s Joint Research Centre analysed AI definitions.
It highlights that they often refer to issues linked to the perception of the
environment (i.e. the way a system receives input/data from its environment,
e.g. through sensors), information processing, decision making and the
achievement of specific goals. Definitions frequently refer to machines
behaving like humans or taking over tasks associated with human intelligence.
Given the difficulty of defining intelligence, many definitions remain vague.
This makes the use of AI hard to measure in practice10 and, equally, challenging
to define in law.11
19

This report discusses the use of AI based on concrete applications. These
differ in terms of their complexity, level of automation, potential impact on
individuals, and the scale of application.
Most of the discussion around, and the actual use of AI, involves deploying
machine learning technologies. These can be seen as a sub-domain of AI.
There is also some confusion around the term “learning”, which implies that
machines learn like humans. In reality, much of current machine learning
is based on statistical learning methodologies.12 Machine learning uses
statistical methods to find rules in the form of correlations that can help to
predict certain outcomes.
This is different from traditional statistical analysis, because it does not involve
detailed checks of how these predictions were produced (often referred to as
‘black boxes’13). Traditional statistical analysis is based on specific theoretical
assumptions about the data generation processes and the correlations used.14
Machine learning is geared towards producing accurate outcomes, and can
be used for automating workflows or decisions, if an acceptable level of
accuracy can be obtained.
The usual example is an email spam filter, which uses statistical methods to
predict if an email is spam. As it is not important to know why a certain email
was blocked and because spam can be predicted with very high accuracy,
we do not really need to understand how the algorithm works (i.e. based on
what rules emails get blocked). However, depending on the complexity of
the task, prediction is not always possible with high accuracy. Moreover, as
this report highlights, not understanding why certain outcomes are predicted
is not acceptable for certain tasks.
The area of machine learning incorporates several approaches. Most often,
machine learning refers to finding rules that link data to a certain outcome
based on a dataset that includes outcomes (supervised learning). For example,
a dataset of emails, which are labelled as spam or not (‘ham’), is used to find
correlations and rules that are associated with spam emails in this dataset.
These rules are then used to ‘predict’ with some degree of likelihood if any
future email is spam or not.
Sometimes, machine learning is used to find hidden groups in datasets
without defining a certain outcome (unsupervised learning) – for example,
segmenting people into groups based on similarities in their demographics.

20

Finally, rules and correlations can be found through trial and error
(reinforcement learning). These systems try to optimise a certain goal
through experimentation, and update their rules automatically to have the
best possible output. Such systems need enormous amounts of data and
can hardly be used on humans, as it involves experimentation. They were
mainly responsible for the success of winning board games against humans,
which were often sensationalised by media.

1.3.	AI AND FUNDAMENTAL RIGHTS IN THE EU POLICY
FRAMEWORK: MOVING TOWARDS REGULATION
Policymakers have for some time highlighted the potential for AI and related
technologies to improve efficiency and drive economic growth. Yet public
authorities and international organisations have only recently reflected on
the fundamental rights challenges associated with such technologies. Coupled
with the growing use and accuracy of AI systems, this has turned attention
to whether and how to regulate their use.
A 2017 European Parliament resolution marked a milestone in the EU’s
recognition of the fundamental rights implications of AI. The resolution
stressed that “prospects and opportunities of big data can only be fully tapped
into by citizens, the public and private sectors, academia and the scientific
community when public trust in these technologies is ensured by a strong
enforcement of fundamental rights”.15 It called on the European Commission,
the Member States, and data protection authorities “to develop a strong and
common ethical framework for the transparent processing of personal data
and automated decision-making that may guide data usage and the ongoing
enforcement of Union law”. 16
Later that year, the European Council called for a “sense of urgency to address
emerging trends” including “issues such as artificial intelligence […], while
at the same time ensuring a high level of data protection, digital rights and
ethical standards”.17 The European Council invited the European Commission
to put forward a European approach to AI.
Responding to these calls, the European Commission published in 2018 its
Communication on AI for Europe18 and set up a High Level Expert Group on
AI.19 Both initiatives include a strong reference to fundamental rights.
The Commission-facilitated High Level Expert Group was made up of 52
independent experts from academia, civil society and industry (including a
representative from FRA). It published ‘Ethics Guidelines for Trustworthy AI’
and ‘Policy and investment recommendations for trustworthy AI’ in 2019.
These were developed further in 2020.20 Its work triggered further discussion
on the importance of framing AI in human rights terms, alongside ethical
considerations. This led to the development of Ethics Guidelines that refer
to the Charter and place fundamental rights consideration with respect to AI.
The Ethics Guidelines include an assessment list for trustworthy AI, which has
been translated into a checklist to guide those who develop and deploy AI.21
Indicating political support at the highest level, the European Council calls
in its Strategic Guidelines for 2019-2024 to “ensure that Europe is digitally
sovereign” and for policy to be “shaped in a way that embodies our societal
values”.22 Similarly, Commission President Von der Leyen committed to “put
forward legislation for a coordinated European approach on the human and
ethical implications of [AI]”.23 This prompted significant moves towards setting
out an EU legal framework to govern the development and use of AI and
related technologies, including with respect to their impact on fundamental
rights.
21

In February 2020, the European Commission published a White Paper on
artificial intelligence. It sets out policy options for meeting the twin objectives
of “promoting the uptake of AI and addressing the risks associated with certain
uses of this new technology”. The paper promotes a common European
approach to AI. It deems this necessary “to reach sufficient scale and avoid
the fragmentation of the single market”. As it notes, “[t]he introduction of
national initiatives risks to endanger legal certainty, to weaken citizens’ trust
and to prevent the emergence of a dynamic European industry”.24 Legal
uncertainty is also a concern of companies planning to use AI.
The Commission White Paper on AI highlights risks to fundamental rights as
one of the main concerns associated with AI. It acknowledges that “the use
of AI can affect the values on which the EU is founded and lead to breaches
of fundamental rights, be it as a result from flaws in the overall design of
AI systems, or from the use of data without correcting possible bias”. It also
lists some of the wide range of rights that can be affected.25
The White Paper on AI indicates the Commission’s preference for the
possible new regulatory framework to follow a risk-based approach, in
which mandatory requirements would, in principle, only apply to high-risk
applications. These would be determined on the basis of two cumulative
criteria: if it is employed in a sector, such as healthcare, transport or parts
of the public sector, where significant risks can be expected to occur; and if
it is used in a manner where significant risks are likely to arise. This latter
risk could be assessed based on the impact on the affected parties, adding
a harm-based element.
The White Paper also highlights some instances where AI use for certain
purposes should be considered high-risk, irrespective of the sector. These
include the use of AI applications in recruitment processes or for remote
biometric identification, including facial recognition technologies.
Following a public consultation, which ran from February to June 2020,26
the Commission is expected to propose legislation on AI in the first quarter
of 2021.27
Ahead of the proposal, the EU’s co-legislators have considered various aspects
of the potential legal framework. In October 2020, the European Parliament
adopted resolutions with recommendations to the European Commission on
a framework of ethical aspects of AI, robotics and related technologies,28
and a civil liability regime for AI.29 It also adopted a resolution on intellectual
property rights for the development of artificial intelligence technologies,30
and continues to work on resolutions on AI in criminal law and its use by
the police and judicial authorities in criminal matters,31 and AI in education,
culture and the audio-visual sector.32 It also established a special committee
on artificial intelligence in the digital age.33
Following their meeting on 1-2 October 2020, the heads of state and
government of the EU Member States declared that the “EU needs to be a
global leader in the development of secure, trustworthy and ethical Artificial
Intelligence” and invited the Commission to “provide a clear, objective definition
of high-risk Artificial Intelligence systems.34 In addition, the Council of the EU
adopted Conclusions on shaping Europe’s digital future35 and on seizing the
opportunities of digitalisation for access to justice, which included a dedicated
section on deploying AI systems in the justice sector.36 The German Presidency
of the Council of the EU published conclusions on the Charter of Fundamental
Rights in the context of artificial intelligence and digital change; the text was
supported, or not objected to, by 26 Member States.37

22

The growing reference to fundamental rights in these discussions indicates
that a fundamental rights framework alongside other legal frameworks38 is
necessary for an effective and human rights compliant evaluation of the many
opportunities and challenges brought by new technologies. Many existing AI
initiatives are guided by ethical frameworks, which are typically voluntary.
A fundamental rights-centred approach to AI is underpinned by legal regulation,
where the responsibility for respecting, protecting and fulfilling rights rests
with the State. This should guarantee a high level of legal protection against
possible misuse of new technologies. It also provides a clear legal basis
from which to develop AI, where reference to fundamental rights – and their
application in practice – is fully embedded.39
In addition to steps towards legal regulation, the EU is taking significant
policy and financial actions to support the development of AI and related
technologies. Alongside the White Paper, the Commission published the
European Data Strategy.40 It aims to set up a single market for data, including
nine common European data spaces, covering areas such as health data
and financial data. The proposal for the 2021-2027 Multiannual Financial
Framework would create a Digital Europe Programme worth € 6.8 billion
to invest in the EU’s “strategic digital capacities”, including AI, in addition to
funding through Horizon Europe and the Connecting Europe Facility.41
Other international actors are also considering steps to regulate AI. Most
notably, the Council of Europe is an active player in the field of AI and related
technologies. In September 2019, the Committee of Ministers of the Council
of Europe set up the Ad Hoc Committee on Artificial Intelligence (CAHAI). It
aims to examine “the feasibility and potential elements of a legal framework
for the development, design and application of AI, based on the Council of
Europe’s standards on human rights, democracy and the rule of law”.42 In
April 2020, the Committee of Ministers of the Council of Europe adopted
recommendations on the human rights impact of algorithmic systems.43
In addition, the Organisation for Economic Cooperation and Development
(OECD) adopted AI principles and created an AI policy observatory.44 At global
level, UNESCO is starting to develop a global standard setting instrument
on AI.45 These are selected examples of the wide range of legal and policy
initiatives aiming to contribute to standard setting in the area of AI. This
includes, amongst others, actual (draft) legislation, soft-law, guidelines and
recommendations on the use of AI, or reports with recommendations for
law and policy.
FRA put together a (non-exhaustive) list of initiatives linked to AI policymaking.46
While these also include legislative initiatives in EU Member States, many
organisations and businesses launched initiatives to tackle ethical concerns
of AI. However, while useful to tackle potential problems with AI, ethical
approaches often rely on voluntary action. This does not sufficiently address
the obligation to respect fundamental rights.
As FRA pointed out in its Fundamental Rights Report 2019: “only a rights-based
approach guarantees a high level of protection against possible misuse of new
technologies and wrongdoings using them.”47 The European Commission’s
initiative on regulating AI helps to avoid disjointed responses to AI across
Member States, which can undermine businesses across the EU and with
entities outside the EU.

23

Endnotes
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
24

Reuters (2018), ‘Amazon scraps secret AI recruiting tool that showed bias against women’, 10 October 2018.
Chatbot or chatterbot is a common AI feature embedded in messaging applications to simulate human conversation through voice or text.
Independent (2017), ‘AI robots learning racism, sexism and other prejudices from humans, study finds’, 17 April 2017.
Prates, M., Avelar, P. and Lamb, L. (2019) ‘Assessing Gender Bias in Machine Translation – A Case Study with Google Translate’, 11 March
2019.
The Gender Shades project evaluating the accuracy of AI powered gender classification products.
See for example: Der Standard (2020), Datenschutzbehörde kippt umstrittenen AMS-Algorithmus, or AlgorithmWatch (2019), Poland:
Government to scrap controversial unemployment scoring system.
Privacy First (2020), Dutch risk profiling system SyRI banned following court decision.
European Commission (2020), European enterprise survey on the use of technologies based on artificial intelligence, Luxembourg, July
2020.
See, for example, the website “AI myths”.
Samoili, S., López Cobo, M., Gómez, E., De Prato, G., Martínez-Plumed, F., and Delipetrev, B. (2020), AI Watch. Defining Artificial
Intelligence. Towards an operational definition and taxonomy of artificial intelligence, Luxembourg.
Schuett, J. (2019), A legal definition of AI, arXiv: 1909.01095
Hastie, T., Tibshirani R., and Friedman, J. (2009), The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer.
See, for example: Pasquale, F. (2015), The Black Box Society. The Secret Algorithms That Control Money and Information, Harvard
University Press, Cambrigde and London; and Rai, A. (2020), ‘Explainable AI: from black box to glass box’, Journal of the Academy of
Marketing Science, Vol. 48, pp. 137-141.
A seminal paper describing this difference is: Breiman, L. (2001), ‘Statistical Modeling: The Two Cultures’, Statistical Science, 2001, Vol. 16,
No. 3, pp. 199-231.
European Parliament resolution of 14 March 2017 on fundamental rights implications of big data: privacy, data protection, nondiscrimination, security and law-enforcement (2016/2225(INI)), para. 1.
Ibid., para. 20.
European Council (2017), European Council meeting (19 October 2017) – Conclusions, EUCO 14/17, Brussels, 19 October 2017, p. 8.
European Commission (2018), Communication from the Commission to the European Parliament, the European Council, the Council, the
European Economic and Social Committee and the Committee of the Regions on Artificial Intelligence for Europe, COM(2018) 237 final,
25 April 2018.
More information is available on the webpage of the High level expert group.
High-Level Expert Group on Artificial Intelligence (2019), Ethics Guidelines for Trustworthy Artificial Intelligence; Policy and investment
recommendations for trustworthy AI.
High-Level Expert Group on Artificial Intelligence (2020), Assessment List for Trustworthy Artificial Intelligence (ALTAI) for selfassessment.
European Council, A New Strategic Agenda 2019-2014, p. 4.
Vonder Leyen, Ursula, A Union that strives for more: My agenda for Europe, p. 13.
European Commission, White Paper On Artificial Intelligence – A European approach to excellence and trust, COM(2020) 65 final, Brussels,
19 February 2020, p. 2.
Ibid., p. 12.
European Commission (2020), White paper on Artificial Intelligence: Public consultation towards a European approach for excellence and
trust,17 July 2020.
European Commission (2020), Adjusted Commission Work Programme 2020, Annex I: New initiatives, 27 May 2020.
European Parliament, Legislative Observatory, Framework of ethical aspects of artificial intelligence, robotics and related technologies,
2020/2012 (INL).
European Parliament resolution of 20 October 2020 with recommendations to the Commission on a civil liability regime for artificial
intelligence, 2020/2014 (INL).
European Parliament resolution of 20 October 2020 on intellectual property rights for the development of artificial intelligence
technologies, 2020/2015 (INI).
European Parliament, Artificial intelligence in criminal law and its use by the police and judicial authorities in criminal matters,
2020/2016 (INI).
European Parliament, Legislative Observatory, Artificial intelligence in education, culture and the audiovisual sector, 2020/2017 (INI).
European Parliament decision of 18 June 2020 on setting up a special committee on artificial intelligence in a digital age, and defining
its responsibilities, numerical strength and term of office, 2020/2684 (RSO).
European Council (2020), Special meeting of the European Council (1 and 2 October 2020) – Conclusions, EUCO 13/20, 2 October 2020.
Council of the European Union (2020), Shaping Europe’s Digital Future – Council Conclusions, 9 June 2020.
Council of the European Union, Council Conclusions “Access to Justice – Seizing the Opportunities of Digitalisation”, 13 October 2020.
Council of the European Union, Presidency Conclusions – the Charter of Fundamental Rights in the context of Artificial Intelligence and
Digital Change, 21 October 2020.
See e.g. Pagallo, U., Casanovas, P. & Madelin, R. (2019), ‘The middle-out approach: assessing models of legal governance in data
protection, artificial intelligence, and the Web of Data’, The Theory and Practice of Legislation 7 (1), pp. 1-25.
See FRA (2019), Fundamental Rights Report 2019, Luxembourg, Publications Office, Chapter 7.
Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the
Committee of the Regions, A European strategy for data, COM/2020/66 final.
European Council, Conclusions from Special meeting of the European Council (17, 18, 19, 20 and 21 July 2020), EUCO 10/20, 21 July 2020.
Council of Europe, Ad Hoc Committee on Artificial Intelligence (CAHAI), Factsheet: Governance for digital transformation.
Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights impacts of
algorithmic systems (adopted by the Committee of Ministers on 8 April 2020 at the 1373rd meeting of the Ministers’ Deputies).
See the dedicated OECD website.
See the dedicated UNESCO website.
See, for an overview by FRA, AI Policy Initiatives, or at the Council of Europe website.
FRA (2019), Fundamental Rights Report, Luxembourg, Publications Office, p. 166.

2

PUTTING FUNDAMENTAL RIGHTS IN
CONTEXT – SELECTED USE CASES OF AI
IN THE EU
In the EU, the use of AI-related technologies is relatively wide-spread.
A recent survey shows that 42 % of companies use AI-related technologies
– and that 18 % plan to do so.

Note on
interviewees

The use cases presented in this
chapter are based on information
obtained in interviews with
both public and private sector
representatives.
The interviewed representatives
from public administration work
in the areas of health services,
infrastructure and energy, the
judiciary, law enforcement,
migration and border management,
social benefits, tax, as well as
transportation and traffic control.
Interviewees from private companies
mainly work in retail, marketing
and pricing, the health sector, in
financial services, energy, insurance,
employment and transport, as well
as in cross-cutting areas with a focus
on AI development for different
sectors.

This chapter presents selected cases of AI use
– typically referred to as ‘use cases’ in the AI
field. FRA collected information on such cases
from five EU Member States: Estonia, France,
Finland, the Netherlands and Spain. They involve
different areas of application across public
administration and private companies. Special
focus is put on the use of AI in the areas of social
benefits; predictive policing; health services;
and targeted advertising.
The chapter provides information on the current
use of AI, as well as basic information on EU
competence, in these select areas. The use cases
provide a good sense of what kind of AI and
related technologies are currently being used.
The examples also offer context for the
fundamental rights analysis. Looking at a
broad variety of use cases provides important
insights on how the actual use of AI can affect
people’s fundamental rights. Chapter 4 includes
a discussion of fundamental rights implications,
and makes reference to the cases described in
this chapter.
25

Use of AI by
companies in
the EU in 2020

According to the European Enterprise Survey, at the beginning of 2020, 42 % of companies
in the EU said they use technologies that depend on AI. This percentage ranges from 27 %
in Estonia and Cyprus to 61 % in Czechia (see Figure 1). Another 18 % of companies are
planning to use AI in the future.
The survey indicates that AI is used mostly in the IT sector (63 %). The technologies used
comprise a variety of IT applications aiming at process or equipment optimisation, anomaly
detection, process automation, and forecasting, price optimisation and decision making.
FIGURE 1:

COMPANIES USING AI IN 2020, BY MEMBER STATE (%)

100
90
80
70
60
50
40
30

26

20

18 21 27

18
48
42

51 50

1
11
54

46
40

12 19
61

8
54

51
43

16 31 21 21
23 14 25 22

9

46

48

40 40
31

20

44
36

33

36

22 19
16

20

10 13
20 20 16
40
36
35
34 35
29
27 27

34

10

Currently using AI

UK

RO
AT
SE
SI
NL
BG
LU
EL
CZ
LT
BE
MT
LV
DK
HR
DE
HU
FR
PT
PL
IT
FI
ES
IE
EE
CY
SK

EU-27

0

Planning to use AI

Notes:	The survey asked about the use or plans for use of ten different AI related technologies, such
as speech recognition, visual diagnostics, fraud detection, analysis of emotions, forecasting
based on machine learning and more. Includes the percentage of companies using at least one
AI technologies. N = 9,640.

Source: FRA, 2020 [based on data extracted from European Commission, European enterprise
survey on the use of technologies based on artificial intelligence, Luxembourg, July
2020]

As noted, this report focuses on four broad AI ‘use cases’:
――social benefits,
――predictive policing,
――health services, and
――targeted advertising.
These areas are particularly sensitive as regards fundamental rights. Two
cover mainly the public administration’s use of AI (social benefits allocation
and predictive policing). The other two concern private companies (health
services and targeted advertising). These use cases provide the basis for
the report’s fundamental rights analysis by offering the necessary context.
Where appropriate, the report also highlights findings from interviews that
cover areas other than these four areas.

“AI and machine learning are
different concepts. AI is an umbrella
term.”

Detailed studies on the taxonomy of AI are available,1 providing further
categorisations of the technology. As noted in the introduction, interviewees
had different views about what AI is and some also stated that there is no
clear definition of AI.

“What you see now is that
everyone doing something with
machine learning is labelling this
as ‘AI’.”

(Private company, Estonia)

(Public administration, Netherlands)

This report discusses specific use cases without further classifying the
technology applied. Yet the use of AI in the cases examined differed: the
26

use of technology described by the interviewees involved both varying levels
of complexity and varying levels of automation.
Figure 2 provides an overview of different examples of use that interviewees
discussed under the heading of AI. Some applications are relatively
straightforward to understand. In rule-based decision making, algorithms
are defined based on ‘if-then-rules’ (for example, if a person has an income
below a certain threshold, then they will be eligible for certain benefits).
Such algorithms were used in the area of social benefits at different levels
of automation, with examples of full, partial or no human review involved.
Other applications used more traditional statistical methods to inform
decisions. These include, for example, regression analysis. This is a classical
statistical method that analyses correlation between several pieces of
information (‘variables’) and an outcome, which is a credit score in this
example. Others used more complex machine learning methodologies to
feed into the production of forecasts and statistics for government reports.
There are also algorithms with much higher levels of complexity, such as
deep learning for diagnosis support in the area of health. Such tools still
include a high level of human review, and hence do not include a high level
of automation.
By contrast, targeted advertising is an example of potentially using highly
complex algorithms without human review of each output and decision, also
using highly complex algorithms including deep learning and reinforcement
learning. (See Chapter 1 for descriptions of these terms.) Human review would
also not be possible in this area due to the scale at which such algorithms
operate.

low

Automation

high

FIGURE 2:

EXAMPLES OF DIFFERENT AUTOMATION AND COMPLEXITY LEVELS IN USE CASES COVERED

rule-based
automated
decision making
on social benefits

regression analysis
predictions for fully
automated credit
scoring

deep learning and
reinforcement
learning for
advertising

rule-based
decision for
positive
outcomes of
social benefits

regression analysis
predictions for
human-reviewed
credit scoring

facial recognition
technology for
identification
with human
review

rule-based
human decisions
for social
benefits

machine learning
supported
production of
forecasts

medical images
analysis for
diagnosis

low

Complexity

Areas of examples
Social welfare
Marketing
Law enforcement
Health services
Financial services

high

Source: FRA, 2020


Notes: The examples from financial
services and the use of facial recognition
technology are not covered in the
detailed use case descriptions, but were
mentioned in other interviews. The
examples illustrate different levels of
complexity and automation, as used in
practice.

AI systems also vary according to the potential harm that could result
from an erroneous decision based on the use of AI. Depending on the area
of application, wrong decisions – based on erroneous outputs from the
system – can have different impacts. When using AI for decision making,
the consequences are different if a decision is affirmative but wrong (false
positive) or negative but wrong (false negative).
These issues are particularly important when machine learning is used, as
it is based on statistical calculations, which always come with some degree
27

of error. While rule-based algorithms can also make mistakes (especially
if they grow more complex), risks are lower because of the deterministic
nature of the rules developed.
For example, when using AI to make decisions on social benefits, a false
positive means that a person may erroneously receive benefits. This does not
necessarily have a negative impact on the person concerned (unless the error
is found out later and the money needs to be
paid back). However, it negatively impacts on the
public administration, as money is paid not in line
with good administration practices. In contrast, a
false negative would have a negative impact on
the individual, because they would not receive
benefits to which they are entitled. Annex 2,
available on FRA’s website, provides further
hypothetical examples of effects of wrong
decisions based on the use cases discussed.
Importantly, when automating tasks, the impact
could also scale up, potentially exacerbating
the negative effect on society as a whole. The
severity and scale of potential harm is one aspect
that needs to be taken into consideration when
analysing potential limitations on fundamental
rights with respect to the use of AI.
For example, small error rates when using facial recognition technology
used by law enforcement might still lead to flagging many innocent people,
if the technology is used at places where many people are analysed. This
might apply to airports or train stations, where thousands of people could
be scanned on a daily basis.2 A potential bias in error rates could then lead
to disproportionally targeting certain groups in society.

Technologies
used across
all cases
identified in
the research

Interviewees mostly mention ‘machine learning’, including the use of neural networks
and its extensions (see Chapter 1 for a description of machine learning). Respondents
either directly mentioned this, or mentioned subfields of machine learning, such as image
recognition or facial recognition technology (FRT).
Most often, interviewees mentioned the use of ‘supervised machine learning’ as mainly
used to optimise for a specifically defined outcome. Yet sometimes ‘unsupervised machine
learning’ was also used to categorise or cluster data. Only one case referred to the use of
‘reinforcement learning’, without going much into detail.
Several respondents used ‘natural language processing (NLP)’. This is a technology to
analyse text and speech, and is sometimes combined with machine learning algorithms.
Few mention examples that involve rule-based algorithms, meaning that the rules for the
algorithm to follow are directly encoded (i.e. based on ‘if-then-rules’).
In some cases, interviewees did not disclose or could not provide detailed information about
the technology used.

Generally, the interviewees referred to more than one use case, but were
asked to focus on one application during interviews.
――Importantly, the fieldwork shows that companies and public administrations
are often still at the beginning of looking into the use of AI. Only about
two thirds of the use cases are actually in use and deployed in practice.
Many of the use cases described by interviewees are at pilot stage, under
development, or still in the research phase.
――Two AI-driven applications were halted after tests.
28

FIGURE 3:

WORDS INTERVIEWEES MOST OFTEN USED TO DESCRIBE THE AI
‘USE CASES’

Notes:	FRA visualisation of the words most frequently used in descriptions
of use cases. The bigger the size of the word, the more often the
interviewees mentioned the terms.

Source: FRA, 2020
Figure 3 shows the most frequently used words to describe the use cases
covered in this report. It highlights the importance of data when using AI
systems as well as its relevance to supporting decision making.
FRA has previously highlighted that a thorough description of the data used by
AI applications is essential for identifying and mitigating potential fundamental
rights challenges.3 A variety of data were used for the AI systems covered
in this report. However, it was difficult to obtain detailed information about
the data used, because most respondents remained rather vague about
their data sources.

“It is mostly used to save time […]
when you have to go through a lot
of material.”
(Public administration, Netherlands)

“The most important is to deal with
cases more efficiently. It’s about
making use of your workforce,
the people who handle cases, as
effectively as possible.”
(Public administration, Netherlands)

Rather generically, many respondents mentioned using ‘open data’, ‘historical
data’ or ‘metadata’. More concretely, respondents mentioned using customer
data, e.g. about purchases or browsing behaviour, or administrative records,
such as data on social benefits and taxes. Interviewees also mentioned
medical records, police records, court records, as well as social media and
traffic data. Data included text data (e.g. e-mails), audio recordings, video,
and geolocation data. Data come from internal databases of companies and
public administration, but also from external sources.
The single most important reason for using AI is increased efficiency. The
vast majority of respondents, across the public and private sector, mentioned
using AI for greater speed, fewer errors and cost reduction, as fewer human
resources are needed. Some interviewees from law enforcement also said
they use AI for safety and security, as well as crime prevention.
Humans previously performed many of the use cases. Some respondents said
they use AI because it entails fewer mistakes than having humans carry out

29

certain tasks. Some respondents also use AI for tasks that humans did not
previously carry out, as the quantity of information could not be processed by
humans – for example, in the area of genome analysis or traffic predictions.
Importantly, for about half of the respondents interviewed, the use of AI is
relevant for decision making. However, AI is mainly used to support decision
making, and the final decisions remain largely in the hands of humans.
Interviewees pointed out that, while enthusiastic, public administration and
companies are still cautious when deploying AI. Many of the use cases are
still in the testing phase. And some, as described below, were stopped during
this phase. Nevertheless, almost no interviewees were aware of any plans
to reduce the level of technology used. In fact, most expressed intentions to
invest in innovation or new ways to employ currently available AI systems.

2.1.	 EXAMPLES OF AI USE IN PUBLIC ADMINISTRATION

[Use case 1]
Automating social welfare systems – using algorithms in the area of
social benefits
Background and EU legal framework
The United Nations Special Rapporteur on extreme poverty and human rights,
Philip Alston, warned in his October 2019 report that introducing a ‘digital
welfare’ state, including the use of AI, can lead to a “digital welfare dystopia”.
Digitalisation of welfare systems is often accompanied with reductions of
overall welfare budgets, narrowing the beneficiary pool, and other measures
that reduce the availability of welfare. Digitalisation also increases the power
of states by offering opportunities to control people. This is particularly
worrying in countries with significant rule of law deficits.4
The use of algorithms by public administration in welfare raises major concerns
with respect to its potentially negative impact on poverty and inequality, if
applied erroneously in the area of social benefits.5 This includes areas such
as child welfare services6 and unemployment benefits.7
Yet public authorities are keen to use new technologies to make decision
making on social security and other benefits more efficient and potentially
fairer. Globally, new technologies are used in many ways to administer
welfare systems. These include identity verification, eligibility assessments,
benefit calculations, fraud prevention and detection, risk scoring and need
classification, as well as communication between authorities and beneficiaries.
The OECD defines social benefits as transfers made to households in need
after certain events or particular circumstances have arisen, including sickness,
unemployment, retirement, housing, education or family circumstances.8
However, there is no commonly agreed definition of social benefits. Social
benefits, in particular social insurance, systems are different from private
insurance schemes, as they involve compulsory contributions made by both
employees and employers, sometimes in the form of taxation.9
Social policy, including social security and social protection, is an area of shared
competence between the EU and the Member States (Article 4 (2) (b) of the
TFEU). Pursuant to Article 151 of the TFEU, the EU pursues the objectives,
among other things, to promote “improved living and working conditions”
and “proper social protection”. To this end, the EU supports and complements
30

the activities of the Member States in a number of fields, including social
security and social protection of workers and combating social exclusion
(Article 153 (1) of the TFEU). EU actions can encourage cooperation between
Member States and adopt directives with minimum requirements. Moreover,
decisions on social security and social protection can only be adopted through
special legislative procedure by a unanimous vote in the Council.10
Against this backdrop, EU Member States are mostly free to shape their social
security and social protection policies. Since there is virtually no harmonisation,
social security systems differ significantly across the EU in terms of what
benefits are provided, conditions for eligibility, how benefits are calculated,
what contributions need to be paid and by whom, etc.
Public administrations in EU Member States are working on implementing AI
and related technologies in the area of public welfare. However, information
about its applications is limited. FRA collected information about use cases
linked to:
――using algorithms when it comes to compensating job seekers,
――processing social benefits applications, and
――machine learning-supported data analysis on the use of pensions.

Private
insurance
companies’
use of AI

Several private insurance companies interviewed for this research use AI and related
technologies. This includes handling requests of customers for complementary health
insurance, insurance compensation decision support, evaluating the credit risk of
individuals, insurance pricing, insurance claims management, and decision-making support
related to management functions and credit decisions.
Private insurance companies generally embrace AI-related technologies, as these help make
their business more profitable. An OECD report highlights the importance of technology
for this sector. But it also argues that risk classification could lead to the exclusion of those
belonging to certain vulnerable groups in ways that are undesirable from a societal and
political perspective.*
* OECD (2020), The Impact of Big Data and Artificial Intelligence (AI) in the Insurance Sector.
Use in practice
The use cases outlined below exemplify some of the challenges when using
or planning to use AI in the area of social benefits, linked to algorithmic
decision making.
Experimenting with new technologies to support jobseekers

Over the course of a three-year project, a public organisation experimented
with several AI-related technologies concerning all of their work related to
processing benefits for job seekers and assisting them to return to work. The
representative interviewed states that the tested technologies can improve
and foster the relationship with job seekers and improve the advice given to
both job seekers and companies. After testing is completed, the organisation
will decide if and how it will apply these technologies in its day-to-day work.
Tests include machine learning-based detection of the attractiveness of
job offers and a system for detecting whether job seekers are still actively
looking for a job.
The tests are also looking into profiling job seekers to provide advice to them.
This would include calculating the probability of someone being offered an
available job within a given time, and identifying parameters that make job
offers relevant. This may also be reflected in advice to companies on best
practices for formulating job offers. The profiling would allow the organisation
31

to determine appropriate services according to the profile and background
of the job seeker, rather than having an analysis and advice drawn up by
employees. Practically, this would be done by requiring job seekers to complete
a monthly diary on their job search. However, it is still under consideration
whether the programme should be limited to providing descriptive analyses, or
whether it should go further and provide recommendations. The organisation
is hesitant about the latter aspect.
Additionally, a natural language processing system is being tested for analysing
the content of job seekers’ e-mails. Here, e-mails are categorised, relevant
data is extracted, and the urgency and relevance of e-mails is identified.
Using a chatbot, and using automatic replies to emails, is being considered.
The data used for the systems come from several sources from within
the organisation. The data on job seekers and their background, including
personal tax data, as well as data on salaries and social security allowances,
are used under very strict conditions. This is because they are derived from
highly regulated data sources (e.g. salary statements cannot be accessed).
Other data, such as job offers from companies, are also used to generate
knowledge about the job market. The organisation currently does not use
external data, such as from (professional) social media networks, because
no legal provisions are in place for using such data.
Processing housing benefits – failure and success

A public body responsible for processing social benefits piloted an AI tool
to process applications and subsequently support their staff in making
decisions on housing benefits. The system selected cases from new benefit
applications that were relatively straightforward to calculate. These include
new applications for housing benefits submitted by an individual living alone
or with children, and by an individual who does not have any other income
than government benefits. Overall, these cases were deemed simple, with
the result always being that the individual receives the benefits.
The technological solution was based on a decision-tree model following the
rules for housing benefits. Calculating general housing benefits requires income
estimates in advance. The data used during the testing stemmed from an
internal database, which contains data on benefit application processes. The
data was pseudonymised as there was no need to use personal information.
A simple statistical model (linear regression) was used, where the input is
the income and the cost limits, and the output is the amount of benefit.
However, even in such simplified cases, they
found it too difficult to use AI in practice
because of the frequent changes in the
legislation. The test was terminated. According
to the interviewee, the lack of a legal basis for
using machine learning does not allow using
it for administrative decisions. There are no
further plans to use AI to support decision
making on social benefits.
While the organisation is not pursuing this
particular project due to the aforementioned
legal challenges, the interviewee noted
potential for further applications and solutions
in this area in the future. It was noted that
AI or related technologies that can support
operations without having a legal impact were
particularly good for the organisation.
32

The SyRI case

In the Netherlands, the so-called
‘System Risk Indication’ (SyRI)* was
developed as a government tool to
alert the Dutch public administration
about fraud risk of citizens, by
processing and linking large amounts
of their personal data from public
authorities.
A broad coalition of civil society
organisations dealing with privacy
issues initiated a lawsuit, prompting
the District Court of The Hague to
scrutinise the algorithm-based SyRI.**
The court ruled that SyRI impinges
disproportionately on the private
life of citizens. The court found that
everyone whose data was analysed
by SyRI was exposed to this risk.
In addition, due to the opacity of
the algorithm used, citizens could
“neither anticipate the intrusion into
their private life nor can they guard
themselves against it.” ***
*A good description of SyRI can be
found in Ilja Braun (2018), High risk
citizens, in: Algorithm Watch.
** The ruling of 5 February 2020 (in
Dutch) is available online.
*** Privacy First (2020), Dutch
risk profiling system SyRI banned
following court decision.

At the same time, the organisation is using
image processing for social benefits applications.
Generally, benefit applicants have to complete
several forms and attachments, which are often
submitted in paper format. For more efficient
and time-saving handling of those documents
by the agency’s staff, the hard copies received
are scanned and then classified by an automated
system.
A first step is to turn images the right way
round. Algorithms re-align documents that
were not aligned properly when they were
scanned, remove spots and clean up and
edit the colouring of the document, identify
columns, paragraphs, tables, and other elements
as distinctive blocks, recognise the script, etc.
Then, the application checks if the received
application form and attachment are marked
correctly (e.g. if a document is marked as an
invoice, the system determines whether this
is correct).
The turning and the classification of the images
are done by image recognition and Optical
Character Recognition (OCR) technologies. They
recognise text stemming from images, including
from photographs and scans of documents
or handwritten notes. OCR technology then
converts the recognised text into text data that is
machine-readable. Here, in a pattern recognition
process, input from the scanned images is
first isolated, then compared to ‘glyphs’ (i.e.
variations of letters) stored by the system on
a pixel to pixel basis.

The agency will continue processing images and further develop it, for
example, by potentially making it possible to scan bar codes from attachments.
This would help to speed up the confirmation of the correctness of documents
and attachments. There will also be more solutions related to natural language
processing.
Automating unemployment benefits

In one of the countries selected, most decisions on unemployment benefits
are fully automated. The national institution responsible for unemployment
insurance benefits updated its system in 2019 to fully automate most of the
processing of benefit applications and decisions. This was done after the
relevant legislation was adapted to allow automated decisions.
If a person registers as unemployed and lodges an application for benefits,
the system draws on information about the applicant from various other
databases. This includes, for example, the population register, and tax
authorities’ databases containing information about salaries and work
experience, etc. If all conditions for receiving unemployment benefits are
fulfilled, the system calculates the period of payments, based on the length
the person has contributed to the insurance system, and the amount of
benefits, based on the average daily salary.

33

The procedure is fully automated. However, an employee of the institution
must intervene if necessary information cannot be extracted from the
databases, if there is contradictory information in the databases or if the
decision on a case involves a level of discretion (i.e. the decision cannot be
definitively determined based on the data available and a human has some
leeway in deciding on the case).
The main reason for using this system is improved efficiency. In addition, the
system is believed to achieve consistency in the processes. This is because
every application, not subject to discretion, is handled in the same way.

[Use case 2]
Predictive policing – trying to anticipate crime in advance
Background and EU legal framework
AI technologies are used in law enforcement, particularly in predictive policing.
Existing research into how such tools can affect fundamental rights has
highlighted particular issues concerning discrimination, among other rights.
One recurrent concern is the potential for predictive policing to reproduce
and entrench existing discriminatory practices, particularly through reliance
on historical crime data that may be biased or incomplete. This is because
many crimes – such as domestic violence or hate crime – remain largely
unreported and therefore are under-counted in official police statistics.11
A focus on certain crimes, such as violence and drug-related crime in public
places – rather than on business fraud and non-payment of taxes, for example –
can also make law enforcement responses less equitable.12 This is because the
former are often associated with certain demographics and neighbourhoods.
Ultimately, this can undermine police relations with particular communities.
Criminological research on crime ‘hotspots’ has been around for several
decades – notably in the UK and USA.13 It uses police data to map certain
crimes and undertakes statistical tests to explore crime probabilities. Various
police forces have used and developed them to address different types of
crime concentrations or clusters (‘hotspots’).
More recently, adaptations of this area of applied research have used AI as a
tool to enhance its effectiveness, with some suggesting that using algorithmic
tools could reduce the police’s reliance on subjective human judgments
that may reflect biases or stereotypes.14 Some studies have also indicated
that predictive policing could potentially reduce unnecessary surveillance,
questioning, and physical checks and searches,15 reducing the humiliation
and harassment of individuals that may occur during these activities.
Predictive policing aims to forecast the probability of crime and anticipate
emerging trends and patterns to inform crime prevention and intervention
strategies.16 It may also be a part of an investigation into a crime that has
already taken place. While there is no authoritative definition of predictive
policing,17 it is typically characterised by analysing data to identify common
patterns and trends in crime by using algorithms to create models based
on the analysis. This is used to forecast criminal activity that may occur in
the future.
AI technologies in this area generally either aim to ‘predict’ crimes or to
‘predict’ which individuals will either commit or be victims of crimes. Tools
aiming to predict crimes are generally fed with historical data – largely from
official sources – on the time, place and type of crimes committed. This can
be complemented by environmental variables, such as population density,
34

FRA ACTIVITY

Preventing
unlawful profiling
today and in the
future: a guide
In developing and using algorithmic
profiling, bias may be introduced at
each step of the process. To avoid this
and subsequent potential violations
of fundamental rights, both IT experts
and officers interpreting the data
should have a clear understanding of
fundamental rights.
This FRA guide explains what profiling
is, the legal frameworks that regulate
it, and why conducting profiling
lawfully is both necessary to comply
with fundamental rights and crucial
for effective policing and border
management.
For more information, see FRA (2018),
Preventing unlawful profiling today
and in the future: a guide.

presence of certain public places or services, and major events or holidays.
They generally do not use personal data when applied.18

FRA ACTIVITY

Facial recognition
technology
on the rise:
fundamental rights
considerations in
law enforcement
EU law recognises as ‘sensitive data’
people’s facial images, which are a
form of biometric data if processed by
facial recognition software. But such
images are also quite easy to capture
in public places. Although the accuracy
of matches is improving, the risk of
errors remains real – particularly for
certain minority groups. People whose
images are captured and processed
might not know this is happening
– and so cannot challenge possible
misuses.
The FRA paper outlines and analyses
these and other fundamental rights
challenges that are triggered when
public authorities deploy live FRT for
law enforcement purposes. It also
briefly presents steps to take to help
avoid rights violations.
For more information, see FRA (2019),
Facial recognition technology:
fundamental rights considerations in
the context of law enforcement.

In contrast, AI systems focused on predicting potential perpetrators or victims
of crime employ both historical and real-time personal data. This could include
criminal records data, addresses, phone numbers, location data, data extracted
from social media, information about known associates and health or income
data. This is then combined with other criminal and environmental data.19
The EU and its Member States have shared competence in the area of freedom,
security, and justice (Article 4 (2) ( j) of the TFEU). This includes judicial
cooperation in criminal matters and police cooperation (Articles 82-89 of
the TFEU). Already when the Treaty of Lisbon was adopted, an annexed
declaration on the protection of personal data in judicial cooperation in
criminal matters and police cooperation observed that “specific rules on
the protection of personal data and the free movement of such data in the
fields of […] police cooperation based on Article 16 of the [TFEU] […] prove
necessary because of the specific nature of these fields.”20
Within the framework of predictive policing, the collection, storage, processing,
analysis and exchange of information is particularly relevant. The processing
of personal data in the context of law enforcement operations is regulated at
EU level by the Law Enforcement Directive (Directive (EU) 2016/680). 21 It sets
out comprehensive standards and safeguards for such processing, including
the safeguarding against and the prevention of threats to public security.
Use in practice
The use cases collected by FRA signal the variety of ways in which law
enforcement authorities already use, or plan to use, AI and related technologies
to support their work.
Examples mentioned by interviewees range from data mining systems
designed to map crime patterns, detecting online hate speech and making
risk assessments on gender-based violence, to automating certain prison
guard duties. Other use cases include detecting illicit objects from satellite
images, and, more generally, recognising objects in images. In addition, a tool
was mentioned in the research used in the private sector for fraud prevention
and crime detection in money transfers.
Interviewees emphasised that AI or related technology systems are used to
automate and speed up tasks previously done by humans, thus freeing up
and/or better distributing resources.
Mapping crime to support the efficient allocation of investigation capacity

A national intelligence agency and public prosecutor’s office employ a datadriven system to help their employees make choices on how, where and
when to use the available investigation capacity. The aim is to improve the
allocation of human resources, ensuring that officers can be present at the
right time and place.
The interviewees suggest that this system could make more precise
assessments compared to humans, who often rely on their gut feeling for
decisions. Still, the system is always used in combination with human appraisal
and other non-AI systems to make operational decisions.
Based on system-generated outcomes, analysts create a ‘heat map’. This
outlines the prevalence of certain crimes in certain areas. This replicates a
long-standing manual version of this crime anticipation system, whereby
35

police officers put pins on a map to indicate specific risk areas. Using AI to
increase the speed of this process also makes it more reliable, users believe,
because it can analyse more data.
The system is based on data mining and machine learning processes. It is
primarily built on unique police data contained in crime reports, witness
statements, and suspect declarations. Gaps are, to the extent possible,
addressed by using other data sources, such as criminology research, and
social and demographic information obtained from the national office of
statistics. The system also uses data from open sources.
The specific parameters for calculation depend on the type of crime, as
predictive factors vary in relevance across crime areas. For example, in the
case of burglaries, data on burglaries is collected and combined with data
on the place of residence of known criminals and their distance to burgled
houses. The relevant criteria are preselected to allow the system to produce
the heat map.
Location-based predictions are made for the next six months, and indicate the
time and location where a burglary may occur. The result is a map of small
squares where the risk of crime occurring is indicated in different shades.
The interviewees indicated that this visualisation helps officers to analyse
neighbourhoods and observe correlations between different locations.
Assessing the risk of gender-based domestic violence

A national police force uses an internal system to track cases of genderbased domestic violence. The system helps police officers take decisions and
distribute resources across domestic violence cases. The system categorises
cases on the basis of the assessed risk of relapse and repetition, in order to
focus on the ‘riskiest’ cases.
A specialist team could complete the risk analysis without using AI. However,
the system is able to compute a large amount of data in a short amount of
time and assist untrained or non-specialist police officers in risk analysis.
When a case of alleged gender-based domestic violence is reported, the police
officer starts an initial investigation. This includes collecting evidence, taking
witness statements and – potentially – making an arrest. Using information
gathered from this process, the officer fills out two detailed questionnaires
to assess the complaints, evaluate the probability of reoffending, examine
the evolution of the case and assess the behaviour of the perpetrator and
the victim. Police officers also indicate the level of gravity, the nature of
threats faced and attitudes concerning the victim.
The system then produces a risk ‘score’ on a three point scale. The police
officer can raise the level of risk manually, but cannot lower the risk level
below that indicated by the system. Once the level is confirmed, specific
measures are applied in line with established police protocols. The system
also informs a judge about potentially ‘severe cases’ through an automated
system.

36

FRA ACTIVITY

Detecting hate
speech online
A public agency combatting hate
crime uses an AI-based tool to detect
online hate speech by analysing
patterns of speech online. On the
basis of the processing, the system
determines which social groups are
targeted. This helps law enforcement
adopt measures to protect them
before threats are realised.
Although the tool aims to identify
potential victims, rather than
perpetrators, law enforcement can
use the information generated by the
system to ask social media providers
for information on users to pursue
criminal investigations.
One particular challenge is
understanding the context in which
statements are made. For example,
journalists or academics may use
words associated with hate speech to
report on or analyse its occurrence.
In 2021, FRA plans to initiate research
on online hate present on social
media. This will allow FRA to provide
input to policy developments in the
area of online content moderation,
which uses AI.

2.2.	 EXAMPLES OF AI USE IN THE PRIVATE SECTOR

[Use case 3]
AI and health – analysing medical records to save lives
Background and EU legal framework
Healthcare is particularly prominent in discussions about the use of AI. Medical
data and online applications have the potential to support improved health
outcomes and – as a result – wider socio-economic benefits. The COVID-19
pandemic has further increased focus and interest in the area, particularly
in terms of the potential for (online) data and applications to enhance the
ability of governments and health services to track the spread of disease.
Health is also prominent in the general population’s views on uses of AI. A
2019 Eurobarometer survey found that every second European thinks that
AI can be best used to improve medical diagnostics, develop personalised
medicine, or improve surgery.22

This use case covers applications of AI or related technologies by public
and private sector stakeholders in the area of medical records and disease
prediction. Feeding data from electronic medical records (EMR) and electronic
health records (EHR) into AI systems and related technologies can support
the development of preventative medicine that recognises early risks of
disease and designs appropriate interventions. Researchers can predict
clinical events such as mortality, hospitalisation, readmissions and length
of stay in the hospital.
Beyond disease prediction, medical record data can be analysed to predict
patients’ adherence to treatment and their keeping of medical appointments.
These technologies have the potential to support improved health outcomes,
as well as increase the efficiency of the healthcare system.
Under Article 6 of the TFEU, the EU has supporting competence in protecting
and improving human health. Member States retain full responsibility for
defining their health policies, organising and managing their health systems,
and for delivering health services (Article 168 (7) of the TFEU).
Within the EU competence, Union action, which has to complement national
policies, is directed towards improving public health, preventing physical and
mental illness and diseases, and obviating sources of danger to physical and
37

mental health. Such action can cover health information and education, as
well as monitoring, early warning of and combating serious cross-border
threats to health (Article 168 (1) of the TFEU). In the latter areas, the EU can
adopt incentive measures, excluding any harmonisation of the laws and
regulations of the Member States.
Other rules and policies adopted at the EU level aim to ensure free movement
of citizens, their equal treatment and non-discrimination abroad, as well as
availability and safety of medical products and services in the single market.
Considering the development of technologies and their application in health
care, exchange of medical records, patients’ rights in cross-border situations
and disease prediction as a matter of public health are particularly relevant.
Under the GDPR, health and genetic data are considered as a special category
of data (Article 9) called ‘sensitive data’.23 These require specific protection as
their processing could create significant risks. Data subjects’ health and genetic
data can only be shared in specific circumstances under Article 9 (2) of the
GDPR. The GDPR provides an exemption to the purpose limitation principle if
data are used for research purposes, in line with its Article 89 (1). Researchers
are required to ensure that technical and organisational safeguards – such
as pseudonymisation and anonymity – are in place when using patient data.
The EU has also taken action regarding the exchange of medical records.
European Commission Recommendation C(2019)800 on a European Electronic
Health Record exchange format24 “seeks to facilitate the cross-border
interoperability of EHRs in the EU by supporting Members States in their
efforts to ensure that citizens can securely access and exchange their health
data wherever they are in the EU.”25 The recommendation lays out technical
specifications for the exchange of such data between EU Member States.
The European Data Strategy (February 2020) also has a strong focus on health
data.26 A ‘Common European health data space’ is one of the nine common
European data spaces whose establishment the European Commission will
support.
The Early Warning and Response System (EWRS) is owned by the European
Commission and operated by the European Centre for Disease Prevention
and Control. It aims at “notifying at EU level on serious cross-border threats
to health”27 and enabling “the European Commission and EU countries to be
in permanent communication for the purposes of alerting, assessing public
health risks and determining the measures that may be required to protect
public health.”28
EMR, which is a computerised medical record created for patients of a
healthcare organisation,29 and EHR, which contains a patient’s medical history
beyond one organisation and involve sharing data across the healthcare
system, can include a large amount of personal data. This can encompass,
among others: the name and contact details of the individual and their next
of kin; demographic information, diagnoses and test results; and medication
and treatment.30 They may also include patient-generated data from wearable
devices.31
There is no uniform EMR/EHR system operating across all EU Member States.32
Some, such as Germany, do not have a national EMR/EHR system. Others –
including Belgium and Denmark – have different EMR/EHR systems at the
regional level. The systems differ considerably depending on what data is
recorded and by whom and who has access to what data.33 The European
Commission and other stakeholders have highlighted the diversity of country-

38

level EMR/EHR systems and their lack of interoperability as a major barrier
to the digital single market in health.34
Studies highlight the potential for AI or related technologies to enable earlier
diagnosis, widen possibilities for disease prevention and improve patient
safety,35 strengthening the right to access preventive healthcare and benefit
from medical treatment. EMR/EHR may also help to make healthcare more
personalised,36 while the possibility for rapid sharing of data can facilitate
more coordinated and timely treatment.
However, use of EMR/EHR presents significant data protection risks. The
healthcare sector leads in terms of personal data breaches.37 The amount of
the personal data stored, the highest among all industries, combined with
the large data-sharing network and number of access points, makes the
healthcare sector an attractive target for hackers.38
The quality of data in EMR/EHR also raises some concern. Studies where
patients were shown their medical files and asked about their accuracy
found that up to 50 % of information was incomplete or erroneous.39 A lot
of important data in EMR/EHR is unstructured in the form of free text, which
further reduces data quality.40 Low levels of accuracy, completeness and
overall data quality increases the risk of medical error.41
Use in practice
The applications described in the interviews include both simple and more
advanced models employed in the public and private sectors. The largest
number of use cases refer to image-based diagnosis tools. However,
interviewees also discussed tools to automate various working procedures,
such as the mapping of text data, filing of medical records, and analyses and
measurements of body tissues and nerve fibres.
A smaller number of examples touched on more advanced projects, such as
systems to monitor remotely certain health indicators, such as heart rate. In
each case, systems complement the expertise of health professionals. The
next sections present examples of diagnostic and remote monitoring tools.
Image-based tools to help detect and diagnose disease

The tools used to support the detection and diagnosis of diseases described
by interviewees work in similar ways. For example, a privately owned hospital
uses an AI system to interpret images from CT-scans of stroke patients. After
a stroke, imaging is used to detect where damage to the brain has occurred
and where there may be blockages in the blood supply to the brain. It can
also generate measures that can be compared to particular values by a
medical specialist.
The interviewee feels that the application helps to determine such
characteristics in images more quickly, potentially – depending on who uses
the tool – improving the quality of the diagnosis. However, they highlight
that it is not necessarily more efficient to rely on the AI application, since a
medical professional must be present and they could examine the image.
Rather, the tool can offer some support – for example, if the specialist finds
it difficult to interpret a certain image or find abnormalities.
The system was built, trained and validated using a dataset partially based on a
large scientific study to which the hospital contributed. This was supplemented
by purchasing foreign datasets. The algorithm will not be further trained or
adapted in the future based on new data. No new versions will be released.
39

The developers feel that allowing the system
to continue to learn would make it difficult to
validate its operation.
A private company developed an algorithm
that supports the detection of breast cancer
from mammography exams. The tool gives a
probability and degree of certainty, which can
help radiologists to speed up their analysis
of the results and decide whether additional
tests are warranted. The algorithm detects and
characterises anomalies in a mammography
as cancerous or not.

Using AI to
target health
inspections

While the interviewee indicates that the
system now has a very low rate of false
negatives or false positives, they note that in
many cases it does not deliver a clear outcome.
The system was trained on radiography and
mammography data from Europe and the EU,
with written reports and past biopsies acting
as control data.
Monitoring patients’ vital statistics remotely

A hospital is piloting a system to support
early detection of potential illness. Monitoring
patients’ health indicators – for example, blood pressure or heart rate –
typically takes place manually and captures the situation at a specific moment
in time. Constantly monitoring such indicators has the potential to identify
trends that doctors may otherwise not recognise and detect health issues
early to prevent illness. The system uses a biosensor – a kind of plaster –
which gathers hemodynamic data from patients continuously by constantly
monitoring heart pulsation and respiration.
The data used by the system come from the hospital and the patient. These
data are anonymised before being shared with the third-party provider.
No other information besides that gathered through the monitoring of the
plaster is used to build and train the system. Data on environmental factors
were not incorporated in the pilot because, the interviewee pointed out,
they could contain biases.
In the future, the system will combine the information gathered by the
biosensor with separate information from patients’ EMR to draw conclusions
from trends observed in the monitoring.

40

A public authority responsible for
inspecting food safety standards in
restaurants uses machine learning to
process customer review data from
major online platforms. This helps to
decide where and when to conduct
inspections. Previously, this process
was based on complaints the
authority received and on previous
reports. Since the introduction of
the tool, the rate of non-compliant
restaurants identified doubled from
around 18 % to 36 %.
The first step involves text mining.
The algorithm identifies reviews
containing key words that may
indicate health and safety issues,
such as ‘sick, ‘nausea’ or ‘rodents’.
For the second step, the authority
compared results coming from
customer reviews with previous
inspection reports to improve the
algorithm’s accuracy and reliability.

[Use case 4]
Targeted advertising – profiling consumers to boost profit
Background and EU legal framework
The internet has transformed the way we live. Many people make use of
internet services, often offered for free, on a daily basis. Companies offering
their services for free mainly generate revenue through advertising, with
adverts automatically targeted to individual consumers based on information
about them.
The availability of data about online individual behaviour combined with
machine learning technologies have considerably improved the ability of
commercial enterprises to target individuals. This could even go as far as
manipulating consumers by predicting their reactions based on irrational
aspects of psychology and not reasoned choice.42
The Cambridge Analytica scandal underscored
the particularly negative impact of such uses
for political purposes. In that case, a company
illegally obtained personal data on millions of
social media users to target political adverts
to different social groups based on certain
psychological profiles.43
A recent declaration of the Committee
of Ministers of the Council of Europe
highlights the lack of knowledge about the
manipulative power of algorithms. “The
effects of the targeted use of constantly
expanding volumes of aggregated data on
the exercise of human rights in a broader
sense, significantly beyond the current
notions of personal data protection and
privacy, remain understudied and require
serious consideration.”44 Concerns have also
been raised about how online advertising, powered by AI technologies, can
affect data protection and privacy,45 consumer protection,46 the right to nondiscrimination,47 and even the way democracies work.48
The word ‘advertising’ is associated with messages designed to influence
consumer behaviour. Advertising in one form or another has always targeted
specific groups based on their characteristics and behaviour.49
The growth of social media, however, has taken targeted advertising to
another level, using direct access to consumer data. Micro-targeting is directed
towards very specific groups – and the more data that is gathered through
online activities, the more targeted these activities can be. As social media
providers and platforms like Google or Amazon gather comprehensive user
data by monitoring the various activities of their users, advertisers can access
more detailed and specific information.50
The area of targeted advertising and systems that recommend content (e.g.
news or movies) is one of the few real life examples that also involves socalled reinforcement learning. This is a technology that is based on optimising
a certain goal through experimenting and updating its rules automatically
to have the best possible output. This means a systems tries out different
ad placements through trial and error and so finds the best way to optimise
revenue – including an element of self-learning.
41

While very little knowledge on the actual use of reinforcement learning is
available for European countries, major companies working in the area are
researching the issue.51
Issues related to targeted advertising fall under consumer protection. This
falls under shared EU competence with Member States under Article 4 (2) (f)
of the TFEU. EU consumer protection measures seek to protect the health,
safety and economic interests of consumers; and promote their right to
information, education and to organise themselves to safeguard their interests
(Article 169 (1) of the TFEU). The EU can adopt minimum harmonisation
measures to achieve a high level of consumer protection (Article 114 (3) of
the TFEU), yet allowing EU Member States to introduce even more stringent
measures nationally.
In secondary EU legislation, rules on advertising are covered by
Directive 2006/114/EC concerning misleading and comparative advertising.52
This directive provides a minimum level of protection from misleading
advertising. It also harmonises rules on comparative advertising across the
EU. The provisions of Directive 2006/114/EC apply to both consumer-tobusiness and business-to-business relations. However, they are practically
only applied to the latter53 after Directive 2005/29/EC on unfair business-toconsumer commercial practices in the internal market practices54 took effect.
Further, Directive 2006/123/EC on services in the internal market55 covers
services that include advertising. Additionally, Directive 2000/31/EC on
certain legal aspects of information society services, in particular electronic
commerce, in the internal market (E-Commerce Directive) also applies. This
directive forms part of the legal framework for digital services in the EU.
To meet significant developments in the area of new online services and
practices, the E-Commerce Directive is currently being revised as part of the
Digital Services Act package. That package aims to “strengthen the Single
Market for digital services and foster innovation and competitiveness of the
European online environment”.56
FRA collected information about actual use cases from six European companies
engaged in placing online ads, content recommendation, and personalised
marketing.
Use in practice
The examples covered include:
――placing ads online based on click predictions (i.e. learning about the
likelihood that online users click on certain links or adverts) and automated
bidding at auctions for online advertisement space
――personalised and targeted marketing and communication via email.
Most tasks were fully automated. The examples concern analyses of user
preferences and activity and calculations of probabilities of clicks and
purchases, including a measurement of the effectiveness of previously made
recommendations. This also includes methods of targeted communication
on the basis of identified target groups to build (long term) trust between
clients and service providers.
Targeted online ads based on click predictions

Business models working with click predictions and targeted advertisements
often follow a ‘click and buy’ policy. Companies purchase advertising space
on media platforms, and optimise the display of adverts by analysing the
interests and preferences of website users and showing them advertisements

42

that interest them. The purpose is to increase the relevance of advertisements
shown by better matching them to the interests of those who see them.
In the present example, the company only gets paid if people click on an
advertisement and buy something. Additionally, the company uses AI to
detect inappropriate content in advertising, such as advertisements for
alcohol, firearms or political content.
The company uses a range of machine learning techniques in the field of
computational advertising. To estimate the probability of a user clicking on
an advertisement displayed in a specific context (optimising the so-called
click-through-rate), customers’ interests and the relevance of products are
measured via a mapping of individuals’ browsing histories and transaction
patterns. Further, information is derived from individuals’ navigation on
merchant websites worldwide, with whom the advertising company works.
This is done via anonymised third party cookies and trackers.These are placed
on these merchant websites and outline individuals’ navigation across them,
and also list the products seen and purchased.
The profiles of individuals are linked to devices used by them, although
IP addresses are anonymised. Once a product has been purchased, a
recommender system algorithm tries to determine other products that
the customer could also buy. In this case, ‘fresh’ data is valued higher than
older data. Browsing histories are stored for a maximum of one year, as
interests change and purchases older than a year are no longer necessarily
considered relevant.
Advertisements shown to the respective person are immediately adapted
accordingly, and they vary across websites, to also match the content of
the latter. Once an advertisement is posted, it is continuously analysed.
The combination of elements taken into account on an individual’s interest
is confirmed when a purchase is made. Data is shared across platforms,
which includes informing others once a purchase has been made, to stop
advertisements of that particular item. If no purchase is made, the formula
is reviewed and the algorithm is further adapted on individuals’ continuous
online behaviour.
In the future, the company covered in this example expects to work more
on optimising its timing in terms of when it places advertisements, within
their given budget for a certain time frame. It also expects to focus more on
displayed ads that have an impact on consumers.
Another example is based on a European online market place, which links
buyers and sellers on a range of specialised products. Here, AI is used
to optimise advertising campaigns, to categorise products based on the
advertisements that are shown on the website of the market place, to
improve the search engine experience by predicting complementary and
substitutable products, and to detect fraud attempts.
The company uses machine learning to predict the value of clicks of customers
to buy advertisement space, which is offered in real-time auctions. With
these examples, the company indicates that AI enables it to make decisions
that otherwise would not be possible without AI, or which would have to be
significantly scaled down.
Targeted communication with customers and clients

In the case of a retail company focusing on specialised supplies sold across
physical stores and online, direct marketing or personalised advertising is
43

used to increase appeal to customers, and at the same time measure the
efficiency of a particular instance of marketing or advertising.
According to the company at issue in this example, marketing emails are
opened at an average of 20-30 %, and particularly so when customers
recognise relevant and favourite products being offered. Marketing emails
are sent to around 250,000 registered individuals, and a system is used to
establish what may be considered relevant by each of these individuals.
This is done by analysing purchases made by the respective individuals in
the previous six months. 80 % of the offers displayed are directly based on
previous purchases. Meanwhile, 20 % are new suggestions, i.e. alternative
products in the same category as the previous purchases.
A similar approach is used by a bank that sends emails to clients. Messages
offering specific services or products are sent only to certain clients. Data
analysts calculate the probability of clients being interested in a service or
product. If this probability is above a certain threshold, the client will receive
the message. The system used does not yet include machine learning models
and is not fully automated. These points will be taken on when they further
develop the system.
In a third example, a grocery retailer uses loyalty cards both to increase the
customers’ interaction and to personalise offers. Loyalty card systems can
predict how many customers are likely to engage with a product offering.
The system covered in this example also suggests new products to customers
and tracks the results of these suggestions. It groups buyers with similar
behavioural patterns into segments to make more personalised suggestions.
Every week, the company’s loyalty card owners receive personalised offers
by email, website or mobile application, and they can access offers through
in-store terminals. The AI system selects the offerings based on the individual
purchase history, and it recommends new items that might catch the buyer’s
interest and prompt a purchase.

44

Endnotes
1
2
3
4
5
6
7
8
9
10
11

12
13
14
15
16
17
18
19
20
21

22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37

See, for instance, Samoili et al. (2020), AI Watch. Defining Artificial Intelligence. Towards an operational definition and taxonomy
of artificial intelligence, Luxembourg; Karanasiou A. and Pinotsis D. (2017), ‘A study into the layers of automated decision-making:
emergent normative and legal aspects of deep learning’, International Review of Law, Computers & Technology, 2017, pp. 170-187.
See FRA (2019), Facial recognition technology: fundamental rights considerations in the context of law enforcement, Luxembourg,
Publications Office, p. 9 and 22.
FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications
Office, June 2019.
UN, Human Rights Council (2019), Report of the Special rapporteur on extreme poverty and human rights, Philip Alston, A/74/48037.
Eubanks, V. (2018), Automating Inequality. How hightech tools profile, police, and punish the poor, St. Martin’s Press.
Redden, Joanna, Dencik, Lina and Warne, Harry (2020), Datafied child welfare services: unpacking politics, economics and power, Policy
Studies.
Panoptykon Foundation (2015), Profiling the unemployed in poland: Social and political implications of algorithmic decision making; see
also Algorithm Watch (2019), Poland: Government to scrap controversial unemployment scoring system.
OECD, Glossary of Statistical Terms - Social Benefits Definition, accessed 5 August 2020.
J. Henry Richardson, CHAPTER IV, SOCIAL INSURANCE, Economic and Financial Aspects of Social Security (University of Toronto Press, 1960).
Pieters, Social Security.
For an overview of the EU competence in this domain and Regulation (EC) No. 883/2004, see Paju, J. (2017), The European Union and
Social Security Law, Oxford, Hart Publishing, Ch. 2.
Erik Bakke (2018), ‘Predictive policing: The argument for public transparency’, New York University Annual Survey of American Law, Vol.
74, pp. 139-140; Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, Washington University Law Review, Vol. 94, pp. 1146-1150. For
example, only one in five women who experienced violence brought the most serious incident to the attention of the police. See FRA
(2014), Violence against women: an EU-wide survey. Main results report, Luxembourg, Publications Office, p. 61.
Elizabeth E. Joh (2015), The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing, UC Davis Legal Studies Research
Paper No. 473, p.18.
Braga A., et al (2019), Hot spots policing of small geographic areas effects on crime, Campbell Systematic Reviews, Vol. 15 (3).
Elizabeth E. Joh (2015). The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing, UC Davis Legal Studies Research
Paper No. 473, pp. 17-18. Available at: SSRN.
Erik Bakke (2018), ‘Predictive policing: The argument for public transparency’, New York University Annual Survey of American Law, Vol.
74, pp. 137-138.
Wim Hardyns and Anneleen Rummens (2017), ‘Predictive Policing as a New Tool for Law Enforcement? Recent Developments and
Challenges’, Eur J Crim Policy Res, p. 3, DOI: 10.1007/s10610-017-9361-2.
Albert Meijer & Martijn Wessels (2019), ‘Predictive Policing: Review of Benefits and Drawbacks’, International Journal of Public
Administration 42:12, p. 1032, DOI: 10.1080/01900692.2019.1575664.
The Law Society Commission on the Use of Algorithm in the Justice System (2019), Algorithms in the criminal justice system, p. 36.
Newbold, J. (N.D.), ‘Predictive Policing’, ‘Preventative Policing’ or ‘Intelligence Led Policing’. What is the future?
Declaration No. 21 Annexed to the Final Act of the Intergovernmental Conference which adopted the Treaty of Lisbon, signed on 13
December 2007.
Directive (EU) 2016/680 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard
to the processing of personal data by competent authorities for the purposes of the prevention, investigation, detection or prosecution
of criminal offences or the execution of criminal penalties, and on the free movement of such data, and repealing Council Framework
Decision 2008/977/JHA, OJ L 119, 4.5.2016, pp. 89-131.
European Commission (2019), Standard Eurobarometer 92, Report, Europeans and Artificial Intelligence, p. 10.
European Patients Forum (n.d.), The new EU Regulation on the protection of personal data: what does it mean for patients? A guide for
patients and patients’ organisations.
Commission Recommendation (EU) 2019/243 of 6 February 2019 on a European Electronic Health Record exchange format, OJ L 39,
11.2.2019, pp. 18-27.
Digital Health Society, Exchange of electronic health records across the EU, 19 February 2020.
Communication from the Commission to the European Parliament, the Council, the European Economic and Social Committee and the
Committee of the Regions, A European Strategy for data, COM(2020)66 final, Brussels, 19 February 2020.
Decision No. 1082/2013/EU of the European Parliament and of the Council of 22 October 2013 on serious cross-border threats to health
and repealing Decision No. 2119/98/EC, OJ L 293, 5.11.2013, pp. 1-15.
See Commission webpage on Communicable diseases.
OECD and European Union (2018), Healthcare at a glance: Europe 2018, p. 192.
Vera Ehrenstein, Hadi Kharrazi, Harold Lehmann and Casey Overby Taylor (2019), ‘Obtaining Data From Electronic Health Records’, in:
Gliklich RE, Leavy MB, Dreyer NA (eds.), Tools and Technologies for Registry Interoperability, Registries for Evaluating Patient Outcomes:
A User’s Guide, 3rd ed., Addendum 2.
On the use of these data in the insurance industry currently and their potential see, for example, A. Spender, C. Bullen, L. Altmann-Richer,
J. Cripps, R. Duffy, C. Falkous, M. Farrell, T. Horn, J. Wigzell and W. Yeap (2019), ‘Wearables and the internet of things: considerations for
the life and health insurance industry’, British Actuarial Journal 24:22, pp. 1-31.
See WHO visualisation.
See a short overview of different EHR systems in Europe from nurses’ perspective in HealthEurope (2019), The world of cloud-based
services: storing health data in the cloud.
College of Europe (2018), Transformation Health and Care in the Digital Single Market. Synopsis report of the public consultation.
European Commission (2016), Study on Big Data in public health, telemedine and healthcare; Roberta Pastorino, Corrado De Vito,
Giuseppe Migliara, Katrin Glocker, Ilona Binenbaum, Walter Ricciardi, Stefania Boccia (2019), Benefits and challenges of Big Data in
healthcare: an overview of the European initiatives, European Journal of Public Health, Vol. 29, Issue Supplement 3, pp. 23–27.
Ministry of Health, Welfare and Sport of The Netherlands (2016), Digitalization in health care and benefits for patient safety: Literature
and web reports (2015-2016).
This is according to multiple reports by different cybersecurity companies and over time. See, for example, SC Magazine (2019),
Healthcare leads in cost of data breaches; Shannon Williams (2020), New report reveals ‘wall of shame’ in health care data breaches;
Tammy Lovell (2019), Statistics reveal healthcare is the sector most affected by personal data breaches.
45

38 SC Magazine (2019), Healthcare leads in cost of data breaches.
39 Annet Sollie (2016), Reuse and Sharing of Electronic Health Record Data with a focus on Primary Care and Disease Coding, Doctoral
dissertation at the Vrije Univesiteit Amsterdam, pp. 28-30.
40 Vera Ehrenstein, Hadi Kharrazi, Harold Lehmann and Casey Overby Taylor (2019), ‘Obtaining Data From Electronic Health Records’, in:
Gliklich RE, Leavy MB, Dreyer NA (eds.), Tools and Technologies for Registry Interoperability, Registries for Evaluating Patient Outcomes:
A User’s Guide, 3rd ed., Addendum 2.
41 Mowafa Househ, Bakheet Aldosari, Abdullah Alanazi Show, Andre Kushniruk and Elizabeth M Borycki (2017), ‘Big Data, Big Problems: A
Healthcare Perspective’, Studies in health technology and informatics 238, p. 38.
42 Sartor, Giovanni (2020), New aspects and challenges in consumer protection, study for the committee on the Internal Market and
Consumer Protection, Policy Department for Economic, Scientific and Quality of Life Policies, European Parliament, Luxembourg.
43 Neudert, Lisa and Marchal Nahema (2019), Polarisation and the use of technology in in political campaigns and communication, study
at the request of the Panel for the Future of Science and Technology (STOA) and managed by the Scientific Foresight Unit, within the
Directorate-General for Parliamentary Research Services (EPRS) of the Secretariat of the European Parliament; Information Commissioner’s
Office (ICO) (2018), Investigation into the use of data analytics in political campaigns.
44 Council of Europe (2019), Declaration by the Committee of Ministers on the manipulative capabilities of algorithmic processes,
Decl(13/02/2019)1.
45 For example, Costello, Róisín Áine (2020), The Impacts of AdTech on Privacy Rights and the Rule of Law, Technology and Regulation,
11–23; EDPS (2018), Opinion 3/2018, EDPS Opinion on online manipulation and personal data.
46 Sartor, Giovanni (2020), New aspects and challenges in consumer protection; Jabłonowska, Agnieszka et al. (2018), ‘Consumer law and
artificial intelligence. Challenges to the EU consumer law and policy stemming from the business’ use of artificial intelligence’ EUI
Working Papers, LAW 2018/11.
47 Wachter, Sandra (2020), ‘Affinity Profiling and Discrimination by Association in Online Behavioural Advertising’, Berkeley Technology Law
Journal, Vol. 35, No. 2, 2020, (forthcoming), available at SSRN.
48 Zuboff, Shoshana (2018), The Age of Surveillance Capitalism, London; EDPS (2018), Opinion 3/2018, EDPS Opinion on online manipulation
and personal data.
49 Martin, Gillian (2011), The importance of marketing segmentation, American Journal of Business Education, Vol. 4, No. 6.
50 Kaili Lambe and Becca Ricks (2020),The basics on microtargeting and political ads on Facebook.
51 See for example, information on the RecSys2020 Workshop on REVEAL 2020: Bandit and Reinforcement Learning from User Interactions
(accessed on 7 August 2020).
52 Directive 2006/114/EC of the European Parliament and of the Council of 12 December 2006 concerning misleading and comparative
advertising, OJ L 376, 27.12.2006, pp. 21-27.
53 European Commission, Misleading and comparative advertising directive: Objective of the directive.
54 Directive 2005/29/EC of the European Parliament and of the Council of 11 May 2005 concerning unfair business-to-consumer commercial
practices in the internal market and amending Council Directive 84/450/EEC, Directives 97/7/EC, 98/27/EC and 2002/65/EC of the
European Parliament and of the Council and Regulation (EC) No 2006/2004 of the European Parliament and of the Council (‘Unfair
Commercial Practices Directive’), OJ L 149, 11.6.2005, pp. 22-39.
55 Directive 2006/114/EC of the European Parliament and of the Council of 12 December 2006 concerning misleading and comparative
advertising, OJ L 376, 27.12.2006.
56 See the European Commission’s webpage on The Digital Services Act package.

46

3

FUNDAMENTAL RIGHTS FRAMEWORK
APPLICABLE TO AI
The use of AI – as presented in the four use cases discussed in Chapter 2 – can
affect specific fundamental rights (as outlined in Chapter 4). Full compliance
with fundamental rights is a prerequisite for using AI-driven technologies,
irrespective of the area concerned.
This chapter introduces the general fundamental rights framework in the
EU that governs the use of AI, including selected secondary EU legislation
and national law (Section 3.1). This fundamental rights framework provides
the normative basis and benchmarks for the design, development and
deployment of AI tools.1 It helps determine whether or not a specific use of AI
is fundamental rights compliant. The requirements for justified interferences
with fundamental rights are outlined in Section 3.3.

3.1.	 FUNDAMENTAL RIGHTS FRAMEWORK GOVERNING
THE USE OF AI
The cornerstone instrument of the EU fundamental rights framework applicable
to the use of AI is the Charter. Together with the unwritten general principles
of EU law, it is the main source of fundamental rights in the EU. The Charter
enshrines a wide array of fundamental rights and has the same legal value
as the EU Treaties. All EU institutions and bodies are bound by the Charter, as
are Member States when they act within the scope of EU law (Article 51 (1)
of the Charter).2

47

Many Charter rights are the same as those set out in the European Convention
on Human Rights (ECHR).3 Their meaning and scope must be the same as
the corresponding ECHR rights (Article 52 (3) of the Charter). However, this
cannot prevent Union law from providing more extensive protection.
Fundamental rights can also be found in provisions of the Treaties (see e.g.
Article 6 (2) of the TEU and Titles V and X of the TFEU), and in EU secondary
law.4 These rights are further safeguarded in different pieces of secondary
EU law.
A central piece of EU secondary law in the context of AI is the General
Data Protection Regulation (GDPR – Regulation (EU) 2016/679).5 It governs
automated processing of personal data in the European Economic Area and
processing of personal data by any other means which form part of a filing
system – within the scope of EU law. (As a result, the GDPR does not apply
to national security-related data processing.)
The GDPR is coupled with the Law Enforcement Directive, which applies to
police and judicial cooperation in criminal matters. Both EU instruments include
numerous provisions on the protection of personal data, determining the key
principles of data processing, such as lawfulness, fairness and transparency.6
Whether EU data protection legislation applies depends on whether personal
data are processed. Some AI-driven applications do not use personal data
(for example, traffic data). Others use anonymised data. In these cases, data
protection laws do not apply, or their applicability is not entirely clear.7 The line
between personal and non-personal data is blurred, because there is some
risk that anonymised data can be ‘re-identified’ – ie, the anonymisation can
be undone. However, re-identification is usually illegal. In addition, persons
re-identifying the data usually have to put in major efforts and potentially need
access to additional information about individuals who might be included in
an anonymised dataset for re-identification. Section 4.2 discusses the topic in
more detail, linked to the results of the interviews carried out for this report.
In addition to the EU data protection acquis, European non-discrimination law
is key for safeguarding fundamental rights in the context of the use of AI and
related technologies. Article 2 of the TEU provides that non-discrimination is
one of the fundamental values of the EU, and Article 10 of the TFEU requires
the Union to combat discrimination on a number of grounds. Moreover,
Articles 20 and 21 of the Charter provide for equality before the law and
non-discrimination.
Beyond this, several EU non-discrimination directives enshrine more specific
and detailed provisions. They have varying scopes of application.8 These
include the Employment Equality Directive (2000/78/EC),9 the Racial Equality
Directive (2000/43/EC),10 the Gender Goods and Services Directive (2004/113/
EC),11 and the recast Gender Equality Directive (2006/54/EC).12
EU Member States are also party to other international human rights
conventions (see the list of conventions in the Key Findings and FRA opinions
section). These contain legally binding standards and safeguards to comply
with when they act in areas that do not fall within the scope of EU competence.
The main such instrument is the ECHR, ratified by all EU Member States. It is
accompanied by additional protocols, to which a great majority of EU Member
States are parties. The ECHR has a wide reach: it also applies to areas not
covered by EU law.
In addition, the Council of Europe Convention for the Protection of Individuals
with regard to Automatic Processing of Personal Data13 is another source of
48

pan-European data protection obligations binding on all EU Member States.
It was recently modernised.14
Sector-specific EU and national legislation also enshrines safeguards for
the protection of fundamental rights. An overview of such more technical
legislation is beyond the scope of this report. However, this chapter provides
a few examples relevant to the use cases discussed in the report. This is
complemented by a couple of examples of national laws from the five EU
Member States covered.
None of the five EU Member States covered currently have horizontal AIspecific laws, although the countries are looking into the potential need for
regulation. Some EU countries, such as Finland, issued recommendations
for self-regulation and the development of responsibility standards for the
private sector.15 In Estonia, an assessment concluded that a separate AIspecific law will not be required in the foreseeable future, since the current
legal framework is sufficient.16 According to the relevant Estonian longterm strategy, however, the legal environment must be adapted to avoid
unnecessary hindrances to implementing AI.17
The situation concerning sectoral legislation relevant to the use of AI in different
sectors varies across EU Member States. However, active policymaking on AI
has recently emerged at the national level. National action plans on AI have
appeared and remain the core policy development in Member States. Some
countries are working on growing entrepreneurship.18 Others are focused on
enacting market-oriented policies compatible with the UN 2030 Agenda for
Sustainable Development.19 Educational activities to promote AI and increasing
public use of AI are often identified as AI-related strategy goals. Investment
in research and development is also frequently outlined as a relevant goal.20
While domestic AI discussions on potential legislative reforms remain attentive
to European initiatives, national, sector-specific fundamental rights safeguards
are also being enacted. For instance, Finland began considering an overhaul of
domestic human rights safeguards in the public sector by proposing a broader,
across-the-board legislative update as opposed to individual AI laws.
In specific reference to the processing of personal data under immigration
law, the Finnish Constitutional Law Committee has put forward a proposal to
strengthen the safeguards of the Finnish Constitution, overriding constitutional
law shortcomings in relation to, among others, protection under the law,
accountability, as well the ambiguity of algorithms in automated decision
making. Whenever public authorities automate their decision-making
processes, these processes must adhere to the constitutional principle of rule
of law, and may not endanger the observance of rules on good administration
and due process.21 This proposal articulated a vision on what requirements
the Finnish Constitution sets for AI use and automated decision making within
public administration.
The research identified other initiatives and policies linked to AI and
fundamental rights in the five Member States examined. For example, the
Estonian e-State charter includes a summary of citizens’ rights for better
communicating with agencies electronically. It also targets AI in relation to
the right to know what data is collected by public authorities.22
Similarly, the Ministry of the Interior of the Netherlands presented a policy
brief to parliament on AI, public values and fundamental rights.23 The brief
stresses a human-centric approach, where AI-applications have a strong
influence on human beings or on society as a whole. It also lists the most
important risks of AI for fundamental rights, such as discrimination as a result
49

of biased data, or reduced interpersonal relations if AI takes over certain
forms of interaction.

3.2.	 ‘USE CASE’ EXAMPLES
Social welfare (Use case 1)
When regulating social welfare, EU Member States enacted rules aiming to
protect fundamental rights specifically in this area in addition to existing
horizontal EU regulations (see Section 2.1). These mostly define rules for
the processing and protection of personal data for the purpose of social
benefits and insurance.
In Estonia, for example, the Insurance Activities Act, applicable to all types
and forms of insurance, regulates the processing and transmission of personal
data in this context. It states that public authorities, health care providers,
insurance undertakings and other third parties may transmit personal data
at the request of an insurance undertaking if the personal health or court
data are necessary for the insurance undertaking to perform an insurance
contract or if the right and obligation to disclose such data derives from law.
The scope of this Act also includes data transfers for the purpose of data
processing within AI systems.
The Social Welfare Act contains more specific provisions on data protection
of persons in need of social assistance. They have to be notified of the
processing of their data and should provide consent for further processing.
Any person in the established target group has the right to opt out of data
processing. The Social Welfare Act also allows local authorities to process
(including using algorithms) personal data of youth between 16 and 26 years
of age stored in state registries to identify the youth not in employment,
education or training.
In Finland, Act No. 552/2019 on Secondary Use of Health and Social Data
applies to using AI in social care and healthcare. This Act is based on the norms
for securing and protecting sensitive personal data as outlined in the GDPR. It
aims to establish conditions for the effective and secure “processing of, and
access to, personal health and social data for certain secondary purposes,
such as research and statistics, innovation and development, knowledge
management, teaching and authority planning.”24 The Act regulates the
manner in which registered health data can and cannot be processed.
Several other laws apply to various types of social benefits. In France, the
2015 Code of relations between the public and the administration applies
for the purpose of processing or accessing personal data related to social
benefits with minor amendments after the entry into force of the GDPR.
This code states “that algorithms used by public administrations must be
published” and “the person subject to automated decision making has a right
to be informed”.25
Predictive policing (Use case 2)
In the context of predictive policing, the EU’s Law Enforcement Directive
contains key fundamental rights safeguards. These stipulate how law
enforcement authorities should apply some of the main data protection
principles set out in the GDPR.26 These include the requirement for data
controllers (i.e. the competent law enforcement authorities) to provide data
subjects with information on the controller’s data processing activities, such
as the identity and contact details of the data controller, the purposes of the
processing and information about the right to lodge a complaint (Article 13).
50

In specific cases, data controllers shall provide further information – for
example, the legal basis for processing – to enable data subjects to exercise
their rights. The right of access (Article 14) requires the data controller to
confirm, upon request of the data subject, whether there are processing
operations related to them. If this is the case, the data subject shall be able
to access this data and also to request additional information, including the
purposes and legal basis of the processing and the categories of personal
data processed. Both the right to information and the right to access can be
restricted in a number of cases, including to avoid obstructing or prejudicing
the prevention, detection, investigation or prosecution of criminal offences;
or to protect public security and national security.27
In addition, Article 11 of the Law Enforcement Directive explicitly prohibits
automated decision making.28 This prohibition is limited if authorised by
EU or national law which safeguards the data subject’s rights, including “at
least the right to obtain human intervention on the part of the controller”
(for more, see Section 4.2).
In some cases, the scope of implementing national legislation is broader than
the directive. For example, the Finnish Act on the Processing of Personal Data
in Criminal Matters and in Connection with Maintaining National Security
strengthens the right to information by not distinguishing between the
information provided in general and in special circumstances.29
Healthcare (Use case 3)
As regards EU-level fundamental rights safeguards when using AI in healthcare,
the GDPR empowers patients with rights to be informed, in part by granting
them more control of their personal health data. Such data qualifies as
‘sensitive data’30, as found, for example, in their medical records.31 The rights
include the rights to access one’s own personal (health) data; to object to
the processing of own personal data; rectification and erasure of data, as
well as rights in case of breach..32
Under the GDPR, administrative fines for breaches of processing data, including
health data, are not allowed. However, in Estonia, for instance, domestic
law allows for a maximum penalty of EUR 400,000 in application of the
misdemeanour procedure in such cases. The Data Protection Inspectorate
can also impose similar fines in the misdemeanour procedure.33
In France, the Data Protection Act and the Public Health Code impose stricter
requirements than those set out in the GDPR regarding health data processing.
The French Data Protection Act has been amended through the Law for the
Modernisation of the Health System, to allow for the processing of personal
health data for various purposes, provided they fall within the scope of one
of the exceptions to the general principle of prohibition of sensitive data
processing under Article 9 of the GDPR.34
Targeted advertising (Use case 4)
When considering fundamental rights safeguards in relation to targeted
advertising and the underlying mechanisms regarding profiling in particular,
the EU legal framework on privacy and data protection provides the most
relevant fundamental rights provisions. The protection of privacy and personal
data holds a status that takes precedence over economic benefits. Hence,
rules on processing of (special categories of) personal data are relevant for
companies operating in the area of or applying targeted advertising in that
they place companies under certain obligations.
51

The main legal provisions setting out rules on protecting personal data in the
EU are the GDPR and the Directive on privacy and electronic communications
(e-Privacy Directive), which is a lex specialis to the GDPR. The GDPR is directly
applicable in all EU Member States whenever a company is based in the EU
and processes personal data, and if a company is based outside of the EU,
but processes data relating to individuals in the EU.
The e-Privacy Directive, with a strong focus on fundamental rights, concerns
the processing of personal data and the protection of privacy in the electronic
communications sector (e.g. when individuals use their computer, smartphone
or tablet). In 2017, the European Commission proposed an e-Privacy Regulation,
which would replace the current e-Privacy Directive.35 The legislative proposal
would broaden the scope of the directive, and include specific provisions
concerning unsolicited marketing, cookies and confidentiality.

3.3.	 REQUIREMENTS FOR JUSTIFIED INTERFERENCES
WITH FUNDAMENTAL RIGHTS
Chapter 4 highlights selected fundamental rights – as covered by the Charter –
that are particularly affected by AI, taking into account the four use cases
discussed in Chapter 2. Most of these rights are not absolute rights, so can
be subject to limitations in line with Article 52 (1) of the Charter. Accordingly,
before analysing to what extent the different fundamental rights are impacted
by the use of AI, this section presents the general steps that need to be
followed to determine whether or not a Charter right can be limited.
Fundamental rights affected by AI that are not absolute can be subject to
limitations. Interferences with such fundamental rights can only be justified
if they respect the requirements of the Charter and of the ECHR, in case of
Charter rights corresponding to rights guaranteed in the ECHR (Article 52 (3)
of the Charter).36
Pursuant to Article 52 (1) of the Charter, any limitation on fundamental
rights must:
――be provided for by law,
――genuinely meet objectives of general interest recognised by the Union
or the need to protect the rights and freedoms of others,
――respect the essence of the right,
――be necessary, and
――be proportionate.37
The Court of Justice of the EU (CJEU) has also emphasised that any limitation
on the exercise of the rights and freedoms recognised by in the Charter
must respect “the essence” of those rights and freedoms.38 This means that
fundamental rights can be limited to a certain extent, but not completely
disregarded.
Once it has been established that the inalienable, essential core of a right
is not violated by a measure, the next step is to conduct the necessity and
proportionality test outlined in the Charter in respect of non-core aspects of
that right.39 Any interference with a Charter right needs to be examined as
to whether the given legitimate aim could not be obtained by other means
that interfere less with the right guaranteed.40 Similar requirements are
also imposed by the ECHR, as interpreted by the European Court of Human
Rights (ECtHR).41 These include the ‘essence of a right’ concept, which can be
derived from the object and purpose of the ECHR as a whole.42 In respect to
the use of new technologies, the ECtHR observed in S. and Marper v. the UK
that States should “strike a right balance” between protecting fundamental
rights and developing new technologies.43
52

Given the wide range of applications of AI systems in everyday life as
presented in the four selected use cases, a wide range fundamental rights may
have to be assessed, taking into account a variety of elements, depending on
the context and the particular area of use. Most notably, the specific purpose
for which AI is used, its functionality, complexity, and the scale at which it
is deployed, are relevant for assessing fundamental rights implications.44

53

Endnotes
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37

38
54

See also van Veen, C. (2018), ‘Artificial Intelligence; What’s Human Rights Got to Do with It?’ Data & Society: Points – blog of Data
& Society Research Institute, 14 May 2018; Barfield, W. & Pagallo, U. (2020), Advanced Introduction to Law and Artificial Intelligence,
Cheltenham/Northhampton, MA, Edward Elgar, 2020, pp. 19-20.
See also CJEU, Åklagaren v. Hans Åkerberg Fransson [GC], 26 February 2013, paras. 17, 20.
European Convention for the Protection of Human Rights and Fundamental Freedoms, as amended by Protocols Nos. 11 and 14, 4
November 1950, ETS 5.
For an overview of the application of the Charter, see FRA (2018a), Applying the Charter of Fundamental Rights of the European Union in
law and policy making at national level, Luxembourg, Publications Office.
Regulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard
to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC (General Data Protection
Regulation), OJ L 119, 4.5.2016, pp. 1-88.
For more, see FRA (2018), Handbook on European Data Protection Law. 2018 Edition, Luxembourg, Publications Office.
See for example, Hacker, P. (2020), A Legal Framework for AI Training Data. Law, Innovation and Technology (forthcoming), available at
SSRN.
For an overview of European non-discrimination law, see FRA (2018), Handbook on European non-discrimination law. 2018 Edition,
Luxembourg, Publications Office.
Council Directive 2000/78/EC of 27 November 2000 establishing a general framework for equal treatment in employment and occupation,
OJ L 303, 2.12.2000, pp. 16-22.
Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial or
ethnic origin, OJ L 180, 19.7.2000, pp. 22-26.
Council Directive 2004/113/EC of 13 December 2004 implementing the principle of equal treatment between men and women in the
access to and supply of goods and services, OJ L 373, 21.12.2004, pp. 37-43.
Directive 2006/54/EC of the European Parliament and of the Council of 5 July 2006 on the implementation of the principle of equal
opportunities and equal treatment of men and women in matters of employment and occupation (recast), OJ L 204, 26.7.2006, pp. 23-36.
Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data, Strasbourg, 28 January 1981 (ETS No.
108).
Protocol amending the Convention for the Protection of Individuals with regard to Automatic Processing of Personal Data, Strasbourg, 10
October 2018 (CETS No. 223).
The AI Finland Project’s ethics working group and the Ethics Challenge added emphasis on companies and self-regulation. AI Finland,
‘Etiikkahaaste (Ethics Challenge)’, Tekoäly on uusi sähkö (in Finnish).
Republic of Estonia (2019), Report of Estonia’s AI Taskforce, p. 38.
The Estonian Government launched the preparation for a long-term strategy.
For example, see the Netherlands, Ministry of Economic Affairs and Climate Policy (2019), Strategic Action on AI Strategic Action Plan AI
(Strategisch Actieplan AI – SAPAI).
For an example of an effort to adapt goals to the development of a sustainable market, see Spain, Ministry of Science, Innovation and
Universities (2019), National AI Strategy (in Spanish).
For a more comprehensive overview, see European Commission (2019), National strategies on Artificial Intelligence; or the OECD AI
policy observatory.
Finnish Constitutional Law Committee (2019), ‘Committee Opinion PeVL 7/2019 Vp ─ HE 18/2019 vp: Draft Proposal to Parliament for the
Law on the Processing of Personal Data in the Immigration Administration and for Related Laws’.
Estonia, National Audit Office and Chancellor of Justice (2018), Everyone’s Rights in e-State: The e-State Charter.
Netherlands, Ministry of the Interior and Kingdom Relations (2019) AI, public values ​​and fundamental rights (in Dutch).
Elina Saxlin-Hautamäki and Johanna Lilja (2019), Secondary use of health data – the new Finnish Act.
de Donno, M. (2017), The French Code “Des Relations Entre Le Public Et L’Administration”. A New European Era For Administrative
Procedure?, Italian Journal of Public Law 2, pp. 220-260.
See FRA (2018), Preventing unlawful profiling today and in the future: a guide, Luxembourg, Publications Office, Tables 2 and 4.
Sajfert, J. and Quintel, T. (2017), Data Protection Directive (EU) 2016/680 for Police and Criminal Justice Authorities, available at SSRN.
Note that Article 11 of the Law Enforcement Directive seems to apply to automated decisions taken solely through automated processing.
This means that this safeguard will not apply if human agency is involved. Orla Lynskey (2019), Criminal justice profiling and EU data
protection law: Precarious protection from predictive policing, p. 21.
The English translation is available via the Finlex website.
GDPR, recital (10) and Art. 9 (1).
European Patients Forum (n.d.), The new EU Regulation on the protection of personal data: what does it mean for patients? A guide for
patients and patients’ organisations.
GDPR, Arts. 15-17, 20-21 and 34.
White&Case (2019), GDPR Guide to National Implementation: Estonia.
Merav Griguer (2019), Processing health data in France: What to look out for after GDPR?
European Commission, Proposal for a regulation of the European Parliament and of the Council concerning the respect for private life
and the protection of personal data in electronic communications and repealing Directive 2002/58/EC (Regulation on Privacy and
Electronic Communications), COM(2017) 10 final, Brussels, 10.1.2017.
Charter, Art. 52 (3): “In so far as this Charter contains rights which correspond to rights guaranteed by the Convention for the Protection
of Human Rights and Fundamental Freedoms, the meaning and scope of those rights shall be the same as those laid down by the said
Convention.”
As also reiterated and explained by the CJEU. See, for example, C-73/07, Satakunnan Markkinapörssi and Satamedia, 16 December 2008,
para. 56; Joined cases C-92/09 and C-93/09, Volker und Markus Schecke and Eifert GbR and Hartmut Eifert, 9 November 2010, para.
77; Joined cases C-293/12 and C-594/12, Digital Rights Ireland Ltd v. Minister for Communications, Marine and Natural Resources and
Others and Kärntner Landesregierung and Others, 8 April 2014, para. 52; C-362/14, Maximillian Schrems v. Data Protection Commissioner,
6 October 2015, para. 92; and C-419/14, WebMindLicenses Kft. v. Nemzeti Adó-es Vámhivatal Kiemelt Adó- és Vám Főigazgatóság,
17 December 2015, paras. 69 and 80-82.
See CJEU, C-362/14, Maximillian Schrems v. Data Protection Commissioner, 6 October 2015, paras. 94-95, which refer to Article 52 (3) of
the Charter. See also Scheinin, Martin and Sorell, Tom (2015), SURVEILLE Deliverable D4.10 – Synthesis report from WP4, merging the

ethics and law analysis and discussing their outcomes, 7 April 2015, p. 9.
39 See e.g. Brkan, M. (2019), ‘The Essence of the Fundamental Rights to Privacy and Data Protection: Finding the Way Through the Maze
of the CJEU’s Constitutional Reasoning’, German Law Journal 20 (2019), p. 867; Lenaerts, K. (2019), ‘Limits on Limitations: The Essence of
Fundamental Rights in the EU’, German Law Journal 20 (2019), pp. 779-794.
40 CJEU, Joined cases C-293/12 and C-594/12, Digital Rights Ireland Ltd v. Minister for Communications, Marine and Natural Resources and
Others and Kärntner Landesregierung and Others, 8 April 2014.
41 See, for instance, Khelili v. Switzerland, No. 16188/07, 18 October 2011; ECtHR, S. and Marper v. the United Kingdom [GC], Nos. 30562/04
and 30566/04, 4 December 2008; ECtHR, K & T v. Finland, No. 25702/94, 12 July 2001; ECtHR, Z v. Finland, No. 22009/93, 25 February
1997; ECtHR, Huvig v. France, No. 11105/84, 24 April 1990; ECtHR, Leander v. Sweden, No. 9248/81, 26 March 1987.
42 Scheinin, Martin and Sorell, Tom (2015), SURVEILLE Deliverable D4.10 – Synthesis report from WP4, merging the ethics and law analysis
and discussing their outcomes, 7 April 2015, p. 9.
43 ECtHR, S. and Marper v. the United Kingdom [GC], Nos. 30562/04 and 30566/04, 4 December 2008, para. 112.
44 See also Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights
impacts of algorithmic systems, Appendix, para. A.8.

55

56

4

IMPACT OF CURRENT USE OF AI ON
SELECTED FUNDAMENTAL RIGHTS
Deploying AI systems engages a wide range of fundamental rights. As
seen in Chapter 2, the use cases presented in this report involve a range
of technologies of varying levels of complexity and automation. They are
in different phases of development and applied in different contexts, for
different purposes and at different scale. While the rights affected depend
on these factors, a number of horizontal and sector-specific fundamental
issues emerge.
The chapter begins with a general overview of risks perceived by interviewees,
and their general awareness of fundamental rights implications when using
AI. The chapter then highlights selected fundamental rights affected by AIrelated technologies, with reference to the four use cases analysed.
The analysis takes into account and presents the views, practices and
awareness of these issues expressed in the interviews conducted for this
report. Interviewees were first asked about the general risks they see when
using AI. They were then asked about general fundamental rights awareness
when using AI and about more concrete fundamental rights implications, which
were mostly linked to data protection, non-discrimination and availability of
complaints mechanisms.

4.1.	 PERCEIVED RISKS
It is important to recognise that many issues cut across different rights. For
example, a potentially biased decision made by an algorithm could involve
the right to non-discrimination, protection of personal data, and the right to an
effective remedy. Similarly, a particular issue can be seen from the perspective
of different rights. For instance, a good explanation of a decision made by
an algorithm is required under the right to protection of personal data, right
to good administration, and the right to an effective remedy and a fair trial.
When asked about general risks when using AI, the interviewees did not always
mention fundamental rights as the main risks, although some highlighted
related topics. Private sector representatives most often mentioned inaccuracy
as a risk of using AI, followed by potential bias and the proper legal basis
for processing personal data. One respondent from an international retail
company stated that one business risk is linked to European customers being
extremely knowledgeable about their rights; namely, people do not hesitate to
ask about data storage and automated decision making. If customers are not
properly informed, they might complain and the company may lose a client.
In addition, the interviewee continued, breaching the law, and possible fines
linked to a breach, is another major business risk.
With respect to public administration, bias was most often highlighted as
a risk associated with using AI. In addition, public authorities often discussed
57

inaccuracy and data re-identification as risks of using AI. For example,
interviewees working on social benefits algorithms stated that incorrect
results in general are a risk. This can occur potentially due to rare cases, which
are not well identified by the algorithm, or due to errors in the input data.
They also highlighted the difficulties associated with moving from testing to
deploying a system, including technical challenges, resources required and
potential different results when deployed.
Respondents working on targeted advertising also highlighted business
risks – for example, when offering irrelevant or inappropriate content. One
respondent mentioned potentially losing control over automated systems.
In addition, interviewees indicate challenges linked to the difficulty of
interpreting results and outputs from AI systems. One interviewee from
the consultancy sector fears that the risk related to the lack or absence of
sufficient AI knowledge and understanding can cause ongoing projects to
be halted, due to a company’s inability to explain clearly what an algorithms
will perform, and for what purpose.
Another interviewee from the law enforcement sector, looking into the
possible use of AI to support decisions about licence applications, explains that
there are inherent risks on how and why such a system proposes a certain
response. For example, when potentially using AI to support decisions about
license applications for firearms, the respondent asserts that it would not
only be critical to understand the reasoning behind negative decisions, but
also positive decisions. Several interviews showed that a major concern is
to assign properly trained staff with sufficient expertise to trace, explain and
interact with the AI system.
This finding is also corroborated by the results of the European Commission
survey among companies in the EU. In that survey, 85 % indicate as an
obstacle to adopting AI technologies the difficulty to hire new staff with
the right skills; 80 % mention the complexity of algorithms as an obstacle.1
With respect to the ability to explain decisions based on algorithms, an
interviewee working in public administration mentioned that there are no
alternatives to being completely transparent when making decisions. There
should not be any room for doubt. In a similar vein, a respondent working
in the area of health for the private sector mentions that ‘self-learning’
algorithms are forbidden in their area of work, because only fixed algorithms
can be traced.
Other risks reported without providing much additional information include
cyber-security, data quality, excessive monitoring of people due to the use
of data and algorithms, job loss due to automation, and profiling.

“The use of AI can bring many
benefits, but also risks, it is like
nuclear energy.”
(Interviewee working in private sector,
Spain)

4.2.	 GENERAL AWARENESS OF FUNDAMENTAL RIGHTS
AND LEGAL FRAMEWORKS IN THE AI CONTEXT
Not everyone in the EU is aware about their fundamental rights. FRA’s
Fundamental Rights Survey shows that slightly more than every second
person in the EU (aged 16 or older) has heard about the Charter. Slightly
more people, two out of three, have heard about the ECHR and the Universal
Declaration of Human Rights. This might be because the ECHR is older and
more established in people’s common knowledge.2
The majority of people interviewed for this project acknowledge that using
AI can generally affect fundamental rights. Only very few mention that
their use of AI does not have a potential impact on fundamental rights or
58

“[Our use of AI] does not impact
[human rights] in any way. In terms
of the decision process, it does not
matter whether the decision is made
by machine or a human.”
(Interviewee working for public
administration, Estonia)

that they were not aware of any such implications. Their responses are
influenced by the different ways they use AI, but also their understanding
of what fundamental rights are.
For example, one respondent working on the production of pension forecasts
based on machine learning says that producing statistics does not have an
impact on fundamental rights, apart from data protection issues, which need
to be addressed. Another respondent working on social benefits algorithms
argues that the impact depends on “how widely human rights are defined” –
for example, the right to receive a correct pension.
“Once all the rights related to data
protection are ensured, I do not see
how human rights are of relevance
here.”
(Private company, Spain)

“We did not touch the topic because
we assume that there are no
human rights issues involved: all
the activities are within the legal
framework, all the activities are
compliant with data protection and
good practices, and therefore we
assume that there are no human
rights issues related to these
systems.”
(Public administration, Spain)

None of the interviewees working on targeted advertising believe that their
use of AI affects fundamental rights negatively. One respondent working
on targeted communication with customers stated that one reason for such
a response relates to a lack of knowledge about what exactly fundamental
rights are.
Practically all interviewees showed awareness about the rights to privacy
and data protection as well as to non-discrimination. Other rights, such as
human dignity, the right to a fair trial and to effective remedy were also
mentioned, albeit very briefly.
A closer look at interviewees’ responses indicates diverging views across
respondent groups. Most respondents working for private companies discuss
data protection and non-discrimination, but rarely mention other rights
challenges. A company working on targeted advertising mentions that they
are attentive to issues linked to freedom of speech and the right to information
in the sense that their company promotes these rights. This is because
posting adverts helps news and other websites obtain funding to continue
their work, one interviewee notes.
The range of rights awareness is much broader among public sector
representatives working on AI, who referred to other rights such as human
dignity and the presumption of innocence.
Those working on AI systems in different fields of application also highlight that
the use of the systems is also covered by sector-specific laws. For example,
the system making decisions about unemployment benefits is regulated by
national legislation on unemployment insurance, on administrative procedures
and on data protection. However, some respondents are not aware of any
legal standards that apply to their use of AI or are unsure about it.
In the absence of AI-specific regulation, several respondents mention ethics
guidelines and certification schemes. Some work with existing guidelines
and standards, not necessarily specifically aimed at AI. This is the case, for
example, with the IT security system ‘ISKE’ in Estonia3 or in the area of financial
services, with the Payment Card Industry Data Security Standard.4 Respondents
also refer to the standards developed by the International Organization for
Standardization (ISO), the Institute of Electrical and Electronics Engineers
(IEEE) or the European Committee for Standardization (CEN).

“I do not think that we should
regulate specific technology like
AI. It is sufficient to have general
principles and technology-neutral
rules.”

A respondent working on targeted advertising argues that certification is not
needed in their field because posting ads is not the same as issues linked
to the health sector or the work of banks. Several interviewees noted that
their organisations are developing (internal) guidelines.
Some respondents mention the guidelines developed at the EU and
international level, such as the guidelines from the European Commission’s

(Private sector, Estonia)
59

High-Level Expert Group on AI, the OECD’s guidelines, or the UNESCO standards.
Some are aware of ongoing developments at EU and Council of Europe level.
Some refer to the need to update sector-specific regulations to be able to
innovate on AI – for example, in the area of health. Yet one interviewee
states that existing standards are sufficient and that AI does not need to be
regulated separately.

4.3.	 HUMAN DIGNITY
Using AI-driven technologies broadly implicates the duty to respect human
dignity, the foundation of all fundamental rights guaranteed by the Charter.5
Article 1 of the Charter states that human dignity is inviolable and that it must
be respected and protected at all times. The CJEU has confirmed in its case
law that the fundamental right to dignity is part of EU law.6
AI-driven processing of personal data must be carried out in a manner that
respects human dignity. This puts the human at the centre of all discussions and
actions related to AI. Rather than the technology, the ‘human being’ creating
and affected by the new technology needs to be the focus. Taking human
dignity as the starting point can help to ensure that the use of AI benefits
everyone – for example, by supporting ageing and access to healthcare in
a dignified manner.
The use of AI also risks infringing on other closely connected Charter rights,
such as the right to life (Article 2) and the right to integrity of the person
(Article 3). In this context, it is important to consider how to avoid the harmful
use of AI to prevent violations of these rights, for example when it comes
to the use of AI by people engaging in criminal activities or when AI is used
for weapons.7
Apart from such extreme cases, preserving dignity includes avoiding subjecting
people to AI without their knowledge and/or informed consent, which is
strongly linked to privacy and data protection. For example, when people’s
applications for social benefits are decided upon through the use of AI, people
need to be made aware (and consent to the use when automated decisions
are taken). To give another example, a certain proportion of the population
does not feel comfortable being subjected to biometric identification systems.
Hence, using it without allowing them to opt out could potentially violate
their dignity.8
Only very few respondents from public administration referred to the right
to dignity when discussing fundamental rights. One respondent, considering
the use of AI in prisons, mentions that in this particular context it first needs
to be assessed whether the risk of violating fundamental rights would be
too high, such as the right to human dignity. Other interviewees made only
general references to this right, without discussing it in relation to a concrete
use of AI.

60

“Yes, there are codes, and yes, there
are procedures, but both these codes
and procedures are out of date,
because we are using something we
created for the analog world in the
digital world.”
(Private sector, Spain)

4.4.	 RIGHT TO PRIVACY AND DATA PROTECTION –
SELECTED CHALLENGES
The right to respect for private life and the protection of personal data
(Articles 7 and 8 of the Charter) are at the core of fundamental rights
discussions around the use of AI. While closely related, the rights to respect
for private life and the protection of personal data are distinct, self-standing
rights. They have been described as the “classic” right to the protection of
privacy and a more “modern” right, the right to data protection.9
Both strive to protect similar values, i.e. the autonomy and human dignity
of individuals, by granting them a personal sphere in which they can freely
develop their personalities, think and shape their opinions. They thus form
an essential prerequisite for the exercise of other fundamental rights, such
as the freedom of thought, conscience and religion (Article 10 of the Charter),
freedom of expression and information (Article 11 of the Charter), and freedom
of assembly and of association (Article 12 of the Charter).10
Given that these two rights are not absolute rights, they can be subject to
limitations. However, any interference needs to be adequately justified11
and cannot compromise the essential, inalienable core of that right,12 as
explained in Section 3.3.
The concept of “private life” or “privacy”
is complex and broad, and not susceptible
to an exhaustive definition. It covers the
physical and psychological integrity of
a person, and can, therefore, embrace
multiple aspects of the person’s physical
and social identity.13 There is also a zone of
interaction of a person with others, even in
a public context, which may fall within the
scope of “privacy”. In other contexts, the
ECtHR has used the concept of “reasonable
expectation of privacy” – referring to the
extent to which people can expect privacy
in public spaces without being subjected
to surveillance – as one of the factors,
albeit not necessarily a conclusive one,
to decide on a violation of the right to
respect for private life. Its relevance and
scope of application, however, appears
to be limited.14 Similarly, according to the
UN Human Rights Committee, the mere fact that participants in assemblies
are out in public does not mean that their privacy cannot be infringed. The
same applies to the monitoring of social media to glean information about
participation in peaceful assemblies.15
The widespread use of AI-technologies may, as the technologies continue
to develop, raise unchartered issues and novel concerns about the right to
respect for private life. AI-driven technologies may change the way we
think about privacy. Algorithmic tools can predict, and reveal information
about, people’s behaviour in unprecedented ways – without people even
realizing that they are giving away such information. Personal data obtained
from the internet may, for instance, then be used for targeted advertising,
raising many fundamental rights concerns.16 Issues linked to personal data
sharing via smart-phone apps particularly raises significant concerns, including
a variety of potential harmful effects, such as manipulation and exploitation
of vulnerabilities, discrimination, security issues and fraud (e.g. identity theft)
and reduced trust in the digital economy.17
61

Using AI-driven technologies often implies computerised processing of large
amounts of personal data. This constitutes an interference with the right to
protection of personal data set out in Article 8 of the Charter (embodying
pre-existing EU data protection law), as well as the right to private life under
Article 7 of the Charter and Article 8 of the ECHR.

Awareness of data protection issues and use of personal data

In the EU, 69 % of people have heard about the GDPR.18 By contrast, virtually
all interviewees are aware of the GDPR and discussed data protection issues.
Data protection rules deriving from the GDPR and national law are clearly
the most well-known and applied rights in the area of AI. Other fundamental
rights are less known.
When discussing the legal framework governing the use of AI, most
respondents only mentioned data protection rules, as well as some sectoral
laws. Some clearly say that there is no other legal framework apart from
the data protection laws. An interviewee working for a Spanish public
administration notes: “we rely on the data protection regulation and norms,
which is all that is available at the moment”.
One interviewee, reflecting on an image-based diagnostic tool, expressed
the view that the GDPR could hinder research. The hospital using the tool
to support diagnosis after strokes had clear rules on data protection, the
interviewee indicated, although they did not know whether data protection
certification was requested.
Others referred to more general data protection guidelines or indicated that
they were not aware of such documents.

“We were a little anxious when the
GDPR was implemented, but in the
end it meant managing datasets
and access rights […] It is a good
reminder that not everything can be
or should be done.”
(Public administration, Finland)

“Actually, I’m concerned that the
GDPR might hinder AI research. I’m
afraid that some large databases
that we have used previously cannot
be used for our research anymore.”
(Private company, Netherlands)

“There is the GDPR but it does not
give you specific rules. It gives
principles but it comes down to
ethical issues and interpretation.”
(Private company, Estonia)

All respondents working in target advertising are aware of privacy and
data protection issues. Although not all are responsible for data protection
issues in their companies, they are all aware of efforts to protect the data
and privacy. One interviewee mentioned that, contrary to earlier years,
personal data are now stored much more securely and handled with more
care. Attention was given to properly handling consent for data processing.
As a consequence, there is a high level of awareness about data protection
and privacy issues linked to AI use.
However, data protection law only applies when personal data are processed.
For example, using anonymised data to develop AI tools (i.e. as training data)
is most likely permissible in many instances and would not trigger the GDPR.
Research shows that data can often be de-anonymised.19 However, such
efforts often require expert knowledge and potentially additional information,
and are illegal. While the illegality of de-anonymisation does not necessarily
preclude the applicability of the GDPR, it is more important to consider if reidentification of anonymised data is reasonably likely.20 Anonymising data is
only one aspect of protecting the privacy of data subjects. When assessing
risks of re-identification, other aspects are also important to consider when
disseminating anonymised data. These include who will use the data, for
what purpose, and what outputs will be produced.21
In the interviews, respondents were not always entirely clear about their use
of personal data. They often only superficially described the data used, as
mentioned in Chapter 2. In several instances, interviewees indicated that they
use non-personal data or anonymised data, arguing that data protection was
not relevant in such cases. For example, a semi-public organisation working
on environmental management uses aggregated data on water consumption

62

for machine learning-based predictions of water consumption. These data
are not available at individual level.
Other interviewees said they did not use personal data, although the data
originally stem from individuals. The tool supporting restaurant inspection
by collecting data from online sources does not use any personal data – the
interviewee indicated. However, they indicated the need to be careful when
mining data online, because, even if publicly available, it might include
personal data, such as usernames.
“It would be great to retrieve some
data from another service so that
the client wouldn’t have to repeat
it all, but where does the line go in
reusing data?”

In another example, an insurance company is using a chatbot to make client
contact more effective. The data used to train the system are chat protocols
(conversation logs), which are not linked to any personal data. However, in
this example, linking these data to personal data might be possible in the
future, according to the respondent.

(Public administration, Finland)

Companies working on targeted online advertising indicate using (pseudo-)
anonymised data. This is done, for example, after excluding names and
social security keys and encrypting data. The identity of the consumers is
not relevant to the company, an interviewee mentioned.
While some indicate that they use non-personal or anonymised data, for
others this is not possible because the data are used to make predictions
or decisions about specific individuals. For example, an interviewee from
a company working on credit rating mentioned they need to know the identity
of consumers for their assessments. In this case, this is even more important
than the right to be forgotten, according to an interviewee.
An exhaustive discussion of data protection issues is not possible in this report.
However, two aspects clearly emerged during the interviews: automated
decision making linked to the right to human review, and the right to obtain
meaningful information when decisions are automated.
Automated decision making
Article 22 of the GDPR and Article 11 of the Law Enforcement Directive
generally forbid automated decision making, meaning any “decision based
solely on automated processing, including profiling, which produces legal
effects concerning him or her or similarly significantly affects him or her.”
Under Article 22 of the GDPR, explicit
consent is needed when decisions are
solely automated and have a legal or
similarly significant effect on people and
if such automated decision making is
not authorised by law. The authorisation
by Union or national law is the sole
precondition under the Law Enforcement
Directive (Article 11) for such processing.
For a decision not to be considered fully
automated, both instruments require human
review by the controller.22

63

However, the concept of ‘automated’ decision making is elusive and requires
further discussion and research. For example, in some cases, human
intervention might be limited to ‘signing-off’ on the outcomes of the AI
system, rendering it virtually automated.23 Importantly, human review must
not mean a human just signing off the recommendations or outputs from
an algorithm. It must be done by someone who has the “authority and
competence to change the decision”, considering all relevant data at hand.24
If humans review and potentially override outcomes of the system, this
must also be evaluated.
Research indicates that humans overrule outcomes from algorithms mainly
when the result from the algorithm is not in line with their stereotypes. 25
This behaviour threatens the possible added value of automated processing
in being potentially more accurate or even fairer than humans. It may also
put minority groups at a disadvantage, and is therefore also relevant for
non-discrimination issues (discussed below).
Overall, there is disagreement about the exact scope of these provisions of
the EU data protection acquis, and whether they impose a general ban on
certain types of automated decisions, or provide data subjects with some
rights in the context of certain types of AI-driven decision making.26
Using algorithms in the area of social benefits, health and predictive policing
clearly have potential legal or other significant consequences. The interviews
suggest that those working in these areas are well aware of the concept of
human review before decisions are taken with the support of AI.
Many interviewees indicate that no automated decisions are taken. One
exception is an automation of unemployment benefits, which is, based on
national law, fully automated for decisions that do not involve any discretion.
In another example, from another country, only positive decisions, based on
pre-defined rules, are automated for student benefits. In this case all negative
decisions are made by humans. Both cases refer to rule-based decisions, not
involving the use of statistics or machine learning.
Another respondent, testing the use of AI systems, including machine learning
in the area of social benefits, mentions that equality could be negatively
impacted. This is because automation makes human behaviour visible,
including existing biased practices. This makes precautions necessary and, as
a consequence, the organisation would only allow decisions made by humans.
Interviewees working in health highlighted risks linked to the automation
of decisions. An interviewee discussing the tool to support stroke diagnosis
feels it is important not to rely on the system to avoid the risk of automation
or confirmation bias. They caution that early positive experiences with
the application could prompt users to rely on it too easily and devote less
attention to their own assessment of the images. Other interviewees raised
similar concerns. One interviewee, discussing a tool that analyses images to
provide a probability for the presence of a certain type of lesion, notes that
the technology supports the diagnosis of simple cases, but that the expertise
of doctors is particularly important – and trusted – in more complex cases.
Targeted advertising is often considered not to have a significant effect
on people. However, this may be the case if, for example, an individual’s
vulnerabilities are used for successful advertising. Considering vulnerabilities
is particularly important for people from disadvantaged groups, who may
not be aware that they can opt out of direct marketing (see box) or of their
right to have a say when decisions are automated.

64

In the absence of case law in this area, more information and research
is needed to identify the impact of such automated decisions (i.e. which
advertisement will be delivered to whom, when, how and why). Answering
these questions is challenging, as targeted advertising is based on highly
complex technology and at scale.

Awareness of
right to opt
out from direct
marketing
among general
population

In 2019, a Eurobarometer survey asked people in the EU if they are aware of their right to
opt out from direct marketing. Overall, only 59 % of EU citizens have heard about this right
(with 24 % having exercised it). But people can only exercise their right if they are aware
of it – which becomes even more important when direct marketing is made much more
efficient through machine learning.
Awareness levels strongly vary across the EU. The percentage of people who know about
their right to opt out from direct marketing ranges from 38 % in Bulgaria to 81 % in the
Netherlands. Figure 4 shows the percentages. It also highlights – based on FRA’s analysis of
Eurobarometer data – that there is a strong variation within countries, when broken down
by regions.
In some regions, fewer than one in four have heard about their right. These are areas with
higher shares of people at risk of poverty. This indicates the general problem that people
who are more disadvantaged in society tend to be less aware of this right. The data show
that people who are not working, who more often struggle to pay their bills, who are living
in rural areas, or who are older, are less aware of this right.
FIGURE 4:

AWARENESS OF GDPR RIGHT TO OPT OUT FROM DIRECT MARKETING, IN THE EU
AND UNITED KINGDOM, BY COUNTRY AND REGION (%)

75

50
FI−80
25

SE−65
EE−62

Insufficient or no data available

LV−60

DK−67

UK−63

LT−54

IE−74
NL−81

PL−64
DE−69

BE−59
LU−62

CZ−55
SK−61
AT−62

FR−47

HU−56

SI−57

RO−50

HR−50
IT−56

PT−51

ES−51

BG−38

EL−53

MT−46
CY−59

Note:	
Map does not show non-EU countries other than the UK. Light shading = more aware of right.
Dark shading = less aware of right. Results for regions within countries represented by light
grey spaces were excluded because there were fewer than 20 respondents, meaning the
numbers of observations were too low for reliable results. N = 26,503. Question: “The General
Data Protection Regulation (GDPR) guarantees a number of rights. Have you heard of each of
the following rights? […] 18.2 The right to object to receiving direct marketing.”

Source: FRA, 2020 [Calculations and presentation based on European Commission (2019),
Eurobarometer, 91.2]

65

Experiences from use cases
In general, interviewed experts highlighted that data protection law is difficult
to interpret and lacks clarity when it comes to the meaning of automated
decision making. One expert from France felt that, because automated
decision making is so difficult to explain, all automated decision making
should be banned, meaning the exceptions in the GDPR that allow for some
automated decision making should be removed. They pointed out that AI
can only be used as a decision support tool.
Another expert, an independent lawyer from the Netherlands, views current
laws and standards as sufficient, but says they need to be concretised per
sector. Particularly, the expert mentions that the scope of the existing rules
on permissible automated decision making are not clear and that it remains
unclear to him what a comprehensive assessment, or a ‘human in the loop’
means. This was also raised in relation to the SyRI case, where it remained
unclear to what extent the decisions were reviewed.
Another expert working at a supervisory authority, more generally, sees
no need for adapting data protection laws as “the legislation is quite
comprehensive. It is more about the organisation of the supervision thereof,
and also the political will behind it”.
These concerns reflect the findings of other research, which also raise serious
issues concerning the right to human review. For example, the responsible
officers questioned the results of an algorithmic system built to profile
unemployed people in Poland in less than one percent of cases. This essentially
makes a supporting tool an automated decision making tool.27
Linked to the question of reviewing decisions or outputs from AI systems is
the challenge of a clear lack of knowledge about how AI works. Interviewees
often could not explain in detail how the system they use works or which
data it uses, be it due to the lack of knowledge or lack of transparency.
Meaningful information about the logic involved, or explaining outcomes
from algorithms, is essential for several fundamental rights. It is crucial
not only for the processing of personal data, but also for ensuring that the
algorithms are fair and do not discriminate. It is also necessary to enable
people to properly challenge decisions and AI systems.
One interviewee working for public administration explains that the complexity
differs depending on the tasks. Licence administration systems can be
relatively straightforward. Crime prevention analysis uses more data sources,
which makes it harder to understand. Another interviewee working in law
enforcement says that the current AI used by police organisations is not yet
so complex that it would make explanations difficult, but that this might be
the case in the future.

“There is a risk of having too much
trust in the machine.”
(Public administration, France)

“There is a huge tension surrounding
the GDPR. So we want to do well,
but might in fact be worse off,
because interpretation of the data
then turns out to be impossible.”
(Public administration, Netherlands)

“If we had to explain the model, we
wouldn’t be able to. The model is
statistical and not very explainable.”
(Public administration, France)

“Internally we can explain the
decisions of the machine learning
models and we have several means
to do that.”
(Private sector, Estonia)

A respondent working on financial data transactions indicates that traditional
models were straightforward to understand. However, new methodologies
are more difficult to explain, and the company has to invest resources into
making these models more explainable. Still, the level of explainability
required by the GDPR is not clear to the respondent.

“If the systems do not have black
boxes of information or processes,
we already take a step forward in
the defence of human rights.”
(Public administration, Spain)

“We are strongly attached to the
idea that AI has to be explainable.”
(Public administration, France)
66

Most people are not aware that they have the right to have a say when decisions are
automated, evidence suggests. A Eurobarometer survey showed that 40 % of Europeans
know about their data protection rights.
FRA’s analysis of the Eurobarometer survey shows that this figure drops considerably
among people with lower socio-economic status. Only 26 % of EU citizens who report
that they are struggling to pay their bills most of the time know about this right. This
lack of rights awareness among those socially disadvantaged could contribute to further
social exclusion if those already disadvantaged are less aware that they can challenge
(automated) decisions about them (see Figure 5).
Gender differences are small, yet women are even less aware of this right (38 % of women
and 43 % of men). Older people are considerably less aware (31 % among those aged 55
and older).
FIGURE 5:

AWARENESS OF RIGHT TO HAVE A SAY WHEN DECISIONS ARE AUTOMATED, BY
AGE, GENDER AND DIFFICULTY IN PAYING BILLS (%)

55 years and older

31

40-54 years
Age

46

25-39 years

50

15-24 years

44

Gender

Women

38

Men

43

Difficulty paying bills

Refusal

39

Almost never/never

42

From time to time

38

Most of the time

26

Total

Awareness of
right to have
a say when
decisions are
automated

40
0

10

20

30

40

50

60

Notes:	N = 26,503. Question: “The General Data Protection Regulation (GDPR) guarantees a number
of rights. Have you heard of each of the following rights? […] 18.5 The right to have a say
when decisions are automated (e.g. an algorithm decides if you will be granted a loan or not).”

Source: FRA, 2020 [calculations and presentation based on European Commission (2019),
Eurobarometer, 91.2]

67

4.5.	 EQUALITY AND NON-DISCRIMINATION
Equality before the law and non-discrimination are enshrined in Articles 20
and 21 of the Charter. Discrimination is “where one person is treated less
favourably than another is, has been or would be, treated in a comparable
situation” based on a perceived or real personal characteristic28 (called
‘protected grounds/characteristics’). Article 21 of the Charter prohibits any
discrimination based on any ground such as sex, race, colour, ethnic or social
origin, genetic features, language, religion or belief, political or any other
opinion, membership of a national minority, property, birth, disability, age
or sexual orientation.

The Charter prohibition reflects corresponding rights in the ECHR (Article 14)
and in Protocol No. 12 to the ECHR (Article 12), but is even broader, as it
establishes a non-exhaustive, open list extending protection to a wide range
of new grounds. Unlike Article 14 of the ECHR, the Charter right to nondiscrimination is a freestanding right that applies to situations that do not
need to be covered by any other Charter provision.29

Main challenges

Discrimination is a crucial topic when it comes to the use of AI, because the
very purpose of machine learning algorithms is to categorise, classify and
separate. As one interviewed expert points out, making differences is not
per se a bad thing. According to this expert, when deciding to grant a loan,
credit history can be used to differentiate between individuals, but not on
the basis of protected attributes, such as gender or religion. However, many
personal attributes or life experiences are often strongly correlated with
protected attributes. The credit history might be systematically different for
men and women due to differences in earnings and job histories.
Interviewees often mention efficiency as the main purpose for using AIrelated technologies. Yet it is important to note that this cannot not justify
unfair, differential treatment.
Often, protected attributes might be highly correlated with risks. For example,
differences in life situations among men and women might often be linked to
68

different insurance risks. This is, however, not acceptable, as the Test-Achats
ruling, 30 shows. In that case, the CJEU put an end to gender discrimination
in insurance pricing.31
Under certain circumstances and in some areas, using algorithms could
positively contribute by reducing bias and stereotyping. Algorithmic data
analysis may produce results that could dispel prejudicial attitudes. For
example, predictive policing might, in some contexts, lead to more equitable
and non-discriminatory policing by reducing reliance on subjective human
judgments.32 Predictive techniques may be used to identify so-called ‘whitecollar crimes’, such as financial crimes that are historically under-policed.33
Nevertheless, direct or indirect discrimination34 through the use of algorithms
that involve big data is considered as one of the most pressing challenges
in the use of AI-driven technologies.35 Bias and discrimination, including
gender-based discrimination, in data-supported algorithmic decision making
can occur for several reasons and at many levels in AI systems. They are
difficult to detect and mitigate.36 Often, the quality of the data and biases
within it are the source of potential discrimination and unfair treatment.37
The discriminatory effects generated on certain groups are, in practice, very
difficult for individuals to challenge.38 So far, only a limited number of court
cases have dealt with discrimination relating to AI systems.39

UK Court of
Appeal: police
use of facial
recognition
violates
human rights

A first instance decision of the Divisional Court of Cardiff in 2019 dismissed a claim
concerning the lawfulness of the South Wales Police’s use of the “AFR Locate” face
recognition system. The Court of Appeal overturned that decision.
It found that the facial recognition programme used by the police was unlawful. The Court of
Appeal ruled that “too much discretion is currently left to individual police officers”. It added
that “[i]t is not clear who can be placed on the watch list, nor is it clear that there are any
criteria for determining where [the technology] can be deployed”.*
The court also held that the police did not sufficiently investigate if the software in use
exhibited race or gender bias.
This judgment is the first in-merit specifically on this matter in Europe. It considerably
narrows the scope of what is permissible and what law enforcement agencies need to do to
fully comply with human rights law.**
* UK, Court of Appeal, R (Bridges) v. CC South Wales, [2020] EWCA Civ 1058, 11 August 2020.
** Ars Technica, ‘Police use of facial recognition violates human rights, UK court rules’, 11 August
2020.

Studies have highlighted the potential for discrimination prompted by the
use of AI-systems across the areas covered by the report.40 In the area of
predictive policing, for example, a particular risk relates to the potential
for automated decision making tools to reproduce and entrench existing
discriminatory practices that undermine equality before the law (Article 20
of the Charter). The historical crime data that underpins predictive policing
may be biased,41 reflecting inherent data gaps (e.g. chronic underreporting
for certain types of crime), alongside issues with how data is recorded (e.g.
human error, but also bias by individual officers).
Crime victimisation surveys consistently show that a large proportion of crime
is never reported to the police by the public – particularly crimes involving
physical and/or sexual violence, and hate crimes. For example, FRA’s survey
on violence against women – with 42,000 respondents – showed that only
one in five women who experienced violence, by their partner or anyone
else, brought the most serious incident to the attention of the police.42 FRA’s
69

EU-MIDIS II survey of 25,500 respondents across the EU showed that only
three in ten reported incidents of racially motivated hate crime to the police
or any other organisation.43
Compared with violent crime and hate crime, property crime – such as
burglary – has a higher rate of reporting to the police, particularly in developed
countries. This may be because this is a requirement when claiming on an
insurance policy.
In sum – relying on official crime statistics (that are based on reported crime)
when looking to develop AI models in the field of predictive policing is
particularly problematic when it comes to specific crimes and specific groups.
Some variables used in AI modelling can be proxies for race, ethnicity, gender
and other protected categories. The complexity of the algorithms makes it
harder to identify and remove such biases. Instead of providing objective
analysis, predictive policing software may turn into an ‘echo chamber’
cementing existing systemic flaws and injustices with the ‘stamp’ of what
appears to be scientific legitimacy.44
The use of predictive policing may also make law enforcement responses
less equitable by focusing on certain crimes or areas.45 Predictive policing
is currently focused on property crimes such as theft and burglaries, which
are often associated with certain demographics and neighbourhoods. This
can result in certain demographics and neighbourhoods – and the individuals
living in them – being further stigmatised.46 Meanwhile, white-collar crime –
typically committed by different demographics – is less prioritised.47 These
patterns of policing – whereby certain neighbourhoods or communities are
disproportionately policed – predates the use of AI. However, the ‘promise’ that
AI is more ‘objective’ and can, in turn, be used to counteract discriminatory
policing, needs to be verified in practice.
Oxford University researcher Sandra Wachter highlights that discrimination may
occur due to information linked to protected attributes in targeted advertising.
Newly created profiles for the purpose of advertising might amount to indirect
discrimination and potentially even require new characteristics to be added
to non-discrimination legislation, and extend for its scope to be expanded
to other areas.48

Experiences from use cases

Many interviewees noted that the use of AI, in general, can discriminate, but
that the systems they are working with do not. Many indicated a belief that
excluding information on protected attributes is sufficient protection against
discrimination. However, discrimination can occur due to other information
contained in datasets that may indicate protected attributes. Traces of
protected groups are often hidden in other information.
An example from a public authority, which uses AI in tax and customs,
shows the challenges linked to identifying possible bias and potential
discrimination when using algorithms. When scrutinising their algorithms,
a public administration body found a higher degree of errors in tax declarations
among recently issued national identification numbers, which have almost
always been attributed to immigrants. This prompted further research into the
correlation. It turned out that the outputs of people with recent identification
numbers more often contained errors because they had never filed their
taxes before, and did not know how to do so (which was also the case for
non-migrants). This is also an example of proxy information, where parts of
a number could indicate immigrant status.

70

“If you want the machine not to
discriminate on the basis of sex,
do not put the variable of sex, as
easy as that, or make the examples
symmetrical if you notice that sex
has certain relevance.”
(Public administration, Spain)

Another interviewee working on the potential use of AI for detecting benefits
fraud mentioned in this respect: “If you want to prevent discrimination based
on ethnicity, for instance, it does not suffice to just remove the ‘ethnicity
label’, because the neighbourhood composition is often also determined by
ethnicity, or ethnicity plays a role in it. So [preventing discrimination] often
goes beyond the ‘direct’ characteristics”.
“[I]f you do not have access
to sensitive personal data, it is
impossible to check if you are
profiling on that basis.”
(Public administration, Netherlands)

Even if most of the respondents were aware of the general potential
for discrimination when using AI, they often ruled out that their system
discriminates against people based on protected characteristics. Some
respondents also believe that their tools have a positive impact in terms of
non-discrimination. One respondent, testing AI for social benefits decisions,
regrets not being able to use AI for data protection reasons, even though, in
the respondent’s view, automation could process big datasets effectively and
without discrimination. While noting that protection of personal data needs
to be observed, the respondent feels it hinders prompt decision making
and non-discrimination – “if it can be automated, it should be automated”.
Some respondents were not clear or not sure about whether their use of AI
could discriminate. Respondents repeatedly stated that their system cannot
discriminate because it does not include data on protected characteristics.
For example, several interviewees working in predictive policing and law
enforcement felt that there was no potential for discrimination, as the AI
systems did not use data on, or return outcomes related to, protected grounds,
or because the system does not aim to identify people.
Others working on predictive policing felt that discrimination could occur, in
particular because of issues in the training data. In relation to the predictive
policing ‘heat map’ case, for example, one interviewee noted that – because
the dataset is never fully neutral, representative or complete – there is
a strong risk of bias and possible discrimination towards particular groups.
They identified sharing datasets to increase the amount of data available
as one way to mitigate this risk, but felt that this was impeded by data
protection regulations. They also indicated that multi-level teams with the
task to travel to different police authorities and check on the quality of the
systems used are being set up.
In the area of targeted advertising, some interviewees mentioned discrimination
as a potential problem, mainly after being asked directly about it. Overall
respondents do not think that their systems discriminate. Three respondents
mention that information on gender and age is not used and consequently
no discrimination in this respect can occur. Another interviewee is not sure
if this information is included or not.

“For discrimination, it’s complicated
because some diseases are more
present in certain ethnic groups.
Predictions take into account the
sexual, ethnic, genetic character. But
it is not discriminatory or a violation
of human rights.”
(Private sector, France)

A respondent working on a breast cancer detection tool highlighted that
age, gender and ethnicity are relevant factors as some population groups
are more likely to develop certain types of cancer. Respondents working in
health highlighted that the potential for discrimination is also linked to who
uses the system, suggesting that this could become a greater challenge if
the system were used by non-medical staff.
A different, but related, example comes from a respondent working on credit
rating for a private company, selling credit scores of individuals created by an
algorithm. The company uses information about gender, age and citizenship
in its credit risk models. This information has some impact on the outcome
of the credit scores. For example, younger people or non-citizens have
a higher credit risk score, but the influence of demographics is much smaller
compared to credit history data. According to the interviewee, their system
“certainly does not impact on the right to non-discrimination, because we
71

do not make any decisions, we sell data and data analytics. Creditors have
to monitor that they do not discriminate”.
Another interviewee working on the data strategy for a financial institution in
the private sector, using AI to analyse financial transactions, clearly mentions
the challenges of understanding what non-discrimination constitutes for their
work. The interviewee mentions, for example, that it is not clear to what
extent it is illegal to exclude older people from receiving credit if their life
expectancy is expected to be lower than the mortgage repayment period
they asked for.
These findings point to uncertainty and ambiguity in the financial sector with
respect to how Article 21 of the Charter – on non-discrimination – translates
into real life situations.49

Vulnerable groups

Much of the discussion and research about discrimination when using AI is
linked to biased results with respect to ethnic origin, gender, and to some
extent, age. Although it is important to analyse potential discrimination against
these groups, the Charter covers several other grounds of discrimination,
which are less often part of discussions or research.
These other grounds include, for example, political opinion, sexual orientation,
and disability. The Charter provides particular rights to some special groups
(beyond Articles 20 and 21), including the rights of the child (Article 24), the
rights of the elderly (Article 25), and the rights of persons with disabilities
(Article 26).
The question of age – with respect to older age groups and younger adults –
came up during the interviews, notably when it comes to insurance and
credit (see above).
However, none of the interviewees or experts directly mentioned the rights
of the child. This might be linked to some extent to the nature of the use
cases investigated, but it clearly reflects the fact that this topic is not high
on the agenda of many of those working in AI.
Article 24 of the Charter emphasises that the best interests of the child
must be the primary consideration in all activities of public authorities and
private actors that concern children, which applies of course – equally – to
the field of AI.50
Only two respondents from public administration mentioned possible use of
AI in the area of child custody and the distribution of children in schools. But
they did not address this in consideration of the rights of the child. These
respondents did not wish to go into more detail concerning these use cases –
potentially reflecting the sensitivity of this topic.
Finally, issues linked to the integration of people with disabilities were not
raised in any of the interviews.

72

A Eurobarometer survey that included questions on AI asked respondents about the areas
they are mostly concerned about when it comes to the use of AI, including discrimination in
decision making, unclear responsibility, and that there is nobody to complain to.
Only around 40 % of EU citizens indicated that they are concerned that using AI could
lead to discrimination in terms of age, gender, race or nationality – for example, in taking
decisions on recruitment, credit worthiness, etc.
Results vary across countries. Higher proportions of people are concerned about
discrimination in the Netherlands (58 %), Luxembourg (48 %) and Sweden (47 %). Lower
proportions expressed concern in Estonia (25 %), Hungary (24 %) and Lithuania (23 %)
(see Figure 6).
However, from this question it is not clear if people do not know that discrimination can
happen, or if they are aware that it can happen but do not think it is a problem.
FIGURE 6:

AWARENESS ABOUT THE RISKS OF DISCRIMINATION WHEN USING AI, BY COUNTRY
(%)

100
90
80
70
58

60
50
40

40

30

48 47 46
45 44 43
41 41 40 40 40 39
38 37 36
35 34 34 34
32 31
29 28

41
25 24 23

20
10
UK

NL
LU
SE
FR
EL
SI
DE
ES
CY
BE
IE
HR
AT
DK
IT
CZ
FI
BG
PT
SK
MT
LV
RO
PL
EE
HU
LT

0
EU-27

Awareness
among general
population of
potential for
AI to lead to
discrimination

Notes:	Includes people who indicated that they are concerned that AI could lead to discrimination
among three possible issues, or all of the three issues.

Source: FRA calculations based on European Commission (2019), Eurobarometer, 92.3

73

Tackling
gender
The Charter stipulates that equality
women and men must
inequality in between
be ensured in all areas, including
work and pay
the design and employment,
(Article 23). Gender discrimination
is a major concern when it comes to
use of AI
the design and use of AI and related
technologies.*

* See also European
Commission, White
Paper On Artificial
Intelligence –
A European approach
to excellence and trust,
COM(2020) 65 final,
Brussels, 19 February
2020, p. 1.
** European Economic
and Social Committee,
Artificial intelligence –
The consequences of
artificial intelligence
on the (digital) single
market, production,
consumption,
employment and society
(own-initiative opinion),
31 May 2017, JO C 288,
p. 43.
*** Aksoy, C., Özcan, B.
and Philipp, J. (2020),
Robots and the Gender
Pay Gap in Europe, IZA
Discussion Paper No.
13482.
****
See the webpage
on data feminism on the
datasociety’s website.
***** Criado Perez,
C. (2020), Invisible
Women. Exposing data
bias in a world designed
for men, London.

74

On the development side, the
European Economic and Social
Committee notes that the
development of AI is taking place
within a homogenous environment
principally consisting of young white
men. This results in cultural and
gender disparities, which are being
embedded in AI technologies. For
example, training data are prone to
manipulation, may be biased, reflect
cultural, gender and other prejudices
or preferences, and contain errors.**
This is also reflected in this research,
where, despite efforts to achieve
gender balance, the majority of
interviewees were men.
Disparities at the design and
deployment stage are linked to the
systematic disadvantages affecting
women in the labour market and
the potential lack of awareness
of gender biases. A recent study
showed that the increased use
of industrial robots could widen
the gender gap, despite both
genders benefitting from increased
automation, as the analysis indicated
that men in medium- and highskill occupations would benefit
disproportionally.***
Looking ahead, using data and
algorithms could help to better
mainstream gender equality into
policies and processes by paying
attention to gendered datasets.
Drawing on discussions around
gender inequalities and the use
of data (‘data feminism’)****
could help to raise awareness that
the male point of view should
not be taken as the default view,
which also then finds its way into
datasets.*****

4.6.	 ACCESS TO JUSTICE
The right to an effective remedy before a tribunal and to a fair trial (Article 47
of the Charter) is one of the most often used Charter right in legal proceedings.
This highlights its importance in upholding fundamental rights and the rule
of law. This right of horizontal character empowers individuals to challenge
a measure affecting any right conferred to them by EU law, not only in respect
of those guaranteed in the Charter.51 The CJEU has underlined that Article 47
of the Charter constitutes a reaffirmation of the principle of effective judicial
protection and that the characteristics of a remedy must be determined in
a manner that is consistent with this principle.52
The right to an effective remedy also covers decisions taken with the support
of AI technologies. EU data protection law reconfirms that the right to an
effective judicial remedy must be provided in relation to decisions by the
controller or the processor53 as well as the supervisory authority.54 Data
processed by AI-driven technologies is no exception.
It is crucial to note that the possibility to lodge an administrative complaint
before a supervisory authority as provided for by the GDPR and the Law
Enforcement Directive55 is not considered an effective judicial remedy under
Article 47 of the Charter. This is because no court is involved in such a review.
Judicial review should always remain available and accessible, when internal
and alternative dispute settlement mechanisms prove insufficient or when
the person concerned opts for judicial review.56
Using AI can challenge the right to an effective remedy in different ways.
One prominent concern is the lack of transparency in the use and operation
of new technologies. Algorithmic decision making is notoriously opaque:
data collection, algorithm training, selection of data for modelling or profiling,
the situation around individual consent, effectiveness and error rates of the
algorithm and other aspects are often not transparently reported.57

75

Without access to this information, individuals may not be able to defend
themselves, assign responsibility for the decisions affecting them,58 appeal
any decision negatively affecting them or have a fair trial, which includes the
principle of equality of arms and adversarial proceedings as established by
the ECtHR.59 These requirements also form part of the corresponding Charter
right (Article 47) in view of Article 52 (3) of the Charter.

Main challenges

These issues are reflected in the specific challenges to the right to an effective
remedy and a fair trial that the interviewed experts outlined. Generally, experts
indicate a difference in accessing remedies at private companies and public
administration. Public authorities are more often forced to be transparent
about their use of AI. Meanwhile, companies appear to be more secretive,
the assessment of several experts suggests. However, an expert from the
Netherlands said that people might more readily complain to companies, but
be reluctant to complain to public authorities. This is because public services
often concern vulnerable people, in need of social benefits, who would be
less inclined to complain about any decisions.
Opportunities to successfully complain about the use of AI and challenge
decisions based on AI are essential for providing access to justice. The
interviews emphasised the following as important in this respect:
――Making people aware that AI is used
――Making people aware of how and where to complain
――Making sure that the AI system and decisions based on AI can be explained
First, everyone needs to know if they are dealing with an AI system. If
a taken decision affects people, e.g. on social benefits, people concerned
might complain in general – but they will not be able to complain about the
use of AI if they do not know AI is involved.
An expert explained that, while there is general willingness to complain,
the biggest problem is that people often do not know that AI is being used,
because organisations are not transparent about this, even though this is
required by the GDPR. Several interviewees indicate that informing people
that any decision made about them is based on (partly) automated tools is
the very first step for providing access to complaints.
Second, everyone needs to know how and where to complain. It may be
difficult for people to know which body deals with what type of complaints.
One expert pointed out that consumers often do not know how to complain –
for example, to a bank that might use algorithms for deciding on financial
matters. A public administration that issues automated decisions decided
to add names of employees to the decisions to provide contact persons to
those potentially challenging the (automated) decision. Most interviewees
indicated that there are ways and procedures for complaints in place, which
are the same procedures as those for any other complaints not linked to the
use of AI. Only few companies or organisations that use AI on anonymised or
aggregated data indicate that they do not have any complaint mechanisms
in place.
Finally, those complaining need enough information to challenge the underlying
decision. Only thorough information about the AI systems provides equality of
arms to meaningfully challenge decisions. However, this is not straightforward
when it comes to the use of AI, particularly because of:
――potential intellectual property rights issues, and
――because complex systems are difficult to explain.

76

Intellectual property rights form one hurdle to providing enough information
about how a decision was made, or how a system works. Algorithms can be
part of an implemented software, or technical invention, that may be subject
to intellectual property rights – a right protected under Article 17 (2) of the
Charter. Actors often seek out copyright, patent and trade secret protection
to safeguard their knowledge on AI.60
One interviewee from the insurance sector claims that, due to the highly
competitive market, “one may not share too much about the workings of
a used technology” as to, for instance, why a particular price was given to
a customer. This is essentially because competitors could benefit from this
knowledge if the underlying software were subject to scrutiny. Another
respondent using AI to handle visa applications notes that using systems
developed by external providers whose algorithms are covered by intellectual
property rights can hinder the necessary transparency at a later stage.
Another challenge for successfully complaining about automated decisions
or the use of AI in general is the challenge to explain the decisions based
on complex systems. The interviewees working for public administration
suggest that there is usually clear guidance on how to complain against an
administrative decision, an area where interviewees highlight the importance
of detailed explanations. For example, for the systems that automatically
provide unemployment benefits for cases that do not involve discretion,
clients can ask for the reasoning behind automated administrative acts. An
interviewee indicates that if clients wish to see the calculations behind financial
decisions, they may do so in the self-service system on the organisation’s
website or in their publications, which contain detailed descriptions of the
calculations used.
Interviewees recognise that an open and transparent logic is essential for
providing explanations regarding AI-supported decisions, but that this is
often challenging or impossible to achieve. One interviewee working for
a bank mentions that more complex machine learning solutions cannot
be used for certain decision making, because the reasoning of the system
cannot be explained easily, and this is why such systems are only used for
other purposes. However, an interviewee working for another bank indicates
that such systems are used, but they use simpler methods in addition to
the complex ones to get an idea of the probable reasons for the decisions.
One expert raised the problem that companies internally might not have
enough information about the way algorithms work themselves. The lack of
expertise and knowledge appears to be a major hindrance in practice when
seeking access to effective remedy.61
“The topic of transparency is very
important nowadays, there are
many procedures on how to publish
the information, many automatic
means that help to upload the
information on the portals, and
there has been a lot of work done in
terms of transparency.”
(Public administration, Spain)

Experiences from use cases

Respondents discussing predictive policing tools highlighted transparency
as important.
In the gender-based violence use case, they felt that sending both the police
file and the outcome of the AI system to the judge, and informing the victim
of the level of risk attributed to the case and the police measures that will
apply as a result, enhances transparency.
Interviewees discussing the heat map example referred to numerous requests
to the police to explain the system’s purpose and how it works, and highlighted
transparency as a way to reduce public anxiety.
A number of interviewees pointed to the possibility for individuals
affected by the system to make complaints to the police, the courts or the
77

Ombudsinstitution. With reference to the domestic violence case, however,
the interviewee indicated that there is no procedure in place to question
a system of police protocol.
In terms of measures to protect fundamental rights in the health services use
cases, several interviewees referred to ethics committees, as well as general
legal safeguards and data protection rules. Checks and controls were primarily
mentioned to take place through external actors. No specific complaints
procedures were in place in the organisations of those interviewees who
responded to this question.
Some interviewees highlighted that doctors ultimately take responsibility for
decisions, and that patients often do not know about the use of an AI tool
in the first place. For example, in the breast cancer detection example, the
interviewee indicated that there is no possibility for legal recourse against
the developer of the tool, as the radiologist makes the decision on diagnosis
and is liable for any errors.
The safeguards in place for the targeted advertising cases mainly follow
data protection requirements, such as ensuring that consent is obtained and
respected. One company makes sure not to have clients engaged in illicit
practices and rejects clients from certain sectors, such as political advertising.

Complaints received

Few of the organisations interviewed received any complaints challenging
their use of AI. In some cases, interviewees claim to have received complaints
by complainants not aware that AI was used, who noticed incorrect outputs
in decision making.
For example, individuals lodged complaints regarding traffic fines, whereby
a police officer stopped a car driver, and upon hearing the car driver’s
explanation as to why the fine was wrongfully administered, proceeded
to manually correct the information in the system, without being able to
update the system’s historical data. In these cases, such fines will remain
visible throughout the system, and this particular person would continue to
be profiled as a high risk on each occasion.
Even though organisations rarely received any formal complaints with respect
to their use of AI, interviewees often state that this is due to the early stages
of their AI implementation. Nonetheless, interviewees reported repeated
requests for access to or rectification of personal data, and some people
requested their information to be removed, as well as explanations as to
why a certain recommendation was made.
The majority of interviewees claim that procedures are the same as to if
a decision had been processed or undertaken by a human. On the other
hand, a few other interviewees showed interest in opening new channels
to analyse, explain and redress decisions involving their AI solutions.
Other rights linked to access to justice set out in the Charter are also impacted,
most notably by the use of AI in law enforcement. These include, for example,
the presumption of innocence (Article 48 of the Charter).
When identifying people who are suspected of having committed a crime,
the police may target their activities specifically against one person or put
them under suspicion based on flawed and fragmented data and algorithmic
profiling.62 Uncritical reliance on automated tools, without proper human review
that takes into account other information, might contribute to discrimination
in decision making.
78

“The number of the complaints
about data use is miniscule, rather
people may have asked to delete
some information about them.”
(Private company, Estonia)

4.7.	 RIGHT TO SOCIAL SECURITY AND SOCIAL
ASSISTANCE
The right to social security and assistance
enshrined in Article 34 of the Charter is a classic
social right,63 inspired by various international
and European legal standards.64 This provision,
combining both elements of a right and of
a principle,65 has a great significance in the
EU in view of the free movement of people
within the Union.
Instead of tying issues of social protection
to the labour market, this Charter right takes
a new, communitarian approach when broadly
referring to “providing [social] protection in
cases such as maternity, illness, industrial
accidents, dependency or old age, and in the
case of loss of employment” (Article 34 (1)).66
It is, however, a primarily programmatic statement that does not prescribe
any minimum standard of protection. It is in principle up to EU Member States
to determine the conditions of entitlement and access to social benefits, with
further clarification needed from the CJEU.67 Yet, Article 34 (1) of the Charter
provides protection against measures restricting or abolishing existing social
security rights.68
In addition, access to social rights is guaranteed to all individuals legally
residing within the EU who exercise their right to free movement, regardless
of their nationality, subject to EU and national laws (Article 34 (2)). This thus
creates justiciable rights before national courts and the CJEU.69
It is becoming increasingly apparent that the impact of AI technologies on
social protection systems and the lives of the many individuals who rely upon
them can be far-reaching and – potentially – very problematic. Introducing
AI-driven technologies in social welfare systems risks creating barriers to
access to this right.70
For example, using AI in social security needs to account for potential
negative – and discriminatory – effects on non-nationals (both EU citizens
and third-country nationals) exercising their right to freedom of movement
in the EU. They could be negatively affected, for example, if a system relies
on data about job histories, which are not available for those moving from
other EU Member States.
Only one respondent addressed the ‘right to receive a correct pension’ as
an aspect of a wider definition of human rights. Meanwhile, none of those
interviewed referred to the fundamental right to social security and social
assistance. This could partly reflect the nature of the use cases. However,
the lack of references to social rights among public sector interviewees was
notable.

4.8.	 CONSUMER PROTECTION
The Charter stipulates that EU policies must ensure a high level of consumer
protection, which is based on Article 169 of the TFEU. EU institutions and
other bodies needs to observe this principle, as do Member State authorities
when implementing EU law.71
79

This Charter principle provides only for the guarantee of a particular goal (“a
high level of consumer protection”). Article 169 of the TFEU is more concrete,
as it also determines the means of how to achieve the stated aim – for
example, protecting the health, safety and economic interest of consumers,
as well as promoting their right to information and education. 72
Among the use cases, the use of AI for targeted advertising, and the use of
medical records by companies, are of particular importance.
When it comes to targeted advertising, consumers need to be aware that
they can opt out from being targeted. If they are not aware, they might be
subjected to advertising they do not want. This is particularly problematic in
combination with highly sophisticated AI systems for advertising, which can
amount to some sort of manipulation of consumer preferences.73
Consumer protection is also of major relevance for the use of health data
(EHRs). The European Consumer Organisation (BEUC) noted that AI in the
area of health brings challenges for consumers. It recommends that AI
technologies must fully respect data protection rules, be transparent to
the consumer, and avoid discrimination. BEUC has also called for updated
regulation and legislative measures for market surveillance, law enforcement,
and efficient redress concerning digital health products and services to fully
protect EU consumers.74
BEUC carried out a survey among consumers about their views on AI in
selected EU Member States. It shows that more than one in two respondents
agree that companies are using AI to manipulate consumer decisions. In
addition, almost half of respondents believe that personalised content and
adverts on e-commerce platforms do not have an added value (44 %).
Slightly more than half of the survey respondents expressed low trust that
governments effectively control AI.75
In the interviews conducted for this study, consumer protection was only
mentioned at the margins, when discussing risks of using AI and fundamental
rights. However, some respondents from businesses refer to consumer
protection legislation as a relevant framework also applying to their use
of AI. Moreover, some respondents deem consumer protection authorities
potentially relevant oversight bodies when AI is used.
In general terms, many interviewees in the business sector stress the
importance of consumer satisfaction. For example, a company using video
surveillance for the security of customers at their premises mention that
consumer protection regulations are relevant for such technical solutions,
and that the use of the systems should aim to improve the situation of
consumers while also preserving their rights. Several AI tools are built to
understand and profile consumers to enable businesses to improve their
services and marketing.
Data protection is an important aspect for business. This is also linked to the
fact that breaching data protection rules is considered a business risk, as
mentioned above. One major concern of companies is obtaining and managing
consent from consumers and customers to process their data, when using
AI tools for marketing purposes. Interviewees report that the GDPR has had
an impact, improving their systems to handle consent.

80

4.9.	 RIGHT TO GOOD ADMINISTRATION
The right to good administration is a well-established general principle of
EU law elaborated by the CJEU. As such, it is binding on all EU Member States.76
It is also a fundamental right enshrined in Article 41 of the Charter, although
only for actions of EU institutions, bodies and agencies.77
As a general principle of EU law, it requires EU Member States to apply the
requirements of the right to good administration in all public action. This right
includes, but is not limited to, the right of an individual to have access to
their file and the obligation of any public authority to give sufficient reasons
for its decisions.78
Access to the file facilitates understanding of the evidentiary basis on which
a decision has been made, and/or of the reasons underlying it. This places
the individual in a better position to put forward counter-arguments when
exercising the right to be heard and the right to an effective remedy.79
The obligation to give reasons makes, from the perspective of the individuals
affected, the decision-making process more transparent, so that the person
concerned can know and understand why a measure or action has been
taken. Transparency is also an enabling principle that provides foundations
for other rights,80 including the exercise of the right to an effective remedy.
According to the CJEU, the context in which individual decisions are made is
important in determining the extent of the duty to give reasons.81 In France, for
instance, the Code on the Relations between the Public and the Administration
requires written explanations of the factual and legal considerations on which
a decision has been based.82
The right to good administration also applies when AI systems process
personal data and support decision making by public authorities. Although
the right to good administration may be subjected to certain limitations,
the question arises of how to ensure that the potentially huge number of
individuals all have access to their files (personal data used in AI systems).
Another question is how to make sure that public authorities always give
sufficient reasons when the operation of AI-driven technologies cannot be
fully explained due to their inherent opacity and complexity.
The use of a system to categorise unemployed people, set up in Poland,
highlighted problems linked to public administration and the use of algorithms.
Based on questions answered by unemployed people, a categorisation was
developed through a statistical algorithm. The system received a lot of criticism
from civil society with respect to the lack of opportunities to complain and
potential discrimination.83 In the end, a complaint by the Ombudsinstitution –
based on administrative grounds – led to a constitutional court ruling that
put an end to the system’s use.84
The intent to increase efficiency drives the use of AI in the public sector – an
aim that directly speaks to improving administration and benefiting citizens.
Respondents in public administration by far most often indicate efficiency
as the reason for considering the use of AI or for presently using AI. One
respondent, who advises ministries on digital strategies and their use of
AI, said that the main reasons for adopting AI are to improve the service to
citizens and to reduce the costs of these services for public administration.
Interviewees also indicate that public administration has particular
requirements, meaning AI cannot be used for all purposes and needs particular
attention when it comes to decision making. However, the efficiency of
a system is also considered an important added value.
81

In this sense, a respondent working on the digitalisation of migration
management indicates that building too complex AI systems is a risk,
because afterwards it would require a lot of work to understand the system
in retrospect. The interviewee indicates that their team needs to be careful
not to allow AI to make final decisions, which have to be taken by a human –
because society and clients are not ready for this, according to the interviewee.
Although some systems are appealing, they do not work effectively, and this
could result in extra work and negative results. However, the interviewee
also indicates that the dimension of efficiency “is often side-lined when
discussing data protection”.
The requirements for good administration also directly link the issues raised
above with respect to data protection, non-discrimination and the right to an
effective remedy and fair trial. Public administration can only process data
on a legal basis. Decisions need to be fair and transparent and pathways
to challenge decisions need to be available and accessible. As a result, the
requirements for good administration are directly linked to the discussion
and analysis above with respect to the legal processing of data (under data
protection), fair decisions (linked to the discussion about non-discrimination),
alongside transparency and ways to challenge and explain decisions (with
respect to access to justice).

82

Endnotes
See European Commission, European enterprise survey on the use of technologies based on artificial intelligence, Luxembourg, July
2020.
2 FRA (2020), What do fundamental rights mean for people in the EU, Luxembourg, Publications Office, p. 28.
3 See webpage on Three-level IT Baseline Security System ISKE on ther website of Estonia’s Information System Authority.
4 See the website of the PCI Security Standards Council.
5 Barak, A. (2019), ‘Human dignity as a framework right (motherright)’, in Barak, A., Human Dignity: The Constitutional Value and the
Constitutional Right, Cambridge, Cambridge University Press, 2015, Ch. 9 (pp. 156-169).
6 CJEU, C-377/98, Netherlands v. European Parliament and Council, 9 October 2001, paras. 70-77.
7 For a discussion on the malicious use of AI, see for example, Brundage, M. et al. (2018), The Malicious Use of Artificial Intelligence:
Forecasting, Prevention, and Mitigation.
8 FRA (2019), Facial recognition technology: fundamental rights considerations in the context of law enforcement, Luxembourg,
Publications Office, November 2019.
9 CJEU, Joined Cases C-92/09 and C-93/09, Volker und Markus Schecke and Eifert GbR and Hartmut Eifert, Opinion of Advocate General
Sharpston, 17 June 2010, para. 71.
10 FRA, Council of Europe and EDPS (2018), Handbook on European data protection law. 2018 Edition, Luxembourg, Publications Office, June
2018, p. 19.
11 See also Ibid., pp. 35-52.
12 ECtHR (2019), Guide on Article 8 of the European Convention on Human Rights – Right to respect for private and family life, home and
correspondence, Strasbourg, Council of Europe, updated on 31 August 2019, paras. 133 and 136.
13 ECtHR, López Ribalda and Others v. Spain, Nos. 1874/13 and 8567/13, 17 October 2019, para. 87. For a comprehensive legal analysis of
the meaning and content of ‘privacy’, see also Koops, B.-J. et al. (2017), ‘A Typology of Privacy’, University of Pennsylvania Journal of
International Law, Vol. 38, Issue 2, pp. 483-575.
14 Vermeulen, M. (2015), SURVEILLE Deliverable D4.7 – The scope of the right to private life in public places, July 2014, p. 2.
15 UN, Human Rights Committee, General Comment No. 37 (2020) on the right of peaceful assembly (article 21), CCPR/C/GC/37, 17
September 2020, para. 62.
16 Costello, Róisín Áine (2020), The Impacts of AdTech on Privacy Rights and the Rule of Law,
Technology and Regulation.
17 Norwegian Consumer Council (2020), Out of Control. How consumers are exploited by the online advertising industry.
18 FRA (2020), Your rights matter: Data protection and privacy – Fundamental Rights Survey, Luxembourg, Publications Office.
19 Rocher, L., Hendrickx, J. M. and de Montjoye Y. (2019), Estimating the success of re-identifications in incomplete datasets using
generative models, Nature Communications 10, No. 3069.
20 Hacker, P. (2020), A Legal Framework for AI Training Data. Law, Innovation and Technology (forthcoming), available at SSRN; Article
29 Data Protection Working Party (2014), Opinion 05/2014 on Anonymisation Techniques; see also Finck, Michèle and Pallas, Frank,
They Who Must Not Be Identified - Distinguishing Personal from Non-Personal Data Under the GDPR (October 1, 2019), Forthcoming,
International Data Privacy Law, 2020, Max Planck Institute for Innovation & Competition Research Paper No. 19-14, available at SSRN; and
Sartor G. and Lagioia F. (2020), The impact of the General Data Protection Regulation (GDPR) on artificial intelligence, study prepared for
the panel for the Future of Science and Technology (STOA) of the European Parliament.
21 See, for example, the UK Data Service’s blog on “Access to sensitive data for research: ‘The 5 Safes’”; see also the discussion in Ohm, P.
(2010), “Broken promises of privacy: responding to the surprising failure of anonymization”, UCLA Law Review, p. 1701.
22 GDPR, Art. 22 (3); and Law Enforcement Directive, Art. 11 (1).
23 Veale, M. and Edwards, L. (2018), ‘Clarity, surprises, and further questions in the Article 29 Working Party draft guidance on automated
decision-making and profiling’, Computer Law & Security Review, Vol. 34 (2), April 2018, pp. 398-404.
24 Article 29 Data Protection Working Party (2018), Guidelines on Automated individual decision-making and Profiling for the purposes of
Regulation 2016/679, Adopted on 3 October 2017, as last Revised and Adopted on 6 February 2018.
25 Green, B. And Chen, Y. (2019), ‘Disparate Interactions: An Algorithm-in-the-Loop Analysis of Fairness in Risk Assessments’, In FAT* ’19:
Conference on Fairness, Accountability, and Transparency (FAT* ’19), January 29-31, 2019.
26 González Fuster, G. (2020), Artificial Intelligence and Law Enforcement – Impact on Fundamental Rights, European Parliament, Policy
Department for Citizens’ Rights and Constitutional Affairs, Directorate-General for Internal Policies, PE 656.295, July 2020, p. 17; Brkan,
M. (2019), ‘Do algorithms rule the world? Algorithmic decision-making and data protection in the framework of the GDPR and beyond’,
International Journal of Law and Information Technology, Vol. 27 (2), p. 98; Article 29 Working Party, Guidelines on Automated individual
decision-making and Profiling for the purposes of Regulation 2016/679, Adopted on 3 October 2017, as last Revised and Adopted on 6
February 2018, WP251rev.0, p. 19.
27 Misuraca, G., and van Noordt, C. (2020), Overview of the use and impact of AI in public services in the EU, European Commission Joint
Research Centre, Luxembourg.
28 Council Directive 2000/43/EC of 29 June 2000 implementing the principle of equal treatment between persons irrespective of racial
or ethnic origin, OJ L 180, 19.7.2000, pp. 22-26, Art. 2; and Council Directive 2000/78/EC of 27 November 2000 establishing a general
framework for equal treatment in employment and occupation, OJ L 303, 2.12.2000, pp. 16-22, Art. 2.
29 FRA and CoE (2018), Handbook on European non-discrimination law. 2018 edition, Luxembourg, Publications Office, June 2018, p. 35.
30 CJEU, C-236/09, Association Belge des Consommateurs Test-Achats ASBL and Others v. Conseil des ministres, 14 January 2011.
31 European Commission (2012), EU rules on gender-neutral pricing in insurance industry enter into force, Press release, IP/11/1581, 20
December 2012.
32 Elizabeth E. Joh (2015), ‘The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing’, UC Davis Legal Studies Research
Paper No. 473, pp. 17-18.
33 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, p. 14. DOI:
10.1177/1477370819876762.
34 See also European Commission, White Paper On Artificial Intelligence – A European approach to excellence and trust, COM(2020) 65 final,
Brussels, 19 February 2020, p. 1.
35 FRA (2018), #BigData: Discrimination in data-supported decision making, Luxembourg, Publications Office, June 2018, p. 3.
36 Ibid.
37 FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications
Office.
1

83

38 Korff, D. and Browne, I. (2013) ‘The use of the Internet & related services, private life & data protection: trends, technologies, threats
and implications’, Council of Europe, T-PD(2013)07.
39 See National Non-discrimination and Equality Tribunal of Finland, decision no. 216/2017 from 21 March 2018. See also the SyRI case
discussed above and UK, Court of Appeal, R (Bridges) v. CC South Wales, [2020] EWCA Civ 1058, 11 August 2020.
40 See also Equinet (2020), Regulating for an equal AI: A new role for equality bodies, Brussels, report prepared by Allen R. and Masters D.
41 Tolan S., Miron M., Gomez E. and Castillo C (2019), ‘Why Machine Learning May Lead to Unfairness: Evidence from Risk Assessment for
Juvenile Justice in Catalonia’, Best Paper Award, International Conference on AI and Law, 2019; Richardson R., Schultz J. and Crawford K.
(2019), Dirty Data, Bad Predictions: How Civil Rights Violations Impact Police Data, Predictive Policing Systems, and Justice, 94 N.Y.U. L.
REV. ONLINE 192 (2019), available at SSRN.
42 FRA (2014), Violence against women: an EU-wide survey. Main results report, Luxembourg, Publications Office, p. 61.
43 FRA (2018), Second European Union Minorities and Discrimination Survey. Main results, Luxembourg, Publications Office, p. 66.
44 Erik Bakke (2018), “Predictive policing: The argument for public transparency”, New York University Annual Survey of American Law, Vol.
74, pp. 139-140; Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, Washington University Law Review, Vol. 94, pp. 1146-1150;
andCouncil of Europe Committee of experts on internet intermediaries (MSI-NET) (2017), Algorithms and Human Rights, Council of Europe
DGI(2017)12, p. 11.
45 Elizabeth E. Joh (2015), ‘The New Surveillance Discretion: Automated Suspicion, Big Data, and Policing’, UC Davis Legal Studies Research
Paper No. 473, p. 18.
46 Gstrein, O. J., Bunnik, A., and Zwitter, A. (2019), ‘Ethical, Legal and Social Challenges of Predictive Policing’, Católica Law Review 3:3, pp.
77-98; Albert Meijer & Martijn Wessels (2019), ‘Predictive Policing: Review of Benefits and Drawbacks’, International Journal of Public
Administration 42:12, p. 1036, DOI: 10.1080/01900692.2019.1575664.
47 Aleš Završnik (2019), ‘Algorithmic justice: Algorithms and big data in criminal justice settings’, European Journal of Criminology, pp. 8-9.
DOI: 10.1177/1477370819876762.
48 Wachter, Sandra (2020), ‘Affinity Profiling and Discrimination by Association in Online Behavioural Advertising’, Berkeley Technology Law
Journal, Vol. 35, No. 2, 2020, (forthcoming), available at SSRN.
49 On the use of AI in financial industries leading to unequal access to financial services, see in the legal literature e.g. Boyd, D., Levy K. &
Marwick, A. (2014), ‘The Networked Nature of Algorithmic Discrimination’ in Gangadharan, S. P., Eubanks, V. & Barocas, S. (eds), Data and
Discrimination: Collected Essays, Open Technology Institute, pp. 53-62.
50 For an overview of child rights issues, see UNICEF Innovation, Human Rights Center, UC Berkeley (2019), Artificial Intelligence and
Children’s Rights.
51 EU Network of Independent Experts on Fundamental Rights, Commentary on the Charter on Fundamental Rights of the European Union,
June 2006, p. 360. See also: FRA and CoE (2016), Handbook on European law relating to access to justice, Luxembourg, Publications
Office, June 2016, p. 92.
52 CJEU, C-432/05, Unibet (London) Ltd, Unibet (International) Ltd v. Justitiekanslern, 13 March 2007, para. 37; CJEU, C-93/12, ET
Agrokonsulting-04-Velko Stoyanov v. Izpalnitelen direktor na Darzhaven fond ‘Zemedelie’ – Razplashtatelna agentsia, 27 June 2013, para.
59; CJEU, C-562/13, Centre public d’action sociale d’Ottignies-Louvain-la-Neuve v. Moussa Abdida, 18 December 2014, para. 45.
53 Law Enforcement Directive, Art. 54; and GDPR, Art. 79.
54 Law Enforcement Directive, Art. 53; and GDPR, Art. 78.
55 Law Enforcement Directive, Art. 52; and GDPR, Art. 77.
56 Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States on the human rights impacts of
algorithmic systems (adopted by the Committee of Ministers on 8 April 2020 at the 1373rd meeting of the Ministers’ Deputies), Appendix,
para. B.4.5.
57 Andrew G. Ferguson (2017), ‘Policing Predictive Policing’, 94 Washington University Law Review, pp. 1165-1167.
58 Gstrein, O. J., Bunnik, A., & Zwitter, A. (2019), Ethical, Legal and Social Challenges of Predictive Policing’, Católica Law Review, 3:3, pp. 8081; Yeung K. (2019), A study of the implications of advanced digital technologies (including AI systems) for the concept of responsibility
within a human rights framework, Prepared by the Council of Europe Expert Committee on human rights dimensions of automated data
processing and different forms of artificial intelligence (MSI-AUT).
59 Council of Europe, Algorithms and human rights, pp.11 and 24.
60 International Technology Law Association (2019), ‘Responsible AI: A Global Policy Framework’, pp. 258-282.
61 The lack of expertise on AI is also reflected in the survey among companies in the EU, where the lack of skills among existing staff and
difficulties in hiring new staff are the most prominent obstacle for further AI adoption (European Commission (2020), European enterprise
survey on the use of technologies based on artificial intelligence, Luxembourg, July 2020, p. 11).
62 See a detailed assessment of the impact of predictive policing on the presumption of innocence in Mendola, Marco (2016), One Step
Further in the ‘Surveillance Society’: The Case of Predictive Policing.
63 See e.g. Egorov, A. and Wujczyk, M. (eds.) (2016), The Right to Social Security in the Constitutions of the World: Broadening the moral
and legal space for social justice, Geneva, ILO Global Study, Vol. 1: Europe, pp. xv-xvii and 1-6.
64 These include Arts. 153 and 156 of the TFEU; Arts. 12 and 13 of the 1961 European Social Charter; as well as points 2 and 10 of the 1989
Community Charter on the Fundamental Social Rights of Workers (see Explanations relating to the Charter of Fundamental Rights, OJ
C 303, 14.12.2007, pp. 17-35).
65 Explanations relating to the Charter of Fundamental Rights (OJ C 303, 14.12.2007, pp. 17-35), Explanation on Article 52 — Scope and
interpretation of rights and principles.
66 Łukasz Bojarski, Dieter Schindlauer and Katrin Wladasch (2014), The European Charter of Fundamental Rights as a Living Instrument –
Manual, Rome/Warsaw/Vienna, pp. 61-62.
67 De Becker, E. (2016), ‘The (Possible) Role of the Right to Social Security in the EU Economic Monitoring Process’, German Law Journal, Vol.
17, No. 3, pp. 297, 304; Paju, J. (2017), The European Union and Social Security Law, Oxford, Hart Publishing, sub-section 7.5.2.
68 Ibid., pp. 297-298; Peers, S. & Prechal, S. (2014), ‘Scope and Interpretation of Rights and Principles’, in Hervey, T., Kenner, J., Peers, S. and
Ward, A. (eds.), The EU Charter of Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1455, 1508.
69 With the exception of Poland and the United Kingdom, see Protocol (No. 30) on the application of the Charter of Fundamental Rights of
the European Union to Poland and to the United Kingdom (OJ C 115, 9.5.2008, pp. 313-314), Art. 1 (2).
70 Christiaan van Veen and Ben Zevenbergen, ‘Conference on Social Protection by Artificial Intelligence: Decoding Human Rights in a Digital
Age’, Freedom to Tinker – Research and Expert Commentary on Digital Technologies in Public Life, 29 May 2019.
71 Art. 51 (1) of the Charter; see also Explanations relating to the Charter of Fundamental Rights (OJ C 303, 14.12.2007, pp. 17-35),
Explanation on Article 52 — Scope and interpretation of rights and principles.
72 Łukasz Bojarski, Dieter Schindlauer and Katrin Wladasch (2014), The European Charter of Fundamental Rights as a Living Instrument –
Manual, Rome/Warsaw/Vienna, p. 67.
84

73 Sartor, Giovanni (2020), New aspects and challenges in consumer protection, study for the committee on the Internal Market and
Consumer Protection, Policy Department for Economic, Scientific and Quality of Life Policies, European Parliament, Luxembourg.
74 The European Consumer Organisation (BEUC) (2018), Digital Health, Principles and Recommendations.
75 BEUC (2020), Artificial Intelligence: what consumers say. Findings and policy recommendations of a multi-country survey on AI.
76 In recent case law, see CJEU, C-604/12, H. N. v. Minister for Justice, Equality and Law Reform, Ireland, Attorney General, 8 May 2014, para.
49.
77 Also confirmed by the CJEU, Joined Cases C-141/12 and C-372/12, YS v. Minister voor Immigratie, Integratie en Asiel, and Minister voor
Immigratie, Integratie en Asiel v. M, S, 17 July 2014, paras. 66-70).
78 These components, initially developed by the CJEU case law, have been codified in Article 41 (2) of the Charter. For more on this right in
leading academic literature, see Craig, P. (2014), ‘Article 41 – Right to Good Administration’, in Hervey, T., Kenner, J., Peers, S. and Ward, A.
(eds.), The EU Charter of Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1069-1098.
79 Ibid., p. 1082.
80 Finck, M. (2019), ‘Automated Decision-Making and Administrative Law’, Max Planck Institute for Innovation & Competition Research
Paper No. 19-10, p. 8.
81 Craig, P. (2014), ‘Article 41 – Right to Good Administration’, in Hervey, T., Kenner, J., Peers, S. and Ward, A. (eds.), The EU Charter of
Fundamental Rights. A Commentary, Oxford and Portland, Oregon; Hart Publishing, 2014, pp. 1086-1087.
82 France, Code des relations entre le public et l’administration, Article L2111-5.
83 Panoptykon Foundation (2015), Profiling the unemployed in Poland: Social and political implications of algorithmic decision making; see
also Algorithm Watch (2019), Poland to scrap controversial unemployment scoring system.
84 See Decision K 53/16, available on the Constitutional Tribunal’s website.

85

86

5

FUNDAMENTAL RIGHTS IMPACT
ASSESSMENT – A PRACTICAL TOOL
FOR PROTECTING FUNDAMENTAL
RIGHTS
Chapter 4 illustrated the extent to which using AI affects different fundamental
rights. This chapter analyses how fundamental rights impact assessments
(FRIA) could reduce the negative impacts that using AI can have on
fundamental rights.
Section 5.1 provides a brief overview of the current discussion on the need
for fundamental rights impact assessments in this field. Section 5.2 analyses
current practices in addressing fundamental rights implications, based on the
interviews conducted for this report. Interviewees were asked about what
sort of testing was done before the system was used, and who controls the
tasks affected by the use of the technology.
The chapter ends with suggestions on how to assess the fundamental rights
impact when using AI and related technologies.

5.1.	 CALLING FOR A FUNDAMENTAL RIGHTS IMPACT
ASSESSMENT – AVAILABLE GUIDANCE AND TOOLS
International organisations,1 academics2 and civil society3 have called for
fundamental rights impact assessments to be conducted when using AI or
related technologies.
For example, the Committee of Ministers of the Council of Europe’s guidelines
on addressing the human rights impacts of algorithmic systems recommend
that states should conduct “impact assessments prior to public procurement,
during development, at regular milestones, and throughout their contextspecific deployment in order to identify the risks of rights-adverse outcomes”.4
There is a need for flexible impact assessments that can adapt to different
situations given that fundamental rights violations are always contextual.
Scholars exemplify this based on EU anti-discrimination law, where equality
is always contextual and depends on the case at hand.5
Fundamental rights compliance cannot be automated and hard-coded into
computer software. Rather, each use case needs separate examination
to determine whether any fundamental rights issue arises. Nevertheless,
assessments can follow a systematic approach and provide similar information.
Existing standards provide guidance on how to do a fundamental rights impact
assessment of AI and related technology. These include hard law, soft law
87

instruments (such as recommendations or declarations), and practical tools
(e.g. guidelines and checklists).
Beyond the requirements flowing from data protection legislation (see box),
there are few examples of laws requiring mandatory assessments of the
effects of AI in general. In view of the increasing uptake of AI, the Canadian
government has issued guidelines, including mandatory requirements for
assessing AI for use by public administration. It applies to any system, tool,
or statistical model used to recommend or make an administrative decision
about a client.6

88

Learning
from data
protection
impact
assessments

European data protection law requires a data protection impact assessment (DPIA).a The
CoE Modernised Convention No. 108 provides for a general obligation to examine the likely
impact of data processing on individuals’ rights and fundamental freedoms before their use.
Following the assessment, controllers should design the processing in such a manner to
prevent or minimise identified risks.b
EU law imposes a similar, more detailed, obligation. The GDPR foresees a Data Protection
Impact Assessment (DPIA) for data processing that is likely “to result in a high risk to the
rights and freedoms of natural persons.”c Therefore, where required by law, a DPIA for an
AI technology could potentially also address the broader fundamental rights implications,
besides the impact on the right to privacy,d and be used as a tool to further investigate
algorithms and their impacts.e
However, under the GDPR (Article 35), the DPIA is limited to ‘high risk’ cases processing
personal data. It therefore may miss other high risk cases that are not primarily or obviously
related to protection of personal data. At the same time, the GDPR is delimited to its specific
field of application, with accompanying expertise in this field. This means that the potential
extension of the scope of a DPIA to other fundamental rights might be limited.
The GDPR also gives some indications about the modalities to undertake a DPIA. First,
a DPIA should be conducted before any high risk processing.f Second, a DPIA should provide
for a systematic description of envisaged operations, the purpose and the legitimate
interests pursued. It must also assess the necessity and proportionality of the processing
and the possible risks to the rights of individuals. In addition, it must contain the planned
security measures to address the risks identified.g
While pointing out that different methodologies can apply, the Article 29 Working Party (WP
29) proposes – in a check list form – minimum criteria that a controller should use to assess
if the DPIA comprehensively complies with the GDPR.h
Finally, the GDPR foresees prior mandatory consultation of the relevant supervisory
authority, if the impact assessment indicates that processing presents risks that cannot be
mitigated. i This gives a crucial role to DPAs, as independent bodies established by law.j
The European Data Protection Supervisor (EDPS) provides guidance on carrying out DPIAs.k
Data protection authorities have also discussed, and provide guidance on, how to assess AI
technologies.l
For more information on Data Protection Impact Assessment, see: FRA, Council of Europe and
EDPS (2018), Handbook on European data protection law. 2018 edition, p. 179-181.
a

b

Council of Europe Modernised Convention No. 108, Art. 10 (2).

c

GDPR, Art. 35 (1).

GDPR, Recitals (2) and (75); Article 29 Working Party, Guidelines on Data Protection Impact
Assessment (DPIA), wp248rev.01, 13 October 2017.
d

Edwards and Veale (2018); FRA (2018), #BigData: Discrimination in data-supported decision
making, Luxembourg, Publications Office, June 2018.
e

GDPR, Art. 35 (1). The WP29 specifies that ‘carrying out a DPIA is a continual process, not a onetime exercise.’
f

g

GDPR, Art. 35 (7), as well as recitals (84) and (90).

Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13
October 2017, Annex 2.

h

i

GDPR, Art. 36.

j

GDPR, Art. 35.

EDPS (2019), Accountability on the ground Part II: Data Protection Impact Assessments & Prior
Consultation, v.1.3, July 2019.
k

See, for example, the Declaration on ethics and data protection in AI, adopted by the 40th
International Conference of Data Protection and Privacy Commissioners (ICDPPC), in 2018.
l

89

There are many more examples of non-binding guidelines. At the global
level, the United Nations Guiding Principles on Business and Human Rights
recommend that enterprises integrate the findings from human rights impact
assessments across relevant internal functions and processes, and take
appropriate action.7 Although they do not refer specifically to AI, the guidelines
are relevant in supporting the development of AI technology in a rights
compliant manner.8
At the EU level, the Ethics Guidelines for Trustworthy AI prepared by the
European Commission’s High-Level Group on Artificial Intelligence9 also
recommend performing a FRIA, before a system’s development, when
“there are risks that fundamental rights can negatively be affected by the
technology”.10 They also emphasise the need to put in place mechanisms
to receive external feedback on AI systems that potentially infringe on
fundamental rights.
In addition, private companies, 11 associations of private companies12 or of both
public and private interests,13 as well as NGOs14 and other organisations15 have
developed different types of guidance to support AI impact assessments. These
documents do not usually contain clear guidelines on impact assessment.
Instead, they highlight the different aspects and criteria that should be taken
into account when developing and carrying out an impact assessment.
Broad categories include the purpose of the system, the description of the
technology, the assessment of the impact and targeted population/individual,
evaluating fairness and diversity, the description of the audits planned or
performed, as well as accountability. Some explicitly refer to applicable
international human rights law standards.16
Various codes of ethics or conducts,17 standards,18 as well as certification
schemes are also in place.19
Several practical tools are available to assess the impact of AI technologies and
mitigate risks, developed by a wide range of actors. These include checklists,20
lists of questions,21 online self-evaluation tools,22 and risk management
frameworks.23
Some focus specifically on assessing fundamental rights risks.24 Others focus
on ethical, societal or economic implications.25 These can be useful references
when performing a thorough fundamental rights impact assessment of AI
technologies.
In July 2020, for example, the High-Level Group on Artificial Intelligence
issued an “Assessment List for Trustworthy AI” (ALTAI),26 after a six month
pilot involving more than 350 stakeholders. ALTAI helps organisations
self-evaluate – on a voluntary basis – the reliability and trustworthiness of
AI and reduce potential risks for users. It supports businesses and public
administrations to ask the right questions around the seven requirements
for responsible AI identified in the Ethics Guidelines for Trustworthy AI.27
ALTAI specifically refers to the need to perform a fundamental rights impact
assessment. It includes examples of questions to assess impact on nondiscrimination/equality; the right to privacy; the rights of the child; the
freedom of expression; as well as the freedom of information and association.28
Several online assessment tools target the use of AI by public authorities.
The Canadian Government developed an Algorithmic Impact Assessment tool
(AIA),29 pursuant to the Canadian Directive on Automated Decision-Making.30
The AIA represents an automated assessment consisting of more than 50
questions that unfold the requirements of the directive. Questions relate to
90

fundamental rights concerns – such as an AI system’s impact on the freedom
of movement, on the likelihood of incarceration of an individual, on the legal
status, on access to funding or benefits, or on indigenous people. A score
is attributed to each reply and a final impact scoring is provided and made
publically available on the government’s website.
As another example, the Ethics Toolkit31 is a freely accessible tool designed
for local governments. Based on a risk management approach, it supports fair
automated decisions and minimising unintentional harm to individuals in the
field of the criminal justice, higher-education, social media and other areas.
Among national human rights bodies, the Danish Institute for Human
Rights proposed a human rights compliance “quick check.32 This involves
an interactive online computer programme that allows companies to select
and modify the information in a database to suit their type of business and
area of operations to check rights compliance. The quick check is based on
the Human Rights Compliance Assessment tool,33 which runs on a database
of over 350 questions and 1,000 corresponding human rights indicators. It
uses international human rights law standards as benchmarks. Applying to
all fields of operations, it can provide guidance when developing impact
assessment for AI technology.
Academic work has also suggested operational frameworks for assessing
risks in using AI technology. Some focus specifically on identifying and
addressing fundamental rights implications by the private sector.34 Some
focus on developing ethical and values-oriented models (analysing the
societal impact of the data used) with the creation of ad hoc expert (review)
committee.35
Others have developed guidance frameworks for specific case studies.
For example, in the field of criminal justice, the ALGO CARE framework36
introduced a step-by-step assessment to evaluate the key legal and practical
concerns that should be considered in relation to police using algorithmic
risk assessment tools.
Some have argued for participatory ways to involve and consider the views
of the affected rights-holders and other stakeholders communities when
developing an impact assessment and publically engage with them from
the start.37 Others have joined cross-discipline expertise of science and law
to design practical frameworks.38

5.2.	 IMPACT ASSESSMENTS AND TESTING IN PRACTICE
Virtually all the systems discussed in the interviews were subject to some
sort of testing, which included elements of impact assessment. However,
these were mainly technical and data protection (impact) assessments. These
rarely address potential impacts on other fundamental rights.

91

Some interviewees argue against conducting a fundamental rights impact
assessment, because in their view the system does not negatively affect
fundamental rights or because they are unsure about it. For example,
a respondent working on traffic management, using cameras for monitoring
traffic, indicated that they only tested for accuracy of the system, but not
fundamental rights, apart from respecting data protection rules.
Some respondents simply did not know if fundamental rights were assessed
as part of a general impact assessment that was carried out.

Testing and stages of development

Much testing is done before any new AI system is used. As respondents
highlighted, moving an AI system into production is a very challenging task.
As mentioned, public administration as well as private companies are usually
careful when using AI. Many projects that interviewees refer to are still in
development or in the pilot phase, and some had not started concrete testing.
Testing can be done in several stages. These include the development stage
(so-called proof-of-concept), pilot stages before deployment, and tests during
and after deployment. If possible, live experimentation is carried out at the
initial stages, which often involves staged deployment.
For example, the organisation interviewed that tests different applications
to support job seekers conducts continuous, step-by-step testing. Selected
members of the organisation test the tool in real situations, using check lists.
The interviewee mentioned that it is challenging to move to the deployment
stage and it is planned to supervise the tool in real time.
In another example, involving automated rule-based granting of social
benefits, different assessments were carried out. Before implementation,
a group of lawyers, data protection specialists, compensation specialists
and accountants performed a general impact assessment. After this, the
department responsible for using the system conducted tests to decide
whether the system could be used.
Following this, the system was monitored in its implementation, using a stepby-step approach. In a first step, about half of the decisions were taken by
the system. In a next step, the decisions taken automatically were expanded
to all negative decisions. After this, another area of decisions was added,
92

“When testing the system, we did
not really look at the legal aspects,
we looked at whether the system is
profitable.”
(Private company, Estonia)

including all decisions on ending compensation payments. At the time the
interviews were conducted, about 95 % of decisions were automated. The
interviewee indicated that, after carrying out these tests, they feel sure that
the system is secure, and that there are no outstanding risks.
A company working on a fraud detection system replaced their rule-based
system with a machine learning tool. Before changing the system, the old
and new system were run in parallel to see if the machine learning system
performs better than the rule-based one. The interviewee mentioned that
“[there] was rigorous analysis behind it and direct feedback where we saw
what would be the impact on losses versus how many good customers we
were impacting negatively”. The interviewee added that, when they “were
comfortable that [the machine learning system] was better [than the static
rule system] in all aspects, we deployed it in its entirety”.
In other use cases, no previous automated system existed and tests were
reviewed by humans. For example, an automated transcription service was
tested during court hearings, when allowed by the judge. This included
regular feedback on the correctness of the transcription services from judges.
One interviewee from law enforcement, working on a tool to detect domestic
violence, identifies issues with precision and accuracy when using the system.
If a police officer does not have sufficient training and knowledge about the
system, the indicators required by the system cannot gather the required
information, which could lead to miscalculation. They highlight that the
robustness of the system is tested annually to assure the quality of the two
questionnaires used, the completeness of the data, and the training of the
police officers using the AI system. This process also considers how personal
data protection laws and protocols are applied. The tests discussed focus
strongly on technical aspects and general operations.

Fundamental rights and data protection impact assessments

Apart from data protection, which all respondents mentioned, other
fundamental rights were typically not considered. Respondents only reflected
about other potential impacts on fundamental rights, or mentioned that these
aspects were considered, when prompted by the interviewer.
Many respondents are generally aware of discrimination issues – but often
discussed this only after being explicitly asked about discrimination. Yet they
gave no information about any formal, in-depth tests for discrimination.
Generally, respondents ruled out the possibility that their system discriminates
based on protected attributes. For example, one interviewee states that they
test the system against data protection laws and specific applicable legal
acts, but not fundamental rights. However, the interviewee did consider
potential discrimination, but ruled it out. It needs to be kept in mind for future
technologies, the interviewee stated.

“Yes, we assess the legality of
personal data protection and the
conformity with their specific legal
acts.”
(Public administration, Estonia)

However, there are cases where non-discrimination was generally considered
during the testing phase of AI systems. One respondent from a municipal
authority mentioned that they cannot assess the fairness of a model, because
they cannot access data needed due to data protection reasons. According
to the interviewee, “there is a huge tension surrounding the GDPR. So we
want to do well, but might in fact be worse off, because interpretation of
the data then turns out to be impossible”.
Most respondents reported that a data protection impact assessment, as
required by law, was conducted, although these took different forms. A bank
tested a tool for analysing speech from customer calls to find out about
reoccurring problems, and carried out a data protection impact assessment
93

(DPIA) specifically for testing the tool. The outcome was that the system
can be tested if data were only used for the testing phase and are deleted
after a certain period after the test, and if access to the data by employees
is restricted to the testing phase and supervised. For the deployment of the
tool, another DPIA is required in this case.
There is sometimes a lack of clarity as to what extent the use of AI and
related technologies, most notably the use of algorithms, belongs to an DPIA.
In the area of predictive policing, for instance, some DPIAs were done for
the underlying architecture of the system, rather than the specific AI tool.
Another interviewee using algorithms in financial services also mentioned
not assessing the machine learning tool as such within the framework of
a DPIA, because of the belief that it does not apply to the machine learning
system (but the underlying data).
One interviewee felt that the data protection impact assessment for the crime
heat map example was not sufficiently in depth to safeguard the quality of
the model, and that the system was not equipped to deal with cross-sectoral
use of data, where different rules might apply. They indicated that further
standards were required.
A respondent working on migration management indicated having data
protection officers involved in their analysis. The legal service has a specialised
quality control AI tool to study the data protection aspects of their system.
However, the respondent also mentioned that more guidance is needed.
The companies working on targeted advertising all looked into data protection
issues, although not all respondents were sure if an impact assessment was
conducted. The companies assessed, for example, whether only people who
consented are approached in targeted communication. For targeted ads,
they assessed whether information on possible re-identification is deleted,
including whether cookies and trackers are anonymised.
With respect to DPIAs generally, some respondents did not know, as this
was not their area of responsibility. Others knew they had a positive DPIA,
but were not aware of any details. It appears that the legal assessment
is sometimes detached from the technical side, with technical people not
knowing about legal assessments. One interviewee from a private company
working on credit risk scoring mentioned: “I make suggestions how some
system could be developed and then the compliance manager tells me if it
is in conformity with the laws”.

Audits and working with external (oversight) bodies

The public administrations and private companies involved in FRA’s research
all carry out tests before deploying any AI. These are often linked to existing
internal and external oversight processes. The use of AI is frequently subjected
to internal review processes within companies and public administration,
although these are not necessarily formalised review processes. Some
interviewees mentioned that they are working on formalising existing internal
review processes for overseeing AI systems.

94

Interviewees from the public sector say
that they have to be particularly cautious
before using any AI to support decisions.
A representative working on migration
management at a public administration
indicates that “[i]n the private sector, [wrong
results] might cause business-related losses,
in the police it impacts people’s lives and their
fundamental rights”.
Yet it is not always clear to public administration,
or to businesses, who is responsible for
checking and overseeing the use of AI. Public
administrations appear to be under stronger
scrutiny when it comes to oversight of their AI
systems. Such oversight is often done through
regular audits, for example connected to
budgetary review.
Some interviewees, from public and private organisations, report that their
AI systems are currently checked in the framework of an existing IT review
(e.g. regular database checks), in the absence of review processes that
specifically look into the use of AI. In addition, interviewees report about
sector-specific certification schemes that also look into the use of AI – for
example, in the area of health or financial services.
Several interviewees mentioned that they were in contact with data protection
authorities. Some companies and public administrations sought permission
from the data protection authorities before using their AI system or at least
were generally in contact with them. For example, one company working on
targeted advertising mentioned discussing their use of personal data with
the national data protection authority.
Experts interviewed for this report further highlighted the relevance of data
protection authorities for overseeing AI systems with respect to the use of
personal data. However, experts strongly highlighted that data protection
authorities are under-resourced for this task for two reasons. Data protection
authorities often do not have relevant AI-related expertise.39 Additionally,
their budgets are overstretched and their workload heavy.
Experts’ views differ with respect to the need of additional oversight bodies,
and the potential creation of an AI specific institution. However, they agree that
existing bodies all have to work on topics linked to AI within their mandates.
Equality bodies, as well as other human rights institutions, are mentioned
by some interviewed experts as providing oversight concerning possible
discrimination when using AI. They highlighted that these institutions need
to build up expertise in this area to better contribute to the oversight of AI.
However, similar to data protection authorities, this is a challenging task for
equality bodies given their lack of resources.
“We are proactive not only among
ourselves to mitigate risks, but we
also get additional audits. We also
see sometimes that some regulatory
audits are quite sloppy. For us that
is not good because we have lots of
customer data.”
(Private company, Estonia)

Several interviewees mentioned consumer protection authorities as potentially
providing relevant oversight on the use of AI. One respondent, working for
a retail company, would like to have an advisory agency that could be consulted
about possible use of AI for innovation without being investigated right away.
At the moment, the company prefers to consult consumer authorities over
data protection authorities about potential future marketing campaigns.
This is because data protection authorities might start an investigation into
their efforts.
95

When discussing oversight, those developing and using AI, as well as experts,
repeatedly mention the challenge to really understand the impact when using
AI. Despite the need to engage existing oversight bodies, responsibilities to
oversee the use of AI from a fundamental rights perspective remain unclear.

5.3.	 FUNDAMENTAL RIGHTS IMPACT ASSESSMENT IN
PRACTICE
Many key actors in the field of fundamental rights have called for conducting
fundamental rights impact assessments before using any AI-driven systems.
This section highlights some of the elements that could be incorporated into
such an assessment.

Fundamental rights impact assessments are needed given that a contextualised
assessment is required. This is because uses of AI vary considerably in terms of
complexity, level of automation, potential errors and harm, scale of application,
as well as area of use. The more complex an AI system is, the more difficult
it is to assess its potential impact.
While the fundamental rights implicated will vary depending on the area of
application, the full spectrum of rights needs to be considered for each use
of AI. However, uses of AI are likely to involve some of the rights most often
affected by AI systems. The discussion in the preceding chapter makes clear
that issues linked to data protection, non-discrimination, as well as access to
effective remedies and a fair trial, are relevant for all uses of AI.
Thus, the following horizontal points could be a basic starting point for
considering the impact of AI on selected rights:
――The legal processing of data needs to be confirmed in line with data
protection laws.
If personal data are used, the full data protection framework applies. This
ensures that processing is legal and does not violate a person’s rights to
respect for a private and family life, and data protection.
――The processing should not lead to unfair treatment or discrimination of
protected groups.
Assessing non-discrimination needs to be at the core of assessing AI. Even
apparently miniscule differences can scale up and create risks contravening
the principle of non-discrimination. The disadvantage to people depends
on the nature (kind of harm), severity (strength of harm) and significance
96

(how many people are put at a disadvantage compared to another group of
people). Statistical assessments on group differences are an important tool
to assess unfair and discriminatory uses of AI.40
――People subjected to AI and related technologies should be able to complain
and receive effective remedies.
There should be accessible ways for people to complain about potential
decisions being made and to effectively access remedies. This includes
availability of information that allows the explanation of decisions.
In addition, other relevant rights in the Charter apply. Public administrations
using AI need to consider good administration principles. Businesses have
to take consumer protection into account.
Other rights are relevant depending on the area of application. Some examples
include:
――the right to social protection, when working with social benefits;
――the right to freedom of expression and information, when using AI to
support online content moderation;
――the right of assembly and of association, when considering the use of
facial recognition technology in public spaces;
――the right to education, when using AI in the education sector;
――the right to asylum, when using AI to support migration management;
――the right of collective bargaining and action, when using AI in the ‘gigeconomy’;
――the right to fair and just working conditions, when using AI at the workplace;
――the right to access preventive health care, when using AI in health services;
――and the right to the presumption of innocence and the right to defence,
when using AI in the justice sector or for law-enforcement purposes.

Information needed to assess the potential impact on fundamental
rights before implementing AI

Given the variety of tools, purposes and area of application, assessments
are contextual. To be able to meaningfully respond to the horizontal points
raised above, and to assess specific rights linked to different use cases, at
least the following information needs to be available:
――A description of the purpose and context of the system, as well as the
legal basis.
――A description of possible harm of using the system, including questions
around false positives, false negatives, and other possible harm due to
the automation and scale of use.
――A description of the technology used. This includes information on the
data used for building the system and its legal basis for processing.
A description of relevant information to include is provided in FRA’s paper
on data quality and AI.41
――An evidence-based description of the accuracy of the AI system in terms
of outcomes based on training data and possible tests and experiments in
real life situations, if appropriate. Here, false positives and false negatives
should be considered separately. These should include breakdowns for
as many groups as possible to allow for checking potential discrimination
(e.g. differences in the accuracy between women and men).
――Where already available, the provision of information about compliance
with existing standards and potential certifications obtained.

Ex-post assessments and safeguards

Lastly, envisaging ex-post safeguards further contributes to the fundamental
rights compliant use of AI. These could include:
97

――Regular repetition of assessments after deployment, where appropriate.
This is important to learn about potential feedback loops and in case
rules are updated. This also requires recording information on the use
and outcomes of the system to the extent data protection is respected.
――Making people subjected to AI systems aware that they are subjected
to this technology, as they can otherwise not challenge any decision
affecting them.
――Making available easily accessible channels for effectively complaining
about decisions made based on the AI system.

Engaging external experts, stakeholders and oversight bodies

The above information could be the basis for consultation with different
stakeholders and experts before a particular AI system is used. Depending on
the nature of the application and its legal basis, a consultation with relevant
stakeholders would ensure that no potential harm has been omitted and
different perspectives are brought into the assessment. Stakeholders could
include civil society; different public and private organisations; as well as
experts from different fields of fundamental rights, including data protection.
As the ten experts interviewed for this report highlighted, existing oversight
bodies are also responsible for AI oversight within their mandates. Sectorspecific bodies and certification schemes are doing this to some extent, the
interviews suggest – for example, in health care and financial oversight.
To monitor, comprehend and effectively respond to the potential impacts
of AI on a wide spectrum of fundamental rights, data protection authorities,
equality bodies, ombuds institutions and national human rights institutions
could play an important role, providing input and oversight from their various
points of expertise. However, as interviews indicated, extensive upskilling
and resource allocation is needed to underpin this.

98

Endnotes
1
2

3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19

20
21
22
23
24
25
26
27
28
29
30
31
32
33
34

Council of Europe, Commissioner for Human Rights (2019), Unboxing Artificial Intelligence: 10 steps to protect Human Rights –
Recommendation, Council of Europe, Strasbourg, May 2019.
Heleen L Janssen (2020), ‘An approach for a fundamental rights impact assessment to automated decision-making, International
Data Privacy Law’, International Data Privacy Law,Vol. 10, Issue 1, February 2020, pp. 76-106; Alessandro Mantelero, ‘AI and Big Data:
A blueprint for a human rights, social and ethical impact assessment’, Computer Law & Security Review Vol. 34, Issue 4, August 2018, pp.
754-772; Edwards, Lilian and Veale, Michael (2017), Slave to the Algorithm? Why a ‘Right to an Explanation’ Is Probably Not the Remedy
You Are Looking For (May 23, 2017), 16 Duke Law & Technology Review 18.
AccessNow (2020), Access Now’s submission to the Consultation on the “White Paper on Artificial Intelligence - a European approach
to excellence and trust”.
Council of Europe, Recommendation CM/Rec(2020)1 of the Committee of Ministers to member States
on the human rights impacts of algorithmic systems, 8 April 2020, para 5.2. (human rights impact assessment).
For a detailed discussion with respect to non-discrimination, see: Wachter S., Mittelstatt, B., and Russel C. (2020), Why fairness cannot be
automated: bridging the gap between EU non-discrimination law and AI.
Government of Canada, (2019), Directive on Automated Decision-Making.
United Nations, UN Guiding principles on Business and Human Rights, endorsed by Human Rights Council Resolution 17/4, A/HRC/
RES/17/4, 6 July 2011, Principles 18, 19, 20.
Heleen L Janssen, An approach for a fundamental rights impact assessment to automated decision-making, International Data Privacy
Law, Vol. 10, Issue 1, February 2020, pp. 76-106.
High-Level Expert Group on Artificial Intelligence, Ethics Guidelines for Trustworthy AI, 8 April 2019, Chapter III.
Ibid, p. 15.
See for example: IBM, Everyday Ethics for Artificial Intelligence, 2019; Sony, Sony Group AI Ethics Guidelines, 2019; Vodaphone,
Vodaphone’s AI framework, 2019; Arborus International and Orange, International Charter for Inclusive AI, 21 April 2020, signed by more
than 40 private companies, including Camfil, Danone, EDF, L’Oréal, Metro, Sodexo, etc.
Information Technology Industry Council (ITI), ITI AI Policy Principles, 2017.
ECP Platform for the Information Society, Artificial Intelligence Impact Assessment, The Netherlands, 14 November 2019.
Amnesty International, Access Now, Human Rights Watch, Wikimedia Foundation, The Toronto Declaration: Protecting the rights to
equality and non-discrimination in machine learning systems, 16 May 2018 at RightsCon Toronto; University of Montreal, Montreal
Declaration Responsible AI, 2018.
Electrical and Electronics Engineers (IEEE), Global Initiative on Ethics of Autonomous and Intelligent Systems, Ethically Aligned Design:
Prioritizing Human Wellbeing with Autonomous and Intelligent Systems, 2019; Future of Life Institute, Asilomar AI Principles, Conference
outcome of the Future of Life Institute’s second conference on the future of artificial intelligence, 2017.
See for example: ECP Platform for the Information Society, Artificial Intelligence Impact Assessment, The Netherlands, 14 November
2019; IEEE Initiative.
Association for Computer Machinery (ACM), ACM Code of Ethics and Professional Conduct, 22 June 2018.
Future of Humanity Institute, University of Oxford, Standards for AI Governance: International Standards to Enable Global Coordination in
AI Research & Development, April 2019.
ISO, Standards by iso/iec jtc 1/sc 42, Artificial intelligence,Sstandard and/or project under the direct responsibility of iso/iec jtc 1/sc
42 secretariat, ISO, ISO/IEC TR 24028:2020 standard Information technology — Artificial intelligence — Overview of trustworthiness in
artificial intelligence, May 2020. It establishes, among others, the “approaches to assess and achieve availability, resiliency, reliability,
accuracy, safety, security and privacy of AI systems.” Other ISO standards under development as of September 2020: ISO/IEC CD
23894 Information Technology — Artificial Intelligence — Risk Management, ISO/IEC AWI TR 24027 Information technology — Artificial
Intelligence (AI) — Bias in AI systems and AI aided decision making, or ISO/IEC AWI TR 24368 Information technology — Artificial
intelligence — Overview of ethical and societal concerns, more information available on ISO’s website; Electrical and Electronics Engineers
(IEEE), IEEE P7003™ Algorithmic Bias Considerations; German AI Federal Association (KI Bundesverband), German AI Federal Association:
seal of quality (KI Bundesverband Guetesiegel), 22 March 2019.
Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13 October 2017, Annex 2 – Criteria for
an acceptable DPIA.
High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17
July 2020.
Government of Canada, Algorithmic Impact Assessment Tool, 2019; Danish Institute for Human Rights, Human rights compliance
assessment quick check, 7 June 2016.
Center of Government Excellence, Johns Hopkins University, Ethics & Algorithm toolkit, 2018; Government of Canada, Algorithmic Impact
Assessment Tool, 2019.
Article 29 Working Party, Guidelines on Data Protection Impact Assessment (DPIA), wp248rev.01, 13 October 2017, Annex 2 (data
protection focus); Danish Institute for Human Rights, Human Rights Impact Assessment Guidance and Toolbox, 2016; AI Pulse, Creating
a Tool to Reproducibly Estimate the Ethical Impact of Artificial Intelligence, 26 September 2019.
Fairness, Accountability, and Transparency in Machine Learning (FAT/ML), Principles for Accountable Algorithms and a Social Impact
Statement for Algorithms, 2019; FAT/ML, Social Impact Statement for Algorithms, 2019.
High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17
July 2020.
See e.g. human agency and oversight; technical robustness and safety; privacy and data governance; transparency; diversity, nondiscrimination and fairness; societal and environmental well-being; accountability. High-Level Expert Group on Artificial Intelligence, Ethics
Guidelines for Trustworthy AI, 8 April 2019.
High-Level Expert Group on Artificial Intelligence, Assessment List for Trustworthy Artificial Intelligence (ALTAI) for self-assessment, 17
July 2020, p.5.
Government of Canada, Algorithmic Impact Assessment tool, 2019.
Government of Canada, Directive on Automated Decision-Making, 2019, Article 6 an Appendix C.
Center of Government Excellence, Johns Hopkins University, Ethics & Algorithm toolkit, 2018.
Danish Institute for Human Rights, Human rights compliance assessment quick check, 7 June 2016.
Danish Institute for Human Rights, Human Rights Impact Assessment Guidance and Toolbox, 2016.
Heleen L Janssen, An approach for a fundamental rights impact assessment to automated decision-making, International Data Privacy
99

Law, International Data Privacy Law,Vol. 10, Issue 1, February 2020.
35 Alessandro Mantelero, AI and Big Data: A blueprint for a human rights, social and ethical impact assessment, Computer Law & Security
Review, Vol. 34, Issue 4, August 2018, pp. 754-772; AI Pulse - Program on Understanding Law, Science, and Evidence (PULSE), UCLA School
of Law, Creating a Tool to Reproducibly Estimate the Ethical Impact of Artificial Intelligence, 26 September 2019. This model includes
a series of questions for Assessing the Human Rights Impact of AI-Enabled Projects.
36 Marion Oswald, Jamie Grace, Sheena Urwin & Geoffrey C. Barnes, Algorithmic risk assessment policing models: lessons from the Durham
HART model and ‘Experimental’ proportionality, Journal, Vol. 27, 2018 – Issue 2.
37 AINOW, Algorithmic Impact Assessments: a Practical Framework for Public Agency Accountability, April 2018.
38 The Institute for Ethical AI & Machine Learning (Ethical ML Network (BETA)), The Machine Learning Maturity Model, 2019.
39 Brave (2020), Europe’s governments are failing the GDPR.
40 See Wachter S., Mittelstatt, B., and Russel C. (2020), Why fairness cannot be automated: bridging the gap between EU nondiscrimination law and AI.
41 FRA (2019), Data quality and artificial intelligence – mitigating bias and error to protect fundamental rights, Luxembourg, Publications
Office, June 2019.

100

6

MOVING FORWARD: CHALLENGES
AND OPPORTUNITIES
This report is published amidst ongoing European legislative and policy
developments on artificial intelligence and the global fight against the
coronavirus. The Covid-19 pandemic has potentially quickened acceptance
of innovative technologies. Yet it has also shown that AI is not the panacea
to all problems, and comes with various challenges.
This report clearly shows that using AI systems engages a wide range
of fundamental rights. It also shows that many businesses and public
administrations are already using or planning to use AI and related technologies.
However, these technologies involve different levels of complexity. Most
examples use relatively simple algorithms. The level of automation also
varies. Most – but not all – decision making is subject to human review.
The applications currently used are also often only in the development
stage. EU and national legislators and policymakers should keep this reality
in mind – especially when presented with optimistic expectations of AI’s
potential vis-à-vis the challenges related to using new technologies and the
need to regulate them.
“We try to look into the future. We
will automate more and more.”
(Private company, Estonia)

“The next steps are related to
transparency and open data: that is
to say, publish not only information
in pdf, but also information in
reusable formatting so that it could
be reused internally and by the
private sector.”
(Public administration, Spain)

“AI is a great thing but we must
learn to use it.”
(Private company, Spain)

The vast majority of public administrations and businesses interviewed plan
to keep working on or using AI. Only two interviewees indicated that they
will not further use or develop AI. Another two interviewees are cautious.
They plan to wait and see what others are doing, including because of a lack
of resources for further work on using AI.
However, most said that they will further develop or continue to test tools
and (data) infrastructure with respect to the use of AI. This includes starting
new or continuing ongoing pilots, evaluating existing efforts, sharing data
and results with others, increasing data quality, or trying to obtain other
data sources.
Some interviewees mentioned that they are engaged in ongoing debates and
expressed the desire to contribute to the further development of legislation.
They still see the current situation – the absence of harmonised law in the
area – as an obstacle to the further use of AI. In addition, some respondents
said they are working on issues linked to the interpretability of AI. This
means that they are working on methods that enhance understanding and
explanation of decisions based on more complex AI. Some indicated a desire
to look more closely into ethical and legal matters.
Figure 7 shows correlations of words interviewees often use when talking
about their future use of AI. The figure indicates topics that are often raised.
For example, interviewees often used the term ‘data’ when discussing future
developments.
101

FIGURE 7:

CORRELATIONS OF WORDS RESPONDENTS MOST OFTEN
MENTION WHEN DISCUSSING FUTURE PLANS TO USE AI
technologies

systems
data

plans
development

company

ai

tool
management

level

solutions

companies
project

system

translation
related

analysis

processing

process

technology

administration

tax

service

organisation

lot

customer
easy

information
quality
business

national

phase

payment
services

application

learning

machine
potential

improve
energy

human
algorithms

support

decisions
ml

continue
people

future

Notes:	Based on text from interview summaries, when respondents spoke about
their future use of AI, including words mentioned at least ten times. The
lines connecting words indicate the strength of word correlations within
text passages. The size of the dots indicate the frequency of the words
used.

Source: FRA, 2020

Effectively and adequately protecting fundamental rights in the EU is a key
objective of the current efforts to better regulate the use of AI. In the context
of upcoming EU legislation on AI, the European Commission’s White Paper
addresses current gaps, helping to mitigate the uncertainty around the use
of AI with respect to fundamental rights, and making the use of AI more
transparent and accountable in terms of fundamental rights. It includes
requirements for AI use that directly link to the information needed to assess
the impact of AI on fundamental rights, as discussed above.
Requirements linked to the description of training data, data and record
keeping, information to be provided to those subjected to AI, robustness and
accuracy, as well as human oversight are all highly relevant when assessing
and protecting fundamental rights. In this respect, the body of evidence
presented in this report offers general insights into how different technologies
can affect fundamental rights and what safeguards are needed to ensure
fully fundamental rights-compliant use of AI in practice.
At the same time, further research into the fundamental rights implications
of the use of AI in specific areas will further support policy and legislative
efforts at the EU level aiming to shape Europe’s digital future more widely.
102

police

develop

time

FRA will continue to look into the fundamental implications of AI by carrying
out more focussed analysis of specific use cases. To increase knowledge
on what can potentially go wrong and consequently help mitigate and
prevent fundamental rights violations, FRA will look into potential simulation
studies. These can showcase how biased algorithms can negatively affect
fundamental rights.
The use of AI often involves automating tasks that were previously carried
out by humans. Here we need to acknowledge that human behaviour is
sometimes not in line with fundamental rights, both when using AI and when
not using AI. For example, the police might engage in unlawful profiling.
Decisions by public administration or companies might sometimes be driven
by negative stereotypes.
Current developments in the use of AI need to acknowledge the potential for
discrimination with respect to the data on which an AI system is built, and
with respect to the underlying assumptions that humans in turn may feed
into the development and deployment of a system. Automating certain tasks
without fully understanding what is being automated could lead to unlawful
processing of data, the use of technology that treats people unfairly, and might
make it impossible to challenge certain outcomes – to name some challenges.
However, the increased availability of data and technological tools can also
be used to better understand where and how unequal treatment occurs.
Current technological developments and the increased availability of data
also provide a unique opportunity to better understand the structures of
society, which can be used to support fundamental rights compliance. The
opportunities created by AI can also contribute to better understanding and
consequently mitigation of fundamental rights violations.

103

Getting in touch with the EU
In person
All over the European Union there are hundreds of Europe Direct information centres.
You can find the address of the centre nearest you at:
https://europa.eu/european-union/contact_en
On the phone or by email
Europe Direct is a service that answers your questions about
the European Union. You can contact this service:
 y freephone: 00 800 6 7 8 9 10 11
—b
(certain operators may charge for these calls),
— at the following standard number: +32 22999696 or
— by email via: https://europa.eu/european-union/contact_en

Finding information about the EU
Online
Information about the European Union in all the official languages of the EU is available
on the Europa website at: https:// europa.eu/european-union/index_en
EU publications
You can download or order free and priced EU publications at: https://op.europa.eu/
en/publications. Multiple copies of free publications may be obtained by contacting
Europe Direct or your local information centre (see https://europa.eu/european-union/
contact_en).
EU law and related documents
For access to legal information from the EU, including all EU law since 1952 in all the
official language versions, go to EUR- Lex at:
http://eur-lex.europa.eu
Open data from the EU
The EU Open Data Portal (http://data.europa.eu/euodp/en) provides access to datasets
from the EU. Data can be downloaded and reused for free, for both commercial and
non-commercial purposes.


PROMOTING AND PROTECTING
YOUR FUNDAMENTAL RIGHTS
ACROSS THE EU
―
Artificial intelligence (AI) already plays a role in deciding what
unemployment benefits someone gets, where a burglary is likely to
take place, whether someone is at risk of cancer, or who sees that
catchy advertisement for low mortgage rates. Its use keeps growing,
presenting seemingly endless possibilities. But we need to make sure
to fully uphold fundamental rights standards when using AI.
This report presents concrete examples of how companies and public
administrations in the EU are using, or trying to use, AI. It focuses on
four core areas – social benefits, predictive policing, health services
and targeted advertising.
The report discusses the potential implications for fundamental rights
and analyses how such rights are taken into account when using
or developing AI applications. In so doing, it aims to help ensure
that the future EU regulatory framework for AI is firmly grounded in
respect for human and fundamental rights.

EU Charter of
Fundamental Rights

Access to justice

Non-discrimination

Information society

FRA – EUROPEAN UNION AGENCY FOR FUNDAMENTAL RIGHTS
Schwarzenbergplatz 11 – 1040 Vienna – Austria
TEL. +43 158030-0 – FAX +43 158030-699

fra.europa.eu
facebook.com/fundamentalrights
twitter.com/EURightsAgency
linkedin.com/company/eu-fundamental-rights-agency

No 5 /2020

Machine intelligence in
insurance: insights for
end-to-end enterprise
transformation

01 Executive summary
03 Machine intelligence:
establishing a common
understanding
07 Machine-intelligence
system implementation
13 Machine intelligence in
insurance
19 Progressing enterprisescale deployment
27 Conclusion

Executive summary
MI offers significant revenue and
cost-saving potential.

Machine intelligence (MI) permeates many industries, bringing significant revenue
creation and cost savings potential. To date in insurance, MI has yielded returns in
areas such as customer analytics and claims processing, based mostly on machine
learning (ML) technology. The scope for industry gain is farther reaching. Among
others, MI can help insurers more efficiently process text from contracts, documents,
email and other online communications tools, and to analyse the massive data sets
becoming available from the digital economy and accumulated from Internet of
Things (IoT) devices. Insurers can use this information to better design, price and
distribute protection covers, and extend their reach into new markets.

Conventional MI methods are already
standard in certain areas of the
insurance value chain. They could be
superseded by more advanced
approaches.

Conventional MI approaches such as generalised linear models have become
standard tools in insurance for risk assessment and prediction models. Even so,
these tools typically only facilitate fragmented, narrowly-focused productivity.
Enterprise-scale transformative benefits could be delivered with more investment in
data engineering. This focus on data engineering is also necessary to realise the
enterprise-wide potential of more advanced ML and artificial intelligence (AI). Early
adopters of such approaches are seeing positive results in select areas like faster
claims settlement, more targeted cross- and up-selling, and better risk scoring. The
foundational technologies necessary to perform MI tasks continue to develop rapidly
as algorithms become easier to use and cheaper. As a consequence, we expect that
some processes currently profitable with conventional MI may well be supplanted by
new ML and AI approaches unlocking new growth trajectories.

Enterprise-scale deployment of
MI-enabled systems remains elusive,
however.

Beyond such progress, however, enterprise-wide deployment of MI-enabled systems
in insurance remains a long way off. In an overview of survey data, we found that less
than 10% of firms in all sectors have managed to scale MI pilots for roll out across
multiple processes. Primary reasons include data availability and quality. Many ML
and AI approaches require large amounts of high-quality data to train algorithms.
Even conventional MI is hampered by data quality. Today, many areas of interest in
MI are working with data sets that are not complete, clean or timely. This further
affirms the importance of data engineering. Without said capabilities, the
performance of models/algorithms has proven to be slow and expensive relative to
existing human-centric processes. If deployed correctly, the models/algorithms can
deliver substantial return on investment but to date they have not been in a readystate for enterprise-scale rollout. COVID-19 has forced consumers and businesses to
become more digitally active. This has accelerated the need to shift to more digitallyoriented business models, further affirming the value of transformative MI.

Issues around data quality and
curation are generating interest in new
areas of MI potential.

In recent years, the issues around data quality and curation have led to development
of new approaches, such as reinforcement learning and ensemble modelling. Socalled hybrid models/algorithms based, for example, on a combination of knowledge
from physics and ML, and causal-inference approaches, are less sensitive to data
quality and compute power inadequacies. These are just two areas of innovative
research seeking to address specific performance and model-interpretability issues,
yielding solutions that could be a new part of future MI applications in insurance.

It's not just models that matter. A
range of issues influence success in
roll out of MI-enabled systems at
enterprise scale.

All told, MI viability is typically assessed on the basis of small-scale proof-of-concept
pilots of models/algorithms. That's not enough. A more holistic view is required
because more often than not, deployment failure can be attributed to organisational
constraints, not model problems. The criteria to evaluate a new process should
include integration of direct (development and running) and indirect (organisational
and opportunity) benefits and costs. While chief data officers and scientists have
become common-place at insurers, inchoate firm-wide data strategies and
inadequate underlying technology hinder their effectiveness. System design,
deployment plans and success criteria should focus on business workflow context,
decision support and enterprise productivity. Regulatory risks regarding tech-linked
innovation in insurance, in particular around data privacy and use, also need to be
considered. Importantly, an MI project also needs clear and understandable
communications across all facets, to secure senior management buy-in and funding.

Swiss Re sigma No 5 /2020

1

Glossary: key machine-learning terminology

Term

Description

Algorithms

A list of computer-implementable instructions.

Machine
intelligence

A collection of programs and processes that enable a machine (eg, computers) to apply data and
information to solve problems.

Conventional
curve fitting

A basic form of MI (eg, generalised linear models). These rely on assumptions to understand how
variables relate to each other, with the aim of creating a curve that best-fits the relationship between
data points. Conventional curve fitting can capture some types of non-linear relationships, also.

Machine learning
(ML)

Algorithms that learn from data and analyse more complex, inter-related and non-linear relationships
among variables. Commonly used in classification, regression and pattern recognition.

Artificial
intelligence

AI goes beyond ML by facilitating adaptive application of understanding. With these algorithms,
machines can store and apply learning flexibly, including to contexts not originally intended.

Supervised
learning

To train a machine using data which are labelled, ie, already tagged with the correct answer. These
labelled data act as supervisor. The machine infers relationships from this sample, which are used to
map new examples.

Unsupervised
learning

Unsupervised learning is used when labelled data are not available. With no teacher to train the
machine, it has to discover hidden structures in the unlabelled data on its own. Used for clustering and
association.

Clustering and
association

A clustering algorithm seeks to discover inherent groupings in the data, eg, grouping policyholders by
purchasing behaviour. An association problem is when an insurer looks to find out rules that describe
the data, eg, policyholders that buy X policy also tend to buy Y policy.

Reinforcement
learning

Goal-oriented algorithms (agents), which answer the question, how can this be optimised? Eg, how
can marketing investment be optimised to extract maximum ROI? Learns by interacting with its
environment.

Ensemble learning

Uses multiple algorithms in combination to obtain better predictive performance than could be
obtained from any one of the algorithms alone.

Data engineering

Data engineering is the process of collecting, curating, storing, and transforming data for analytical
purposes.

Deep learning

Imitates the human brain to learn without human supervision, with data that is unstructured and
unlabelled.

False positive

A prediction which wrongly indicates that a particular condition or attribute is present.

False negative

A prediction which wrongly indicates that a particular condition or attribute is absent.

Physics-based ML

Machine learning that incorporates a model (eg, hydrodynamic) built using a valid scientific theory
based on physical systems understanding into an ML algorithm/process to provide more structure to
the model than would be the case for a less constrained ML model (eg, supervised or unsupervised
learning.) This hybrid approach is often easier to interpret and diagnose.

Generative
adversarial
networks

Generative adversarial networks (GANs) involve learning patterns in data so that the model can
generate new examples that seem credible enough to belong to the original dataset. The original data
and the generated data can then be played off each other in the context of competing neural networks
to develop better models.

Causal inference

Causal inference in ML refers to approaches that provide more structure for control and prediction by
building capabilities that identify actual drivers of outcomes to make an ML process more robust to
changing circumstances, eg, attempting to sort out causal drivers of obesity to distinguish what can
be controlled across different sub-populations or analysing what design choices lead to more clicks on
a website.

Source: Swiss Re Institute

2

Swiss Re sigma No 5 /2020

Machine intelligence: establishing a
common understanding
Machine intelligence (MI) is an umbrella term covering a range of data processing and manipulation techniques, from
conventional logistic regression to sophisticated deep learning. The more advanced MI are generally categorised as
machine learning (ML) and artificial intelligence (AI). Today, conventional techniques can be more easily scaled up to
augment existing processes. However, in recent years there has been exponential growth in the ease of use and
effectiveness of algorithms, and more sophisticated ML and AI could eventually supplant conventional approaches.

Information and data processing
MI encompasses programs and
processes that use data to enable a
machine to solve problems.

Machine intelligence (MI) is often used as a synonym for artificial intelligence (AI), a
term for which common understanding is equally variable and/or vague. To establish
a consistent reference understanding for the purposes of this report and public
debate, we define MI as a collection of programs and processes that enable a
machine (most often computers) to apply data and information to solve problems. In
most cases, human intervention is required to make the MI-enabled process useful.
We include the following categories within this umbrella definition of MI:
̤̤ Conventional curve-fitting or traditional statistical approaches, such as
generalised linear models (eg, linear or logistic regression). These approaches rely
on assumptions to understand how variables relate to each other, with the aim of
constructing a curve or mathematical function that best fits the relationship
between data points. Note that conventional curve fitting can capture some types
of non-linear relationships, also.
̤̤ Machine learning: Algorithms that learn from data and analyse more complex,
inter-related and non-linear relationships among variables. Commonly used in
classification, regression and pattern recognition.
̤̤ Artificial intelligence goes beyond ML by facilitating adaptive application of
understanding. In AI, algorithms mirror human-like qualities such as the ability to
respond to contextually ambiguous situations. With these algorithms, machines
can store and apply learnings flexibly, including to contexts not originally intended.
In this vein, more advanced AI is sometimes called neuromorphic or cognitive
computing. Some newer AI appear to reflect new kinds of networked intelligence
called hive intelligence.

ML and some kinds of AI are more
complex analytical approaches to data
processing.

Conventional curve-fitting, ML and AI can be independent and interdependent. As
Figure 1 shows, AI typically incorporates both ML and conventional curve-fitting
methods. Within ML, supervised learning has seen widespread adoption. Here
human intelligence is used to embed each piece of sample data with meaningful
tags that help an algorithm understand the data. Unsupervised learning is the
method used when labelled data are not available, such as to detect data clusters
(eg, in insurance to group policyholders by purchase behaviour) or anomalies (eg,
fraud detection, in partnership with human claims experts).1

Newer types of ML are still in early
stages of development.

Reinforcement learning (RL) exhibits more adaptivity and has been successfully
applied in augmented reality tools such as in gaming.2 However, the field is still in
early stages of development in other sectors. RL algorithms are not restricted to
existing data, but search out optimised solutions based on rewards or penalties
related to each action taken. RL can be combined with simulations and data
augmentation to compensate for incomplete, messy, non-stationary or biased data.

1
2

A key task is to detect any specific grouping or clustered behaviour in the observed data.
Reinforcement learning is an area of machine learning concerned with how software agents ought to
take actions in an environment in order to maximise the notion of cumulative reward.

Swiss Re sigma No 5 /2020

3

Machine intelligence: establishing a common understanding

Figure 1
Schematic showing overlapping
areas within machine intelligence

Machine intelligence

Supervised
learning

Machine
learning

Unsupervised
learning

Artificial
intelligence

Reinforcement
learning

Deep learning

Conventional
statistical
approaches

Source: FSB, Swiss Re Institute

Conventional MI models can be
more easily scaled-up to augment
existing processes in a firm.

The objective of MI, particularly when deployed in enterprises, is to supplement or
emulate human deduction, reasoning and problem solving. An MI-enabled solution
can be more successful and transformative when based on a conventional approach
like logistic regression, even if ML or AI models, given the greater number of
variables they can analyse, perform better in predicting outcomes. This is because
the conventional model can be more easily scaled-up to augment existing processes
in an organisation. Figure 2 ranks different MI techniques in terms of complexity
criteria: interpretability of model results, ease of implementation, stability of models
to changes in data, and execution speed. These factors determine success of MI
solution deployment. We expect that as algorithms become faster and cheaper,
more sophisticated ML and AI models could supplant conventional methods.

Figure 2
Complexity spectrum of different categories of MI
Complexity Approaches/Algorithms*

Low

High

Interpretation

Implementation

Stability

Execution
speed

Category

Generalised linear models
Naïve Bayes classifiers
Instance-based learning
Support vector machines
Decision trees
Random forest
Gradient boosting

Supervised learning
Supervised learning
Supervised learning
Supervised learning
Supervised learning
Supervised learning
Supervised learning

Deep learning (CNNs, RNNs, NLPs, BERT^ etc)
Generative adversarial networks (GANs)
Optimal classification trees
Reinforcement learning
Ensemble learning

Supervised/unsupervised/semi-supervised
Supervised learning
Reinforcement learning
Combination of techniques

Supervised/semi-supervised

High

Low

*Certain approaches/algorithms may be better or less suited than others for solving certain problem types
Notes: Interpretation refers to algorithm transparency and explainability. Implementation refers to effort required to develop algorithms and
the volume and complexity of data needed to train models. Stability refers to sensitivity of performance to change in data and assumptions.
Execution speed refers to time to complete end-to-end execution, starting from data ingestion to final results.
^ Bidirectional encoder representations from transformers (BERT) is a substantial improvement on natural language processing (NLP). CNNs
are convolutional neural networks; RNNs are recurrent neural networks; GANs are generative adversarial networks.
Source: Swiss Re Institute

4

Swiss Re sigma No 5 /2020

Beware the hype
The evolution of AI has seen periods of
very intense and lesser interest.

AI as a term was first coined in the 1950s. This technique has proliferated since then,
interspersed with periods of reduced funding and interest (“AI winters”). There was a
spike in interest in the early 1990s with the development of neural networks.3
However, the new potential was not fully realised leading to another period of AI
winter in the 2000s, which has only thawed in the last 10 years.

Figure 3
MI development timelines
1888 Francis Galton:
corrélation, standard
deviation, regression

1950 Alan Turing:
Learning machine

1763 Thomas Bayes:
Underpinnings of
Bayes theorem

1970 Seppo
Linnainmaa:
Backpropagation

1900 Pearson:
statistical hypothesis
testing
1957 Frank
Rosenblatt:
Perceptron

1812 Pierre-Simon
Laplace: Bayes
theorem

First neural net and learning programs
Pre 20 century
Pre AI era

1995 Corinna Cortes &
Vladimir Vapnik:
Support vector
machines

2011 IBM Watson
wins Jeopardy

1980 Vector
Autoregression (Sims)

1995 Tin Kam Ho:
Random Forest
algorithm

2012 Andrew Ng
& Jeff Dean: Deep
learning

1972 Hazard
models

1987 Cointegration

2016 Googleʼs
AlphaGo beats
human Go champion

1972 Generalized
linear models

1989 Christopher
Watkins: Reinforcement learning

1997 Hochreiter &
Schmidhuber: Long
short-term memory
(LSTM)

2017 Googleʼs
AlphaZero (developed out of AlphaGo)
generalized to Chess
& other two-player
games

Speech recognition

Deep learning

Data mining, Big data

1970 ARIMA

1944 Logit model

th

1982 John Hopfield:
Recurrent neural
network

1950s

Visual guidance and
logic based learning
1970s

Genesis of AI and early hype

1980s
AI winter

Expert
systems

1990s
AI winter

2000
Algorithmic
innovation

AI winter

2015

2020

Big data, better compute,
algorithmic efficiency

Source: Swiss Re Institute

The periods of interest can also
generate a certain level of hype.

There has been notable progression in AI and ML capabilities in the last years due to
improvements in processing power, developments in cloud computing, an explosion
in data, and digital transformation. This has also led to, in our view, an unwarranted
level of hype. For example, a 2019 study found that two-fifths of Europe's labelled AI
start-ups that claim to use AI actually do not. And when they do, the AI use cases are
often quite basic.4 In some instances, mention of AI and ML have been included in
business pitches to improve chances of securing financing, although the level of
sophistication of the AI being sold is still rudimentary.

Even so, indications are that spending
on AI will grow strongly over the
coming years.

Often technology spending tracks hype. According to estimates from International
Data Corporation (IDC), spending on AI systems will reach USD 98 billion in 2023
(a compound annual growth rate (CAGR) of 27% from 2019).5 These surveys do not
necessarily reflect all MI spending, some of which gets lumped in with general IT
budgets, so it is difficult to find a comprehensive estimate. Of late, many categories
of MI have passed through the peak of inflated expectations (the hype), and some
are falling into the trough of disillusionment.6 With respect to insurance specifically,
not much has changed over the last 30 years. If anything, the industry has yet to
enter the “slope of enlightenment”.

3
4
5
6

A network of artificial neurons mimics the connections and associated responses of the human brain.
The State of AI 2019: Divergence. MMC Ventures, March 2019.
Worldwide Spending on Artificial Intelligence Systems Will Be Nearly USD 98 Billion in 2023, IDC,
September 2019.
Hype Cycle for Artificial Intelligence, Gartner, 25 July 2019.

Swiss Re sigma No 5 /2020

5

Machine intelligence: establishing a common understanding

Nevertheless, the foundational technologies necessary to facilitate successful
enterprise-scale MI deployments continue to develop rapidly, as algorithms become
faster, leaner and cheaper.7 The consequence will be that some enterprise processes
that are profitable with conventional MI could suddenly become more so with newer
MI (eg, ML and AI). Figure 4 (LHS) shows what has been an exponential increase
(see Y-axis) in efficiency of algorithms (eg, AlexNet, ShuffleNet) over recent years, far
outpacing the rate of capability improvement according to Moore's law.8 For
example, the processing performance of newer algorithms such as EfficientNet in
2019 on a computer vision task far exceeds that of AlexNet in 2012.9

Exponential improvements in
algorithm efficiency could make
deployment of more advanced MI
techniques more profitable.

Figure 4
Increase in algorithmic efficiency (LHS) and progress in AI implementation (RHS)
Measure of efficiency, teraflop/s-day
0.05

46%

Academia (2200 business leaders,
managers, and key contributors)

49%

20%

10%

24%

EfficientNet b0
ShuffleNet_v1_1x
MobileNet_v1

ShuffleNet_v2_1x
ShuffleNet_v2_1_5x
MobileNet_v2

0.5

AlexNet

Vgg-11

2014

AI Solutions Provider (senior decision
makers in North America)

DenseNet121
Squeezenet_v1_1
ResNext_50

GoogleNet

5
2012

AI ML Analyst Firm
(1500 decision makers across regions)

Tech Research &
Advisory Co (2900 global CIOs)

Wide_ResNet_50

2016

Lowest compute points at any given time

2018

2020

All points measured

Note: a teraflop refers to the capability of a processor to
calculate 1 trillion floating-point operations per second.
Source: D. Hernandez, et.al. Measuring the Algorithmic
Efficiency of Neural Networks, 2020

To date, digital-native firms have been
the most successful in transformative
MI deployment.

Planning but no action

6%
23%

22%
2%

27%

52%

19%
4%

32%

53%

12%

Experimenting & piloting

Implementing narrowly (in near-term)

Implementing widely

Source: Swiss Re Institute (based on multiple surveys, data anonymised,
standardised and rescaled to make them comparable).

We looked at a collection of surveys (see Figure 4 RHS) and found that some
enterprises (less than 10%) have managed to build on successful pilots to deploy MI
in multiple processes across the organisation. Enterprise-wide transformative MI
demands initial investment in digitising firm-wide operations. This will lay the
foundation for firms to (1) apply MI to automate processes; and (2) introduce new
offerings by cutting across company-wide silos to integrate data. Examples of
advanced deployments include MI-native digital-first platforms/firms like Uber and
Airbnb. Also included are firms that started off with traditional technology (eg,
Microsoft, Amazon) but have since invested heavily in MI capabilities.10 Firms in
more traditional sectors like financial services (including insurance) are playing catch
up. To reap large-scale benefits from MI, they first need to digitise operations and
break down data silos. We expect to see progress in this direction over the coming
decade.

7
8
9

A. Agrawal, J. Gans, et.al, Prediction Machines: The Simple Economics of Artificial Intelligence, 2018.
According to Moore's Law, the speed and capability of computers increase every two years.
By 2020, with more efficient algorithms, it took 44x less compute power than in 2012 to train a neural
network to the level of AlexNet. Over the same time span, Moore's Law yields 11x improvement.
10 AI from exploring to transforming: Introducing the AI Maturity Framework, Element AI, May 2020.

6

Swiss Re sigma No 5 /2020

Machine-intelligence system
implementation
MI applicability and implementation are not uniform across industries. Successful implementation is dependent on
data availability, interpretability, system complexity and regulation. Implementations require a strong business case,
competent system architects and developers, supportive regulations, committed management, and an enterprise-wide,
production-ready data strategy. In a post-COVID-19 lower growth environment, ROI will be a key consideration as
analytics projects are evaluated. Investment in data engineering capabilities is critical for successful deployments.

Criteria for success
Things to think about include...

When companies think about MI-enabled systems, there are many things to
consider: which MI tools to implement; how to address constraints arising from
legacy systems; where to find staff to resolve capability deficits; and how to secure
more benefits than costs. Table 1 presents four overriding criteria we recommend
companies should consider as they prepare for MI-related technology adoption.

Table 1
Criteria for successful implementation of enterprise-wide MI for the long term
Criteria for success

Present inadequacies

What successful projects look like

Long-term positive
ROI for end-to-end
processes

Underestimation of implementation costs, poorly
designed MI-ready workflow processes,
scalability difficulties arising from poor planning

Workflow process architecture matches
deployed MI strengths, recurring costs detailed
in implementation plan, integration questions
prioritised, security and privacy built into
planning from the beginning

Production-ready
data strategy

Underinvestment in data ingestion and curation,
lack of detailed data stewardship, insufficient
number of data engineers.

Appropriate focus on data strategy, data
engineering, data tools, and data modelling.

Use cases that fit
business and
regulatory contexts

Among others, use cases do not consider data
constraints, and organisational issues that prevent
MI from adding value.

Deployment leverages MI strengths in the
context of existing organisation and workflow
processes. Deployment plan includes clear
match of MI and pain points, data and technical
feasibility evaluated from the start.

Management
commitment

Senior management not adequately briefed on
proposed MI-enabled deployment, poor
implementation by frontline staff, poor
coordination and buy-in among business units,
and shallow understanding of MI operations.

Regular and detailed updates to management,
willingness to change the process to
accommodate new findings, and ongoing
investment and talent continuity to extract value.

Source: Swiss Re Institute

Long-term return on investment
...return on investment.

To secure positive return on investment (ROI) from spending on MI-transformation of
end-to-end processes across a firm over the long-term, factors to consider include:
̤̤ Net benefits from workflow transformation vs recurring costs: The benefits
from the transformation exercise in terms of reduced cost, increased revenue and
new business opportunities must be greater than the costs of organisational
integration, both direct (development and running) and indirect (organisational
and opportunity costs). In a recent survey, 93% of respondents at US insurers
going through MI transformation expressed concern around the costs of
implementation and ROI.11 Many ROI calculations look only at the cost of vendor
solutions, not the added costs to the business (eg, data curation, training, etc).
Further, ROI is an ongoing calculation as new inputs (eg, regulation and changes
in costs of key data) can render initial cost/benefit estimates inaccurate. Figure 5
presents key cost/benefit considerations for deployment of such systems.
11 State of Artificial Intelligence and Machine Learning in the Insurance Industry Study, LexisNexis® Risk
Solutions, 3 December 2019.

Swiss Re sigma No 5 /2020

7

Machine-intelligence system implementation

Figure 5
Cost/benefit considerations for deployment of MI-enabled systems
Financial model build:
For example, a model with
anticipated ROI is available
for review

Potential upside return:
eg, Increased customer
retention of 5%. Decreased
cost per transaction of 5%

Potential downside risks:
eg, decreased customer
retention (10%). Increased
cost per transaction (5%)

Worst-case downside:
eg, Automated claims
processing crashes or does
not filter fraudulent claims

Liability: eg, Automated
underwriting system does
not capture the risk accurately

Cost of building model:
eg, six months for a team of
two data scientists

Cost of maintaining model:
eg, 10 hours a month

Quality of predictions vs
expectations: eg, model
assumes 100% correct
predictions, but results 85%

Uncertainty in model
predictions: eg, prediction
might be accurate +/–10%

Source: Adapted from V. M. Megler, Managing ML Projects, Balance Potential with the Need for Guardrails, February 2019

̤̤ Performance and efficiency of re-engineered end-to-end processes: MI
implementation must be scrutinised in technological terms and with respect to
business drivers and constraints. For example: (1) a new MI-enabled process
should be at least as accurate and robust as the current process, and more time
efficient; and (2) the new process should generate new and sustainable business
opportunities. At times, the cost expectations of realigning human resources to
accommodate forecast benefits may not be realistic. For example, anecdotal
evidence suggests that ML fraud detection systems sometimes flag far more new
cases than existing staff can verify. Measurement metrics must be tied to realistic
business outcomes rather than mere model performance.
̤̤ Investment needed to maintain system quality and robustness: A key
criterion for success of MI-enabled systems is an effective continuous monitoring
framework for model lifecycle management. Model development – even for most
advanced MI – is often the more straightforward and less costly aspect of
enterprise MI deployment. Integrating a new MI system into an organisation, on
the other hand, will likely require workflow process re-engineering and constitute
most of the system deployment costs. Further, maintaining the integrity, security
and privacy of a new system will require a large budget at first (although for wellarchitected systems these running costs should decrease over time).
̤̤ Exception handling: In the post COVID-19 environment, models may give
unexpected results due to major shifts in consumer behaviour, data inputs and the
way businesses are run. However, the renewed emphasis on digitalisation will
create more and diverse data sets to further refine MI-models, and widen the
scope of training information available to better exception handling capabilities.
On the other hand, companies already investing heavily but facing cost pressures
may now prioritise projects more carefully and continue with projects that already
deliver positive ROI or are close to doing so. Several areas will continue to see
greater attention and investment, including automation and fraud detection
capabilities.

8

Swiss Re sigma No 5 /2020

Production-ready data strategy
MI-enabled system performance
materially depends on data-valuechain management

Often, deployment fails because of poor data engineering. In an end-to-end
enterprise process, low-quality algorithms with high-quality data engineering will
tend to outperform high-quality algorithms with low-quality data engineering. In
financial services, firms typically start off with developing an algorithm and then
under-invest in data engineering. For transformative impact enterprise wide, they
should do the reverse. Figure 6 presents findings from a recent survey on the low
maturity levels of insurers in terms of accessing and curating data for AI models.12

Figure 6
Data maturity of insurers
Are you able to access all
the data you need for AI?
16%

Is accessible data cleaned and
consolidated for the use with AI?
5%

11%

24%

16%

11%
11%

0%
11%

51%
43%

Access is very difficult and a barrier to AI
Some data is accessible to start building POCs
Core set of data consistently accessible to build models
Extended set of data seamlessly accessible to many BUs
Proactive and efficient access across the firm
Not aware

We don’t know if our data is ready to use
Some cleansing and consolidation for specific use cases
Begun to standardize data cleansing and
consolidation accross the firm
Standard data cleansing and consolidation pipeline, with tools
Actively evolving data cleansing and
consolidation, with automated tools

Source: The Five Dimensions of Enterprise AI, Element AI, May 2020, Insurance respondents only

Lack of data strategies can detract
from the effectiveness of
newly-created CDO roles.

Some of the most innovative AI under development (eg, reinforcement learning and
ensemble modelling) can be robustly trained without extensive high-quality training
data. In these cases, systems use simulations, data augmentation algorithms and
synthesise subject-matter expertise.13 However, these new approaches are not yet
ready for enterprise scale and nearly all successful MI-enabled system deployments
still depend heavily on data quality and quantity. Here, the capacity and tools
available today to process structured and unstructured data open new doors of MIrelated opportunity. To optimise the potential, a firm-wide data strategy and system
architecture such as represented in Figure 7, is essential. The lack of said strategies
and architectures hamper the effectiveness of the also critical newly-created CDO
roles. For example, in a recent survey, less than 10% of CDOs across industries said
they are able to measure the financial value of their information and data assets.14

12 Survey with senior decision-makers at large organisations in the US and Canada. See Element AI, May
2020, op. cit.
13 Integrating subject matter expertise in this way reflects a Bayesian updating approach to ML or AI in the
sense that subjective priors are incorporated into the algorithm to reduce the scope of evaluated
parameter space, which can counteract (assuming the priors are sound) data inadequacies.
14 Gartner Survey Finds Chief Data Officers Are Prioritizing the Right Things, But Higher Strategic Focus
Is Required, Gartner, 10 June 2019.

Swiss Re sigma No 5 /2020

9

Machine-intelligence system implementation

Figure 7
MI-data strategy schematic
Data acquisition

Curation
Data
exploration

Feature engineering

Model engineering

Business review

Features

AI models

Evaluation

Feature selection

Model architecture

Extraction

Train, validate, test

Data
preprocessing

Convert to
MI-usable data
IoT

Data engineering pipeline

Compliance
reviews

Exploratory
analysis

Data scrubbing,
transformation

Cataloguing,
storage,
permissioning

Data validation

Deduplication,
scaling, normalisation,
anonymisation

Higher level
of effort

Transform,
encoding

Source: Swiss Re Institute

Data engineering is often duplicated
across functions.

For insurers investing in data architecture, a common characteristic of poor strategy,
is duplicative data engineering across different teams. In these cases, data scientists
rarely feel confident about a centralised curation process. In some cases, there is no
centralised curation process, or none that the data scientists are aware of, leading
them to create their own, often duplicative, processes. IDC found that data
professionals spend 67% of their time searching for and curating data.15 In another
insurance specific survey, nine out of 10 employees at the 100 largest firms in the
US said that managing increased volumes of data was their number one challenge.16

Insurers also lack comprehensive data
ontologies that define relationships
among data.

In our view, a centralised data ingestion and curation capability can generate sizable
ROI by overcoming such inefficiencies. This is an area where insurers in particular,
have a long way to go. A recent survey found that as many as 75% of insurers lack a
taxonomy to harmonise different types of data.17 Most also do not have
comprehensive data ontologies that define multi-dimensional relationships among
the classified data in a data taxonomy (or in multiple taxonomies), an issue that is set
to become more complex as data sources grow in number and diversity.

Factors to consider for best-fit use cases
Many failed implementations arise
from a mismatch of algorithms to use
case.

Successful MI-enabled implementations require matching of desired outcomes with
the best-suited enterprise-ready algorithms and techniques. Not every algorithm
works for every use case, and many failed implementations arise from a mismatch of
algorithms to use case. Well-calibrated traditional statistical methods can offer
similar results in terms of accuracy to advanced models, suggesting that data quality
matters more than algorithmic innovation. Importantly, business use case and data
availability should drive technique selection. Even with best efforts to match
technique to use cases, a trial and error process ensues as different techniques are
implemented and tested to determine which approach works best. Over time we

15 End-User Survey Results: Deployment and Data Intelligence in 2019, IDC, November 2019, sourced
from “Talend Accelerates Path to Revealing the Intelligence in Data”, 27 February 2020.
16 LexisNexis® Risk Solutions, op. cit.
17 Building New Data Engines for Insurers, BCG, 5 November 2018.

10

Swiss Re sigma No 5 /2020

expect consensus to develop with respect to which techniques work best with
specific use cases. Factors to consider when matching technique/use case include:
̤̤ Interpretability: Questions to ask include: How much does the business need to
understand? How much would it normally understand? What are the regulatory
requirements and professional standards? In insurance, about a third of firms that
have adopted MI are concerned that if regulators do not understand newer
techniques, they could block or limit efforts to use new applications.18 Also,
depending on use case, firms may be required to be transparent to both regulators
and customers. Typically, a high degree of model/process transparency is
required, and the line between collecting data to improve service and
compromising privacy is very thin. Getting the balance right can impact funding.
Gartner forecasts that by 2022, projects will be twice as likely to receive funding if
they have built-in transparency.19
̤̤ Use case selection should consider the costs of different errors: While
models process large volumes of data rapidly, they can also lead to relaxed human
oversight. Each error has a cost, and management must decide on acceptable
levels of error tolerance to identify the point at which economic value can turn
negative. In some use cases, all types of errors may have an equal impact, but in
others one can prove more costly (eg, if a self-driving car ignores a pedestrian or an
automated credit evaluation system extends credit to a company that later
defaults). At other times, the cost of a false prediction may be greater than the
savings associated with a true prediction. For example, an insurer looking to rapidly
assess property claims using MI-based aerial image analysis may later have to
increase reserves significantly because of non-visible damages (eg, under a roof).
A false positive may prove less expensive in cross-selling campaigns (where the
cost is a wasted email) than in underwriting or pricing (where the cost is accepting
sub-standard risks). Table 2 demonstrates this trade-off in two scenarios: 1)
propensity to buy in a cross-selling campaign; and 2) classifying a critical illness
(CI) risk for underwriting decision. In cross-selling, the cost of approaching an
unwilling prospect based on a less accurate propensity to buy prediction (false
positive) is far lower (eg, USD 10) than the false-positive in the underwriting of a CI
policy, when a risk classified as good is actually bad (eg, USD 1100).
Table 2
Assessing the impact of error costs
Propensity
to buy
Actual
(unlikely to buy)
Actual
(likely to buy)
Critical illness
classification
Actual
(Bad risk)
Actual
(Good risk)

Predicted
(unlikely to buy)
(True negative)
3 000

Predicted
(likely to buy)
(False positive)
600

(False negative)
400

(True positive)
6 000

Predicted
(Bad risk)

Predicted (Good
risk)

(True negative)
3 000

(False positive)
600

(False negative)
400

(True positive)
6 000

Cross-selling
scenario
True positive

Number of
predictions
6 000

Gain (Loss) per
prediction in USD
100

False negative

400

Don’t approach

–

False positive

600

(10)

(6 000)

3 000

Don’t buy

True negative
Total

10 000

Total gain (loss)
in USD
600 000

–
594 000

Underwriting
scenario
True positive

Number of
predictions
6 000

Gain (Loss) per
prediction in USD
100

False negative

400

Don’t underwrite

–

False positive

600

(1,100)

(660 000)

3 000

Don’t underwrite

True negative
Total

Note: This is a simplified illustration and may not capture all possible scenarios.
Source: Swiss Re Institute

10 000

Low cost of error

Total gain (loss)
in USD
600 000

–
(60 000)

High cost of error

18 LexisNexis® Risk Solutions, op. cit.
19 Can learnings from early projects give CIOs a head start with AI technologies?, Gartner, 9 February
2018.

Swiss Re sigma No 5 /2020 11

Machine-intelligence system implementation

Organisational maturity and willingness
Transformative, successful MI
deployment requires a cohesive
strategy that explicitly includes details
of necessary capabilities.

Transformative MI deployments require much more than just model/algorithm
development. Equally critical for success is a cross-functional, detailed strategy with
senior executive sponsorship. An important part of the strategy is a focus on
capability, in terms of technology, process re-engineering and staffing. An MI
implementation workflow requires capabilities across data engineers, model
engineers and software developers/IT operations (see Figure 8). It is also desirable
to have staff with multiple skill sets who can translate requirements clearly across
functions. In insurance, firms sometimes hire actuaries with programming skills to
reduce miscommunication when actuaries hand over their models to development
engineers without knowledge of statistics.

Figure 8
MI implementation workflow
1
Bootstrap
data
sources

Updated
data, feedback data

Data
ingestion

Model

weights
Source: Swiss
4 Re Institute

2
Unprocessed
data
sources

Data
curation

Training and
testing

Curated
data

Hyperparameter
tuning

Web
services

6
Apps,
service
consumers

Deployment
Reference
matrixes,
etc.

Model
architecture

5

Library
APIs

Performance
accuracy
metrics

3
Modeling

Core
libraries

Source: Swiss Re Institute

12 Swiss Re sigma No 5 /2020

Open
source subcomponents

Proprietary,
domain-specific
sub-components

Data operations
Model engineering
Developer operations

Machine intelligence in insurance
Insurers continue to experiment with newer MI approaches to build upon (and possibly replace) conventional MI
techniques that are becoming standard practice in areas like customer analytics and claims processing. However,
unlike in sectors such as social media, end-to-end transformation of insurance processes through MI-enabled
systems remains elusive. Data availability, model interpretability, and privacy issues remain barriers to large-scale
adoption. The cost of errors in insurance can also be high.

How things stand
Insurance executives remain
optimistic about MI.

The insurance industry has lagged in implementation of MI-enabled systems. Still, a
2019 survey found that industry executives have high expectations about adopting
ML in 2021. They were optimistic in the past too, with past surveys projecting high
expectations for where they would be in 2019, although actual adoption last year
was well below predictions.20 It also appears that insurers have become more vocal
about MI. Figure 9 shows that mention of MI-related terms (eg, AI, ML and data
science) in investor annual reports has risen significantly, from two citations in 2015
to 116 in 2019. The growth in Insurtechs using AI/ML technology and MI-related
patents filed by insurers in recent years mirrors this trend.

Figure 9
MI-related mentions in insurer investor reports (LHS), and trend in Insurtechs using AL/ML technology
150

10%

100

120

8%

80

90

6%

60

60

4%

40

30

2%

20

0

0

0
2015

2016

2017

2018

Usage of the terms “ML”, “AI” or “data science”
Ratio of usage of “AI” to the term “technology”

2019

2012 2013 2014 2015 2016 2017 2018 2019
Note: Insurtech start-ups with MI-related terms used in the
CB Insights company description.

Source: Annual reports of 30 leading insurers, CB Insights, Swiss Re Institute

Strong growth in insurance-related AI/ML patent filings
MI-related patents filed by insurers
have increased exponentially in recent
years.

We analysed patent databases and found that the number of MI-related patents filed
by insurers has increased since 2010. Focusing on the most prolific patent filers
among insurers in the US, in 2018 and 2019 more than half were for motor business,
some in the area of autonomous vehicles. As MI-enabled processes enter businesscritical vehicle systems, it expands demands for greater innovation in MI-enabled
monitoring. For instance, many patent applications are for remote sensing, image
processing and drone use for damage assessment.

20 Machine Learning: Today and Tomorrow, Willis Towers Watson, 25 February 2020.

Swiss Re sigma No 5 /2020 13

Machine intelligence in insurance

Figure 10
Growth (LHS) and composition of patents (RHS, 2018) at insurers
800
693
600

Customer
service/VAS

400

11%

6%

225
12

25

38

82

95

92

121

system
Underwriting

2010 2011 2012 2013 2014 2015 2016 2017 2018
Number of filings

5%

Operations
Early warning

30%

11%

Claims

200
0

6%

38%

51%

23%

Motor
24%

Distribution

Multiline
Property

Note: by insurance value chain (outer circle) and
line of business (inner circle)

Source: Google Patent Database, Swiss Re Institute

Most recent innovations have sought
to improve customer service, claims
and operations.

In terms of use cases, most MI patents in insurance have been for functionality
designed to improve customer service, claims efficiency and reduce losses. Some
were aimed at generating early warning signals, such as to alert drivers to pedestrian
or cyclist presence, or to alert vehicle operators of malfunction. Both reduce claims
frequency as well as severity. Comparing China and the US, patent filing
concentration is high in both markets, according to the Google patent database.
However, the number of MI-related patents filed between 2010 and 2019 was more
evenly distributed in the US, with 10 insurers accounting for 80% of activity. In
China, less than five insurers accounted for 85% of MI-related patents filed.

Already-live AI and ML application deployment in insurance
The use of some conventional MI tools
is standard in insurance.

Conventional MI such as generalized linear models have become standard tools in
insurance for risk assessment and prediction models. More recently, enthusiasm for
a range of AI and ML techniques such as deep and reinforcement learning has led
some insurers to run pilots. In a few cases, early adopters of AI and ML are seeing
benefits in select areas, such as faster claims settlement, more targeted cross- and
up-selling, improved fraud detection and better risk scoring.
̤̤ Modernising claims analytics: Much of claims processing is still manual. A
number of insurers now have pilots in triaging, routing, validating and
corresponding with third parties, the ambition being that some degree of
automation will materially reduce the cost of claims processing.21 Simpler tasks
like assessing high-volume losses and processing well-specified items are more
likely to be successfully executed by MI-enabled systems. Areas where insurers
report higher savings from MI include those where information is better
structured, such as documentation in standardised formats.
̤̤ Fraud detection and claims mitigation: ML techniques are well suited to use
cases involving large classification of data and anomaly detection, such as fraud
detection. Increasingly, insurers are evaluating and deploying ML-based fraud
solutions that augment internal data with new sources of information, including
third-party IoT and public data. Insurers are also using ML to create entirely new
loss mitigation offerings, which can in turn lead to lower claims. Such is the
thinking behind Direct Line's telematics programme, for example, which uses ML
to identify individuals who need coaching to become better drivers.22
21 “The challenge of full automation”, insuranceinsider.com, 2 April 2020.
22 Direct Line Group saves young drivers over £50 million in motor premiums, Direct Line, 1 Feb 2019.

14

Swiss Re sigma No 5 /2020

̤̤ Distribution channel optimisation: Another area where ML is seeing application
is in agent recruitment and retention. Insurers have started using ML-enabled
systems to identify individuals most likely to become successful producers. These
systems can also improve producer-client matching. For example, Discovery does
real-time, automatic matching of call centre agents to members with whom they
are likely to have the highest affinity. The model has been operational since 2018
and customers on calls where affinity was matched reported greater
satisfaction.23
̤̤ MI in customer experience: MI has been deployed at enterprise scale in many
social media and online retail contexts. Some insurers have sought to do the
same, with the ambition to increase the effectiveness of targeted marketing.
Despite early successes, insurers discovered that rushing out MI-inspired
initiatives may not necessarily generate the desired outcome. For instance,
anecdotal evidence suggests that targeted digital advertising based on previous
interaction with a product can actually turn a customer off. This result suggests
that MI models could benefit by using insights gained from behavioural
economics to disentangle interaction effects.
̤̤ Underwriting: Given the level of confidence needed to deploy new technologies
in underwriting, fully AI and ML-enabled underwriting systems still do not exhibit
levels of accuracy necessary to be used at scale. This also means that MI cannot
be relied on to completely replace risk assessments, except in simpler lines. This
said, some examples related to supervised learning, can complement and or
eventually replace parts of existing processes in insurance. These include smarter
mechanisms for triage and routing, which may be more effective than current
business rules, eg, triage between depths of investigation (full vs. simplified
underwriting), safely waive additional evidence (lab tests, physician statements)
or allocate referrals to the right level of seniority in the organisation (junior
underwriter vs. medical officer).24
̤̤ Pricing: This is subject to regulatory approval, and the traditional approach
involves fitting a GLM to historical claims and premiums. More accurate pricing
models based on newer machine learning techniques cannot be put into
production immediately, as results may be difficult to explain both internally and
externally to regulators. There may also be other constraints to using the data like
cost and lack of access to data.

Inadequacies in existing implementations
Challenges to scale use of newer MI
tools remain.

The challenge to scale AI and ML models continues to hinder deployment of newer
MI technologies at the enterprise level across core workflows in the insurance value
chain. The following are processes where MI could potentially be implemented at
scale and the associated still-existing obstacles that hold back broader adoption:
1. C
 ollecting and curating relevant structured and unstructured data. Here
obstacles include data privacy regulations and incentives (eg, firms or agents
unwilling to share relevant data), fragmented access processes, inadequate data
usage contracts, and still-difficult to systematise data curation processes.
2. A
 ssessing, understanding, and processing relevant input information. NLP
techniques are still inadequate given the difficulties in interpreting tacit and
subtle informational aspects, and data quantity and quality remain poor.

23 Insurance trend #1: Get to know me, EFMA, 14 November 2019.
24 What's new? The next wave of insurance automation complemented with new technologies, Swiss Re,
25 November 2019.

Swiss Re sigma No 5 /2020 15

Machine intelligence in insurance

3. U
 nderwriting approval and pricing: Intelligent automation integrating humans
and machines is still a massive design challenge. Seasoned executives and
underwriters do not trust algorithms given examples of “obvious” misses.
4. M
 onitoring risk portfolios and managing claims: Challenges to efficiency
improvement remain in terms of creating lower-cost systems that have better
false positive/false negative trade-offs than human-centric methods. Data
processing architectures still treat data in “pools” rather than the “rivers”
necessary to accelerate time between data collection and usage. Data not
transitioned into actionable insights near immediately hinder MI-enabled system
usefulness.
5. Improving capital allocation across liability segments: Prediction models still fall
short in terms of reliably supporting better capital allocation. Data are incomplete
and biased in many liability segments, and systems are still designed around
current processes that are not MI-ready.

Poor MI integration hinders system deployment potential
Poor integration of MI-enabled
systems across processes can hurt
project outcomes.

System design and management often fall short when insurers attempt to implement
MI into existing cross-functional processes. Too few resources are dedicated to
integrating models and algorithms into workflows, leading to poor cross-functional
coordination. In an interview with Swiss Re Institute, one insurer seeking to eliminate
unnecessary underwriting questions said it leveraged banking transaction data to
offer accelerated underwriting to prospects. The MI-enabled underwriting model
performed well in classifying individuals into standard and sub-standard risks. The
marketing department, however, did not invest in a propensity-to-buy exercise nor
modify its sales process, which nullified the benefit of the system.

Data collected from IoT devices
currently have limited integration into
underwriting and pricing.

Another challenge is that new data (especially collected from wearables) for
underwriting and pricing purposes may not necessarily lead to more accuracy in
underwriting (See Figure 11). For example, tracking the number of steps one walks
may not materially improve one's health. In many cases, the outcome is the opposite:
an individual who walks more may also think he/she has license to eat more because
he/she is fitter. To this end, there has been a tendency to over-estimate the extent to
which collecting and crunching these data actually changes risk profiles. The
industry will struggle to adopt IoT data without a clearer understanding of how these
insights on behaviour correlate with actual risk experience.

16

Swiss Re sigma No 5 /2020

Figure 11
Degree of difficulty in incorporating new data sources into processes

Easily obtained-traditional
data
Data via application, FNOL,
medical info, motor vehicle
records

Now easier to obtain
digitally-traditional data
Tax filings, property
registrations, births & deaths,
crime records, government
data

New data sources
Electronic health records
(EHRs), third party data bases,
photos, images, voice,
data from wearables & IoT
devices, behavioural
insights, reviews

Predictive power

Established

Partially established

Exploratory phase

Integration challenges

Addressed

Being addressed

Less known

Acquisition cost

Low

Stabilising, was high

Still expensive

Regulatory barriers

Minimal

Moderate

High

Source: Swiss Re Institute

Recommendations for current-day MI initiatives
We expect that successful implementation of MI-enabled systems in end-to-end
processes will reap many productivity benefits for insurers, with the ultimate
outcome of boosting profitability. However, for many firms there is still a long way to
go to being fully “MI-ready”. This does not negate the positive benefit that existing,
often small-scale, MI projects can deliver. The following are some recommendations
to improve the likelihood of success in current initiatives.
Focus first on components of a
process that are amenable to MI.

Invest incrementally. Insurers should start with a focus on process steps amenable
to MI, rather than attempt large-scale transformations. Successful MI-enabled
system implementations should start with narrowly defined objectives and follow
clear milestones rather than aim for full automation. A number of processes - even in
higher-volume lines - can be too complex to fully automate. A good example is auto
insurance: one accident can include several smaller claims, each of a different type
(eg, bodily injury, vehicle damage, car rental) involving different parties and
suppliers, and therefore requiring expert human intervention.

Choose use cases that augment
employee effort.

MI can be deployed in functions with fewer regulatory restrictions. While
wholesale replacement of some insurance processes may require regulatory
approval, augmenting existing processes with selective MI is possible with few
regulatory restrictions. Important here is how an MI-enabled system is deployed.
Many MI deployments to augment human-centric processes fail by adding process
costs without improving overall efficiency or profitability. Involving staff in reengineering process discussions and introducing small deployment steps can be the
difference between successful and unsuccessful implementation.

Swiss Re sigma No 5 /2020 17

Machine intelligence in insurance

Use newer approaches like deep
learning to complement more
conventional techniques.

Combine new and conventional model approaches. Some newer MI (eg, deep
learning) methods can be used to supplement more conventional ones (eg, GLM).
AI or ML methods may improve data curation, facilitate better process design, and
address weaknesses in aspects of the conventional MI (eg, incorporate output from
unstructured data.) Insurers should use simple, interpretable models as a baseline for
AI or ML, especially in areas that are regulated. For example, a large US insurer
acknowledged that because the industry relies so heavily on GLMs, its experiments
with deep learning are still focused on developing a multi-variate rating plan. In this
case, the end result was to use deep learning to develop new insights; final
implementation incorporated these insights to improve the GLM process.25

Centres of excellence should foster
connections between centralised and
local teams.

Foster collaboration between centralised and distributed data science teams.
Best practice programs bring uniformity across divisions. At many insurers, if an
analytics team in one division builds a successful algorithm for a particular issue,
there is little structure to facilitate its adaptation in other divisions. Larger insurers like
QBE are building playbooks that all divisions can consider, including algorithms to
accelerate claims settlements, identify fraud, improve loss reserving, and suggest
when claims cases may become lawsuits.26

25 Trick or Treat? Application of Neural Networks in Insurance, KPMG, 10 January 2019.
26 “QBE, Unlocking the secrets to technological transformation”, Claims Magazine, April 2019,

18 Swiss Re sigma No 5 /2020

Progressing enterprise-scale deployment
With emerging understanding of how MI-enabled systems can improve data ingestion and curation, and augment
existing analyses, there has been growing recognition of the applicability of new approaches. These include, for
instance, hybrid physics/ML-based models and causal-inference algorithms to improve the predictive power of MI
systems. However, failure in enterprise scale MI-system deployment is more often due to larger organisation
constraining characteristics. To this end, insurers should focus more on trust, technology, talent and tenacity.

Innovation and new approaches
New and innovative MI can improve
existing approaches.

More advanced MI techniques often require more and better data, and more
compute power. The lack of one or both can hold back deployment of MI-enabled
systems at enterprise scale. Often the difficulties are specific to models or
algorithms, which in turn can (not always) be the reason for failed deployment.
Where improvement to data quality and/or compute power is challenging, an
alternative way to address model problems, which has been the focus of more recent
research, is to develop a new approach less sensitive to these issues. Examples are
reinforcement learning or ensemble modelling, such as hybrid physics-based and
ML models. Table 3 highlights exciting areas of innovation in MI that have the
potential to help overcome key problems in existing approaches.

Table 3
Schematic showing positive developments in MI
Key development

Challenge it is addressing

Combining physics-based
models with ML

Improve accuracy, interpretability of MI models, while improving predictive and exception-handling
capabilities of physics-based models. Strong applications in critical maintenance activities, early
warning systems, etc.

Progress around using ML for
causal inference

More informed decision-making with higher level of confidence. Better understanding of the impact of
interventions. Huge application in sensitive domains, like, healthcare, defence and even insurance.

Advances in visualization tools
for decision support

Improve interpretability and diagnosability of complex MI systems. Applications in NLP, image
processing, etc.

Better model interpretation
techniques

Improve interpretability of current black box MI techniques, while improving accuracy of more
interpretable but currently low accuracy techniques like CART.

Intelligent automation:
Re-designing workflows

Automated data curation, insight discovery and sharing. Model prototyping in production languages.
Potential to save significant time on development as well as ongoing maintenance.

Privacy-preserving analytics

Governments, corporations, academia all join hands to help improve weighting of model parameters and
thus the model performance without compromising on data privacy.

Source: Swiss Re Institute

Combining physics-based models with data-driven approaches
Data-driven AI and ML systems often
fail to incorporate physical and
scientific knowledge.

Purely data-driven AI and ML-enabled systems are not robust. As insurers move from
“detect and restore” to “predict and prevent”, they may find that data-driven AI and
ML-enabled systems for complex applications are not straightforward because they
fail to incorporate physical and scientific knowledge into learning and prediction.
Often available data are insufficient, noisy and/or biased, which makes it even more
important to compensate with theory-based models. Using (typically inadequate)
data with current AI and ML algorithms leads to inconsistent results, with the
outcome that trained models do not generalise well to out-of-sample testing.

The community is exploring the
continuum between physics-based
and ML models.

On the other hand, pure mathematical physics-based models may fail to capture the
full range of complex interactions characterised by physical systems of interest to
insurers (eg, climate, behaviour, urban resilience, health, etc.) To bridge this gap,
some insurers and technology developers are exploring hybrid physics- and AI/ML
algorithm-based models This hybridisation is called theory-based data science, or
physics-based ML, or ML that incorporates the laws of physics. Newer AI such as

Swiss Re sigma No 5 /2020 19

Progressing enterprise-scale deployment

reinforcement learning, GANs, neuromorphic computing, and agent-based
simulation techniques will further expand the possibilities at this hybrid intersection
of physics-based models and MI.
Figure 12
Dichotomy between theory-based models and data science models

High

High

Low

Low
to Medium

Low

Data
dependent

Medium
to High

Data
dependent

Current approach

Inputs

Predictive
power

Δʏ

An improved approach

Black-box
neural network (BBNN)
Low

ƒBBNN

Physics-guided
machine
learning

Use of data

Inputs

Scalability

Physics based models (PHY)

Robustness

Low

Datadriven MI
model

Interpretability

Use of theory

Physicsbased
model

Benefits of combining theory-based models and data science

High

Characteristics of physics-based
versus data-driven models

ƒPHY

ƒPNN

Δʏ

High

Note: PNN refers to physics-guided neural networks
Source: A. Karpatne et al., Theory-guided data science: A new paradigm for scientific discovery, Cornell University, 2016

Successful combination of the two
requires clear lines of communication.

Figure 12 is a two-dimensional view of the dichotomy between physics-based and
data science models. Science theory-based models (y-axis) can have knowledge
gaps with respect to certain processes that are either too complex to understand or
too difficult to observe directly. At the other end of the spectrum, data-driven models
(x-axis) use large volumes of data but are agnostic to underlying scientific theories.
A complementary approach can take advantage of the unique ability of ML to extract
patterns from data, while also benefiting from scientific knowledge.

This combination is being
experimented in areas such as
predicting breakdowns.

This combined approach is being experimented in areas such as predicting
breakdowns and remaining useful lifetime for industrial systems. These are areas
where physics-based models can be incomplete and data-driven models can be
hampered by poor representativeness of training data. Researchers use physicsbased performance models to infer unobservable model parameters related to
equipment health, which can be combined with sensor readings to generate a datadriven prediction model.27

Progress in combining causal-inference tools with MI
MI-enabled systems learn
connections, but typically cannot
reason cause and effect.

A fundamental assumption of classical statistics and ML is that the distribution of the
training data is the same as the distribution of the data in practice. This is often not
the case in real life as, for example, new regulation or any other intervention can
change the distribution of the data. A general property of causal models is that they
are robust to such changes and more interpretable.

27 M.A. Chao, C. Kulkarni, O. Fink, et.al. Fusing Physics-based and Deep Learning Models for Prognostics,
Cornell University, 2 March 2020.

20 Swiss Re sigma No 5 /2020

Three levels of causality: seeing, doing
and imagining.

Figure 13 shows three levels in the ladder of causality. Level 1 is associational, and
asks "how will seeing X change my belief in Y?" For instance, what does a particular
symptom tell me about the presence of a disease. Level 2 explores questions which
cannot be answered from past data alone. The questions address behaviour changes
in response to interventions ("what happens to Y if I do X?"). Level 3 involves
imagining, answering counterfactual queries like “what if I had acted differently?"

Causal inference facilitates more
adaptive intelligence…

Shifting to such a causal-inference paradigm creates more adaptive intelligence,
which aligns with a more precise definition of AI. Computer scientist Judea Pearl's
book from 2000 (Causality: Models, reasoning, and inference, Cambridge
University Press, 2013) and his more recent The Book of Why (Basic Books, 2018)
explore a collection of techniques that can be used in conjunction with various MI
techniques to extract causal connections in contrast to just identifying associations.
It is important to note that associations arise from almost all MI ranging from
conventional techniques to the newer AI and ML algorithms.

Figure 13
Levels of causality needed in insurance
Three levels of causality
Counterfactuals – imagining
Intervention – doing
Association – seeing

Eg, if I join a wellness program will it
reduce my risk of heart failure?

Eg, what does a symptom tell me about a
disease?

Estimates the effect if one performs an
action

Invokes statistical relationships defined by
the data

Ability to reason about causal structure of
the variables

Eg, what if I wasn’t a smoker?
Retrospective reasoning – ie, about
hypothetical situations
Counterfactual inference enables the
estimation of unobserved outcomes

Majority of ML methods

Source: Swiss Re Institute

… and can thereby improve the
predictive power of MI systems.

Causal inference arises from hypothesising causal relationships with a range of
drivers in the context of directed acyclic graphs (DAGs) based on the best available
scientific understanding.28 Then, different techniques can be used to “prune” the
graph to distinguish causal drivers from confounders. Combined with other MI
techniques, causal inference can be a powerful tool to improve the predictive power
of particular models and feed into more robust risk-management systems. This
combination follows nicely from hybridising physics-based models with newer MI
approaches as scientific theories provide guidance as to which variable interrelationships should be targeted in training, fitting, or estimating relevant MI models.

Using visualisation to generate actionable insight
Data visualisation tools help non- data
scientists understand the output from
MI-enabled systems.

Even if an enterprise successfully implements an MI-enabled system, the output
often remains restricted to discussion among the firm's data scientists. This limits the
system's influence. Inflexible decision-making processes and immature software
make it hard for data scientists to transform system output into actionable insights
that decision-makers can use. In a recent survey, more than 70% of US insurers said
they were concerned that non-data science staff did not understand AI and ML
28 The directed acyclic graph causal framework allows for the representation of causal and counterfactual
relations amongst variables.

Swiss Re sigma No 5 /2020 21

Progressing enterprise-scale deployment

outcomes.29 Insurers will need to develop more customised visualisation and
decision-support tools that work for their specific needs. One seemingly counter
intuitive recommendation is to include non-technical designers as part of the MIenabled system deployment team. These non-technicians should work with
executive decision makers from project beginning, not end. Many powerful systems
are not productively used because the output is confusing to decision makers.
Off-the-shelf business intelligence
tools now allow for custom visuals that
can be used to communicate model
findings.

Many new features were added to visualisation tools in 2019, based on well-known
JavaScript visualisation libraries such as D3, jQuery and R. Gartner predicts that by
2022, 40% of ML model development and scoring will take place in tools (eg,
business intelligence (BI tools) that do not have ML as their primary goal.30 For
instance, Microsoft has made it possible to integrate Python scripts within PowerBI,
its popular BI tool.31 AutoML is already available in visualisation and BI tools and
currently supports classification and regression models.32 There will likely be
additional model types in the future, and the ability to export ML models to
interactive computing environments like Jupyter Notebooks, facilitating model
refinement “on the fly.”

Progress in model explainability and interpretation techniques
Today there is more emphasis on
explainable AI and ML, especially for
techniques with higher accuracy.

As newer MI tools demonstrate productive potential in the enterprise context, more
emphasis is placed on “Explainable AI and ML”. That is, algorithms with higher
accuracy levels (relevant for specific business use cases) need more explanation
before they will be acceptable across a broader range of business contexts.
Successful enterprise-wide and decision-critical system deployments require
explainability and interpretability. The past years have seen progress in explaining
complex models, such as SHAP (Shapley Additive exPlanations) values and Local
Interpretable Model-Agnostic Explanations (LIME). Optimal classification trees are
also being proposed to improve accuracy while dealing with the problem of
interpretability (see Case study: Optimal Classification Trees). Whether these
approaches are sufficient for regulators and internal governance units at insurers is
still unclear.

Case study: Optimal Classification Trees
Decision Trees are interpretable but
may have lower accuracy.

Decision trees are highly interpretable and explainable to a non-technical audience.
However, such models may lack stability. A slight change in data can cause a large
change to tree structure, making them less appropriate for regulated areas like
insurance pricing. Another shortcoming is that every split in the tree is decided on a
standalone basis without considering the possible impact of future splits in the tree.
This can lead to trees that do not adequately capture underlying characteristics of
data sets, potentially leading to weak performance when classifying future data.

Optimal classification trees improve
accuracy, while maintaining
interpretability.

A helpful solution associated with a top-down approach is to create the tree in a
single step (ie, jointly decide all tree nodes). Each split is therefore determined with
complete information of all other tree splits. In 2017, Bertsimas and Dunn proposed
a technique called optimal classification trees to improve decision-tree accuracy.33
This technique uses mixed-integer programming (MIP) to learn optimal classification
trees. MIP comes with a suite of off-the-shelf solvers and algorithms that can be
leveraged to effectively prune-out the search space.

29
30
31
32

LexisNexis® Risk Solutions, op. cit.
Gartner Magic Quadrant for Analytics and Business Intelligence Platforms, Gartner, February 2020.
“A Tour of Artificial Intelligence Features in Power BI”, blue-granite.com, 5 December 2019.
AutoML is a ML capability that enables developers with limited ML expertise to train models specific to
their business needs.
33 D Bertimas, J. Dunn, “Optimal Classification Trees”, Machine Learning, vol 106, July 2017.

22 Swiss Re sigma No 5 /2020

Regression, CART

Interpretability

High

Figure 14
How techniques map with regard
to interpretability and accuracy.

Optimal
Classification Trees,
Optimal Classification
Trees – Hyperplane
Accuracy
High

Low

Low
Random Forest,
Gradient Boosting,
Deep Learning

Source: D. Bertsimas,J. Dunn, Machine Learning under a Modern Optimization Lens, Dynamic Ideas, 2019

Intelligent automation: re-designing workflows
It is now possible to model
prototyping in the same programming
language used in deployment.

The data science vendor space has matured to cater to both expert and citizen data
scientists to build, train, deploy and manage analytical models.34 MI techniques are
increasingly used to simplify analytical processes such as data preparation, insight
discovery and insight sharing. Newer AI and ML techniques still face the challenge
that language program prototypes cannot scale at an enterprise level, but we expect
that new developments could help overcome such obstacles. Model prototyping will
be possible in the same AI and ML-oriented languages that are used for industrial
grade deployment. For example, Amazon Sagemaker recently announced an open
source library and API to prototype deep learning models in Java.35 Internal
engineers now expect to save 30% in development time.36

Better tools can augment existing data
curation workflows.

Inadequate data curation workflows continue to materially hamper successful
deployment of enterprise MI-enabled systems. Fortunately, better tools are becoming
available to improve existing data curation. These platforms augment collecting,
labelling and feeding data into supervised learning models and standardised
workflows. More sophisticated libraries and software packages allow models that are
better able to generalise, meaning that a wider set of problems can be solved (eg,
Tensor Flow for ML models). Even with better tool availability, the feedback is mixed:
some platforms facilitate seamless integration across diverse tools, while others still
struggle with a plethora of tools that do not necessarily work together.

Privacy-preserving analytics
New protocols offer better data
privacy protection than standard
anonymisation techniques.

Given that MI-enabled system performance is often boosted with more data,
industry players would benefit if they were to share data. That said, standard
anonymisation protocols are not secure enough. New protocols are creating new
opportunities. Secure multi-party contribution protocols can unlock derived analytics
from non-public data across multiple insurance companies. These new protocols
facilitate a higher level of data privacy protection beyond what is typical for standard
anonymisation techniques. In this way, a consortium of insurers can contribute data
to generate derived analytics for MI applications that would benefit all contributors
(see Figure 15). The differential privacy techniques eliminate the possibility that even
34 Solution Criteria for Data Science and Machine Learning Platforms, Gartner, 6 September 2019.
35 Introducing Deep Java Library: Develop and deploy Machine Learning models in Java, Amazon Web
Services, 3 December 2019.
36 S. Sivasubramanian, Leadership session: Machine learning, Amazon AI Amazon Web Services,
December 2019.

Swiss Re sigma No 5 /2020

23

Progressing enterprise-scale deployment

the researchers and data engineers working on the pooled data can look back into
individual contributions while still facilitating more sophisticated MI.
Figure 15
Federated learning to achieve privacy preserving analytics
3

2

1

7

a) Train
model
locally with
local data
b) Calculate
loss
c) Backpropagate
gradients
locally

a) Send model
and initial parameters
b) Split data between nodes

Share updated
global model version
and parameters (weights)

Continue
federated training loop

Local data

Insurer 1
local
ML Model

Local data

Insurer 2
local
ML Model

Local data

Insurer 3
local
ML model

Edge/
federated
compute
service

Master
parameter
server

Share learned gradients with
master parameters server
(no transfer of local data)

Average the submitted
gradients from many
node/edge devices

Update
the master weights

4

5

6

Central compute
resources

Global model updates

Source: Swiss Re Institute

Master models leverage local data at
each insurer to help them learn from
each other without sharing data.

Figure 15 shows how once the model has been instantiated, the parameters and
weights are pushed out (in steps 1 and 2) through a web service to individual
insurers, which each run the global model on their local data (eg, claims records), to
find out how accurate the master model is (step 3). After this is completed, each
insurer (step 4) offers feedback and shares the findings or learned gradients (ie, what
is different between the two models). This feedback is combined across insurers,
and updated weights are submitted to the federated services, and reflected in the
global model (steps 5 and 6). The cycle can continue until a certain level of accuracy
is obtained in the master model.

Trust, technology, talent and tenacity
Often organisational constraints are
the reason for failure in the
deployment of MI-enabled systems.

Model-related problems are not the only reason for failure in deployment of MIenabled systems at enterprise scale. More often, organisational constraints such as
poor use case planning, lack of properly trained staff and poor communication are
the sources of failure. In the insurance sector, a change in mindset could help.
Insurers need to better understand the value that MI-enabled-systems can deliver
from an end-to-end, enterprise perspective. Small-scale pilot projects for emerging
technologies make sense as part of an initial R&D project or targeted assessments. A
tendency among insurers thereafter has been to launch enterprise-wide deployment
of the pilot, without due consideration of other non-technology design-related
issues.

Insurers should also focus on nonmodel issues holding back wider
adoption.

To successfully transform their enterprises with MI-enabled technology, we
recommend that insurers stop relying on proof-of-concept, small-scale pilots of
model/algorithm approaches. They also need to focus on the salient, non-model
characteristics of end-to-end enterprise deployment: trust, technology, talent and
tenacity (see Table 4).

24

Swiss Re sigma No 5 /2020

Table 4
Non-model considerations for enterprise-scale of MI-enabled systems
Key findings

Implications for the current model

Outlook

Trust: Develop an algorithmic risk and digital
ethics framework

Better equip MI-enabled systems against
risks, eg, adversarial attacks

Technology: Balance internal versus
external expertise

Understand how procuring MI differs from
traditional software to reduce risks and
maximise ROI
Identify how MI can complement current
actuarial-science- based approaches
Use sandbox approaches to test MI at scale

Balance different definitions of fairness and
incorporate self-monitoring into MI-enabled
systems from the design phase
Develop approaches to harmonize
fragmented technologies.

Talent: Develop talent and skills
Tenacity: Foster a dynamic tech-informed
culture; engage with regulators

Encourage all staff to learn new MI-related
tools and leverage citizen data scientists
Educate regulators. Keep humans in the
loop

Source: Swiss Re Institute

Trust: develop an algorithmic risk and digital ethics framework
Insurers must gain deeper
understanding of the consequences
MI may have on the services they
provide.

Life-altering decisions can be automated via algorithms, and embedded biases
within algorithms may often be inadequately monitored and documented. This can
result in liability for companies using decision-support algorithms that incorporate
bias (in most cases, unintentionally) should victims choose to litigate. Seven out of
10 US carriers are already concerned about bias in ML models.37 Even if an MIenabled-system outcome is solely or mostly responsible for undesired
consequences, “the algorithm did it” is not an acceptable excuse. In a survey carried
out in 2019, nearly 50% of firms using MI solutions across sectors said they have a
formalised framework to consider ethical use, bias risks, and trust implications; 25%
had created a senior management position specifically to ensure compliance.38

Existing frameworks were not
designed to govern behaviour by
large-scale algorithmic systems.

There is also scope for automated technology-based solutions that detect bias and
generate risk scores for algorithms, which allow insurers to assess the malpractice
risk of specific algorithms. Insurance solutions can be considered to protect
companies using such algorithms against liabilities resulting from embedded bias.
Insurers should have a stronger voice in the societal debate about questions of
fairness in algorithmic decisions and join forces with researchers to address these
issues (eg, 'FAT machine learning community39). In the last decade, academics have
published several definitions of fairness, not all of which can be achieved at the same
time. Since creating a generalised state of fairness is not feasible, insurers may need
to choose which conditions to keep and which to discard.

Technology: balance internal versus external expertise
With increasing commoditisation of
MI categories, insurers need a
detailed MI procurement and
knowledge-transfer strategy.

IT support for MI will be especially challenging as technology teams try to manage
the balance between: (1) running the business in the face of increasing requests for
various IT services, along with; (2) innovation and research. More than half (59%) of
CIOs and IT decision makers surveyed recently were unable to deliver on all their
projects in 2019, creating a backlog for 2020.40 As the range of MI-related offerings
continues to grow, IT units will need to modify procurement approaches designed for
buying traditional software to reflect MI procurement. For example, insurers may
need to restrict agreement terms to shorter period (eg, no more than three years to
protect from lock-in.41) There will need to be more emphasis on tool/system flexibility
37 LexisNexis® Risk Solutions, op. cit.
38 Global survey of 2 473 firms that use AI solutions. IDC Survey Finds Artificial Intelligence to be a
Priority for Organizations But Few Have Implemented an Enterprise-Wide Strategy, IDC, 8 July 2019.
39 Fairness, Accountability, and Transparency in Machine Learning, FAT/ML, see https://www.fatml.org
40 New report shows 3 out of 4 organizations expect negative revenue impact if they don't digitally
transform in next 12 months, MuleSoft, 13 February 2020.
41 Lack of Focus on AI Licensing Will Result in Higher Costs, Risks and Long-Term Headaches, Gartner,
11 September 2019.

Swiss Re sigma No 5 /2020 25

Progressing enterprise-scale deployment

and interoperability. Insurers will also need to create and execute knowledge-transfer
plans to ensure continuity between external providers and their own staff, both in IT
and the business.42

Talent: develop talent and skills
Attracting and retaining people with
MI skills remains a major challenge for
insurers.

Lack of sufficient staff to analyse data is among the three biggest challenges
preventing insurers from becoming more data-driven, according to a recent survey.43
Awareness of MI is growing among actuaries. MI is endorsed at prominent actuarial
conventions, with papers on how actuarial science can incorporate deep learning in
areas such as mortality modelling, claims reserving, telematics analysis and non-life
pricing. Still, retaining MI talent remains a challenge. Insurers invest in skills
development programmes for employees, but many struggle to create near-term
opportunities and incentives to apply MI in a way that interests skilled MI developers
and data scientists. About one-sixth of respondents in a survey cited difficulty in
hiring and retaining people with AI skills as a significant barrier to broader AI
adoption in their organisation.44

Tenacity: a dynamic, tech-informed culture and engage with
regulators
Involving the business and executives
throughout the MI development
lifecycle is key.

Beyond investing in foundational MI-enabled capabilities, insurers must focus on
high-level business workflows and opportunities productively transformed by these
new technologies. In recent years, many insurers have funded proofs of concept and
pilots in the MI space. These efforts provide preliminary guidance but do not
transform business. Going forward, key project components for making MI-enabled
systems productively transformative go well beyond the technology. They include
enterprise technology architecture design, business workflow re-engineering, cocreation with executives data on visualisation, and extensive change management
programs. Having business people involved throughout the identification, testing,
evaluation, and implementation process is key to achieving success.

Some issues require more discussion,
such as complying with regulatory
requirements.

Regulatory risks regarding tech-linked innovation in insurance present challenging
hurdles. The risks mostly centre on questions of data management and use. The
General Data Protection Regulation (GDPR) in Europe emphasises important
questions for managing data privacy, which is particularly relevant for MI-enabled
systems, which often merge and mix different data sources for risk assessment.
Some issues require more development and discussion such as complying with
GDPR principles focused on “use for legitimate purposes only” and conditions for use
in “high risk” cases (eg, medical and health, profiling).

Sandbox approaches could help
overcome barriers to adoption of
enterprise-scale MI in insurance.

Further, restrictions on cross-border data transfers can also impede development
and application of cross-border solutions, and slow regulatory approval of new tech
components like cloud solutions. Given the complicated and subtle nature of many
MI-enabled solutions, inadequate understanding of MI possibilities and drawbacks
could slow industry adoption. More sandbox efforts – particularly experiments at
enterprise scale – are required to overcome regulatory barriers and foster a deeper
understanding related to data privacy management and MI capabilities among
regulators and insurance executives.

42 Gartner, op. cit. 9 February 2018.
43 Willis Towers Watson, 25 February 2020, op. cit.
44 AI adoption in the enterprise 2020, O’Reilly, 18 March 2020.

26 Swiss Re sigma No 5 /2020

Conclusion
Insurers must shift focus from
technology development to enterprise
transformation to realise the potential
of MI-enabled systems.

Despite significant advances in MI-enabled image recognition and customer
analytics for example, productive, enterprise-scale transformation based on MIenabled systems in the insurance sector has proven elusive. Some trends have a
long arc and will most likely continue current trajectories such as integrating
computer vision into underwriting systems; other shorter-term trends like semiautomating fragmented data curation systems could change quite quickly.

Investments in data collection and
curation capabilities will be a key
differentiating factor.

Data have become paramount in any strategy to fully exploit the potential of MI in
insurance. While longer time series of structured data and efforts to find novel data
continue to be an important component of this narrative, unstructured data (eg, text,
audio, and video) have become a new opportunity not yet fully exploited.
Incumbents with proper tools and organisation will differentiate themselves as better
curated and novel data become a component of their competitive edge.

Figuring out which specific tools are
realistic and deserve investment is
also critical.

Newer MI in the AI and ML spaces are among the over-hyped technology areas that
have yet to be implemented in a materially profitable and transformative way within
the insurance value chain. For example, chatbots powered with the best in natural
language processing are still rolled out as the solution to confusing menu systems
and as a tool to reduce the size of call centres. The predictions for customer support
transformation were wildly unrealistic. This said, the collection of tools available to
insurers will continue to evolve. Much work remains in determining the specific tools
to use in these spaces.

Insurers should accept that projectcompletion timelines will be longer
than many expect.

An important consideration is the difference between targeted proof-of-concept
value and successful enterprise deployment. Insurers and their technology partners
will benefit from more investment and experimentation with MI at the enterprise
scale. Many insurers already experiment with MI in narrow contexts. The failures to
date result from the inability to scale profitably these narrowly focused experimental
projects. Everyone involved in enterprise MI deployment should accept that projectcompletion timelines will continue to be much longer than most executives expect.

Effective MI deployment will rely on a
range of factors including cultural and
regional attitudes towards privacy and
regulation.

Regulatory compliance will continue to be a critical component of any strategy to
leverage data and digital tools. One area in this context that will be particularly
onerous for any firm expanding its use of data, particularly in the area of
personalisation and customisation, is data privacy. New regulations will continue to
come at a fast and furious pace, furthering the advantage of large insurers already
equipped to manage compliance. Cultural norms, attitudes with respect to data
privacy, and regulation differ substantially across regions. Multi-national insurers that
robustly address this heterogeneous, and often fragmented regulatory landscape in
their MI implementations will differentiate themselves from competitors.

Building “trust” will encompass how
data are managed, and how customer
needs are met.

MI-enabled systems have profitably transformed other industries. This promise
continues to drive MI-related investments in the insurance industry. Executives,
technology architects, project managers, and analysts must shift their focus from
technology development to enterprise transformation to realise this business-value
potential. Key success factors include building trust with clients and regulators,
implementing enterprise-oriented technology, fostering cultures that retain suitable
MI-trained talent, and engendering hurdle-clearing tenacity.

Swiss Re sigma No 5 /2020 27

Recent sigma publications
2020

No 1
No 2
No 3
No 4
No 5

2019

Data-driven insurance: ready for the next frontier?
Natural catastrophes in times of economic accumulation and climate change
Power up: investing in infrastructure to drive sustainable growth in emerging markets
World insurance: riding out the 2020 pandemic storm
Machine intelligence in insurance: insights for end-to-end enterprise transformation

No 1
No 2
			
No 3
		
No 4
No 5
No 6

Emerging markets: the silver lining amid a challenging outlook
Natural catastrophes and man-made disasters in 2018: “secondary” perils
on the frontline
World insurance: the great pivot east continues
Advanced analytics: unlocking new frontiers in P&C insurance
Indexing resilience: a primer for insurance markets and economies
Global economic and insurance outlook 2020/21

2018
No 1
			
No 2
No 3
No 4
No 5
No 6

Natural catastrophes and man-made disasters in 2017:
a year of record-breaking losses
Constructing the future: recent developments in engineering insurance
World insurance in 2017: solid, but mature life markets weigh on growth
Profitability in non-life insurance: mind the gap
Global economic and insurance outlook 2020
Mortality improvement: understanding the past and framing the future

2017

No 1
No 2
			
No 3
No 4
No 5
No 6
2016

No 1
No 2
No 3
No 4
No 5

2015

No 1
No 2
			
No 3
No 4
No 5
No 6

Cyber: getting to grips with a complex risk
Natural catastrophes and man-made disasters in 2016:
a year of widespread damages
World insurance in 2016: the China growth engine steams ahead
Insurance: adding value to development in emerging markets
Commercial insurance: expanding the scope of insurability
Life in-force management: improving consumer value and long-term profitability
Natural catastrophes and man-made disasters in 2015:
Asia suffers substantial losses
Insuring the frontier markets
World insurance 2015: steady growth amid regional disparities
Mutual insurance in the 21st century: back to the future?
Strategic reinsurance and insurance: the increasing trend of customised solutions
Keeping healthy in emerging markets: insurance can help
Natural catastrophes and man-made disasters in 2014:
convective and winter stormss generate most losses
M & A in insurance: start of a new wave?
World insurance in 2014: back to life
Underinsurance of property risks: closing the gap
Life insurance in the digital age: fundamental transformation ahead

2014

No 1	Natural catastrophes and man-made disasters in 2013:
large losses from floods and hail; Haiyan hits the Philippines
No 2 Digital distribution in insurance: a quiet revolution
No 3 World insurance in 2013: steering towards recovery
No 4 Liability claims trends: emerging risks and rebounding economic drivers
No 5 How will we care? Finding sustainable long-term care solutions for an ageing world

2013

No 1	Partnering for food security in emerging markets
No 2 Natural catastrophes and man-made disasters in 2012:
A year of extreme weather events in the US
No 3 World insurance 2012: Progressing on the long and winding road to recovery
No 4 Navigating recent developments in marine and airline insurance
No 5 Urbanisation in emerging markets: boon and bane for insurers
No 6 Life insurance: focusing on the consumer

28 Swiss Re sigma No 5 /2020

Published by
Swiss Re Institute
Swiss Re Management Ltd
Mythenquai 50/60
P.O. Box
8022 Zurich
Switzerland
Telephone
Email

+41 43 285 2551

institute@swissre.com

Authors
Jonathan Anchen
Dr Jeffrey Bohn
Rajeev Sharan

sigma editor
Paul Ronke

Explore and visualize sigma data on natural catastrophes and the world
insurance markets at www.sigma-explorer.com
© 2020 Swiss Re. All rights reserved.
The editorial deadline for this study was 30 June 2020.
sigma is available in English original language and Chinese.

Managing editors
Dr Jeffrey Bohn
Chief Research and Innovation Officer

sigma is available on Swiss Reʼs website: www.swissre.com/sigma
The internet version may contain slightly updated information.

Dr Jerome Jean Haegeli
Swiss Re Group Chief Economist

Translations:
Spanish: Traductores Asociados Valencia S.L.
Graphic design and production:
Corporate Real Estate & Logistics / Media Production, Zurich

The authors thank Evangelos Avramakis,
Luca Baldassarre, Binay Biswal, Aakash Kiran
Raverkar and Jürg Schelldorfer, and also Julia
Brandenberg, Katherine Chen, Charlie Dang,
Ashish Dave, Mustafa Dinani, Yannick Even,
Nate Jensen, Nuno Mesquita, Sudipto Pal,
Ashok Shetty, Charilaos Tsarouchas,
Francesca Volpe, Guan Wang, Kelvyn Young,
colleagues from Swiss Re Client
Management and Solutions, and Gianluca
Antonini from IBM, for reviewing and
providing ideas to improve this publication.

Printing: Multicolor Print AG, Baar

The entire content of this sigma edition is subject to copyright with all
rights reserved. The information may be used for private or internal
purposes, provided that any copyright or other proprietary notices are not
removed. Electronic reuse of the data published in sigma is prohibited.
Reproduction in whole or in part or use for any public purpose is
permitted only with the prior written approval of Swiss Re Institute and if
the source reference “Swiss Re Institute, sigma 5/2020” is indicated.
Courtesy copies are appreciated.
Although all the information used in this study was taken from reliable
sources, Swiss Re does not accept any responsibility for the accuracy or
comprehensiveness of the information given or forward looking
statements made. The information provided and forward-looking
statements made are for informational purposes only and in no way
constitute or should be taken to reflect Swiss Reʼs position, in particular in
relation to any ongoing or future dispute. In no event shall Swiss Re be
liable for any loss or damage arising in connection with the use of this
information and readers are cautioned not to place undue reliance on
forward-looking statements. Swiss Re undertakes no obligation to
publicly revise or update any forward-looking statements, whether as a
result of new information, future events or otherwise.
Order no: 270_0520_EN

Swiss Re Management Ltd.
Swiss Re Institute
Mythenquai 50 /60
P.O. Box
8022 Zurich
Switzerland
Telephone + 41 43 285 2551
swissre.com/institute

No 1 /2020

Data-driven insurance:
ready for the next frontier?

01 Executive summary
02	The battle for customer
touchpoints
15 The digital insurer
23	Marketing, distribution
and servicing
28	Product development
and underwriting
38 Claims management
43 Outlook
47 Conclusion

Executive summary
Digital technology will direct change
across the insurance value chain.

The rapid spread of internet-enabled devices and universal connectivity has
changed consumer behaviours and expectations across all industries, particularly
among the younger generations. The digital age has also brought an explosion of
heterogeneous data from different sources and platforms, which providers of risk
protection solutions can use to broaden the reach and boundaries of insurability. As
their value chain becomes more digitally connected, insurers will be able to better
understand customer segments and partners and adapt in near real-time.

Insurers will become hyper-aware of
customer needs and preferences...

In the short-term the digital insurance consumers will likely be young, educated and
with higher levels of income. Over time, innovative digital cover options for all
consumers will become increasingly available, making income and education less
relevant factors in purchasing decisions. With advanced analytics capabilities, the
insurer of the future will be very aware of customer needs and preferences, and
provide personalised and real-time service, with flexible product offerings. Artificial
intelligence (AI) will be used to interact and build understanding of the customer,
and servicing will be through (personal) virtual assistance, 24/7.

...and to meet customer expectations,
they will need to adapt to provide
coverage across life-cycle stages.

Insurers will be able to move away from a product-focused sales approach to a one
closely tied to the broader needs across the life time of a customer, with greater
focus on the human experience. New generations of systems will deliver
unprecedented levels of proximity and influence on customers, and insurers will go
beyond mere channel management to optimising interaction across a diverse range
of customer touchpoints. This will have implications for business models, how
insurers interact with their customers, and the nature of the services they provide.

Digitalisation will also help create new
underwriting and portfolio risk
management techniques.

Over time, Big Data and sophisticated models will allow risk pricing at increasingly
granular level. Emergence of new risks will create new underwriting and portfolio
risk management techniques. Insurers will create early-warning systems to gather
practical insights that prevent incidents to simplify and accelerate claims processing.
Data-enabled processes will minimise friction and streamline the customer
insurance journey, from request for coverage to claim. Digitalisation will thus help
improve the customer experience and also the efficacy of back office processes.

At the same time, using data from
multiple providers could drive new
business models...

Further, digitalisation will enable development of new data-driven business models
impacting the entire insurance value chain. Access to data and the capability to
model risks will be key. True leverage will come from utilising other assets, such as
key data supplier partners, entry points to ecosystems, and the know how to
generate customer insights. Insurers will need to decide whether to be suppliers of
coverage, or to collaborate with and/or own new areas of business operation.

...and value propositions in insurance.

Technology could foster just ongoing incremental industry change by broadening
the scope and affordability of insurance. Alternatively, potential for more radical
transformation in the provision of risk protection services to households and
businesses is up for grabs if some typical hurdles to innovation can be overcome.

Swiss Re sigma No 1/2020 1

The battle for customer touchpoints
Digital customers expect different levels of service. What is second nature to digital natives presents a new world of
opportunity for incumbent insurers still operating paper systems. Combining digital solutions with advanced analytics
will deliver new insights, allowing insurers to provide deep, holistic engagement across customer lifetimes.

Digital interaction: it’s life
Digital technology has already
transformed many industries…

Digital interaction has become a norm of daily life. Across the world, of the
approximately 4 billion people connected online, more than 90% use mobile devices
and spend several hours in the virtual world each day.1 New technologies and digital
facilities are being developed for consumers, including, for example, location and
other sensor-based services (for smart homes, cars and factories). Developments in
cognitive systems and artificial intelligence (AI) are also creating innovation
opportunities (see Figure 1), including for insurers.

Figure 1
New technologies impacting insurance
Impact on business
Revolutionary

High

Medium

● Digital business
technology platforms
● IoT platforms
● Digital experience
platforms

● Intelligent process
automation

● Next generation
personal health
records (PHR)

● Conversational
platforms
● Behavioural analytics
● Electronic health
Record data extraction
● Genomics and
epigenetics
● Administration/
management SaaS

● Digitally-engineered
underwriting
● Autonomous vehicles

● Digital advisors
● Advanced analytics
solutions

● Insurance wallets
● Reward and loyalty
platforms

0 to 2 years

● P&C related technologies
● L&H related technologies

● Full life-cycle
API management

2 to 5 years

● Sensing & analytics related technologies
● Customer interaction related technologies

5 to 10 years

10+ years
Time to impact

● Productivity related technologies
● Customer loyalty/wallet related technologies

Source: Swiss Re Institute, adapted from Gartner’s Hype Cycle for Digital and P&C Insurance, 2019

1

2

Swiss Re sigma No 1/2020

Connected Commerce: Connectivity is Enabling Lifestyle Evolution, Nielsen,19 November 2018.

New technologies are impacting engagement and insight generation
… and will change how insurers
acquire and service customers.

Figure 2
Forecast growth of digital data

The rapid spread of internet-enabled wearable devices and ubiquitous connectivity
are enabling new ways of communication and information sharing. This has led to
exponential growth in the volume of digital data being generated automatically,
cheaply and non-intrusively. International Data Corp estimates that there will be 41.6
billion IoT devices around the world by 2025, each generating data (see Figure 2).2
New tools to analyse the data and extract useful insights are also proliferating, and
will change the way insurers interact with consumers.

200
CAGR (2019–25)
Real-time data 39%
Non-real time data:24%

150

100

50

0
2010

2012

2014

Real time (zettabytes)

2016

2018

2020

2022

2024

Non real time (zettabytes)

Source: IDC, Swiss Re Institute

The task facing insurers is to upgrade
the online experience for their
customers.

Consumer preferences and buying behaviours are also changing rapidly, and many
industries have adopted more customer-centric business models. Insurers will have
to work hard to keep customers loyal. A recent survey found that while insurers
remain the most trusted source for new risk coverage solutions, consumers are
switching carriers more often than they used to, and are more open to new entrants,
including from InsurTech and Big Tech.3

2
3

The Growth in Connected IoT Devices Is Expected to Generate 79.4ZB of Data in 2025, International
Data Corporation, 18 June 2019.
Insurers: How to Lead in the New Era of Connectivity, Bain and Company, 26 September 2019.

Swiss Re sigma No 1/2020 3

The battle for customer touchpoints

Digital insights into life events, new insurance opportunities
Insights from data analytics can boost
customer retention...

Figure 3 shows the cycles of customer acquisition, and also what is involved in
customer retention. Digitalisation can support these cycles by capturing insights
from the end-to-end consumer experience.

Figure 3
Customer acquisition and
loyalty lifecycle

Prospect customer experience
Finding “good” customers

Existing customer experience
Maximising customer life time value

Advise

Advise
Purchase

Purchase/
use

Evaluation

e

ienc

Trigger
ie, service use

Ba
expde
r

Trigger
ie, life event

ive
sit ce
Po rien
pe
ex

Consideration

Decision
ie, upselling, service
use, etc
N
ex ega
pe tiv
rie e
nc
e

Decision
buy/not buy

Evaluation

Research

Exit

Consideration

Research

Customer acquisition cycle

Customer servicing cycle

Evaluating other offers –
competitors have to put very
little effort to gain attention

Continuous engagement –
Customer pays very little attention
to competitors offerings. Improves
customer life time value

Source: Swiss Re Institute

... by triggering actions that enable
new sales or servicing opportunities.

4

Swiss Re sigma No 1/2020

Sources of digital data can also provide information on what has changed in a
customer’s life (eg, family, location or job change, see Figure 4). This helps insurers
better understand life events, knowledge they can use to develop personalised
marketing strategies and guidance on next best actions (both predictive and
prescriptive) for individual customers. This can entail cross-selling of other risk
mitigation and value-added services, in addition to traditional insurance cover.

Figure 4
Access to life events enabled by digital interactions
4) Predictive: The aim of
predictive analytics is to detect
problems before they occur.

Data
sources

5) Prescriptive: Prescriptive
analytics takes predictive analytics
one step further by offering specific
and actionable next steps on how
to solve the issues brought up in
the predictive data analysis.1

3) Identify and communicate
the next best action

6) Customer online interaction
2) Social media
interaction

Insurance event trigger

Insurance Prevention Value add
4)
Life event trigger

Familyrelated events

Life
related events

Education/jobrelated events

Free timerelated events

Mobilityrelated events
1

Love
Marriage
Pregnancy
1. Kid

2.Kid
Kids leaving
Separation
Death

First apartment
Same apartment
Relocation

Emigration
House purchase
House sale

Education
Re-skilling
(post graduate)
Job change

Self-employment
Workload
eduction
Retirement

Hobbies/interest
Travel
Media &
entertainment

Food/nutrition
Culture
Commerce

On-demand
mobility
Daily commute
Ride hailing

Car/bike/
purchase/sale
Business travel
On-demand
micromobility

1) Pregnancy
6)

5)

Source: https://www.proponent.com/predictive-analytics-vs-prescriptive-analytics/

Source: Swiss Re Institute

Swiss Re sigma No 1/2020 5

The battle for customer touchpoints

New actors, not insurers, own many
touchpoints.

However, these digital signals are not embedded in the environments that insurers
naturally access, not least because insurance is a low-touch business. For example, it
has been estimated that buyers of insurance in the US average just 2.7 information
interactions with their provider per year.4 Data on consumer actions is more readily
available from other agencies. For instance, data on mobility behaviour is owned by
mobility companies, not insurers. And operators of smart-home digital platforms
know more about insurers’ customers properties than insurers do. The more urban
the area of operation, the greater the likelihood that insights are generated by noninsurers. What insurers need to do is embed themselves as providers of risk
protection within the broader ecosystems of digital touch points with consumers.

Optimising interaction across multiple touchpoints
Insurers need to access both owned
and not-owned touchpoints.

To achieve this, insurers need to optimise their interaction with customers across
multiple providers and lifestyle areas like health, education, mobility and leisure. In a
single journey, customers switch touchpoints to fulfil various information needs.
Insurers need to access insights across the different types of touchpoints, both
owned and not owned (paid, earned and social, see arrows in Figure 5). Topperforming insurers will operate across touchpoints as a unified customer-facing
brand, with consistent messaging and experience to provoke recall.

Figure 5
Dynamic and personalised protection enabled by digital-touchpoint insights and digitally-augmented assistants

Sensors and data sources
Customer journey

Context & relevance

Customer lifestyle

Digital augmented
interface

Health

Lifestyle clusters

Education/working

Digital augmented
assistant

Living

Dynamic, personalised,
and augmented
protection

Free time
Consumers touchpoints:
Owned

Mobility
Risk, asset & liability
management

Consumers touchpoints:
Not owned (paid,
earned, social)

Source: Swiss Re Institute
4

6

Swiss Re sigma No 1/2020

Customers Know What They Want. Are Insurers Listening? Bain and Company, 10 October 2018.

̤̤ Owned touchpoints: The insurer invests and controls these consumer
touchpoints. They include agents, websites, apps. The goal is long-term
relationship building; the benefits are cost efficiency and control.
̤̤ Paid touchpoints: The insurer pays for the digital touchpoint to reach the
customer, such as through paid searches and sponsorships. Paid touchpoints can
include various channels like bancassurance and agents.
̤̤ Earned touchpoints: Here the customer becomes the touchpoint. This is when
satisfied customers become social media influencers, and spread awareness.
̤̤ Social touchpoints: An insurer interacts via third party channels but uses its own
profile, such as on Facebook or Twitter. Social media is forecast to be the fastestgrowing digital advertising channel globally over the next five years.5

The changing nature of insurance touchpoints
With the growth of multiple touchpoints, industry susceptibility to disruption is rising
(see Figure 6). We expect insurer interactions will – necessarily – increasingly begin
to cross conventional boundaries. Big Tech and digital ecosystems (eg, Grab, GoJek,
ecommerce marketplaces) are changing the touchpoints in the insurance context.
For example, in the UK four of the leading insurance providers are retailers selling
“white-label” insurance.6 Some insurers will orchestrate digital and physical
ecosystems by combining, in one offer, services that are normally delivered by
different providers.
Figure 6
Speed of disruption across sectors: insurance to be more disrupted and volatile
Current level of disruption vs. susceptibility to future disruption
18 industry sectors

Current level of disruption
0–1 scale

Viability

Volatility
More disrupted

Comms & media
Infrastructure & transposrtation
services

Retail

Insurance

CG & S

High-tech
Software & platform

Banking
Life sciences

Travel
Automotive
Energy Utilities
Capital markets
Health

Chemicals

Natural resources
Industrial equipment & machinery

Less disrupted
Durability

Vulnerability

Less susceptible

Susceptibility to future disruption
0–1 scale
Less susceptible yet more disrupted

More susceptible
Less susceptible and less disrupted

Source: V. Savic and B. Moussavi, Good things come to those who don’t wait, Accenture, 2019, Swiss Re Institute

5
6

Social Will Be The Fastest-Growing Digital Advertising Channel Globally, Forrester, 15 August 2019.
Bain and Company, 10 October 2018, op. cit.

Swiss Re sigma No 1/2020 7

The battle for customer touchpoints

Front-runners are set to outperform.

An early-mover strategy into new technology could prove expensive in the medium
term, but deliver strong returns over the longer run. A BCG MIT survey found that
65% of executives across industries do not yet see value from AI investments made
in recent years.7 Activities like collecting and curating data, building a knowledge
base that is specific to the company, training systems, getting employees to
augment them and developing strong governance, takes years. These are rarely outof-the-box solutions that can be rapidly implemented. Figure 7 demonstrates one
future pay-back scenario: companies that invest early will burn cash for a few years,
but then begin to rapidly see benefits of accumulated learning and investments.
Those players adopting the “fast follower” strategy that has worked with other
technologies, could find themselves long-term laggards.8

Figure 7
Relative changes in cash flow by AI adoption cohort
140% % change per cohort, cumulative
120%

Front-runners
(absorb within 5–7 years)

100%
80%
60%
40%
20%

Followers
(absorb by 2030)

0%
–20%
–40%
2017

2020

2025

2030

Laggards
(do not absorb by 2030)

Source: Jacques Bughin et al., Notes from the AI frontier: Modeling the impact of AI on the world economy, McKinsey Global Institute,
2018, and Swiss Re Institute

7

8

8

Swiss Re sigma No 1/2020

MIT Sloan Management Review and Boston Consulting Group surveyed ~2,500 executives, See
Winning With AI: Pioneers Combine Strategy, Organizational Behavior, and Technology, MIT Sloan
Management Review, October 2019.
V. Mahidhar,T. H. Davenport, Why Companies That Wait to Adopt AI May Never Catch Up, Harvard
Business Review, 6 December 2018.

The digital consumer – today and the future
Millennials are the typical digital
insurance consumers of today.

Based on a sample from three key markets where digital insurance is making
headway– the US, Sweden and China – today’s digital consumers are typically
between 20 and 36 years old, affluent and educated.9,10,11,12,13 The digital generation
(Generation Y (born between the early 1980s and late 1990s) and subsequent ones)
expect rapid access to information, and clarity around the value proposition.
Proactive servicing is key to maintaining loyalty and to maximise customer lifetime
value among these clients.

Most have above-average incomes
and education levels.

Higher-income consumers can afford insurance. There is room for growth in lowerincome segments, but value propositions need to be adapted. In the sample markets,
younger consumers with higher incomes and education tend to make more online
purchases than those with low- and mid-level incomes, at least in China and the US.
In China, 44% of consumers with incomes in excess of RMB 20 000 use online tools
to buy health insurance (including medical expenditure and critical illness). Only 30%
of low-income consumers (< RMB 5 000) do so (see Figure 8). Along similar lines,
45% of consumers with doctorate degrees buy health insurance digitally compared
with 34% with technical/vocational training.14 The same can be seen in the US. For
example, the median income of consumers buying insurance from Haven Life is USD
80 000, and more than 80% have (at least) a college degree.15

Figure 8
Digital purchases of health insurance products across income and education levels in China, 2018
Income level
50%

Education level

% of consumers using digital purchase methods

40%
30%

50%

% of consumers using digital purchase methods

44%

41%

40%

40%

44%

45%

Master
degree

Doctorate/
professional
degree

34%

30%

30%

20%

20%

10%

10%

0%

0%
Low income
(<RMB 5 000)

Mid income
(RMB 5 001 – 20 000)

High income
(>RMB 20 000)

Technical/
vocational
training

Bachelor/
associate
degree

Source: Swiss Re Institute

9
10
11
12
13
14
15

At Avanza, a Swedish digital life insurer; most customers were between 20 to 30 years old. See
Annual Report 2016 – Sweden’s most satisfied savings customers 7 years in a row”, Avanza, 2016.
In China, the majority who signed up for Alipay’s digital critical illness product were born
post- 1990. See Online Insurance in China, General Reinsurance AG, 2017.
The majority of Haven Life customers are millennials (21 to 36). See Happy Birthday to us, Haven Life,
16 May 2018.
81% of customers at US insurer Lemonade are between 24 to 44. See S. Wininger, “Lemonade’s First
Quarter in Market”, lemonade.com, 18 January 2017.
The majority at ZhongAn, a digital P&C insurer in China, are aged 18 to 39. What can insurers learn
from insurtech giant Zhong An, Accenture, 15 January 2019.
Chinese non-life personal lines: consumer perspectives, Swiss Re Institute, 2019.
Haven Life, 16 May 2018, op. cit.

Swiss Re sigma No 1/2020 9

The battle for customer touchpoints

Why do differences in online buying behaviour exist across markets?
The internet is the most used medium to research L&H insurance in China (59%)16
and Sweden (68%).17 However, the penetration of online insurance purchases is still
uneven in some countries. For example, online L&H insurance distribution accounts
for less than 8% in the US18,19 and China20 but already above 30% in Sweden.21 The
findings are similar for P&C insurance. In both Sweden and China, around 10% of
P&C premiums written were distributed online in 2016. 22, 23 In other markets, levels
of digital insurance purchases are much lower despite the internet being used by
many as a research tool. For example, in Spain, 52% use the internet to research L&H
insurance,24 but online penetration of life insurance is only around 0.02%,25 and only
2%26 of P&C premiums came from online channels in 2015 (see Figure 9). For some
commoditised lines such as motor, online purchases are higher: 25% of consumers
in the US,27 11% of consumers in China28 and 5% of consumers in Spain29 purchased
their motor insurance online in 2015/2016.

Many use the internet to research
insurance but penetration of online
purchases is low.

Figure 9
Online insurance purchases
Online distribution of life insurance in 2017
35%
30%

P&C online distribution in 2015/2016

% online distribution of annual
life insurance premiums written

12%

Online distribution of premiums written in %

32.4%

10%

25%

10%

10%

8%

20%

6%

15%

4%

10%
5%

<7.6%

6.5%

US

China

>1%

0%
Sweden

2.2%

2%

Spain

0%
Sweden

China

Spain

Source: Verdict, 2015; China Statistic Press, 2018; GenRe, 2017; Axco, 2019; Statista, 2019, Swiss Re Institute

16
17
18
19
20
21
22
23
24
25
26
27
28
29

10

Swiss Re sigma No 1/2020

Chinese non-life personal lines: Consumer perspectives, Swiss Re Institute, 2019.
European Insurance Report, Swiss Re, 2015.
Online distribution includes life, health and accident insurance.
United States Special InsurTech: Online Distribution, Statista, 2019.
Insurance in China, forty years of reform and opening, China Statistics Press, 2018.
Sweden Distribution Channels Life, axco.co.uk
Sweden Distribution Channels Non-life, axco.co.uk
“How Tencent and Ant Financial are rushing into China’s insurance industry”, cbinsights.com,
23 October 2017.
European Insurance Report, Swiss Re, 2015.
Spain Distribution Channels Life, axco.co.uk
Spain Distribution Channels Non-life, axco.co.uk
US Insurance Shopping Study, J.D. Power, 2016.
sigma 3/2017 – World insurance in 2016: The China growth engine steams ahead, Swiss Re Institute.
Ibid.

One reason is that availability of online
purchase tools is limited.

One reason for the low penetration of online life cover could be that many insurers do
not offer the tools to buy online. Looking at provider websites, we found that in
Spain, neither the biggest insurers nor InsurTechs sell life insurance online directly to
consumers. Similarly, in China and the US, very few large insurers sell life insurance
policies digitally.30, 31 However, in Sweden, six of the biggest financial institutions do
so, which could explain the higher purchase penetration there (32%). 32

Online purchases are higher in markets
with low power concentration,
high individualism, low uncertainty
avoidance…

Nevertheless, equal access to online purchasing facilities may not necessarily lead to
similar penetration across markets. Cultural characteristics based on the Hofstede
Cross Cultural Index33 and preferred leisure activities may also have an impact. This
comes through when comparing online insurance penetration to more general
e-commerce penetration. We use e-commerce buyer penetration as a proxy for
online insurance purchases because countries with a higher e-shopping penetration
also demonstrate higher online distribution of life insurance. One reason might be
that consumers who use the internet often for different purposes, including online
shopping, feel comfortable going online to buy insurance too.34, 35
̤̤ Country-specific cultural characteristics such as low “power distance”, defined as
the degree to which individuals expect and accept that power is distributed
unequally,36 high individualism and low uncertainty avoidance could influence the
adoption of technology.37, 38, 39, 40
̤̤ Countries with low power concentration have flatter hierarchies and people make
decisions by themselves, contrary to hierarchical societies where it is
unacceptable to disagree with decisions of a superior.41 Further, in individualistic
cultures people prefer to collect information on their own from direct sources such
as the internet, while in collectivist cultures people rely on subjective advice from
family and friends.42 And finally, in countries with lower uncertainty avoidance,
people feel comfortable trying new things, including new methods of buying.43, 44

30
31
32
33
34
35
36
37
38
39
40
41
42
43
44

The insurers in China are AnBong Life and PingAn; Insurtechs are Bihubao, HeTai and Xiaoyusan.
The US insurers include Massachusetts Mutual; Insurtechs are Ladder Life, Haven Life, Spot Life, Ethos.
The insurers in Sweden are Folksam, Scandia, Avanza, Nordea Liv, Nordnet and Idun Liv.
The Hofstede Cross Cultural Index analyses the national culture of a country based on six cultural
dimensions relative to other countries.
Bellman, S. et al., “Predictors of online buying behavior”, Communications of the ACM, 1992.
Lee, H. et al., “Consumer Lifestyles and Adoption of High-Technology Products: A Case of South Korea”,
Journal of International Consumer Marketing, Vol 21, 2009.
See Hofstede Cross Cultural Index at https://www.hofstede-insights.com/models/national-culture/
Matusitz, M. et al., “Power Distance, Uncertainty Avoidance, and Technology: Analyzing Hofstede’s
Dimensions and Human Development Indicators”, Journal of Technology in Human Services, 2013.
Özbilen, P., “The Impact of Natural Culture on New Technology Adoption by Firms: A Country Level
Analysis”, International Journal of Innovation and Technology, Vol 8, 2017.
Peterson, M. et al., “The Influence of National Culture in Information Technology Product Adoption”,
AMCIS 2003 Proceedings, Paper 119, 2003.
Lee, S. “The impact of cultural differences on technology adoption”, Journal of World Business, 2013.
Zakour, A., 2004, Cultural differences and information technology acceptance, Proceedings of the
7th Annual Conference of the Southern Association for Information Systems.
Lee, S., 2013, The impact of cultural differences on technology adoption, Journal of World Business,
Vol 48.
Laukkanen, T., “How Uncertainty Avoidance Affects Innovation Resistance in Mobile Banking?” 48th
Hawaii International Conference on System Science, 2015.
Garbarino, E. et al., “How cultural differences in uncertainty avoidance affect product perceptions”,
International Marketing Review, Vol 24, 2007.

Swiss Re sigma No 1/2020 11

The battle for customer touchpoints

̤̤ Every country has a unique combination of cultural characteristics. China’s society
is highly collectivist and there is high-power distance, indicating that inequalities
are accepted. Yet the Chinese also appear comfortable with uncertainty, much like
the Swedes. This could explain why China has a high e-commerce buyer
penetration. So too could what count as popular leisure activities. Consumers that
engage with technology more frequently in their daily lives and free-time also
shop more online.45, 46 Noteworthy is that web surfing is a very popular free-time
activity in China (ranked first)47 and Sweden (ranked third)48; not so in Spain.49
…and where surfing the web is a
popular leisure activity.

All told, we believe that more insurance will be bought online once consumers have
easier access. The degree of online insurance purchases will grow faster in markets
with certain cultural characteristics (low power distance, high individualism, low
uncertainty avoidance), and where use of the internet is a preferred leisure activity.
To this end, we see more opportunities for online insurance purchases in markets like
Sweden, the US, the UK, Germany and China (see Figure 10).

Figure 10
Culture and leisure across markets
Factor

High individualism
Culture Low power concentration
Low uncertainty avoidance
Leisure activities*
E-commerce consumption

Sweden

US

UK

Germany

China

Spain

Italy

Turkey

✔
✔
✔
✔

✔
✔
✔
✔

✔
✔
✔
✔

✔
✔
✘
✔

✘
✘
✔
✔

✘
✘
✘
✘

✔
✘
✘
✘

✔
✘
✘
✘

80%

78%

89%

82%

71%

59%

53%

33%

Note: Digital e-commerce buyers are defined as consumers who use the Internet and have made at least one purchase online in 2017;
* 1 of 3 most preferred leisure activities includes web surfing.
Source: Swiss Re Institute

45 Li, H. et al., “The impact of perceived channel utilities, shopping orientations, and demographics on the
consumers online buying behavior”, Journal of Computer-Mediated Communication, Vol 5, 1999.
46 Bellman, S. et al, 1999, op. cit.
47 Chen, M. et al., “Leisure activities and leisure motivations of Chinese residents”, PLoS ONE, Vol 13,
2018.
48 En undersökning om svenska folkets tidsanvändning år 2010/11, Statistiska CentralByrån, 2011.
49 2009-2010 Time Use Survey, Instituto National de Estadistica, 2011.

12 Swiss Re sigma No 1/2020

What drives online purchasing behaviour if access is provided?
A theoretical framework helps
understanding of what affects the
decision to buy cover online.

Models based on consumer value and need theories can help answer the above
question.50, 51, 52 Figure 11 depicts a theoretical model tailored to online life insurance
buying behaviour. It consists of three value drivers: functionality, emotions and
personal growth. To date, insurance product design has mostly focussed on
functionality, with little consideration of emotional and personal growth values.

Figure 11
Schematic showing of drivers of insurance purchasing behaviour model
Functionality
̤̤ Ease of purchase process
̤̤ Convenience
̤̤ Immediate and fast purchase process
̤̤ Better price

Emotions
̤̤ Low pressure
̤̤ Enjoyment
̤̤ Painless process
̤̤ Feeling of relief
̤̤ Feeling of comfortability

Personal growth
̤̤ Empowerment
̤̤ Epistemic value

Consumption behaviour
Source: Swiss Re Institute

Consumers value simplicity and want
to be emotionally fulfilled.

Analysing 400 consumer reviews of two US digital life insurers (Ladder Life and
Haven Life), we find that when consumers buy cover online, they value functional but
also emotional and personal growth elements (eg, empowerment).53 The most
commonly mentioned emotions are “no pain” and feeling of “happiness” and
“delight”; 23% said they find buying online “painless” while 22% said they feel
“great” to buy online. Another 20% of consumers said they “enjoy” completing their
application and purchase online. Nearly 17% said they feel lower pressure not being
in contact with an insurance agent.

They enjoy products that address
personal growth values.

The personal growth values perceived by consumers mostly relate to empowerment.
Consumers value the ability to acquire information on product prices and terms, and
to be able to decide by themselves which policy best fits their needs. One consumer
stated, for instance, “it was great (…) to get the information to make a decision
yourself”.54 These empowered consumers are no longer passive shoppers
dependent on an agent’s advice. They are active participants who want information
to make their own decisions.55

50 M. Holbrook, Customer Value: A framework for analysis and research, Routledge, 1999.
51 J. Sheth, et al, “Why We Buy What We Buy: A theory of consumption value”, Journal of Business
Research, Vol 22, 1991.
52 A. Maslow, “A Theory of Human Motivation”, Psychological Review, Vol 50, 1943.
53 “Reviews”, trustpilot.com, and consumeraffairs.com last accessed 22 August 2019.
54 Consumer review quoted from consumeraffairs.com, 1 February 2018.
55 P. Bühler, P. Maas, “Consumer empowerment in insurance”, International Journal of Bank Marketing,
Vol 36, 2018.

Swiss Re sigma No 1/2020 13

The battle for customer touchpoints

Insurers need to better relate with
consumers on higher-level emotional
attributes.

These value drivers play an important role in understanding consumers’ choice
behaviour and future purchasing intentions.56, 57 For example, Alibaba’s critical illness
insurance58 and Lemonade’s home insurance59, 60 offer a simple and immediate
purchase process, and also address emotional and personal growth values (see
Figure 12). In the case of Alibaba, consumers emotionally experience insurance as a
community product since they only contribute to a pay-out when someone is ill. The
personal growth value (empowerment) is fulfilled by allowing a large group of preapproved consumers decide through a collective voting system whether a pay-out
should be approved.61, 62

Figure 12
Examples of insurance products based on the three value drivers
Cases: designing insurance based on functional, emotional and personal growth values
Case 1
Alibaba's critical illness insurance
The purchase process is simple and immediate via
the transaction app Alipay.

Case 2
Lemonade's home insurance
The purchase process is simple and immediate via
the Lemonade app or a laptop/tablet.

Emotional value

Consumers experience insurance as a community
product since they only collectively contribute to
the payout when someone is ill.

Customers choose a non-profit organisation for the
leftover money from the monthly flat fee and join a
community that believes in the same cause.

Personal growth value
(empowerment)

(Empowerment): Preapproved consumers decide
themselves through a collective voting system
whether a payout for claimants should be
approved.

(Empowerment): consumers are empowered by
deciding which social cause they want to choose
for their leftover money.

Functional value

Source: Swiss Re Institute, based on information from company websites

In the future, all ages will be attracted
to digital insurance.

In the next three to five years, the digital insurance consumer will likely remain the
Millennials, with higher levels of income and education. In the longer-term, as
millennials become seniors, all age groups will be attracted to digital insurance, with
income and education less relevant factors in purchase decisions. New, innovative
digital cover options for consumers with lower income are already available. For
instance, micro L&H insurance products offered by China Life63, PROSUR64 or
Jubilee,65 and also Alibaba’s critical illness product “Xiang Hu Bao” attract
consumers from all backgrounds, irrespective of income and education levels.66

56 J. Sheth, et al, “Why We Buy What We Buy: A theory of consumption value”, Journal of Business
Research, Vol 22, 1991.
57 M. Holbrook, Customer Value: A framework for analysis and research, Routledge, 1999.
58 “Alipay Health Plan Aiming For 300m Users”, alizila.com, 11 April 2019.
59 “Instant everything” on Lemonade.com
60 “The Lemonade Giveback” on Lemonade.com
61 Ant Financial Aims to Disrupt Health Insurance with New Chinese Health ‘Collective’,
insurancejournal.com, 11 April 2019.
62 “Jack MA is Selling Cancer Coverage for Pennies a Month in China”, bloomberg.com, 20 May 2019.
63 Microinsurance and rural development in China: A Q&A with Associate Professor Yi Yao (Kitty) of
Peking University, MicroInsurance Center at at Milliman, 21 March 2019.
64 “POSUR Micro-Insurance” on prosur.com.kh/
65 “Micro Insurance: Enabling people to overcome uncertainty”, jubilee.com, 23 August 2014.
66 insurancejournal.com, 11 April 2019, op. cit.

14

Swiss Re sigma No 1/2020

The digital insurer
The future insurer will be hyper-aware of customers needs and preferences, and offer personalised, flexible products
and services in real time. Data-enabled processes will minimise friction and streamline the customer insurance
journey, from request for coverage to claim. Artificial intelligence will be used to interact and build understanding of
the customer, and servicing will be through (personal) virtual assistance, 24/7.

The offering
Personalised, social, sustainable and engaging
Digitisation can provide more timely
feedback loops.

Digitalisation can provide far more timely feedback to insurers by leveraging a range
of daily interactions, for example across mobility, health and recreational behaviour.
Feedback loops have always existed in insurance, influencing purchasing patterns,
insureds’ risk behaviours and how they make claims. The feedback has traditionally
been ex-post, and that has sometimes meant a long time lag.

Balance autonomous insights and empathetic care
These constant feedback-based
processes provide better insights for
risk management…

Digitalisation goes beyond data collection. It also gives insurers power to understand
risks more dynamically.67 Insurers may discover uncovered exposures that they can
seek to close (see Figure 13). For example, combining traffic, weather patterns and
other factors, insurers may determine that a driver spends the majority of time on
higher-risk roads. Insurers are taking first steps in engaging customers in an
empathetic manner based on such observed behaviours. For example, Generali has
a real-time coaching tool for motor insurance clients, helping them drive better and
work on accident prevention. The focus is currently on mobility services, with plans
to expand to other areas.68

Access risk insights across new touchpoints and channel formats
…and allow insurers to propose
innovative coverages quicker.

With digitalisation, insurers can obtain risk-related data from a variety of touchpoints,
and can identify and monitor existing and new risk exposures. New data sources and
analytical capabilities can provide instant in-depth view of risks across touchpoints,
which facilitates quicker risk assessment and allows insurers to propose innovative
and relevant coverage. Pre-digital, insurers have had to wait till loss data is available
to assess if a customer is adequately covered.

67 For example, when someone stops working due to a disability, insurers can use data to suggest ways to
help the customer recover more quickly. See “MetLife hooks into external data feeds to help curb
customer risk”, CIO.com, 15 October 2018.
68 “We innovate for our customers”, on generali.com

Swiss Re sigma No 1/2020 15

The digital insurer

Figure 13
Insights improve the existing value chain
Smart interactions

Smart mobility
living / workin
g
Smart
Smart health

Active break

Start work
Park & ride
(mobility)

End work

Breakfast with
family
Sleep

Outdoor
activity

Shopping
Family

Risk-related interactions
Digital risk-related
interactions
Identification/monitoring of existing
and clients risk exposure
Reducing /adapting existing protection gap
growth / profitability adoption on existing
and new customers
Product
management

UWR
pricing

Identification /monitoring of new/
future risk exposure
Generating new protection related products
and services (value propositions)

Marketing
& sales

Claims
management

Retail business

L

H

Source: Swiss Re Institute

16

Swiss Re sigma No 1/2020

P

Customer
service

Corporate business

C

Risk exposure
Over-insurance
Under-insurance
(protection gap)

L

H

P

C

Shift from product to service orientation
Insurance will thus become more
proactive.

Digital interactions will enable insurers to create personalised insurance and new
protection-related services. Traditional risk selection will remain a core activity, but
will be faster, simpler and based on real-time data. Greater volumes of digital data
will help quantify different kinds of exposure, (eg, behavioural, external and internal,
see Figure 14).
̤̤ Behavioural exposure is the risk of aberrant actions, such as driver speeding,
failing to maintain safe distance, or choosing dangerous routes.
̤̤ Internal risk exposure covers the inherent risk of each insured or asset (eg, quality
of construction for a property).
̤̤ External risk exposure covers environmental factors like catastrophe risks for
property, or bad weather and street conditions for drivers.

Figure 14
Representation of an ideal state of risk quantification
Life & health insurance

Property insurance

Automotive insurance
offline
online

Customer journey
Data
collection

Classical lines
of business

Risk exposure:
internal

external

Sensor

behavioural
Identification of risk exposure: contextual risk exposure
(evidence based analytics and decisions – timing – relevance)
Relevant product/service offering: contextual content, products and services
(risk transfer, risk mitigation and preventive services, value adding services)

Risk-related offerings

Partner and community integration/intervention: access/sharing/steering
(to the support community, partner network)

Life area clusters
Health

Education/working

Living

Free time

Mobility

Source: Swiss Re Institute

Swiss Re sigma No 1/2020 17

The digital insurer

This ideal state is still a way off.

The key task for insurers is to develop their digital capabilities to access relevant
information about changes in risk exposures, convert them into actionable insights
and then package and communicate these to customers in a user-friendly manner.
To do this, insurers need to upgrade digital capabilities across three areas.
1	Identify true risk exposures: Not all data can be used for risk exposure
identification. Insurers need to understand risk exposure (evidence-based
analytics and decisions), based on timing, relevance and true signalling power.
2 C
 reate relevant product/service offerings: Insurance cover should adapt to
changing needs, ensuring that cover is appropriate across different life stages. To
this end, insurers will need to be able to contextualise products and services (risk
mitigation and preventive services, value adding services).
3	Partner and community integration/intervention: Insurers can steer the
support community and partner network. With advanced analytics, they will be
able to identify customers most likely to buy based on risk sub-segments.

Higher-quality service will lead to
greater loyalty.

As insurers refine these capabilities, they will be able to provide higher-quality
service interactions, which should lead to greater loyalty. For example, if a property
insurer can access and analyse data on changes in risk exposure, whether from
owned or third-party data, it can enable safety and security services to prevent
damage and losses. It could also advise on real estate purchases across hundreds of
relevant risk variables. For example, one insurer is exploring how to engage with
customers on predictive indicators such as if a property is in public transit corridors,
the number of emergency calls made from the property, hygiene of nearby
restaurants, and infrastructure quality in the area.69

Insurers in emerging markets are
leapfrogging peers in advanced
markets in some innovative areas...

Technology is accelerating the development of insurance in emerging markets. What
took many years of development in advanced markets is being compressed into just
a few years in the emerging territories. This is “leapfrogging”, where some
technology steps are skipped. These disruptive technologies are facilitating
convenient access to insurance products and services as part of local requirements.

...and are working with digital
ecosystems to extend their reach.

Insurers in emerging markets are taking the lead in leveraging new digital
ecosystems; these offer multiple touchpoints to capture user attention, and often
evolve from their original businesses, to move into new domains. Such as in the case
of Grab based in Asia, which started with passenger mobility, then entered food and
goods delivery, and is now expanding into healthcare.70 Insurers in Asia are forming
partnerships with such firms: the insurer brings the underwriting expertise and the
partner firm an entry point to ecosystems, allowing the insurer to extend its reach,
target specific segments and mine user behaviour. Along these lines, in China, Aviva
has partnered with internet major Tencent to sell critical illness policies online. In
another example, in Africa insurers are selling insurers through ride sharing drivers in
mobility ecosystems.71

69 “MetLife hooks into external data feeds to help curb customer risk”, CIO Magazine, 15 October 2018.
70 “Ping An Good Doctor and Grab Form Joint Venture to Deliver Transformative O2O Healthcare Solutions
in Southeast Asia”, grab.com, 16 August 2018.
71 “How Nairobi-Based Ridesharing Service Little Competes With Uber in Africa”, skift.com,
19 June 2018.

18 Swiss Re sigma No 1/2020

Insurers can use digital platforms that are modern and flexible
Flexible and modern platforms can
help insurers enter new markets.

Insurers can leverage flexible, product-agnostic and fully-integrated digital platforms
to engage with customers through the buying journey. Core value chain components
like underwriting can be transformed into insurance as a service (see Figure 15). For
example, Swiss Re has a partnering solution, iptiQ, that allows insurers to harness
the power of cloud applications and data analytics to make buying cover easier.72

They also connect market services for
different parties.

Platforms can also act as intermediaries connecting market services for different
parties, (eg, enabling brokers to manage and deliver cross-border programs from a
single online platform).73 Two types of platforms have been established. First, core
service platforms. which enable part or the whole value chain of an established
industry as a service (IaaS – Insurance as a Service solutions). Second, service
platforms, which act as intermediaries between supply and demand. In rare cases a
combination of both platform types may be established.74

Figure 15
Depiction of opportunities for platforms in insurance
Customers

Primary
insurance

Distribution
partner

Reinsurance

Distribution
partner

Distribution
partner

Retrocessions

B2C
Retail

B2B

B2B

B2B

B2B

B2B

B2B
Corporate
*)

Core
service
platform
Market
service
platform

Product
management
Service interaction platform (services)
Process platform (value chain)
Technology platform (enablement)
Platform management (coordination & governance)

UWR
pricing

Marketing &
Claims
sales
management

Customer
service

*) Industry, insurance, banks,
intermediaries, aggregators,
brokers, platforms, ecosystems

Source: Swiss Re Institute

72 Swiss Re's partnering solution iptiQ allows insurers to harness the power of cloud-based applications
and data analytics to make buying insurance easier, and to help more people to become insured. See
iptiQ: The technology making it easier to buy insurance, Swiss Re, 7 November 2018.
73 Swiss Re Corporate Solutions announces collaboration to build a multinational insurance program
management platform for Brokerslink, Swiss Re Corporate Solutions, 18 October 2019.
74 Starling’s Marketplace delivers an ‘app store’ experience for financial services, NS Banking,
27 February 2019.

Swiss Re sigma No 1/2020 19

The digital insurer

Insurers can use expose these
platforms to influence brokers to
partner with them. This allows a
seamless customer journey across the
value chain.

The benefit of these platforms is that insurers can use APIs to integrate with external
partners, eg, ecommerce to bring significant efficiency across the value chain due to
seamless information exchange. Different components (from need analysis to policy
issue) can be customized and co-branded for these distribution partners who can
choose which products they want to offer to consumers. This allows a seamless
customer journey across the value chain by virtue of remaining on single platform.
These platforms can be multi-device (optimised for desktop, tablet and smart
phone), and multi-channel, ie, customers can switch between face-to-face with an
agent to self-service and can even work with an agent in a call centre with their data
being saved and shared.

The value chain will become more integrated
To iterate quickly, processes will
become flexible and open to
innovation.

The first wave of digitalisation has made the insurance value chain more efficient
(see Figure 16). However, multiple digital silos remain unconnected and information
is compartmentalised. In the future, critical processes will be connected, and
insurers will progress from being mere “digitalised insurance providers” to “digital
insurers”. The potential for transformation does not end there. Looking further ahead,
as data quality and algorithms improve, the next generation will be AI insurers with
value chains that “learn” from data generated by consumers, ecosystems and
governments. Developments in cognitive technologies will help such insurers
integrate learning to adapt value propositions in real-time, thereby providing a
holistic and unique customer experience.

Figure 16
Insurance products growing into a comprehensive risk service
Involved stakeholder

Value chain (example)

Insurance

(Icon)

Product
management

UWR
pricing

Marketing &
sales

Claims
management

Customer
service

Digitalised
insurance

(Icon)

Product
management

UWR
pricing

Marketing &
sales

Claims
management

Customer
service

Digital
insurance

(Icon)

Product
management

UWR
pricing

Marketing &
sales

Claims
management

Customer
service

Data & AIdriven
insurance

(Icon)

Source: Swiss Re Institute

20 Swiss Re sigma No 1/2020

Product = service

Better data and digital triggers will
accelerate the adoption of parametric
insurance.

The availability of such high-quality underlying data will also make it easier to create
products with digital triggers, and will speed up the adoption of parametric
insurance. Property-level flood insurance that utilises data-feeds from sensors in
homes and businesses for parametric triggers is already available.75 Basis risk should
be much lower because the trigger is based on a data source in the property itself.
Over time this will virtualise the value chain and lead to products becoming more
parametric in nature (see Smart contracts).

Smart contracts
Smart contracts can automatically
recognise that the terms of an
insurance policy have been met.

The concept of smart contracts aligns with the idea of an AI-driven insurer
constructed on an interplay between multiple value-chain components. Smart
contracts are self-executing software programs that, for instance, can make a
payment when triggered by occurrence of a risk event. Legal (contract) logic is put
into computational logic, such that a string of computer code recognises that the
terms of a contract or insurance policy have been met. It automatically transfers
funds (insurance payouts) at the agreed time and registers those transfers.

However, as of yet there is no
international regulation specific to
smart contracts.

Existing insurance contracts need to be translated into computational logic. So far,
the most promising cases have been in parametric insurance. Lines of business
where telematics can be used, such as motor, aviation, cargo and agriculture, offer
potential. However, certain challenges stand in the way of wider adoption of smart
contracts. Notably, as of yet there is no international regulation specific to smart
contracts. As Lloyds recently pointed out, it is unclear how policyholders who
disagree with an automatic decision not to trigger cover can defend themselves.76

Challenges to digitalisation
Regulation
Monitoring yet facilitating innovation
is a priority for regulators.

The regulatory framework will play an important role in shaping the integration of
new technology into the insurance space. In monetising the potential of
digitalisation, insurers could face regulatory challenges on data protection and
privacy, providing incorrect advice, and records retention. Errors or bias in algorithms
that might contribute to systemic risk or prompt inappropriate insurance decisions is
also an area of regulatory scrutiny. Likewise, it might be difficult for regulators to
understand why a complex and proprietary algorithm decided to deny coverage or
reject a claim, undermining their ability to fulfil their supervisory and consumer
protection tasks.

Insurers could also face countryspecific regulatory limitations.

Regulators are also wary of unsolicited use of data. Insurers also face countryspecific regulatory limitations and high implementation efforts for product
development and modifications. They will have to construct a robust system of data
governance to win both customer trust and abide by regulation. There may be
additional legal and compliance challenges, such as integrating multiple vendors into
one proposition.

75 JBA & FloodFlash launch sensor-based parametric flood cover first, Artemis, 27 June 2019.
76 Triggering innovation, Lloyds, 2019.

Swiss Re sigma No 1/2020 21

The digital insurer

Ownership of data and willingness to share
Consumer reticence to share data
varies by country and there may be
resistance in some contexts.

As data become easier to collect, an increasingly important consideration will be
ownership. For example, ownership of images of properties surveyed by drones may
rest with clients, not insurers. Consumer reticence to share data varies from countryto-country, and there may be resistance to do so in some contexts. For example,
some employers that offer workers boots, gloves and vests with wearables have
found some employees refuse to be monitored.77 This behaviour differs by line of
business; a recent survey found US consumers more willing to share health-related
data than home-related data.78

The focus should be on loss
prevention rather than personalised
premiums.

Another survey found that consumers are cautious about sharing data, even when
they know it will lead to a more personalised experience.79 However, an Accenture
survey found that eight of 10 would share personal data (eg, on income, location and
lifestyle habits) with insurers if it lowers risk of injury or loss.80 This suggests
messaging around data sharing is more effective if the focus is more on loss
prevention rather than personalised premiums.

Better to ask fewer, more focused
questions.

Data is not easily available in the required depth and detail. Insurers will have to strike
a balance between more and relevant data. Data can be easily misinterpreted if the
background context is not fully understood. For example, a non-smoker who on
occasion buys cigarettes for a parent could be identified as a casual smoker, and
could be subject to higher premium rates when buying health insurance. Does more
data help or confuse? In some cases, asking fewer but more focused questions could
lead to more precise answers.

Data availability

Consumer wariness about automation
But some policyholders may be less
comfortable with digitisation.

Some consumers may be less comfortable in an automated world. For example,
48% of surveyed consumers said they worry about mistakes while filling out
insurance forms, and 46% fear that claims pay-outs might not be accurately
calculated by machines.81 Policyholders may be divided in their views on
digitalisation. It is vital to win customer confidence by reassuring them about
machine accuracy and developing checkpoints to help them avoid mistakes.

77 “The Exciting, New Ways Technology Is Streamlining the Claims Management Process”,
Risk&Insurance, 5 August 2019.
78 U.S. Consumer Survey: A Connected-Devices Insurance Roadmap, Aite Group, 20 May 2019.
79 “Promise of personalization has little impact on consumer willingness to share data, study reveals”,
Marketing Dive, 14 August 2019.
80 64% of consumers are willing to share data in exchange for adjusted car insurance premiums based on
safe driving and 52% are willing to share data in exchange for life insurance premiums tied to a healthy
lifestyle. See Six in Ten Consumers Willing to Share Significant Personal Data with Banks and Insurers
in Exchange for Lower Pricing, Accenture Study Finds, Accenture, 14 March 2019.
81 “Future of Claims: Customers Still Want Humans In Process – But Not Too Many”, Carrier Management,
24 March 2019.

22 Swiss Re sigma No 1/2020

Marketing, distribution and servicing
Insurers are moving away from a product-focused sales approach to one closely tied to the broader needs of the
customer, with greater focus on the human experience. New generations of systems will grant unprecedented levels
of proximity and influence on customers, and insurers will be able to optimise interaction across a diverse range of
customer touchpoints.

Marketing to become more granular
Insurers can use new data to build a
consumer journey based on human
experience.

Historically, insurance marketing has been about aggregating prospective buyers
into groups likely to have common needs and respond similarly to marketing. Typical
attributes include geography, demographics, psychographics (eg, values, attitudes,
lifestyles) and behavioural aspects (eg, price sensitivity). This approach leads to a
relative broad spectrum of consumers in a segment, which can limit effective
targeting. With new sources of digital data on life events such as purchase histories,
travel behaviour, customer service records, data from devices, wearables and also
social networks, insurers can employ advanced analytics techniques to yield more
granular classification of existing and prospective customers (see Table 1).82, 83

Table 1
Evolution of marketing and distribution innovation timeline
Marketing and
distribution journey

Customer experience
Market size/sentiment
analysis
Personalisation
Channel evolution

Fulfillment

Past

Present

Future

Key enablers

Fragmented, limited
channels
Time consuming,
Physical
Not possible, very
expensive
Agent-driven, door-todoor

Multichannel, somewhat
integrated
Tech-based, still takes
time
Possible, limited scope

Seamless, continuous

Direct (online, contact
center, bots), Tech
enabled agents
Direct to home

Ecosystem based with
virtual connect

Platform economy,
insights available
Online surveys, digital
behavior
Shorter development
time, data insights
Chatbots and machine
learning

Exhausting & time
consuming

Tremendous efficiency
and accuracy gains
Complete customisation

Needs-based “just in
time”

Source: Swiss Re Institute

82 sigma 6/2015, Life insurance in the digital age: fundamental transformation ahead, Swiss Re,
December 2015
83 Ibid.

Swiss Re sigma No 1/2020 23

Marketing, distribution and servicing

Smart interaction: the future of marketing
This can boost customer loyalty.

This personalisation can improve loyalty management and impact customer lifetime
value (CLTV).84 Historically the focus of insurer engagement with customers has
been on sales and claims. With new data sources and advanced analytics, insurers
can interact with customers over their lifetime on a wide range of relevant issues,
developing systems that help them listen, engage and act both proactively and
reactively (see Figure 17).

Figure 17
Schematic showing how smart interaction increases loyalty and customer equity
Without/only limited interaction management

With intensive interaction management

CLTV segment

CLTV segment
Customer journey
Mailing
Claim

Customer journey

Potential

Historical focus only on
sales (distribution) and claims
CLTV

CLTV

Listen

Engage

Act

CLTV
Owned

Paid

Paid

Earned

Earned

Social

Social

Reactive

Owned

CLTV
Proactive

Invoice

Value for the customer
Content
Communication

Interaction

Collaboration
Community

Content

Collaboration

Communication

Community

Insurance products and services

Source: Swiss Re Institute

Lifestyle related interactions will help
build deeper one-to-one relationships,
at scale.

The more high-quality interactions there are, the more loyal customers will be. To this
end, insurers can pro-actively use digital engagement to reduce policyholder churn.
For example, recently a major P&C carrier ran a three-month pilot, sending
personalised automated texts to delinquent customers and followed up with a phone
call, creating a new level of proactive digital and human engagement. Less than 2%
of customers opted out, and a large majority of delinquent customers were retained.
There was less manual work: the approach yielded results with 50% fewer outbound
calls.85 Insurers can broaden the scope of these interactions with policyholders to a
wider sets of prospects. For example, some insurers have decoupled insurance from
their telematics apps. The app is available to all drivers rather than insureds alone.86

84 Customer lifetime value or CLTV is a measure of net profit attributed to the entire future relationship.
85 Strategy Meets Action. Proactive Digital Engagement: From Reactive to Proactive: A Paradigm Shift for
Insurers, Hearsay Systems, August 2019.
86 Insurance Innovation of the Month: AIG on the Go, EFMA, 15 January 2019.

24

Swiss Re sigma No 1/2020

Distribution: traditional touchpoints will adapt
Capitalising on unique moments will
be key to increasing loyalty and
customer lifetime value.

With digitalisation, insurers will be better able to identify where human interactions
play a significant role. Capitalising on unique moments will be key to increase CLTV.
Currently digital interactions are mostly transactional (post-sale interactions focused
on billing and claims handling), and miss opportunities for insurers to address the full
range of customer needs. Insurers need to understand where and how empathy is an
important component of brand value by offering a unified customer experience.
Automated data capture and synchronisation with customer relationship
management (CRM) systems offer actionable insights. For example, agents are
alerted when a customer files a digital notice of loss claim and can use that event as
an opportunity to call on the same day to better understand other life events (eg, a
baby on the way) and, if appropriate within the contextual setting, cross-sell.87

Intermediaries: their role will change
Agents will act as risk consultants with
a fuller view of customers' life events.

As customers become more comfortable with buying insurance online, the role of
intermediaries will evolve. Agents will act as risk consultants with an overview of
what’s happening with each customer across the value chain. Insurers and agents
will also need to rethink traditional cooperation models, including how intermediaries
are compensated. For example, a client acquired online may require advice from an
agent. A compensation model that rewards agents for these services via advisory
fees that consumers are willing to pay, may help alleviate channel conflicts that could
arise within a pure commissions-based system. 88

In commercial, small and medium
sized business are also becoming
more open to digital distribution.

New distribution technology has not impacted wholesale commercial insurance
markets as much as retail/personal lines. However, there are some initiatives to
simplify parts of the value chain in commercial insurance. Lloyd’s of London for
example, mandated its syndicates to use electronic placements for no less than 30%
of their written risks by the end of 2018.89 Online brokers for small and medium
businesses (SME) are also seeing traction. For instance, Embroker and CoverHound
use data verification technology to obtain immediately bindable quotes, allowing
customers to complete the process almost seamlessly online.90 These platforms offer
additional services (eg, help SMEs upload and compare policies, generate vendor
certificates, and asset tracking), which can strengthen customer loyalty.

87 [Insurance] The Next-Gen Customer Experience – A Dreamforce Case Study, Hearsay Systems,
16 October 2018.
88 sigma 6/2015 op. cit.
89 “Lloyd’s issues electronic placement mandate”, Lloyds.com, 20 March 2018.
90 See Embroker, CoverHound.

Swiss Re sigma No 1/2020 25

Marketing, distribution and servicing

Digitally-augmented channels on the rise
Digitally-augmented channels,
combining man and machine will
become more common.

Figure 18
Data and AI-driven augmented
advisors

Smart-augmented
platform
Digitally-augmented
empathic human interface

New digital interactions will be driven by next-generation empathetic advisory tools,
augmented by AI (see Figure 18). Agents and brokers will leverage these to refine
their communication to adapt to the customer’s emotional, situational and personal
contexts. The power is in combining both functional and empathetic interfaces. The
functional interface performs the analytics and provides the intelligence, such as
next best action. The agents/brokers provide the human or empathetic interface.

Retail customer/
Commercial customer
Intermediaries
Platform/ecosytem
Broker/3rd Party
Marketplace
Aggregator

Provider
Bank
Insurer/reinsurer

Interactions

Digitally-augmented
digital interface
Source: Swiss Re Institute

Big Tech, intermediaries and insurers
will invest in different interaction
channels.

We believe the use of digital advisors which combine these two capabilities will
drive digital interactions in a targeted manner in the future. Generating this
intelligence will become a core capability, and not just for insurers. Depending on
who the provider is, there could be three types or levels of digital interaction channel.
These could work in co-existence, although the one perceived by consumers as
most trustworthy and unbiased will likely hold a distinctive competitive advantage.
̤̤ “Direct” customer advisors (insurer agnostic), offered by technology companies.
For example, these could be enhanced versions of today’s Siri/Alexa/Google
duplex assistants.
̤̤ The “intermediary advisor”, which feeds intermediaries with better insights so that
back-office responsibilities are minimised, and agents can focus on human
relations and empathy, augmented by better risk insights and data.
̤̤ The “product advisor”, which is fully insurer-owned. Such as, for example, GEICO’s
mobile virtual assistant Kate, which intuitively guides customers to the relevant
information about their policy and helps in self-service tasks.
Customers may use all three set ups. For example, for L&H insurance they may go to
an intermediary, for home insurance direct to an insurer, and for mobility and travel
insurance use an enhanced version of customer advisors like Google Assistant.
Intermediaries will continue to play an important role, although human interaction
may in some cases become less part of the standard process. Where human
involvement can be of added value is where the emphasis is on contextual “empathic
interactions”. In the new competitive landscape, insurers will need to balance these
customer interactions in the right manner using new loyalty and interaction systems.

26 Swiss Re sigma No 1/2020

Servicing: loyalty and new risk management needs
Additional digital tools and valueadded services will help brokers push/
distribute to customers.

Servicing is all about building deeper 1-2-1 relationship with customers at scale
through risk prevention and value adding services (VAS).91 This is especially critical
in commercial lines where specialised knowledge can be leveraged. Whichever
party along the value chain possesses specialised knowledge and provides robust
engagement will control servicing.
There is significant scope for insurers to enable agents and brokers with digital tools
and value-added services. J.D. Power measures satisfaction scores among business
relationships, and finds that insurance agents currently report low satisfaction with
the service they receive from personal lines and commercial lines insurers.92 The
advent of digital engagement in servicing has several implications for insurers:
̤̤ The boundaries of traditional insurance are becoming blurred, and distribution is
moving towards an integrated ecosystem focusing on an end-to-end buying
experience. A survey by Accenture found that three in five insurers (59%) are
forming relationships with non-traditional partners to reach customers in new
ways and create new value.93 This will have an impact on all aspects of the value
chain. To fully realise the benefits of these partnerships, insurers will need to
upgrade their predictive analytics capabilities, obtaining risk-related data from a
variety of touchpoints and providing tailored content.
̤̤ Customers want a positive experience across multiple channels and touchpoints.
Gartner believes that in the coming five years, event-triggered and real-time
marketing will make the biggest impact across industries.94 However loyalty will
be hard to come by. Digital-only customers still give their insurers lower loyalty
scores than do multi-channel customers.95 There is a misalignment between what
customers expect from digital channels and what insurers provide. For example,
some insurers fail to replicate certain service components, such as their Contact
Us page, from their web to mobile sites.96
̤̤ Shifts toward platform plays and ecosystems appear inevitable, in both personal
and commercial lines. E-commerce firms, retailers, automotive OEMs and other
non-traditional players are developing, owning and offering new and unique
product propositions to customers. They build platforms based on plug-and-pay
principles to allow third-party connections. Insurers need to be more cognizant
that different elements in the value chain own different customer touchpoints.

91 Srikanth Madani, Hypotheses on Innovation Direction in Insurance, St Gallen Risk & Finance, 2019.
92 “Insurers Come Up Short for Independent Agents Despite Critical Role Agents Play in Driving Business,
J.D. Power Finds”, prnewswire.com, 31 January 2019.
93 Accenture, Technology Vision for Insurance 2019, Accenture, 10 April 2019.
94 Gartner Identifies Four Emerging Trends That Will Transform How Marketers Run Their Technology
Ecosystems, Gartner, 29 August 2019.
95 Bain and Company, 10 October 2018. op. cit.
96 Gartner Says U.S. Insurance Brands Underperform in Digital, Despite Customers’ Growing Willingness
to Provide Data, Gartner, 10 July 2019.

Swiss Re sigma No 1/2020 27

Product development and underwriting
From bikes to trainers, customers like tailored rather than off-the-shelf products. The same will become increasingly
prevalent in insurance. Over time Big Data and sophisticated models will allow risk pricing at increasingly granular
levels. Emergence of new risks will create new underwriting and portfolio risk management techniques.

Digital-enabled product development
Real-time demand assessment
Digitisation will enable direct
touchpoints with customers and
real-time demand assessment.

Increasing digitalisation will present opportunities to connect directly with
customers and assess product demand in real-time, rather than rely solely on
channel partners (see Table 2). For example, in-app surveys and tracking search
behaviour will provide clues about unmet customer needs. Based on granular
customer data, insurers can more precisely segment customers and develop tailored
products. Moreover, timing of product searches, when correlated with personal data,
may provide the context in which a product is demanded. Such constant feedback
will help insurers develop new products but also refine/adjust existing ones.97

Table 2
Impact of digitisation on product development journey
Product development
journey

Demand assessment

Coverage design

Past

Present

Future

Key enablers

Time consuming,
dependent on channel
partners
Fixed coverage for longer
duration

More efficient, based on
limited customer data
collection
Partially modular
coverage for shorter
duration
Short-period pricing
based on limited
behavioral data
Extended partnerships
across insurance value
chain
Sandbox
experimentation on
data-based evidence
Limited feedback based
mainly on claims
experience

Real -time, based on
changing customer life
stages
Completely modular and
real-time coverage

Online surveys, digital
behavior tracking

Pricing

Fixed pricing for defined
risk categories

Partnership strategy

Mainly distributionbased partnerships

Regulatory approach

Finite experimentation
due to lack of data

Customer feedback

Almost non-existent

Hyper connectivity
enabled by sensors, 5G,
LPWAN95 and satellites

Dynamic pricing based
on real time behavioral
data
Ecosystem partnerships
driven by cross-industry
value chains
Approval for wide range
of data in risk modelling

APIs96, cross industry
ecosystems

Continuous feedback at
multiple touchpoints

End to end digital
delivery of products

Sandboxes and
innovation hubs

Source: Swiss Re Institute

9899

97 K. Cool, C. Angoulvant, et.al., “ZhongAn’s Micropremium Model: The Future of Insurance?”,
knowlege.insead.edu, 18 May 2018.
98 Low Powered Wide Area Network, a low bandwidth wireless technology covering wide areas.
99 An interface or communication protocol that allows two software programs to talk to each other.

28 Swiss Re sigma No 1/2020

Data-driven and granular insights to facilitate new risk covers
Digitisation has enabled coverage
customisation, while preserving base
product structure.

At present, digitalisation drives modest coverage customisation in products. Such
incremental innovation preserves the basic product structure, while using short time
scales to price short duration covers and add-ons to provide coverage carve outs.
These solutions are either white labelled or written directly by insurers.100 Most of
the covers are fixed for shorter durations. Products may use sensors to track asset
usage or movement of individuals to activate coverage or offer minor premium
credits, but this information is not widely used for real-time underwriting and pricing.

Real-time risk data will facilitate more
innovative insurance solutions.

In the sharing economy, modularisation and real-time coverage adjustment will
enable new products to adapt to the frequently changing risk profiles.101
Modularisation means the de- or re-coupling of coverage and services to suit the
changing needs of a customer.102 Personal risk exposures will be tracked and
modelled more easily so that insurers can more confidently de-couple coverage
elements within existing monolithic products. Commercial risks are more complex
and do not change as frequently. Digitalisation can help in coverage re-coupling
across products to reduce protection gaps and achieve price efficiencies. In this
dynamic environment, product life cycles will get shorter and speed-to-market will
be crucial.103

Better data quality is facilitating
construction of new risk indices and
creating new markets.

Some innovative products for new risk pools are emerging with granular data
collection and analysis enabled by digitalisation. For example, we see first solutions
emerging for personal cyber, use of cryptocurrency exchanges, or IoT infrastructure
risks.104 Better data quality also facilitates construction of new risk indices and
parametric products, such as covers to protect against economic impacts of
infectious disease outbreaks. These use pathogen sentiment indices from digital
sources to gauge public fear and behavioural changes to measure the cost of
epidemics.105 In the future, we think algorithmic risk products will become
mainstream, with assets and processes becoming more autonomous and intelligent.

100 Toffee insurance, Trov and Element are examples of agencies providing pay-as-you go insurance via
online portals. Metromile and Lemonade are licensed insurers with usage-based products.
101 Traditional asset insurance mandated a clear commercial or personal insurable interest. However, this
classification is blurring with the sharing economy allowing individuals to put personal assets to
commercial use via online platforms.
102 See Mobility ecosystems striving towards a seamless interface for customers. Swiss Re, 2018.
103 Current average speed-to-market is eight months for new Life and Annuity products, and four months
for modifications. For Property and Casualty insurers, it is seven months for new products and three
months for modifications. See Speed to market for life/annuity insurers, Novarica, March 2019 and
Speed to market for property/casualty insurers, Novarica, March 2019.
104 Bitflyer and Mitsui Sumitomo are selling insurance for users of Cryptocurrency, while Munich Re and
Relayr have developed customized insurance products to facilitate IoT infrastructure investments.
105 Andrew W. Singer, “The Evolution of Parametric Insurance”, rmmagazine.com, 1 April 2019.

Swiss Re sigma No 1/2020 29

Product development and underwriting

Dynamic risk pricing to drive behavioural change
Pricing based on granular data will
generate premium efficiencies and
mould consumer behaviour.

Insurers will be able to micro-segment risk pools, thus accurately reflecting the risk of
each unit within an insured pool. For example, Zurich Insurance in Spain partners
with Klinc to offer coverage that can be turned on and off. This is for personal devices
and other items from a catalogue of over 2 000 products, and there are plans to
expand into categories like auto and home, all enabled by availability of granular
behavioural data and advanced modelling.106, 107 Such pricing may change
dynamically, based on changing risk profiles.108 Going forward, cross-industry
platforms and seamless insurance portability can integrate all the risks, exposures
and covers for an insured under a universal insurance policy. This policy will be
customised according to the insured and it will adapt the coverage in real-time on
the basis of a change in the insured’s risk profile (see Figure 19).

Figure 19
Tech-enabled insurance product and service opportunities
Off-the-shelf coverage

Digitalisation-enabled personalised coverage

Risk levels

Risk levels

Policy period
Static pricing level
Exposure inefficiency for insurer
Insured risk profile
Coverage and pricing inefficiences for insured

Policy period
Coverage and pricing
Insured risk profile
Coverage and exposure efficiences for both insurer and insured

106 Zurich’s innovation efforts recognized with Global Innovator Gold Award; Klinc takes top award in
Products & Services, Zurich, 25 June 2019.
107 In absence of granular data, insurers use proxies such as driver age, type of car or gender to assess
driving behaviour.
108 Products like Pay as You Live (Health Insurance) or Pay as You Drive (Auto Insurance) rely not on the
dynamic pricing, but short-period pricing based on limited behavioral data, which is a finer version of
static pricing.

30 Swiss Re sigma No 1/2020

Tech-enabled insurance product and service opportunities (cont.)
Modularisation global insurance coverage in retail business
Cyber

Travel

Cyber

Liability

Home

Mobility

Travel
Liability

Home

Mobility

25-year
adult

Family,
two kids

NatCat

Life

NatCat

Life

Health

Health

Over-exposure
(inefficiency for insurer)

Modularisation and global insurance coverage in commercial insurance
Liability

Cyber

Liability

Cyber

Marine

Marine
Fire

Mid/large
corporates

SMEs

NatCat

Fire

NatCat
Engineering
Business interruption
Risk exposure

Engineering
Business interruption

Risk coverage and price
Under-insurance (protection gap)
No insurance coverage

Note: The green (risk exposure) line in the circular charts indicates an insured’s dynamic risk profile revealed by processing of granular
insured data. The segments and blocks in the same chart indicate the extent to which coverage can be modularized across exposures under
different risk areas (eg, cyber, fire etc.)
Source: Swiss Re Institute

Swiss Re sigma No 1/2020 31

Product development and underwriting

Extending insurance reach with preventive and value-added services
Insurers may need to focus on
preventing accidents and improving
quality of life.

Insurers with strong distribution networks, brand and ability to adapt to meet
customer needs (eg, simplifying tasks, offering outcomes that reduce pain/improve
gain) could develop new relationships based on trust and value add. They will
reinvent themselves from risk protection to value-add and preventive service
providers. This will help counter any decline in premiums caused by reduction in loss
incidences due to various safety features enabled by technology. Prevention could
contain future claims while value add services build loyalty. These services (see
Table 3) will grow more sophisticated as insurers harness network effects within
cross-industry ecosystems, and should also increase customer stickiness through
increasing touchpoints.109

A wide range of services could be
provided as part of the overall
insurance package.

In commercial lines, insurers could partner with providers of technology to monitor
industrial assets to improve efficiency, safety and early warnings. For example, one
insurer offers a free online tool that helps SMEs assess risk of cyberattacks and
become more proactive on security.110 In personal lines, insurers can enable access
to technologies for coping with or mitigating morbidity. For example in Italy, Axa has
prototyped a cancer profiling service that allows patients with advanced and/or
metastatic tumours to access personalised cancer care.111 And South African insurer
Discovery has a service that provides additional pay-outs to insureds stricken with
permanent disabilities to help them adjust to a new way of life.112

Table 3
Examples of value-added and preventive services
Service Type

Auto

Property

Life

Health

Preventive services

̤̤ Safe driving alerts
̤̤ Maintenance rewards
̤̤ Anti-theft and
breakdown alerts
̤̤ Geo fencing (eg, alert if
driven outside of safe
neighbourhood)
̤̤ Assistance to buy car
̤̤ Concierge services
̤̤ Alert when vehicle is
moved, towed or hit
when parked

̤̤ Remote monitoring
and alerts
̤̤ Flood monitoring in
home basements
̤̤ Automatic shutdowns
of gadgets during ﬁres

̤̤ Online diagnostics
̤̤ Help ﬁnding doctors
̤̤ Scheduling
appointments

̤̤ Medical consultations
̤̤ Screening and
counselling
̤̤ Behavioural
assessments

̤̤ Property security
advice
̤̤ Facility maintenance
̤̤ Emergency repair
services

̤̤ Meal and fitness
vouchers
̤̤ Retirement, financial
planning
̤̤ Healthcare services

̤̤ Digital health records
̤̤ Fitness club
memberships and
discounts

Value added services

Source: Compilation of value-added and preventive services by Swiss Re Institute

109 Digital ecosystems: extending the boundaries of value creation in insurance, Swiss Re, 2019.
110 “Cyber Insurance” on armourinsurance.ca
111 Efma and Accenture Reveal Winners of Innovation in Insurance Awards 2019, Accenture, 25 June
2019.
112 The Purple Life Plan, Discovery, 2019.

32

Swiss Re sigma No 1/2020

Digitalisation and underwriting
Digitisation will enable more forwardlooking underwriting.

Digitalisation of external and internal data will improve risk selection and pricing,
while automation will streamline manual submissions, triaging and binding (see
Table 4). Overall, these developments should reduce underwriting costs, loss ratios
and free up time for underwriters to engage in softer aspects of business (ie,
negotiation and relationship building). Also, developing repositories of digitised risk
information will enable more efficient underwriting than decisions based on limited
claims experience data. For example, Helvetia Insurance enables logistics
companies to purchase transport insurance for their customers through a
straightforward online app in less than two minutes.113

Table 4
Impact of digitisation on underwriting value chain
Value chain

Past

Present

Future

Key enablers

Submissions

Lengthy proposal forms
and human interactions.
No data collection for
post submission analysis

Proposal auto-filling
through third party data.
Chatbot interaction but
limited data collection

Triaging

Delays due to inefficient
workflows and timeconsuming decisions on
referrals

Better quote to market
speed on some LOBs due
to auto-referrals to
appropriate authority

Risk assessment

Manual, time consuming
and limited at the point of
underwriting

Usage of third-party data
for external risk features,
and automated report
interpretation

Fraud detection,
behaviour analysis for
spotting dishonest
disclosures. Complete
data collection
Submission ranking
based on conversion
probability and auto
suggestions on capacity
management
Evolution into a risk
foresight activity with
frequent touch-points
and real-time sensor data

Coverage and pricing

Coverage governed
strictly by insurability,
while pricing based on
static risk matrix

Binding

Delayed binding due to
data entry of paperbased information in
multiple systems

Flexible cover and
pricing for some LOBs,
driven by data on asset
usage and individual
movements
Reduction in binding
time and errors due to
higher flow of digitised
submission data in to
binding process

̤̤ Advanced text and
speech analysis (AI/
ML)
̤̤ Efficient information
storage systems
̤̤ Intelligent business
workflows (AI / NLP)
̤̤ Better readability of
digitized information
(OCR)
̤̤ Digitisation of asset and
biological data
̤̤ Text and image analysis
̤̤ Processing of sensor
data
̤̤ Real-time risk modelling
̤̤ Capacity sharing (P2P,
Digital risk consortiums)
̤̤ Connected sensors

Real-time coverage and
pricing adjustments,
Seamless cover
portability across
insurers
Instant binding on the
basis of end to end
digitised submission and
risk assessment data

̤̤ Same as submission
and triaging

Source: Swiss Re Institute

Submissions triage and routing to become more automated efficient
Submission level accuracy will be
much improved with availability of
more granular customer data.

Increasing digitalisation of geographical, personal and asset information will be
leveraged by insurers to auto-fill proposals, offer risk scores, and even identify more
honest brokers.114 Already pin-code and license plate numbers reveal location
specific hazards and vehicle details, while business registration numbers linked with
municipal data are used to provide occupancy and previous loss history for
commercial clients. For example, about 85% of submissions for Chubb’s business
113 Helvetia transforms logistics and freight companies into insurance professionals, Helvetia,
7 October 2019.
114 Berkshire Hathway Guard Insurance now obtains needed information based on data available online
and offline, which has reduced time from submission to quote. See “Berkshire Hathaway GUARD
Insurance Companies partners with Planck to create full digital underwriting for their commercial lines”,
prnewswire.com, 7 March 2019.

Swiss Re sigma No 1/2020 33

Product development and underwriting

owners policy for SMEs can now flow straight through without any human
intervention on Chubb’s part.115 In the future, intelligent and automated workflows
for triage and routing will be more effective than current business rules.

Risk assessment and underwriting to be more cost effective, engaging
Availability of digitised risk information (eg, biomarkers, activity data, building
footprints, occupancy, vessel/vehicle data) will create risk profiles without physical
risk inspection or medical underwriting requirements (see Figure 20). These data
sets will be especially useful for geographically-scattered assets and individuals.
More complex proposals may still require traditional risk assessment, but intelligent
workflows can automatically assign tasks to suitable risk engineers/doctors, and
natural language processing can enable underwriters to interpret results faster.

Digitisation will reduce the cost of
insurance and turn risk assessment
into an early warning tool.

Figure 20
Different levels of underwriting maturity
Data

UW evidence

AI module

Risk data

§
Learning

Learning

Increasing data

Modelling insights

UW models

Inputs for underwriting

Effort required from customer

Decision/outcome

Dynamic underwriting
– Social media data
– IoT data, fitness, home
– Activity/lifestyle
data/mobility
– Electronic health record

Self optimizing AI driven
underwriting algorithms
using parametrised indexes
accessed through platforms
– Minimal customer input

Automatically
assign risk class

Accelerated underwriting
– Prescription history
– Credit data
– Driving records/
telematics
– Claim experience

The insurer may request
either third parties or the
customer for additional data
if needed

Traditional underwriting
– Proposal
– Tele interview

Underwriting decision solely
based on customer inputs

‘No’ –
assign risk class

‘Yes’ –
additional underwriting

Inspection report,
lab test, physician statement

Machine learning expertise

Emerging
sources of data

Data sources
currently available

Data-Input from
customer

Data

Emerging complexity

Source: Swiss Re Institute

115 Andrew G. Simpson, “InsurTechs Take Note: Chubb’s Digital Marketplace Serves 1,000 Agents a Day”,
insurancejournal.com, 13 February 2019.

34

Swiss Re sigma No 1/2020

Dynamic underwriting can provide
mutual benefits to insureds and
insurers alike.

Standard and accelerated underwriting approaches differ in speed, but both are
static. They evaluate the risk at the time of underwriting but do not track future
behaviour, either positive or negative. This results in price inefficiencies for both
policyholders and insurers. In life and health, greater availability and quality of
Electronic Health Records (EHR), and fitness and social media data can allow
dynamic underwriting and provide predictive insights on state of health. With this
information, insurers can evaluate the change in risk factors (ie, smoker status, BMI,
blood glucose, blood pressure, cholesterol) over time, and predict claims probability.

External data helps digital risk
assessment for SME risks, even for
niche products.

SMEs and mid-corporates have diverse business activities and many exposures,
which are sometimes too small to justify complete risk assessment by an insurer. For
this reason, insurers leverage third-party data sets to auto-fill proposal forms and
offer risk scores. For example, online reviews related to small businesses can be used
as a proxy for their operational risk levels. Some insurers use social media to improve
customer experience by tracking changes in SME payrolls, office premises and
revenue to indicate growth or retrenchment, so that brokers can refine target
strategies.116 Others leverage digital footprints to deliver niche products like trade
credit insurance to SMEs by tracking live financial data. In cyber, new tools can
ingest and analyse data to generate cyber risk scores for SMEs in less than two
minutes.117

Digital underwriting for large
corporate risks is less advanced.

Large corporate risks require bespoke solutions and critical underwriting information
for such risks is not yet sufficiently digitised. As such, the underwriting approach is
more case based. Digitalisation of geo-spatial information has assisted largecorporate underwriting to some extent. For example, InsurTechs provide property
risk engineering solutions to insurers, especially to assess natural hazards.118 In
another case, cargo sensor data can be used to provide enhanced marine
coverage.119 Remote risk inspection can be enabled by allowing risk engineers to
transmit live feeds to underwriters.120 The full value of digitalisation in dynamic
pricing will only be realised when live data from industrial control systems and
facility monitoring systems is integrated with underwriting systems.

116
117
118
119
120

“A move from static data to live data in insurance”, digitalfineprint.com, 31 July 2019.
Cyence for Small Business, guidewire.com.
Insurtech Impact 25, Oxbow Partners, 2018.
Smart Cargo+Cyber Insurance, Covus Insurance, see info.corvusinsurance.com/.
“Virtual i Lets You See” on virtualitechnologies.com

Swiss Re sigma No 1/2020 35

Product development and underwriting

Pricing and coverage determination to accelerate, be more risk based
Price and coverage are being
automated based on better
assessment of risk data.

Automation of submissions and risk assessments may trigger faster determination of
coverage and better pricing. Automated price and coverage determination are more
common for personal (L&H) and retail (P&C) business. Several examples are
beginning to emerge. In China, Ping An claims that of the nearly 20 million
applications for insurance received in 2018, 96% were auto-underwritten by AI.121
Continued testing, focused on scalable and replicable use cases will inform insurer
strategies in the next few years. Mid-market and commercial insurance segments
still require significant manual effort due to underwriting complexity and price
sensitivity.

This is valuable when there is wide
variability in costs for similar
procedures.

In corporate health insurance, automated pricing and coverage determination is
improving. This sector struggles with high costs and wide variability in treatment
costs for similar procedures. Using Big Data and a modern tech stack,
UnitedHealthcare and Bind, on-demand health insurance start-ups, are designing
better health coverage plans for big employers. To lower costs and better match
risks to employees, they identify about 45 top-up optional procedures that have the
widest range of cost, treatment and effectiveness. Members can opt for these
options only if they need it. Customers pay a base monthly premium that can be as
much as 40% less than the other options that their employer offers.122

Fraud detection and managing moral hazard will become more critical
Underwriting fraud is a risk in the
digital world with decisions needed in
near real time.

In a digital world, underwriting decisions on large volumes of insurance applications
need to be made in near real-time. If insurers are not careful to filter out fraudulent
applications, they can build up a wrong customer portfolio. Some are successfully
using new sources of data to prevent high risk profiles from entering their portfolios.
For example, online insurer InShared, an Achmea initiative, has fully automated its
risk assessment, and determines risk at the point of purchase using multiple
indicators such as the person’s conduct, payment risk and claim risk.123

Insurers will look for innovative
technology to help identify high risk
cases.

As digital insurance becomes more widespread, insurers will look for new
approaches to help identify high-risk cases. About one out of five people admit to
lying on insurance claims applications.124 To counter this, insurers are experimenting
with new digital tools to reduce fraud at the underwriting stage (eg, by utilising facial
biometric tools that leverage machine learning and computer vision technology). In
another example, using third-party data insurers may be able to determine – without
medical evidence – whether a customer is a smoker or not, in order to avoid
expensive medical tests in some cases.

Adverse selection could increase if
insureds withhold new
information from their insurers.

With the spread of digital data, insurers are also more susceptible to rising risks of
anti-selection (adverse selection) as consumers build more understanding of their
own health with different healthy-living apps, genetic tests cheaply available direct
to consumers (DTC), and testing devices available on the market. These advances
bring equal promise and risks, including over-diagnosis and subsequent unwarranted
treatments. The major challenge for insurers is to obtain adequate and risk-relevant
information during the underwriting process, since existing regulation was mostly
enacted before the widespread adoption of DTC genetic tests.125

121 Announcement Of Audited Results For The Year Ended December 31, 2018, Ping An, 2018.
122 Susan Morse, “Insurtech entrepreneurs to lead panel at Health 2.0”, healthcarefinancenews.com,
12 September 2019.
123 How insurers are in control of risks at underwriting, celent.com, 2016.
124 Lying on an insurance application, www.finder.com, October 2018.
125 SONAR: New emeging risk insights, Swiss Re Institute, May 2019

36 Swiss Re sigma No 1/2020

Underwriting implications, for insureds and insurers
Constant feedback-based processes
will provide better insights for new
products.

Product development and underwriting will increasingly interact and be dependent
on customer facing functions. For example, marketing models will integrate data
obtained from the initial underwriting experience, customer feedback and other
external data to make targeted and pre-approved offers of cover at the right time,
based on specific customer needs and behaviours. Offers will focus on increasing
existing cover to suit needs but also cross-selling into new products that match new
risks, however small, and thereby increase engagement. This can have a direct
impact on new product development, with implications for insureds and insurers.
̤̤ Insureds: Other than for large corporate risks where placement negotiations
involve underwriters visiting and explaining technical aspects to clients,
underwriting has never been truly customer-facing. We expect modularisation of
products and dynamic pricing adjustment will bring higher understanding of
underwriting and risk assessment to retail customers. For example, life or SME
risk scoring apps will make the opaque and cumbersome underwriting process
more consumer-friendly and transparent. Underwriting bundled with proactive
risk management through value added or preventive services will change the
insurer-customer relationship from risk transfer to risk partnership.
̤̤ Insurers: Insurers can benefit from dynamic and more accurate risk pricing, but
they will need to upgrade their systems to process terrabytes of data coming
every day from sensors. Many insurers are still ironing out legacy system
challenges.126 Also, insurers will need to become more responsive to digital
feedback on product development, tests and experiments. The digital world offers
immediate and sometimes ruthless feedback. For instance, one on-demand digital
insurer saw a massive drop in conversions and discovered it was caused by a
small change in the process flow requested by a small segment of customers.127

126 K. Harris-Ferrante, 2019 CIO Agenda: Insurance Industry Insights” Gartner, 15 October 2018.
127 A Khusid, “How we built a customer feedback loop that works”, medium.com, 15 January 2018.

Swiss Re sigma No 1/2020 37

Claims management
Insurers are using advanced analytics and machine learning to create early warning systems and gather practical
insights that prevent accidents, and simplify and speed-up claims processing. Examples include using AI to detect and
verify accident hot spots, estimate repair costs, and identify potential fraud. Historically claims processing has been a
form filling-in exercise. Digitalisation will help improve customer experience and efficiency of back office processes.
A modular approach is key for
end-to-end claim digitalisation.

Digitalisation can improve the claims experience both in terms of trust, lower friction
in customer experience, and complexity in back office operations. In a survey, only
57% of adults online in the US expressed confidence that their insurer would treat
them fairly in the event of a claim.128 A modular approach will enable end-to-end
digitalisation and ongoing claims management processes (see Table 5). For example
in 2018, USAA launched a 21-member innovation team to digitise components of
claims operations and reduce the full claims cycle to days, and even hours in some
cases.129

Table 5
Claims innovation timeline
Claims journey

Past

Present

Future

Key enablers

FNOL
(First Notice of Loss)

Complex forms which
require explanation
Manually by insurer staff

Data gathering/
fraud detection

Rudimentary fraud
analytics

Claims estimation

Manual claims
adjudication

Automatically: IoT,
satellite, weather
stations, etc.
Digitally augmented
support in complex
situations; AI advice
Automated using
analytics like, big data
and AI
Automated claim
adjudication

Satellite imagery,
weather stations, IoT, etc.

Claims admin

Customer uses the
phone, post, online/
mobile apps
Data entry by insurer
staff

Settlement

Reimbursement via bank
cheques /drafts

Instant payout,
automated validation,
options (eg, replacement,
second hand)

Linked payment systems,
intelligent algorithms

Combination of manual
and models to identify
fraud
Mix of manual inspection
or advanced
technologies
Direct to vendor (garage,
hospital) and
reimbursement

Connected systems,
intelligent algorithms
Computing power,
access to varied data
sources
Computing power, better
algorithms

Source: Swiss Re Institute

128 Forrester Analytics Consumer Technographics® North American Financial Services Survey, Forrester,
Q4 2017 (US).
129 Inside USAA’s claims innovation team, DIG IN, 15 August 2018.

38

Swiss Re sigma No 1/2020

End-to-end digitalisation of claims management
Digital loss prevention
Loss prevention is an important but
undervalued aspect of the insurance
value chain.

Loss prevention is an important but, arguably, undervalued aspect of the insurance
value chain, partly due to lack of research and understanding, and because of its
capital-intensive nature. Investing in loss prevention technologies requires significant
investment with possibly little to no short-term impact on claims reduction.

Fast-growing internet and smart
phone penetration have made loss
prevention action more feasible.

Across lines of business, it is becoming more feasible financially and technologywise to alert policyholders to the potential threat of damage to life and/or property.
For instance, on receiving notification from meteorological departments or sensors
about a catastrophe, insurers can send push safety warning notifications to
customers’ smartphones. This will help insureds take preventive measures to
mitigate damage. Figure 21 illustrates where an insurer can use external and internal
systems to alert insureds to potential threats.

Figure 21
Illustration of a potential digital prevention process for a natural catastrophe event
System integration

Data sources

Build platform

APIs

Sensors

Finding the
nearest safe zone
and routes in case
of evacuation

Notifications
and alerts sent
to policy holders

GIS systems

Policy holders

Lower claims

Consumer takes
steps to prevent
loss of life and
property

Insurers systems
to capture, process
and prioritise
information

Meteorological/
Government agencies

Real-time application

Real-time
situation
monitoring

Cloud-based
network
External systems

Insurer systems

Source: Swiss Re Institute

Digital first-loss notification
New insights are enabling automatedrisk management processes via Digital
FNOL.

There has been significant innovation in First Notification of Loss (FNOL) processes
given IoT sensors, high-speed internet and a wide range of automated data-capture
technologies such as drones and satellite imagery. This data can be shared with a
range of audiences. For instance, CSAA Insurance partnered with Owlcam on a
Video First Notice of Loss (vFNOL) to send videos to a driver’s mobile phone when a
car crashes or is broken into. The driver can share it with the police.130

130 CSAA Insurance Group and Owlcam Partner to Accelerate InsuranceClaims Process for Customers by
Utilizing Video Technology, CSAA Insurance Group, 5 June 2019.

Swiss Re sigma No 1/2020 39

Claims management

Insurers can initiate contact with
policyholders even before a claim is
reported.

Insurers now pro actively initiate contact with customers and offer help even before
a claim is reported. PZU, the largest insurer in Central and Eastern Europe, has
launched a Before-You-Call Service where operational coordinators actively scan
digital and analog information for news of loss events from the web, emails, calls
from witnesses, TV and radio. If they identify customers who could potentially have
been impacted by the loss event, they verify the scope of those customers’
insurance, and offer assistance services covered by their policies. A customer can
register a claim during this first contact or at a later convenient date.131

Claims administration
AI applications can accelerate the
customer through the next stage of
the claims process.

In this phase of the claims journey, a case is analysed based on the information
received in the FNOL (eg, checking coverage details, screening for fraudulent
claims). With AI assistance, digital assessments can automatically verify coverage
details like sum assured, deductibles, co-payments and past claims. The reduction in
manual touchpoints can speed up the process significantly, and also lower the costs
associated with manual inspections. For example, Farmers Insurance adjudicates
claims using only photos and video-estimating technology in 95% of total-loss auto
claims. It aims to expand this capability to property as well.132

Fraud detection
Advanced analytics can improve fraud
detection…

Insurance fraud costs billions of dollars each year. Traditional fraud-detection
statistical models have drawbacks, not least that sampling methods to analyse data
can lead to undetected fraud. Also, these methods rely on experience: when a new
type of fraud occurs, insurers are vulnerable. With Big Data and AI, new tools can
combine thousands of variables from the moment a claim is filed, and identify
incoherence between data relating to the claim and the wider context. Many
insurers use social media analytics to detect fraudulent individual claims. For
example, Allstate is leveraging online information to identify fraud faster (like
claimants who say they are too injured to work, yet engage in strenuous activity such
as horseback riding).133 Analytics can help uncover large frauds by flagging certain
suspicious events like when smaller medical clinics originate large volumes of claims.
While individual insurers may lack sufficient data to detect mass fraud, law
enforcement agencies co-ordinate among carriers with analytics know-how to
uncover patterns (eg, search for comparable bills coming from the same parties).134

Digital loss assessment, claims estimation and settlement
…and also help automate loss
assessment.

Digitalisation can increase transparency in dealing with customers, with provision of
a comprehensive overview of the calculation which could also prevent litigation and
recalculation costs. Moreover, automation of claims assessment can help reduce
administration costs, making insurance more affordable. During the California
wildfires of 2018, USAA used images from drones to settle many home insurance
claims on the same day. Previously, it could take weeks or even months for a team to
be safely mobilised to inspect and adjudicate the loss to a property damaged by
fire.135 Other interesting experiments are emerging. For example, Allianz ran a pilot
with Amazon to enable claims handlers to make an offer on the day a customer
makes a claim by assessing the content loss based on live Amazon prices.136

131 How PZU has successfully built a culture of innovation, EFMA, 5 February 2019.
132 “How Insurers Are Digitizing Claims with Apps, Photos, Videos, E-Payments and More”,
insurancejournal.com, 11 June 2019.
133 “Allstate Finds Fraud Faster with Innovative Data Company”, prnewswire.com, 28 November 2017.
134 sigma 4/2019 - Advanced analytics: unlocking new frontiers in P&C insurance, Swiss Re Institute,
August 2019.
135 Inside USAA’s claims innovation team, DIG IN, 15 August 2019.
136 Allianz Replacement Solution – Allianz France, EFMA, 2019.

40 Swiss Re sigma No 1/2020

There has been wider adoption of
touchless loss assessment and claims
settlement in personal and small
business insurance…

Figure 22 is a graphical representation of touchless claims settlements at work in
parametric insurance policies. Claims for personal lines and small-business
insurance can be largely automated, enabling carriers to achieve straight-through
processing rates and reduce claims processing times from days to hours or minutes.
In motor insurance, use of AI-assisted assessment can significantly reduce the time
and cost of loss assessment.137 Ping An has employed AI to settle motor claims
across the spectrum of activities, including first notice, sharing digital pictures, loss
assessment and document handling. Surveys can be completed within 5–10
minutes for 95.5% of daytime accidents and, using facial recognition, payments can
be made in seconds.138

Figure 22
Touchless claims settlement
in parametric insurance

Smart
distribution

Flight
delay

Earthquake

Tropical
cyclone

Agriculture

Smart
policy

Smart claims
trigger

Smart
payout

$

Travel
platform

Buy

$

(Digital) agent/
broker

Buy

$

Direct (digital)
mobile

Buy

$

Agriculture
ecosystem

Buy

$

$

$

$

Source: Swiss Re Institute

…and some early experimentation in
life lines.

Insurers are experimenting with new claims technology in life insurance too. MetLife
piloted an Ethereum blockchain pilot known as Lifechain to help families determine if
a deceased is protected by a policy. Once an obituary is placed in the press, the
family is automatically informed of this MetLife programme and, if they participate,
Lifechain encrypts the deceased’s national ID number and searches for a matching
policy. If a match is found, an automatic notification initiates the claims process.139

137 In case of an accident, the policyholder just needs to take few photos and streaming videos of the
vehicle and damaged area and upload it on insurer’s app together with the description of the damage
and the accident.
138 “China’s Ping An P&C Unveils Credit-Based Smart Auto Insurance Claim Solution”, insurancejournal.
com, 30 January 2019.
139 “How does it make faster life insurance with blockchain?”, medium.com, 13 August 2019.

Swiss Re sigma No 1/2020 41

Claims management

Claims implications, for insureds and insurers
Processes will become flexible and
open to innovation

In the future, greater use will be made of characteristics of claims submissions to
better inform product pricing and underwriting decisions. Insurers can generate data
that leads to better risk assessments for policy underwriting. Using real time claims
data, insurers will also be able to better monitor which partners and customer
segments offer most growth potential, and adapt marketing and distribution
initiatives in real time. To iterate quickly, processes will need to be flexible and open
to innovation.
̤̤ Insureds: The quality of the claims experience has a strong impact on loyalty and
renewal behaviour. In a property claims satisfaction study by JD Power, 90% of
“highly satisfied” claimants said they would definitely renew their policies. On the
other hand, 60% of claimants who were “indifferent” or “displeased” said they
would shop for a new carrier within the year.140 Insurers have built customer
experience and digital channels development units to introduce design skills and
thinking methodologies into key areas. They have redesigned services and
processes by putting themselves in customers’ shoes.141
̤̤ Insurers: Loss prevention will become a bigger theme in claims management.
Insurers will combine claims and behavioural data to action loss prevention
measures. As different components of the value chain digitally share information
and feedback, this could compress the value chain as digital distribution platforms
connect directly with capital providers without depending on traditional
capabilities of primary insurers. For example, providers of parametric insurance
may dispense with large claims departments and assessors, except in cases of
suspected fraud. In another example, a global insurer used an InsurTech program
platform to transfer part of a >USD 100 million layer of parametric risk to multiple
risk markets.142

140 Satisfaction with Property Insurance Claims Surges, Even as Number of Catastrophes Reaches
10-Year High, J.D. Power Finds, J.D. Power, 1 March 2017.
141 We innovate for our customers, Generali. op. cit.
142 “Tremor places world’s first programmatic parametric swap transaction”, Artemis, 20 May 2019.

42

Swiss Re sigma No 1/2020

Outlook
Digitalisation will enable development of new data-driven business models that impact the entire insurance value
chain. Access to data and the capability to model risks will be core to success. True leverage will be through
collaboration with other assets such as key supplier partners, entry points to ecosystems and the know how to
generate customer insights.

Refining existing and developing new competencies
The insurance value chain can benefit
from new data and analytics.

In recent years, as premium growth has slowed, insurers have been looking to a
variety of approaches to differentiate products and services within the rapidly
expanding new economy sectors (see Figure 23).

Figure 23
New capabilities to be established
Business transformation
Mindset

Partner network
Superior partner
network (access to
risk and access to
capabilities)

Culture

Data & insights
Unique data-sets
and/or insights access

Customer insights
Deep customer
(behavioural) insights

Analytics
Data analytics &
machine intelligence,
collaborative,
autonomous robotics

Underwriting
expertise
Scalable risk modelling
and pricing expertise

Capabilities
Source: Swiss Re Institute

Insurers will need to build capabilities in the following areas:
̤̤ Building a superior partner network: Identifying innovative and appropriate
ecosystem partners and suppliers, including in the technology space, to offer
unique value propositions to customers. Competition for suitable partners is high,
and relationships need to be fostered, the focus being on creating mutual value.
̤̤ Accessing unique and large data sets: Besides existing and accessible data,
owning unique data sets for superior risk modelling is becoming an important
differentiating factor. The key is to assemble data layers in a way that creates
unique insights that cannot be easily copied by competitors.
̤̤ Developing deeper customer insights: Putting the customer at the centre of the
ecosystem and aligning all actions to customer needs. Using behavioural insights
to personalise offerings is essential. This holds true even if an insurer does not
own a customer relationship directly.

Swiss Re sigma No 1/2020 43

Outlook

̤̤ Analytical expertise: The more insurers understand data and related context, the
more sophisticated risk modelling can become. AI stands to reshape the risk
landscape, resulting in better simulations.
̤̤ Scalable underwriting expertise: Insurers can build superior risk models based
on existing and unique data sets, combined with analytical methods and tools that
can be scaled across ecosystems and value chains. This will enable insurers to
make appropriate adjustments to value chains and respond more dynamically.

Impact on the existing insurance value chain
The focus of customer insights within
the value chain is changing.

There will be a close interplay between product design, claims management and
distribution touchpoints to generate customer insights. The changing location of the
yellow box across the three panels in Figure 24 shows how the sources of customer
insights are changing. In the past, the product factory (the insurer) was disconnected
from the direct customer. There was instead close collaboration between the insurer
and the physical sales channel, the intermediary. Intermediaries were in direct
contact with the customer and, with their customer insights, could go back to the
insurer to ask for products to be designed in a particular way.

Figure 24
Decreasing distance between supplier and customer via digital channels
Yesterday

Today

Interaction
channel

Customer

Customer journey

Corporation
customer

Personal
direct

offline
online

Retail
customer

Personal
3rd party

Tomorrow

Customer journey

Corporation
customer

Direct
online offline

offline
online

Customer journey

Retail
customer

Corporation
customer

Phone
Personal
chat direct 3rd party

Retail
customer

offline
online

Digital
platform

Digitally
Digitally
augmented
augmented
empathic human
digital
interface
interface

Producer
(supplier)

Smart
augmented Platform
Product factory
(insurance)

Product factory
(insurance)

Digital
augmentation

Critical insights
Source: Swiss Re Institute

The product factory accesses insights
from multiple channels.

44

Swiss Re sigma No 1/2020

Today, the landscape has shifted towards a multi- and omni-channel approach, and
the product factory is getting closer to the end customer. This is because insurers
have greater influence over product design. Products must be customised to be sold
via different channels: simpler products are sold direct, while products with greater

complexity are largely sold through personal channels (eg, life insurance combined
with investment management). The greater the complexity of products, the lower
the willingness on the part of consumers to buy online. At the same time, the more
insurers move to direct channels, the simpler their products need to become.
Product portfolios will therefore begin to display a wide mix, as insurers create
products that can be modularised and sliced and diced for different channels.
In the future, product factories will be able to directly access all the ingredients of
product design and will be much closer to the risk. These insights will be customised
based on digital touchpoints, and inputs about exposure will be generated in real
time. We think this will result in new business models being created purely around
data access.

In the future, product factories will
directly access all the ingredients for
policy design...

New business models around data
...including access to secure data.

These business models will require secure access to data that can be combined in
different ways, including insights from connected objects, platform providers,
behavioural insights from the consumer and environmental data. Categories 1 to 10
in Figure 25 show examples of data that will be available to an insurer. Some of these
are newer data (eg, behavioural data based on customer context), while others are
classical traditional data (eg, from claims and preventive services).

Figure 25
Illustration of a potential digital proposition and digital business models along the customer journey
Customer journey
1)

Behavioural &
contextual data insights

2)

Smart assistant
services insights

3)

Touch-points and
distribution insights

4)

Channel/market
places insights

5)

Aggregated
data insights

6)

Environmental
data insights

7)

Contextual
provider insights

8)

Sensor data insights
(customer lifestyle cluster)

offline
online

BM1

BM2

Health

9)
10)

Insurance value chain
insights (ie, claims)
Preventive service/value
adding service insights

Education/
working

Product
management

UWR
pricing

Living

Free time

Marketing &
Claims
sales
management

Mobility

Customer
service

Note: BM stands for Business Model
Source: Swiss Re Institute

Swiss Re sigma No 1/2020 45

Outlook

Data from multiple providers via open
Application Programming Interfaces
(API) may be combined to enable
different business models.

In the future, insurers will operate in an environment where they will need continuous
access to different data sources. This is a strategic issue, taking insurers beyond their
existing value chain. No single firm or market place currently provides all these
sources of data. While many data vendors focus on extraction and distribution of
data, few concentrate on data refinement.143 We expect that this will give rise to
specialised aggregators focused on integration and refinement. The more integrated
and refined the data, the wider the service offering to a customer. This has many
implications for the insurance industry, in particular the need for modular products,
personalisation and better distribution.

Insurers may need to invest in
separate new data businesses.

Insurers will need to decide if they are mere suppliers of coverage, or whether they
are willing to collaborate or own other business models to gain control over the key
areas that impact their business. Different stakeholders will seek control over data
driven businesses. For example, if all telematics data were put onto one platform
anonymously, who would be most likely to own that market place? Will brokers stake
a claim? Players that cannot influence such data aggregators will find the going
difficult. If data brokers become omnipresent, the insurers dependent on them could,
over time, be relegated to the status of suppliers.

143 How data will shape the new urban future, Swiss Re Institute, 15 January 2019.

46

Swiss Re sigma No 1/2020

Conclusion
Consumers are more demanding than
ever before. Using digital technology,
insurers can respond.

The insurance landscape is changing as consumers seek more engaging and
personalised purchase experiences, relevant to their lifestyle. Empowered with
digitally-facilitated information and greater choice, consumers have become more
informed and self-directed than ever before. On the supply side, tools like AI enable
more effective customer interaction, allowing insurers to better understand
consumer preferences to develop customised and flexible product offering, with
dynamic pricing and servicing through (personal) virtual assistance, 24/7. New
sources of data also offer opportunities for more granular client segmentation

Boundaries are increasingly blurred
across sectors.

The impact of digitalisation extends beyond the insurance value chain itself to the
whole business ecosystem in which insurers operate. Industry boundaries are
becoming blurred as firms in several sectors build digital platforms that can connect
to different market places, supply chain hubs and financial networks. Non-insurer
participants in business ecosystems like manufacturers and telecom companies too
are gaining access to customer data, and digital analytics capabilities can enhance
their client product offering.

Lasting change will require more than
incremental thinking.

Utilised more fully and intelligently, new data and technology can reinforce and
secure the relevance of the insurance industry to future customers. The same is true
of leveraging cross industry ecosystems. This sigma depicts the end version of what
a digital insurer will look like. While high barriers to entry offer protection against
competitive threat to some extent, incumbents must continue to embrace both
incremental and sometimes more radical innovation to optimise the potential of
consumers touchpoints from both the insured and the insurer perspective.

Innovation will help transform the
effectiveness and raison d’ étre of the
insurance industry.

Full-scale disruption of existing insurers seems unlikely, at least in the near term.
Incumbents have time to adjust to the changing risk environment, shifts in customer
attitudes and accelerating advances in technology, but there is no room for
complacency. Successful insurers will be those that can leverage insights from their
investments, partnerships and collaborations to upgrade their business practices.
Forward thinking and innovative insurers could build on the new infrastructure being
created today to offer compelling risk protection solutions aligned with evolving
regulation and, in doing so, genuinely transform industry effectiveness.

Swiss Re sigma No 1/2020 47

Recent sigma publications
2020

No 1

2019

Data-driven insurance: ready for the next frontier?

No 1
No 2
			
No 3
		
No 4
No 5
No 6

Emerging markets: the silver lining amid a challenging outlook
Natural catastrophes and man-made disasters in 2018: “secondary“ perils
on the frontline
World insurance: the great pivot east continues
Advanced analytics: unlocking new frontiers in P&C insurance
Indexing resilience: a primer for insurance markets and economies
Global economic and insurance outlook 2020/21

2018
No 1
			
No 2
No 3
No 4
No 5
No 6

Natural catastrophes and man-made disasters in 2017:
a year of record-breaking losses
Constructing the future: recent developments in engineering insurance
World insurance in 2017: solid, but mature life markets weigh on growth
Profitability in non-life insurance: mind the gap
Global economic and insurance outlook 2020
Mortality improvement: understanding the past and framing the future

2017

No 1
No 2
			
No 3
No 4
No 5
No 6
2016

No 1
No 2
No 3
No 4
No 5

2015

No 1
No 2
			
No 3
No 4
No 5
No 6

Cyber: getting to grips with a complex risk
Natural catastrophes and man-made disasters in 2016:
a year of widespread damages
World insurance in 2016: the China growth engine steams ahead
Insurance: adding value to development in emerging markets
Commercial insurance: expanding the scope of insurability
Life in-force management: improving consumer value and long-term profitability
Natural catastrophes and man-made disasters in 2015:
Asia suffers substantial losses
Insuring the frontier markets
World insurance 2015: steady growth amid regional disparities
Mutual insurance in the 21st century: back to the future?
Strategic reinsurance and insurance: the increasing trend of customised solutions
Keeping healthy in emerging markets: insurance can help
Natural catastrophes and man-made disasters in 2014:
convective and winter storms generate most losses
M & A in insurance: start of a new wave?
World insurance in 2014: back to life
Underinsurance of property risks: closing the gap
Life insurance in the digital age: fundamental transformation ahead

2014

No 1	Natural catastrophes and man-made disasters in 2013:
large losses from floods and hail; Haiyan hits the Philippines
No 2 Digital distribution in insurance: a quiet revolution
No 3 World insurance in 2013: steering towards recovery
No 4 Liability claims trends: emerging risks and rebounding economic drivers
No 5 How will we care? Finding sustainable long-term care solutions for an ageing world

2013

No 1	Partnering for food security in emerging markets
No 2 Natural catastrophes and man-made disasters in 2012:
A year of extreme weather events in the US
No 3 World insurance 2012: Progressing on the long and winding road to recovery
No 4 Navigating recent developments in marine and airline insurance
No 5 Urbanisation in emerging markets: boon and bane for insurers
No 6 Life insurance: focusing on the consumer

Published by
Swiss Re Management Ltd.
Swiss Re Institute
Mythenquai 50/60
P.O. Box
8022 Zurich
Switzerland
Telephone
Email

+41 43 285 2551
institute@swissre.com

Authors
Dr. Evangelos Avramakis
Jonathan Anchen
Ashish Dave
Aakash Kiran Raverkar
Binay Biswal
Rajeev Sharan
Sophia Steinmetz
sigma editor
Paul Ronke

© 2020 Swiss Re. All rights reserved.
The editorial deadline for this study was 10 November 2019.

Managing editors
Dan Ryan

sigma is available on Swiss Re’s website:
www.swissre.com/sigma

Dr Jerome Jean Haegeli
Swiss Re Group Chief Economist

The internet version may contain slightly updated information.
Graphic design and production:
Corporate Real Estate & Logistics / Media Production, Zurich

Printing: Multicolor Print AG, Baar

The entire content of this sigma edition is subject to copyright with all
rights reserved. The information in this edition may be used for private or
internal purposes, provided that any copyright or other proprietary
notices are not removed. Electronic reuse of the information published in
sigma is prohibited.
Reproduction in whole or in part or use for any public purpose is
permitted only with the prior written approval of Swiss Re Institute and if
the source reference “sigma 1/2020 Data-driven insurance: ready for the
next frontier?“ is indicated. Courtesy copies are appreciated.
Although all the information used in this sigma edition was taken from
reliable sources, Swiss Re does not accept any responsibility for the
accuracy or comprehensiveness of the information given or forward
looking statements made. The information provided and forward-looking
statements made are for informational purposes only and in no way
constitute or should be taken to reflect Swiss Re’s position, in particular in
relation to any ongoing or future dispute. In no event shall Swiss Re be
liable for any loss or damage arising in connection with the use of this
information and readers are cautioned not to place undue reliance on
forward-looking statements. Swiss Re undertakes no obligation to
publicly revise or update any forward-looking statements, whether as a
result of new information, future events or otherwise.
Order no: 270_0120_EN

Swiss Re Management Ltd.
Swiss Re Institute
Mythenquai 50 /60
P.O. Box
8022 Zurich
Switzerland
Telephone + 41 43 285 2551
swissre.com/institute

