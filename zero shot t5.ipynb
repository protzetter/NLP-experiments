{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422120cb-36e8-4ea6-b3cb-5c02fc830cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d1f374e-a959-48fb-a1ea-6d57130cc7b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a837f01-9ba4-4678-8740-a1ca29ef9143",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe457372-c06d-46b5-b00e-b1146bf16b51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#path of first input test file\n",
    "path='./sampledocs/txt/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caae552-a1ee-4a7b-abc9-0b5425077724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pkg_resources/__init__.py:119: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  PkgResourcesDeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "# let us scan the full directory, read the text files and clean them using texthero\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.txt')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    f = open(file_name,'r')\n",
    "    fileText=f.read()\n",
    "    docName.append(file_name)\n",
    "    docType.append('txt')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0210fda-5f1c-4e07-ad86-1314212d5ddc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/txt/Technology-and-innovation-in-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/txt/ai-360-research.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>ai insights next frontier business corner offi...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/txt/Module-1-Lecture-Slides.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>application ai insurtech real estate technolog...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/txt/Insurance-2030-The-impact-of-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Insurance Practice\\n\\nInsurance 2030—\\nThe imp...</td>\n",
       "      <td>insurance practice insurance -- impact ai futu...</td>\n",
       "      <td>4424</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/txt/sigma-5-2020-en.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>machine intelligence insurance insights end en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/txt/Technology-and-innovation-in-...  txt   \n",
       "1               ./sampledocs/txt/ai-360-research.txt  txt   \n",
       "2       ./sampledocs/txt/Module-1-Lecture-Slides.txt  txt   \n",
       "3  ./sampledocs/txt/Insurance-2030-The-impact-of-...  txt   \n",
       "4               ./sampledocs/txt/sigma-5-2020-en.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Insurance Practice\\n\\nInsurance 2030—\\nThe imp...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  technology innovation insurance sector technol...            16742   \n",
       "1  ai insights next frontier business corner offi...             5281   \n",
       "2  application ai insurtech real estate technolog...             3728   \n",
       "3  insurance practice insurance -- impact ai futu...             4424   \n",
       "4  machine intelligence insurance insights end en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               4228  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               1782  \n",
       "4               4329  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39db2113-ab1d-4f52-9a69-bc17ce6a32ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fa254023d5f4d18a23f2202603ea421"
      },
      "text/plain": [
       "                                                 Name Type  \\\n",
       "0   ./sampledocs/txt/Technology-and-innovation-in-...  txt   \n",
       "1                ./sampledocs/txt/ai-360-research.txt  txt   \n",
       "2        ./sampledocs/txt/Module-1-Lecture-Slides.txt  txt   \n",
       "3   ./sampledocs/txt/Insurance-2030-The-impact-of-...  txt   \n",
       "4                ./sampledocs/txt/sigma-5-2020-en.txt  txt   \n",
       "5   ./sampledocs/txt/Issues_Paper_on_Increasing_Di...  txt   \n",
       "6   ./sampledocs/txt/Data machine the insurers usi...  txt   \n",
       "7   ./sampledocs/txt/Artificial Financial Intellig...  txt   \n",
       "8          ./sampledocs/txt/ai_in_insurance_web_0.txt  txt   \n",
       "9                 ./sampledocs/txt/sigma1_2020_en.txt  txt   \n",
       "10  ./sampledocs/txt/Impact-Big-Data-AI-in-the-Ins...  txt   \n",
       "11                  ./sampledocs/txt/ai-insurance.txt  txt   \n",
       "12  ./sampledocs/txt/Digital-disruption-in-Insuran...  txt   \n",
       "13  ./sampledocs/txt/Innovation_Artificial-Intelli...  txt   \n",
       "14  ./sampledocs/txt/WEF_Governance_of_Chatbots_in...  txt   \n",
       "15  ./sampledocs/txt/AI-bank-of-the-future-Can-ban...  txt   \n",
       "16  ./sampledocs/txt/Kaggle State of Machine Learn...  txt   \n",
       "17  ./sampledocs/txt/fra-2020-artificial-intellige...  txt   \n",
       "\n",
       "                                                 Text  \\\n",
       "0   Technology and\\ninnovation in the\\ninsurance s...   \n",
       "1   AI 360: insights from the\\nnext frontier of bu...   \n",
       "2   Application of AI, Insurtech and Real Estate\\n...   \n",
       "3   Insurance Practice\\n\\nInsurance 2030—\\nThe imp...   \n",
       "4   No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "5   Issues Paper on Increasing Digitalisation in\\n...   \n",
       "6   Data machine: the insurers using AI to reshape...   \n",
       "7   Texas A&M University School of Law\\n\\nTexas A&...   \n",
       "8   Promoting Responsible\\nArtificial Intelligence...   \n",
       "9   No 1 /2020\\n\\nData-driven insurance:\\nready fo...   \n",
       "10  The Impact of Big Data and\\nArtificial Intelli...   \n",
       "11  AI in Insurance\\nTop Use Cases, Challenges, an...   \n",
       "12  Digital disruption\\nin insurance:\\nCutting thr...   \n",
       "13  From mystery to mastery:\\nUnlocking the busine...   \n",
       "14  In collaboration with\\nMitsubishi Chemical Hol...   \n",
       "15  Global Banking & Securities\\n\\nAI-bank of the ...   \n",
       "16  State of Machine\\nLearning and Data\\nScience 2...   \n",
       "17  REPORT\\n\\nGETTING THE\\nFUTURE RIGHT\\n―\\nARTIFI...   \n",
       "\n",
       "                                            cleanText  text_word_count  \\\n",
       "0   technology innovation insurance sector technol...            16742   \n",
       "1   ai insights next frontier business corner offi...             5281   \n",
       "2   application ai insurtech real estate technolog...             3728   \n",
       "3   insurance practice insurance -- impact ai futu...             4424   \n",
       "4   machine intelligence insurance insights end en...            14478   \n",
       "5   issues paper increasing digitalisation insuran...            15370   \n",
       "6   data machine insurers using ai reshape industr...             1454   \n",
       "7   texas university school law texas law scholars...            22240   \n",
       "8   promoting responsible artificial intelligence ...             9087   \n",
       "9   data driven insurance ready next frontier exec...            18818   \n",
       "10  impact big data artificial intelligence ai ins...            13471   \n",
       "11  ai insurance top use cases challenges trends w...             6558   \n",
       "12  digital disruption insurance cutting noise con...            34485   \n",
       "13  mystery mastery unlocking business value artif...            10921   \n",
       "14  collaboration mitsubishi chemical holdings cor...            10301   \n",
       "15  global banking securities ai bank future banks...             5774   \n",
       "16  state machine learning data science enterprise...             3660   \n",
       "17  report getting future right -- artificial inte...            49748   \n",
       "\n",
       "    text_unique_words  \n",
       "0                4228  \n",
       "1                1746  \n",
       "2                1506  \n",
       "3                1782  \n",
       "4                4329  \n",
       "5                3667  \n",
       "6                 684  \n",
       "7                6349  \n",
       "8                2706  \n",
       "9                5309  \n",
       "10               3467  \n",
       "11               2297  \n",
       "12               7049  \n",
       "13               3356  \n",
       "14               2941  \n",
       "15               2144  \n",
       "16               1287  \n",
       "17               8458  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import the Data Wrangler widget to show automatic visualization and generate code to fix data quality issues\n",
    "\n",
    "import sagemaker_datawrangler\n",
    "display(fullDocs)\n",
    "\n",
    "# Display Pandas DataFrame to view the widget: df, display(df), df.sample()... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93d54364-7193-4ba0-aaec-fe7b906a510c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_id, model_version, = (\n",
    "    \"huggingface-text2text-flan-t5-xl\",\n",
    "    \"*\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "781f02e4-0b91-4ad1-9679-2149f2cd036d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from ipywidgets import Dropdown\n",
    "from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "# Retrieves all Text Generation models available by SageMaker Built-In Algorithms.\n",
    "filter_value = \"task == text2text\"\n",
    "text_generation_models = list_jumpstart_models(filter=filter_value)\n",
    "\n",
    "# display the model-ids in a dropdown to select a model for inference.\n",
    "model_dropdown = Dropdown(\n",
    "    options=text_generation_models,\n",
    "    value=model_id,\n",
    "    description=\"Select a model\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    "    layout={\"width\": \"max-content\"},\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0e87fea-2b0c-43d9-ac02-091310edd029",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f250148324c4c00b06c830221fcb6fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Select a model', index=8, layout=Layout(width='max-content'), options=('huggingface-text…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "display(model_dropdown)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b4add3e-d4c7-45f6-8a45-e31e3a4087e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# model_version=\"*\" fetches the latest version of the model\n",
    "model_id, model_version = model_dropdown.value, \"*\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b944667-931c-4acd-b52d-f9b032b8cc77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_sagemaker_session(local_download_dir) -> sagemaker.Session:\n",
    "    \"\"\"Return the SageMaker session.\"\"\"\n",
    "\n",
    "    sagemaker_client = boto3.client(\n",
    "        service_name=\"sagemaker\", region_name=boto3.Session().region_name\n",
    "    )\n",
    "\n",
    "    session_settings = sagemaker.session_settings.SessionSettings(\n",
    "        local_download_dir=local_download_dir\n",
    "    )\n",
    "\n",
    "    # the unit test will ensure you do not commit this change\n",
    "    session = sagemaker.session.Session(\n",
    "        sagemaker_client=sagemaker_client, settings=session_settings\n",
    "    )\n",
    "\n",
    "    return session\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2d836d0-c353-40aa-8a29-ba75fe233013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir -p download_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7b95c08-b7d8-46cd-a02e-fee98cccdcc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "\n",
    "endpoint_name = name_from_base(f\"jumpstart-example-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.p3.2xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri. This is the base HuggingFace container image for the default model above.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,  # automatically inferred from model_id\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the inference script uri. This includes all dependencies and scripts for model loading, inference handling etc.\n",
    "deploy_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"inference\"\n",
    ")\n",
    "\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "if model_id in [\n",
    "    \"huggingface-text2text-flan-t5-xl\",\n",
    "    \"huggingface-text2text-flan-t5-large\",\n",
    "]:  # For those large models,\n",
    "    # we already repack the inference script and model artifacts for you\n",
    "    # Create the SageMaker model instance\n",
    "    model = Model(\n",
    "        image_uri=deploy_image_uri,\n",
    "        model_data=model_uri,\n",
    "        role=aws_role,\n",
    "        predictor_cls=Predictor,\n",
    "        name=endpoint_name,\n",
    "    )\n",
    "else:\n",
    "    # Create the SageMaker model instance\n",
    "    model = Model(\n",
    "        image_uri=deploy_image_uri,\n",
    "        source_dir=deploy_source_uri,\n",
    "        model_data=model_uri,\n",
    "        entry_point=\"inference.py\",  # entry point file in source_dir and present in deploy_source_uri\n",
    "        role=aws_role,\n",
    "        predictor_cls=Predictor,\n",
    "        name=endpoint_name,\n",
    "        sagemaker_session=get_sagemaker_session(\"download_dir\"),\n",
    "    )\n",
    "\n",
    "# deploy the Model. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    predictor_cls=Predictor,\n",
    "    endpoint_name=endpoint_name,\n",
    "    volume_size=30,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3beda62d-fe36-4291-9d86-366d2987a064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "\n",
    "def query_endpoint(encoded_text, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/x-text\", Body=encoded_text\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_text\"]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "afcc4327-8767-43d2-a493-844f41e1623e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "input text: Translate to German:  My name is Arthur\n",
      "generated text: \u001b[1mIch bin Arthur.\u001b[0m\n",
      "\n",
      "Inference:\n",
      "input text: A step by step recipe to make bolognese pasta:\n",
      "generated text: \u001b[1mIn a large saucepan, combine the ground beef, onion, garlic, tomato paste, tomato\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "text1 = \"Translate to German:  My name is Arthur\"\n",
    "text2 = \"A step by step recipe to make bolognese pasta:\"\n",
    "\n",
    "\n",
    "for text in [text1, text2]:\n",
    "    query_response = query_endpoint(text.encode(\"utf-8\"), endpoint_name=endpoint_name)\n",
    "    generated_text = parse_response(query_response)\n",
    "    print(\n",
    "        f\"Inference:{newline}\"\n",
    "        f\"input text: {text}{newline}\"\n",
    "        f\"generated text: {bold}{generated_text}{unbold}{newline}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e23b334e-4e75-4024-8f22-6e40a7168079",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['To make cheese fondue, first gather your ingredients. In a small bowl, combine the melted butter, sour cream, salt and pepper. Stir in the cheeses until melted. Transfer the fondue to ', 'Step 1: Preheat the oven to 350 degrees F (180 degrees C). Line a baking sheet with parchment paper. Step 2: In a large bowl, combine the melted butter, sour cream, salt, and pepper', 'To make cheese fondue, melt 2 sticks butter in a small saucepan over medium heat. Add 2 cups shredded Cheddar cheese and 1 cup chopped chives. Cook, stirring frequently, until the cheese is melted and', 'To make cheese fondue, combine 1 cup melted butter, 2 cups shredded cheddar cheese, 1 cup chopped chives, and 1 cup chopped parsley in a medium saucepan. Bring to a boil over medium heat,', 'Step 1: Preheat the oven to 375 degrees F (190 degrees C). In a large bowl, combine the melted butter and cheese. Stir to combine. Step 2: In a small bowl, combine the cornstarch']\n"
     ]
    }
   ],
   "source": [
    "# Input must be a json\n",
    "payload = {\n",
    "    \"text_inputs\": \"A step by step recipe to make cheese fondue:\",\n",
    "    \"max_length\": 50,\n",
    "    \"num_return_sequences\": 5,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "generated_texts = parse_response_multiple_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370a545d-50f5-465e-a24e-4b699f8ade02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Input must be a json\n",
    "payload = {\n",
    "    \"text_inputs\": \"Tell me the steps to make pizza\",\n",
    "    \"max_length\": 50,\n",
    "    \"num_return_sequences\": 3,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "query_response = query_endpoint_with_json_payload(\n",
    "    json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "\n",
    "def parse_response_multiple_texts(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "generated_texts = parse_response_multiple_texts(query_response)\n",
    "print(generated_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5cfb850-9204-412f-9975-5c51025a9508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text=fullDocs.iloc[6]['cleanText']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bbacef57-32a8-4a68-bd1e-ace72d02fb2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = \"\"\"Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document. Use Amazon Comprehend to create new products based on understanding the structure of documents. For example, using Amazon Comprehend you can search social networking feeds for mentions of products or scan an entire document repository for key phrases. \n",
    "You can access Amazon Comprehend document analysis capabilities using the Amazon Comprehend console or using the Amazon Comprehend APIs. You can run real-time analysis for small workloads or you can start asynchronous analysis jobs for large document sets. You can use the pre-trained models that Amazon Comprehend provides, or you can train your own custom models for classification and entity recognition. \n",
    "All of the Amazon Comprehend features accept UTF-8 text documents as the input. In addition, custom classification and custom entity recognition accept image files, PDF files, and Word files as input. \n",
    "Amazon Comprehend can examine and analyze documents in a variety of languages, depending on the specific feature. For more information, see Languages supported in Amazon Comprehend. Amazon Comprehend's Dominant language capability can examine documents and determine the dominant language for a far wider selection of languages.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27dabcbb-b8f7-4c55-b5d8-21637d233cb0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mNumber of return sequences are set as 3\u001b[0m\n",
      "\n",
      "\u001b[1m For prompt: 'Briefly summarize this sentence: {text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Amazon Comprehend is a natural language processing (NLP) tool that analyzes text documents. Use Amazon Comprehend to create new products based on understanding the structure of documents. Access Amazon Comprehend using the\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Understand how Amazon Comprehend works. Use Amazon Comprehend to analyze documents.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Understand how Amazon Comprehend works. Use Amazon Comprehend to analyze documents.\n",
      "\n",
      "\u001b[1m For prompt: 'Write a short summary for this text: {text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Amazon Comprehend helps you analyze documents to extract insights about their content.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. Use Amazon Comprehend to create new products based on understanding the structure of documents. Access Amazon Comprehend document analysis\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents.\n",
      "\n",
      "\u001b[1m For prompt: 'Generate a short summary this sentence:\n",
      "{text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. Use Amazon Comprehend to create new products based on understanding the structure of documents. Access Amazon Comprehend document analysis\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Amazon Comprehend is a text analysis service that uses natural language processing to extract insights about the content of documents. Use Amazon Comprehend to create new products based on understanding the structure of documents.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. Use Amazon Comprehend to create new products based on understanding the structure of documents. Access Amazon Comprehend document analysis\n",
      "\n",
      "\u001b[1m For prompt: '{text}\n",
      "\n",
      "Write a brief summary in a sentence or less'\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Amazon Comprehend extracts insights about the content of documents.\n",
      "\n",
      "\u001b[1m For prompt: '{text}\n",
      "Summarize the aforementioned text in a single phrase.'\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Use Amazon Comprehend to analyze documents.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Use Amazon Comprehend to extract insights about the content of documents. Use Amazon Comprehend to run real-time analysis for small workloads or to start asynchronous analysis jobs for large document sets. Use Amazon Comprehen\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Use Amazon Comprehend to analyze documents.\n",
      "\n",
      "\u001b[1m For prompt: '{text}\n",
      "Can you generate a short summary of the above paragraph?'\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document.\n",
      "\n",
      "\u001b[1m For prompt: 'Write a sentence based on this summary: {text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Understand how Amazon Comprehend works. Understand how Amazon Comprehend works. Understand how Amazon Comprehend works.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Understand how Amazon Comprehend works. Use Amazon Comprehend to create new products. Access Amazon Comprehend.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Understand how Amazon Comprehend works. Learn how to use Amazon Comprehend.\n",
      "\n",
      "\u001b[1m For prompt: 'Write a sentence based on '{text}''\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Amazon Comprehend uses natural language processing (NLP) to extract insights about the content of documents. It develops insights by recognizing the entities, key phrases, language, sentiments, and other common elements in a document.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Amazon Comprehend is a natural language processing (NLP) tool that analyzes documents and extracts insights about the content of the documents.\n",
      "\n",
      "\u001b[1m For prompt: 'Summarize this article:\n",
      "\n",
      "{text}'\u001b[0m\n",
      "\n",
      "\u001b[1m The 3 summarized results are\u001b[0m:\n",
      "\n",
      "\u001b[1mResult 0\u001b[0m: Understand how Amazon Comprehend works. Understand how Amazon Comprehend works. Understand how Amazon Comprehend works.\n",
      "\n",
      "\u001b[1mResult 1\u001b[0m: Understand how Amazon Comprehend works. Access Amazon Comprehend. Input documents into Amazon Comprehend. Analyze documents in a variety of languages.\n",
      "\n",
      "\u001b[1mResult 2\u001b[0m: Understand how Amazon Comprehend works. Access Amazon Comprehend. Input documents into Amazon Comprehend. Analyze documents in a variety of languages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"Briefly summarize this sentence: {text}\",\n",
    "    \"Write a short summary for this text: {text}\",\n",
    "    \"Generate a short summary this sentence:\\n{text}\",\n",
    "    \"{text}\\n\\nWrite a brief summary in a sentence or less\",\n",
    "    \"{text}\\nSummarize the aforementioned text in a single phrase.\",\n",
    "    \"{text}\\nCan you generate a short summary of the above paragraph?\",\n",
    "    \"Write a sentence based on this summary: {text}\",\n",
    "    \"Write a sentence based on '{text}'\",\n",
    "    \"Summarize this article:\\n\\n{text}\",\n",
    "]\n",
    "\n",
    "num_return_sequences = 3\n",
    "parameters = {\n",
    "    \"max_length\": 50,\n",
    "    \"num_return_sequences\": num_return_sequences,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "print(f\"{bold}Number of return sequences are set as {num_return_sequences}{unbold}{newline}\")\n",
    "for each_prompt in prompts:\n",
    "    payload = {\"text_inputs\": each_prompt.replace(\"{text}\", text), **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} For prompt: '{each_prompt}'{unbold}{newline}\")\n",
    "    print(f\"{bold} The {num_return_sequences} summarized results are{unbold}:{newline}\")\n",
    "    for idx, each_generated_text in enumerate(generated_texts):\n",
    "        print(f\"{bold}Result {idx}{unbold}: {each_generated_text}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd3fb7-2495-4aeb-b6ff-805bd5742c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "premise = \"The world cup has kicked off in Los Angeles, United States.\"\n",
    "hypothesis = \"The world cup takes place in United States.\"\n",
    "options = \"\"\"[\"yes\", \"no\"]\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc75a17c-f21c-417d-a8a5-5ae02a2f2430",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"\"\"{premise}\\n\\nBased on the paragraph above can we conclude that \"\\\"{hypothesis}\\\"?\\n\\n{options_}\"\"\",\n",
    "    \"\"\"{premise}\\n\\nBased on that paragraph can we conclude that this sentence is true?\\n{hypothesis}\\n\\n{options_}\"\"\",\n",
    "    \"\"\"{premise}\\n\\nCan we draw the following conclusion?\\n{hypothesis}\\n\\n{options_}\"\"\",\n",
    "    \"\"\"{premise}\\nDoes this next sentence follow, given the preceding text?\\n{hypothesis}\\n\\n{options_}\"\"\",\n",
    "    \"\"\"{premise}\\nCan we infer the following?\\n{hypothesis}\\n\\n{options_}\"\"\",\n",
    "    \"\"\"Read the following paragraph and determine if the hypothesis is true:\\n\\n{premise}\\n\\nHypothesis: {hypothesis}\\n\\n{options_}\"\"\",\n",
    "    \"\"\"Read the text and determine if the sentence is true:\\n\\n{premise}\\n\\nSentence: {hypothesis}\\n\\n{options_}\"\"\",\n",
    "    \"\"\"Can we draw the following hypothesis from the context? \\n\\nContext:\\n\\n{premise}\\n\\nHypothesis: {hypothesis}\\n\\n{options_}\"\"\",\n",
    "    \"\"\"Determine if the sentence is true based on the text below:\\n{hypothesis}\\n\\n{premise}\\n{options_}\"\"\",\n",
    "]\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 50,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "for each_prompt in prompts:\n",
    "    input_text = each_prompt.replace(\"{premise}\", premise)\n",
    "    input_text = input_text.replace(\"{hypothesis}\", hypothesis)\n",
    "    input_text = input_text.replace(\"{options_}\", options)\n",
    "    print(f\"{bold} For prompt{unbold}: '{input_text}'{newline}\")\n",
    "    payload = {\"text_inputs\": input_text, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} The reasoning result is{unbold}: '{generated_texts}'{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2f46c593-cf5d-4c3e-a681-a8db4034ab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "question = \"Which plan is recommended for GPT-J?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fec0e77-acb1-4633-b747-00fcf7c4a350",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m For prompt\u001b[0m: 'Context: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n",
      "    Question: When was NLP Cloud founded?\n",
      "    Answer: 2021\n",
      "    ###\n",
      "    Context: NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n",
      "    Question: What did NLP Cloud develop?\n",
      "    Answer: API\n",
      "    ###\n",
      "    Context: All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n",
      "    Question: When can plans be stopped?\n",
      "    Answer: Anytime\n",
      "    ###\n",
      "    Context: The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n",
      "    Question: {question}\n",
      "    Answer:'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['GPU']'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompts = [\n",
    "    \"\"\"Context: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n",
    "    Question: When was NLP Cloud founded?\n",
    "    Answer: 2021\n",
    "    ###\n",
    "    Context: NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n",
    "    Question: What did NLP Cloud develop?\n",
    "    Answer: API\n",
    "    ###\n",
    "    Context: All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n",
    "    Question: When can plans be stopped?\n",
    "    Answer: Anytime\n",
    "    ###\n",
    "    Context: The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "]\n",
    "\n",
    "\n",
    "parameters = {\n",
    "    \"max_length\": 350,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_p\": 0.1,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "for each_prompt in prompts:\n",
    "    input_text = each_prompt.replace(\"{question}\", question)\n",
    "    print(f\"{bold} For prompt{unbold}: '{each_prompt}'{newline}\")\n",
    "\n",
    "    payload = {\"text_inputs\": input_text, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} The reasoning result is{unbold}: '{generated_texts}'{newline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c1c3169a-9df4-4d04-92ab-9a5687f61e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m For prompt\u001b[0m: 'Context: NLP Cloud was founded in 2021 when the team realized there was no easy way to reliably leverage Natural Language Processing in production.\n",
      "    Question: When was NLP Cloud founded?\n",
      "    Answer: 2021\n",
      "    ###\n",
      "    Context: NLP Cloud developed their API by mid-2020 and they added many pre-trained open-source models since then.\n",
      "    Question: What did NLP Cloud develop?\n",
      "    Answer: API\n",
      "    ###\n",
      "    Context: All plans can be stopped anytime. You only pay for the time you used the service. In case of a downgrade, you will get a discount on your next invoice.\n",
      "    Question: When can plans be stopped?\n",
      "    Answer: Anytime\n",
      "    ###\n",
      "    Context: The main challenge with GPT-J is memory consumption. Using a GPU plan is recommended.\n",
      "    Question: {question}\n",
      "    Answer:'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['GPU']'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for each_prompt in prompts:\n",
    "    input_text = each_prompt.replace(\"{question}\", question)\n",
    "    print(f\"{bold} For prompt{unbold}: '{each_prompt}'{newline}\")\n",
    "\n",
    "    payload = {\"text_inputs\": input_text, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} The reasoning result is{unbold}: '{generated_texts}'{newline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5129d746-fa0a-46e9-8be1-9afa1c442b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question='What is the recommended cheese for a Swiss cheese fondue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2122c554-7f2d-4a41-9f37-d485fe649ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"\"\"Context: The Swiss cheese fondue is best made of 50% of Gruyere cheese .\n",
    "    Question: What percentage of Gruyere is needed in the Swiss cheese fondue?\n",
    "    Answer: 50%\n",
    "    ###\n",
    "    Context: The other 50% of a Swiss cheese fondue is Vacherin Fribourgeois.\n",
    "    Question: What should be the other 50% in teh Swiss cheese fondue?\n",
    "    Answer: Vacherin\n",
    "    ###\n",
    "    Context: The swiss cheese fondue with vacherin and gruyere is called moitié-moitié\n",
    "    Question: How is the swiss chesse findue called ?\n",
    "    Answer: Moitié-moitié\n",
    "    ###\n",
    "    Context: The recommended cheese for a Swiss cheese fondue moitié-moitié are Gruyere or Vacherin\n",
    "    Question: {question}\n",
    "    Answer:\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ddd16b0d-a197-4ae4-8cea-a3568519e0bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m For prompt\u001b[0m: 'Context: The Swiss cheese fondue is best made of 50% of Gruyere cheese .\n",
      "    Question: What percentage of Gruyere is needed in the Swiss cheese fondue?\n",
      "    Answer: 50%\n",
      "    ###\n",
      "    Context: The other 50% of a Swiss cheese fondue is Vacherin Fribourgeois.\n",
      "    Question: What should be the other 50% in teh Swiss cheese fondue?\n",
      "    Answer: Vacherin\n",
      "    ###\n",
      "    Context: The swiss cheese fondue with vacherin and gruyere is called moitié-moitié\n",
      "    Question: How is the swiss chesse findue called ?\n",
      "    Answer: Moitié-moitié\n",
      "    ###\n",
      "    Context: The recommended cheese for a Swiss cheese fondue moitié-moitié are Gruyere or Vacherin\n",
      "    Question: {question}\n",
      "    Answer:'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['Gruyere']'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = {\n",
    "    \"max_length\": 350,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_p\": 0.1,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "for each_prompt in prompts:\n",
    "    input_text = each_prompt.replace(\"{question}\", question)\n",
    "    print(f\"{bold} For prompt{unbold}: '{each_prompt}'{newline}\")\n",
    "\n",
    "    payload = {\"text_inputs\": input_text, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} The reasoning result is{unbold}: '{generated_texts}'{newline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1adf2716-9119-441d-865a-8b83f3d0219a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question2='What is the other recommended cheese apart from Gruyere  for a Swiss cheese fondue'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "74038a75-e69b-4fa7-84f7-084cd783ed8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"\"\"Context: The Swiss cheese fondue is best made of 50% of Gruyere cheese .\n",
    "    Question: What percentage of Gruyere is needed in the Swiss cheese fondue?\n",
    "    Answer: 50%\n",
    "    ###\n",
    "    Context: The other 50% of a Swiss cheese fondue is Vacherin Fribourgeois.\n",
    "    Question: What should be the other 50% in teh Swiss cheese fondue?\n",
    "    Answer: Vacherin Fribourgeois\n",
    "    ###\n",
    "    Context: The swiss cheese fondue with vacherin and gruyere is called moitié-moitié\n",
    "    Question: How is the swiss chesse findue called ?\n",
    "    Answer: Moitié-moitié\n",
    "    ###\n",
    "    Context: The recommended cheese for a Swiss cheese fondue moitié-moitié are Gruyere or Vacherin Frbourgeois\n",
    "    Question: {question2}\n",
    "    Answer:\"\"\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "90118e10-0fc3-40fc-8bd1-a14e2841d801",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m For prompt\u001b[0m: 'Context: The Swiss cheese fondue is best made of 50% of Gruyere cheese .\n",
      "    Question: What percentage of Gruyere is needed in the Swiss cheese fondue?\n",
      "    Answer: 50%\n",
      "    ###\n",
      "    Context: The other 50% of a Swiss cheese fondue is Vacherin Fribourgeois.\n",
      "    Question: What should be the other 50% in teh Swiss cheese fondue?\n",
      "    Answer: Vacherin Fribourgeois\n",
      "    ###\n",
      "    Context: The swiss cheese fondue with vacherin and gruyere is called moitié-moitié\n",
      "    Question: How is the swiss chesse findue called ?\n",
      "    Answer: Moitié-moitié\n",
      "    ###\n",
      "    Context: The recommended cheese for a Swiss cheese fondue moitié-moitié are Gruyere or Vacherin Frbourgeois\n",
      "    Question: {question2}\n",
      "    Answer:'\n",
      "\n",
      "\u001b[1m The reasoning result is\u001b[0m: '['Vacherin Frbourgeois']'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "parameters = {\n",
    "    \"max_length\": 350,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_p\": 0.1,\n",
    "    \"do_sample\": True,\n",
    "}\n",
    "\n",
    "\n",
    "for each_prompt in prompts:\n",
    "    input_text = each_prompt.replace(\"{question2}\", question2)\n",
    "    print(f\"{bold} For prompt{unbold}: '{each_prompt}'{newline}\")\n",
    "\n",
    "    payload = {\"text_inputs\": input_text, **parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response_multiple_texts(query_response)\n",
    "    print(f\"{bold} The reasoning result is{unbold}: '{generated_texts}'{newline}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02880bf3-dafe-4e45-870c-f50ced4d391b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
