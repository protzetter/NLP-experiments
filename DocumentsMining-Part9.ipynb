{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on December 28th  2021 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "## Small experiment of document mining with various techniques Part 9\n",
    "\n",
    "Let us use AWS built-in NTM algorithm for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting texthero\n",
      "  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting gensim<4.0,>=3.6.0\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "     |████████████████████████████████| 24.2 MB 40.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.1.3)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.7/site-packages (from texthero) (0.22.1)\n",
      "Collecting spacy<3.0.0\n",
      "  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
      "     |████████████████████████████████| 10.4 MB 60.6 MB/s            \n",
      "\u001b[?25hCollecting unidecode>=1.1.1\n",
      "  Downloading Unidecode-1.3.2-py3-none-any.whl (235 kB)\n",
      "     |████████████████████████████████| 235 kB 33.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.3 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.4.5)\n",
      "Requirement already satisfied: plotly>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (5.4.0)\n",
      "Requirement already satisfied: tqdm>=4.3 in /opt/conda/lib/python3.7/site-packages (from texthero) (4.42.1)\n",
      "Collecting wordcloud>=1.5.0\n",
      "  Downloading wordcloud-1.8.1-cp37-cp37m-manylinux1_x86_64.whl (366 kB)\n",
      "     |████████████████████████████████| 366 kB 67.7 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /root/.local/lib/python3.7/site-packages (from texthero) (1.21.5)\n",
      "Collecting pandas>=1.0.2\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "     |████████████████████████████████| 11.3 MB 53.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.14.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.2->texthero) (2019.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /root/.local/lib/python3.7/site-packages (from scikit-learn>=0.22->texthero) (1.1.0)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
      "     |████████████████████████████████| 1.0 MB 61.5 MB/s            \n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "     |████████████████████████████████| 9.9 MB 26.4 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (59.5.0)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
      "     |████████████████████████████████| 125 kB 53.1 MB/s            \n",
      "\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
      "     |████████████████████████████████| 184 kB 73.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (2.26.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from wordcloud>=1.5.0->texthero) (8.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (1.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (2.2.0)\n",
      "Installing collected packages: murmurhash, cymem, wasabi, srsly, preshed, plac, catalogue, blis, thinc, wordcloud, unidecode, spacy, pandas, gensim, texthero\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.0.1\n",
      "    Uninstalling pandas-1.0.1:\n",
      "      Successfully uninstalled pandas-1.0.1\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.1.2\n",
      "    Uninstalling gensim-4.1.2:\n",
      "      Successfully uninstalled gensim-4.1.2\n",
      "Successfully installed blis-0.7.5 catalogue-1.0.0 cymem-2.0.6 gensim-3.8.3 murmurhash-1.0.6 pandas-1.3.5 plac-1.1.3 preshed-3.0.6 spacy-2.3.7 srsly-1.0.5 texthero-1.1.0 thinc-7.4.5 unidecode-1.3.2 wasabi-0.9.0 wordcloud-1.8.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install NLTK and gensim if required\n",
    "#%%sh\n",
    "!pip3 -q install nltk gensim\n",
    "!pip3 install texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# Import require libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup S3 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize some parameters depending where you are running the experiment, adapt the parameters to your AWS environment\n",
    "bucket='mymltextarticles'\n",
    "subfolder=''\n",
    "region='us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI-bank-of-the-future-Can-banks-meet-the-AI-challenge-1.pdf.txt\n",
      "Artificial Financial Intelligence.pdf.txt\n",
      "Data machine the insurers using AI to reshape the industry Financial Times.pdf.txt\n",
      "Digital-disruption-in-Insurance.pdf.txt\n",
      "EPAM_Streamlining_the_Auto_Claims_Process_via_Integrated_IA.pdf.txt\n",
      "Insurance-2030-The-impact-of-AI-on-the-future-of-insurance-F.pdf.txt\n",
      "Issues_Paper_on_Increasing_Digitalisation_in_Insurance_and_its_Potential_Impact_on_Consumer_Outcomes.pdf.txt\n",
      "Kaggle State of Machine Learning and Data Science 2020.pdf.txt\n",
      "Module-1-Lecture-Slides.pdf.txt\n",
      "Technology-and-innovation-in-the-insurance-sector.pdf.txt\n",
      "WEF_Governance_of_Chatbots_in_Healthcare_2020.pdf.txt\n",
      "ai-360-research.pdf.txt\n",
      "ai-insurance.pdf.txt\n",
      "fra-2020-artificial-intelligence_en.pdf.txt\n",
      "sigma-5-2020-en.pdf.txt\n",
      "sigma1_2020_en.pdf.txt\n"
     ]
    }
   ],
   "source": [
    "# let us list the files available for analysis in the S3 bucket\n",
    "s3 = boto3.client('s3')\n",
    "contents = s3.list_objects(Bucket=bucket, Prefix=subfolder)['Contents']\n",
    "number_of_docs=0\n",
    "for f in contents:\n",
    "    number_of_docs=number_of_docs+1\n",
    "    print(f['Key'])\n",
    "    target_filename='./sampledocs/'+f['Key']\n",
    "    s3.download_file(bucket, f['Key'], target_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of text files\n",
    "path='./sampledocs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "docNLP=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.txt')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    f = open(file_name,'r')\n",
    "    fileText=f.read()\n",
    "    docName.append(file_name)\n",
    "    docType.append('txt')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text:90011.8125\n",
      "Min length of text:9170\n",
      "Max length of text:328295\n"
     ]
    }
   ],
   "source": [
    " print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/Digital-disruption-in-Insurance.p...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Digital disruption\\nin insurance:\\nCutting thr...</td>\n",
       "      <td>digital disruption insurance cutting noise con...</td>\n",
       "      <td>34485</td>\n",
       "      <td>7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Data machine the insurers using A...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Data machine: the insurers using AI to reshape...</td>\n",
       "      <td>data machine insurers using ai reshape industr...</td>\n",
       "      <td>1454</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.pdf.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>machine intelligence insurance insights end en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/ai-insurance.pdf.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI in Insurance\\nTop Use Cases, Challenges, an...</td>\n",
       "      <td>ai insurance top use cases challenges trends w...</td>\n",
       "      <td>6558</td>\n",
       "      <td>2297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/Digital-disruption-in-Insurance.p...  txt   \n",
       "1  ./sampledocs/Technology-and-innovation-in-the-...  txt   \n",
       "2  ./sampledocs/Data machine the insurers using A...  txt   \n",
       "3               ./sampledocs/sigma-5-2020-en.pdf.txt  txt   \n",
       "4                  ./sampledocs/ai-insurance.pdf.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Digital disruption\\nin insurance:\\nCutting thr...   \n",
       "1  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "2  Data machine: the insurers using AI to reshape...   \n",
       "3  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "4  AI in Insurance\\nTop Use Cases, Challenges, an...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  digital disruption insurance cutting noise con...            34485   \n",
       "1  technology innovation insurance sector technol...            16742   \n",
       "2  data machine insurers using ai reshape industr...             1454   \n",
       "3  machine intelligence insurance insights end en...            14478   \n",
       "4  ai insurance top use cases challenges trends w...             6558   \n",
       "\n",
       "   text_unique_words  \n",
       "0               7049  \n",
       "1               4228  \n",
       "2                684  \n",
       "3               4329  \n",
       "4               2297  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16 entries, 0 to 15\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Name               16 non-null     object\n",
      " 1   Type               16 non-null     object\n",
      " 2   Text               16 non-null     object\n",
      " 3   cleanText          16 non-null     object\n",
      " 4   text_word_count    16 non-null     int64 \n",
      " 5   text_unique_words  16 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 896.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    text = text.lower().split()\n",
    "    text = [w for w in text if not w in stop_words] \n",
    "    text = [wnl.lemmatize(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.7 s, sys: 78.3 ms, total: 2.78 s\n",
      "Wall time: 2.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fullDocs['cleanText'] = fullDocs['cleanText'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/Digital-disruption-in-Insurance.p...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Digital disruption\\nin insurance:\\nCutting thr...</td>\n",
       "      <td>[digital, disruption, insurance, cutting, nois...</td>\n",
       "      <td>34485</td>\n",
       "      <td>7049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>[technology, innovation, insurance, sector, te...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Data machine the insurers using A...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Data machine: the insurers using AI to reshape...</td>\n",
       "      <td>[data, machine, insurer, using, ai, reshape, i...</td>\n",
       "      <td>1454</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.pdf.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>[machine, intelligence, insurance, insight, en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/ai-insurance.pdf.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI in Insurance\\nTop Use Cases, Challenges, an...</td>\n",
       "      <td>[ai, insurance, top, use, case, challenge, tre...</td>\n",
       "      <td>6558</td>\n",
       "      <td>2297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/Digital-disruption-in-Insurance.p...  txt   \n",
       "1  ./sampledocs/Technology-and-innovation-in-the-...  txt   \n",
       "2  ./sampledocs/Data machine the insurers using A...  txt   \n",
       "3               ./sampledocs/sigma-5-2020-en.pdf.txt  txt   \n",
       "4                  ./sampledocs/ai-insurance.pdf.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Digital disruption\\nin insurance:\\nCutting thr...   \n",
       "1  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "2  Data machine: the insurers using AI to reshape...   \n",
       "3  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "4  AI in Insurance\\nTop Use Cases, Challenges, an...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  [digital, disruption, insurance, cutting, nois...            34485   \n",
       "1  [technology, innovation, insurance, sector, te...            16742   \n",
       "2  [data, machine, insurer, using, ai, reshape, i...             1454   \n",
       "3  [machine, intelligence, insurance, insight, en...            14478   \n",
       "4  [ai, insurance, top, use, case, challenge, tre...             6558   \n",
       "\n",
       "   text_unique_words  \n",
       "0               7049  \n",
       "1               4228  \n",
       "2                684  \n",
       "3               4329  \n",
       "4               2297  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.6 ms, sys: 3.12 ms, total: 98.7 ms\n",
      "Wall time: 97.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(fullDocs['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(10603 unique tokens: ['abandon', 'ability', 'able', 'abound', 'abraham']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1024 unique tokens: ['academic', 'accelerate', 'accelerating', 'accept', 'accessed']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(keep_n=1024)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for index in range(0,len(dictionary)):\n",
    "        f.write(dictionary.get(index)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 49.6 ms, sys: 4.02 ms, total: 53.6 ms\n",
      "Wall time: 56.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fullDocs['tokens'] = fullDocs.apply(lambda row: dictionary.doc2bow(row['cleanText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0, 1), (1, 7), (2, 1), (3, 1), (4, 2), (5, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(1, 2), (2, 1), (10, 1), (13, 2), (15, 2), (1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(14, 1), (16, 1), (58, 1), (60, 1), (77, 1), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(0, 1), (1, 2), (3, 3), (7, 1), (8, 1), (9, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(2, 1), (6, 1), (8, 4), (9, 1), (12, 4), (14,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens\n",
       "0  [(0, 1), (1, 7), (2, 1), (3, 1), (4, 2), (5, 5...\n",
       "1  [(1, 2), (2, 1), (10, 1), (13, 2), (15, 2), (1...\n",
       "2  [(14, 1), (16, 1), (58, 1), (60, 1), (77, 1), ...\n",
       "3  [(0, 1), (1, 2), (3, 3), (7, 1), (8, 1), (9, 1...\n",
       "4  [(2, 1), (6, 1), (8, 4), (9, 1), (12, 4), (14,..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fullDocs.drop(['cleanText'], axis=1)\n",
    "data = data.drop(['Name'], axis=1)\n",
    "data = data.drop(['Type'], axis=1)\n",
    "data = data.drop(['Text'], axis=1)\n",
    "data = data.drop(['text_word_count'], axis=1)\n",
    "data = data.drop(['text_unique_words'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.amazon.common as smac\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "prefix = 'training'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protobuf_dataset(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    token_matrix = lil_matrix((num_lines, num_columns)).astype('float32')\n",
    "    line = 0\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            token_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_protbuf_dataset(buf, bucket, prefix, key):\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    buf.seek(0)\n",
    "    s3.upload_fileobj(buf, bucket, obj)\n",
    "    path = 's3://{}/{}'.format(bucket,obj)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/training/training.protobuf\n",
      "CPU times: user 75.7 ms, sys: 3.13 ms, total: 78.8 ms\n",
      "Wall time: 139 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_buf = build_protobuf_dataset(data, dictionary)\n",
    "s3_training_path = upload_protbuf_dataset(training_buf, bucket, prefix, 'training/training.protobuf')\n",
    "print(s3_training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/output/\n"
     ]
    }
   ],
   "source": [
    "s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = 'us-east-1'  \n",
    "container = retrieve('lda', region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::012086180905:role/service-role/AmazonSageMakerServiceCatalogProductsUseRole\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "lda = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.2xlarge',\n",
    "    output_path=s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.set_hyperparameters(\n",
    "    num_topics=10, \n",
    "    feature_dim=len(dictionary), \n",
    "    mini_batch_size=1,\n",
    "    alpha0=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-01 13:57:20 Starting - Starting the training job...\n",
      "2022-01-01 13:57:22 Starting - Launching requested ML instancesProfilerReport-1641045440: InProgress\n",
      "...\n",
      "2022-01-01 13:58:18 Starting - Preparing the instances for training.........\n",
      "2022-01-01 13:59:45 Downloading - Downloading input data...\n",
      "2022-01-01 14:00:12 Training - Downloading the training image..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mUsing mxnet backend.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'alpha0': u'1.0', u'max_restarts': u'10', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'allow_svd_init': u'true', u'epochs': u'1', u'tol': u'1e-8', u'_kvstore': u'local', u'max_iterations': u'1000'}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'1024', u'mini_batch_size': u'1', u'num_topics': u'10', u'alpha0': u'0.1'}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Final configuration: {u'alpha0': u'0.1', u'max_restarts': u'10', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'allow_svd_init': u'true', u'epochs': u'1', u'feature_dim': u'1024', u'num_topics': u'10', u'tol': u'1e-8', u'_kvstore': u'local', u'mini_batch_size': u'1', u'max_iterations': u'1000'}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Using default worker.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Running LDA for 13 topics (10 requested)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"LDA.initialize.time\": {\"count\": 1, \"max\": 0.39386749267578125, \"sum\": 0.39386749267578125, \"min\": 0.39386749267578125}}, \"EndTime\": 1641045635.764828, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.749782}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1641045635.765008, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.764957}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Loaded all data into matrix with shape: (16, 1024)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"get_all_batches_from_iter.time\": {\"count\": 1, \"max\": 14.017820358276367, \"sum\": 14.017820358276367, \"min\": 14.017820358276367}}, \"EndTime\": 1641045635.781521, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.764909}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"moment1.time\": {\"count\": 1, \"max\": 1.8918514251708984, \"sum\": 1.8918514251708984, \"min\": 1.8918514251708984}}, \"EndTime\": 1641045635.788861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.781592}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Number of iterations: 3\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Seed of random SVD : ('MT19937', array([         0,          1, 1812433255, 1900727105, 1208447044,\n",
      "       2481403966, 4042607538,  337614300, 3232553940, 1018809052,\n",
      "       3202401494, 1775180719, 3192392114,  594215549,  184016991,\n",
      "        829906058,  610491522, 3879932251, 3139825610,  297902587,\n",
      "       4075895579, 2943625357, 3530655617, 1423771745, 2135928312,\n",
      "       2891506774, 1066338622,  135451537,  933040465, 2759011858,\n",
      "       2273819758, 3545703099, 2516396728, 1272276355, 3172048492,\n",
      "       3267256201, 2332199830, 1975469449,  392443598, 1132453229,\n",
      "       2900699076, 1998300999, 3847713992,  512669506, 1227792182,\n",
      "       1629110240,  112303347, 2142631694, 3647635483, 1715036585,\n",
      "       2508091258, 1355887243, 1884998310, 3906360088,  952450269,\n",
      "       3647883368, 3962623343, 3077504981, 2023096077, 3791588343,\n",
      "       3937487744, 3455116780, 1218485897, 1374508007, 2815569918,\n",
      "       1367263917,  472908318, 2263147545, 1461547499, 4126813079,\n",
      "       2383504810,   64750479, 2963140275, 1709368606, 4143643781,\n",
      "        835933993, 1881494649,  674663333, 2076403047,  858036109,\n",
      "       1667579889, 1706666497,  607785554, 1995775149, 1941986352,\n",
      "       3448871082, 2109910019, 1474883361, 1623095288, 1831376534,\n",
      "       2612738285,   81681830, 2204289242, 1365038485,  251164610,\n",
      "       4268495337, 1805601714, 1262528768, 1442526919, 1675006593,\n",
      "        965627108,  646339161,  499795587,  840887574,  380522518,\n",
      "       3023789847, 1457635507, 1947093157, 2600365344, 2729853143,\n",
      "       1550618999, 1390905853, 3021294812,  882647559,  838872117,\n",
      "       1663880796, 4222103589, 2754172275, 3844026123, 3199260319,\n",
      "       4176064873, 3591027019, 2690294242, 2978135515, 3172796441,\n",
      "       3263669796, 1451257057, 1427035359, 4174826006, 2171992010,\n",
      "       1537002090, 3122405306, 4162452508, 3271954368, 3794310005,\n",
      "       3240514581, 1270412086, 3030475836, 2281945856, 2644171349,\n",
      "       3109139423, 4253563838, 1289926431, 1396919653,  733220100,\n",
      "       2753316645, 1196225013, 3699575255, 3569440056, 2675979228,\n",
      "       2624079148, 3463113149,  863430286,  623703199, 2113837653,\n",
      "       2656425919,  175981357, 4271478366, 4238022735, 1665483419,\n",
      "         86880610, 2963435083, 1830392943,  847801865, 3237296945,\n",
      "        332143967, 3973606945, 2671879697, 2236330279, 2360127810,\n",
      "       3283955434,  203240344, 4048139172,   13189264, 2263058814,\n",
      "        247241371, 1566765783, 3084408095, 3719371299, 1958375251,\n",
      "       1985924622, 1712739232, 1861691451, 2644502937, 2337807839,\n",
      "        784993770, 2962208780, 2190810177, 1523122731,  714888527,\n",
      "        578678761, 3698481324, 1801168075,  534650483, 3390213921,\n",
      "       3923356461, 3586009066, 2059432114,   52511333, 1969897376,\n",
      "       3630122061,  524661135, 3513619765,  563070233,  501359785,\n",
      "        477489274,  658768624,  938973567, 1548584683, 1345287459,\n",
      "       2488691004, 3441144905, 3849305094, 2430000078,  855172178,\n",
      "        614463281, 2092744749,  176381493, 1655802051, 2273888101,\n",
      "       2474494847, 3471978030, 2138918303,  575352373, 1658230985,\n",
      "       1675972553, 2946663114,  915579339,  284981499,   53939948,\n",
      "       3022598146, 1861218535, 3403620774, 4203516930, 2360471119,\n",
      "       3134536268, 1383448498, 1307602316, 3847663247, 3027225131,\n",
      "       3597251613, 3186237127,  725127595, 1928526954, 1843386923,\n",
      "       3560410503,   54688266, 1791983849, 2519860352, 4256389699,\n",
      "       2328812602,  486464275, 3578698363,  301279829, 1303654791,\n",
      "       4181868765,  971794070, 1933885487, 3996807464, 2144053754,\n",
      "       4079903755, 3775774765, 3481760044, 1212862354, 1067356423,\n",
      "       3764189132, 1609862325, 2209601551, 2565747501,  161962392,\n",
      "       4045451782, 2605574664, 2520953090, 3490240017, 1082791980,\n",
      "         44474324,  101811128, 4268650669, 4171338684,  772375154,\n",
      "       3920460306, 2319139534,  599033750, 2950874441, 3373922995,\n",
      "       1496848525, 4095253594, 1271943484, 1498723121, 3097453329,\n",
      "       3698082465,  281869581, 3148270661, 3591477288,  747441437,\n",
      "       2809508504, 3896107498,  303747862, 2368081624, 1844217645,\n",
      "        886825352,  287949781, 1444561207, 2512101757, 2062331723,\n",
      "        741720931, 1383797313, 3876746355, 2041045348, 2627599118,\n",
      "       1124169970,  200524822, 3484820454,   55883666, 1135054804,\n",
      "        669498692, 2677215504, 3097911127, 1509628615,  617580381,\n",
      "       2229022193,   85601568, 3243896546, 3715672328,  912168347,\n",
      "       2359163500, 1180347564, 4243175048, 2092067103,  880183327,\n",
      "       4000664709, 2045044777, 3500474644, 1515175520, 1862207123,\n",
      "        186628841, 3337252925,  708933575, 4015964629, 3136815297,\n",
      "       3314919747, 2891909013, 3316567785, 3944275369, 3608506218,\n",
      "       2884839110, 3054055598, 2707439927, 1381111877, 3275487281,\n",
      "       4292456216, 2639563270, 3327301876, 3576924628,  721056309,\n",
      "       2002808140,  748967365,   52380958, 2200261692,  763456477,\n",
      "       1708381337, 2038446433, 2682979402, 1526413779, 2211263302,\n",
      "       3879771969,   75966584, 3645059271, 2985763524, 4085690255,\n",
      "         82390958, 1883631385, 1647521260, 1598026998, 3038041577,\n",
      "       2501913134, 3279302868, 1738888524,  805035483,  756399074,\n",
      "       3863810982, 1097797270, 1505792529,  898904527,  583561003,\n",
      "        717152376, 3333867738, 1099456544, 1663473545, 1242141229,\n",
      "       3828627682, 1966201676, 1713552361, 3852160017, 1584965284,\n",
      "         21695908, 1013262144,  145341901, 3995441263, 3462066219,\n",
      "       2239637848, 1214086163, 2428868268, 1650037305, 1545513388,\n",
      "       1621198806, 4232947817, 1823092073,  256414624, 1745018809,\n",
      "       1357102386, 2055139770, 3280958307, 2482431613, 1664870585,\n",
      "        859130423, 4097751123, 3079768369, 2470211009, 2984880786,\n",
      "       2808568948, 2877071923, 1984903163,  302768457, 1866396789,\n",
      "        869566317, 3746415787, 4169433075, 3025005404, 3980733379,\n",
      "       3539207278, 3953071536,  876960847, 2548872156,  800507464,\n",
      "       1865466907, 1273317878, 3754712872, 1757188269, 3229950355,\n",
      "       3731640200, 2283390608, 2204990292,  411873449,  447423849,\n",
      "       1852437802,  472825525, 3044219944, 2913114194, 1859709265,\n",
      "       4053786194,  574820536, 2104496732,  865469814, 2438352724,\n",
      "       4208743605, 4215067542, 1364015250, 4139974345, 3838747005,\n",
      "       1818502786, 2914274940, 1402365828, 1751123528, 2302578077,\n",
      "       2463168652, 1968705496, 1730700144, 3023943273, 1139096844,\n",
      "       2658667767, 2063547264,  705791165, 1444775274, 2415454225,\n",
      "       1575664730,  921044163,  648101324, 1212387162, 4191962054,\n",
      "       1787702169, 1888718041, 1518218010, 3398792842, 4079359729,\n",
      "        149721439,  750400353, 2661036076, 3802767886,  520152586,\n",
      "        951852508, 2939585975, 1375969109,  385733137, 3523607459,\n",
      "       1902438415, 4250996086, 2712727066,  484493674, 3932107461,\n",
      "       1428488210, 1764242548, 3424801055, 4004904451, 2226862072,\n",
      "       2393366939, 3609584727, 3614444319,  317349896, 3826527525,\n",
      "        204023804,  981902443, 3356042039, 3051207045, 1869902661,\n",
      "        561831895, 3706675415, 1527687593, 1227610446, 2596341042,\n",
      "       3191717368, 3269246891,  557877074, 4062070629, 3052520266,\n",
      "       3772487029,  400039836, 3195205275, 4085394797, 1655557239,\n",
      "       1345770144, 2864727192,  449281238,   73189507,  528365765,\n",
      "       2727400656,  247880434, 2408277395,  777039183, 2210179398,\n",
      "       1088433648, 2124356402, 1555630141,  604790219,  195012151,\n",
      "       3312518356,  923728373, 3999251660, 3313059535, 3478133921,\n",
      "       3395026960,  383464614, 3425869222, 2446885186, 4032184426,\n",
      "        157195416, 3158909476, 1663750443, 2046427584, 1658453076,\n",
      "       1784483001, 3146546889, 1238739785, 2297306523, 3472330897,\n",
      "       2953326031, 2421672215, 1221694592, 1588568605, 2546987845,\n",
      "       3375168573, 2137961649, 3056565164,  330165219,  235900365,\n",
      "       1000384800, 2697255904,  579122283, 3050664825,   73426122,\n",
      "       1232986102, 2940571064, 3076486824, 1708182873, 2796363264,\n",
      "        292154131, 4280019913, 1102652157, 1185393592, 1494991690,\n",
      "       4270076389, 2384840717,  425785147, 2385321880,  317514772,\n",
      "       3926962743,  392176856, 3465421709, 1878853468,  122662664,\n",
      "       2958252160, 1858961315, 2244939588, 2361884409, 2860936803,\n",
      "        683833250, 3291277128, 1686857206, 1112632275, 1200680507,\n",
      "       3342928196, 2677058150,  939442136, 3407104669, 2906783932,\n",
      "       3668048733, 2030009470, 1910839172, 1234925283, 3575831445,\n",
      "        123595418, 2362440495, 3048484911, 1796872496], dtype=uint32), 624, 0, 0.0)\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Krylov method iteration: 0.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Krylov method iteration: 1.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Krylov method iteration: 2.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Krylov method iteration: 3.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Krylov method iteration: 4.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Krylov method iteration: 5.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Krylov method iteration: 6.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Covariance matrix min value: -0.000000\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Covariance matrix max value: 0.000000\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Starting SVD...\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"svd.time\": {\"count\": 1, \"max\": 9.449958801269531, \"sum\": 9.449958801269531, \"min\": 9.449958801269531}}, \"EndTime\": 1641045635.878905, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.788908}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Finished SVD.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"rand_svd.time\": {\"count\": 1, \"max\": 90.36803245544434, \"sum\": 90.36803245544434, \"min\": 90.36803245544434}}, \"EndTime\": 1641045635.879295, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.878963}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"GetMoments.time\": {\"count\": 1, \"max\": 92.59486198425293, \"sum\": 92.59486198425293, \"min\": 92.59486198425293}}, \"EndTime\": 1641045635.879563, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.879334}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e3_term1.time\": {\"count\": 1, \"max\": 2.928018569946289, \"sum\": 2.928018569946289, \"min\": 2.928018569946289}}, \"EndTime\": 1641045635.888046, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.879617}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e3_term2.time\": {\"count\": 1, \"max\": 1.168966293334961, \"sum\": 1.168966293334961, \"min\": 1.168966293334961}}, \"EndTime\": 1641045635.889296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.888098}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e3_term3.time\": {\"count\": 1, \"max\": 3.759145736694336, \"sum\": 3.759145736694336, \"min\": 3.759145736694336}}, \"EndTime\": 1641045635.893681, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.889341}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m3_contribution_e3.time\": {\"count\": 1, \"max\": 13.510942459106445, \"sum\": 13.510942459106445, \"min\": 13.510942459106445}}, \"EndTime\": 1641045635.894989, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.89373}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e2m1_term1.time\": {\"count\": 1, \"max\": 0.14281272888183594, \"sum\": 0.14281272888183594, \"min\": 0.14281272888183594}, \"from_numpy.time\": {\"count\": 3, \"max\": 0.13208389282226562, \"sum\": 0.34618377685546875, \"min\": 0.10609626770019531}}, \"EndTime\": 1641045635.896591, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.895034}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e2m1_term2.time\": {\"count\": 1, \"max\": 0.9510517120361328, \"sum\": 0.9510517120361328, \"min\": 0.9510517120361328}}, \"EndTime\": 1641045635.898001, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.896644}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m3_contribution_e2_m1.time\": {\"count\": 1, \"max\": 3.5800933837890625, \"sum\": 3.5800933837890625, \"min\": 3.5800933837890625}}, \"EndTime\": 1641045635.898636, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.898046}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m1_3.time\": {\"count\": 1, \"max\": 0.11110305786132812, \"sum\": 0.11110305786132812, \"min\": 0.11110305786132812}}, \"EndTime\": 1641045635.899, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.898679}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m3_contribution_m1_3.time\": {\"count\": 1, \"max\": 0.4799365997314453, \"sum\": 0.4799365997314453, \"min\": 0.4799365997314453}}, \"EndTime\": 1641045635.899252, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.899045}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m3.time\": {\"count\": 1, \"max\": 19.855022430419922, \"sum\": 19.855022430419922, \"min\": 19.855022430419922}}, \"EndTime\": 1641045635.899507, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.899304}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"M3Whitening.time\": {\"count\": 1, \"max\": 20.067930221557617, \"sum\": 20.067930221557617, \"min\": 20.067930221557617}}, \"EndTime\": 1641045635.899713, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.899569}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] Restart 1/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:35 INFO 139762203416384] [CPDecomp] Using svd initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=7, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 2/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=0, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using svd initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=3, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 3 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 3/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=7, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 4/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=8, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 8 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 5/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=7, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 6/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=11, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 11 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 7/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=7, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 8/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=8, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 8 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 9/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=7, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Restart 10/10\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] [CPDecomp] iter=7, error=0.0318264849484, var=0.0\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Best reconstruction error: 0.031826485\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"CPDecomposition.time\": {\"count\": 1, \"max\": 704.7159671783447, \"sum\": 704.7159671783447, \"min\": 704.7159671783447}}, \"EndTime\": 1641045636.604521, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045635.899766}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"LDA.update.time\": {\"count\": 1, \"max\": 840.2040004730225, \"sum\": 840.2040004730225, \"min\": 840.2040004730225}, \"to_numpy.time\": {\"count\": 4, \"max\": 0.04315376281738281, \"sum\": 0.141143798828125, \"min\": 0.023126602172851562}}, \"EndTime\": 1641045636.607697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045636.604587}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}, \"Total Batches Seen\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}, \"Total Records Seen\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 16, \"sum\": 16.0, \"min\": 16}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1641045636.607986, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"LDA\", \"epoch\": 0}, \"StartTime\": 1641045636.607946}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] #throughput_metric: host=algo-1, train throughput=19.0342929396 records/second\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 WARNING 139762203416384] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"LDA.finalize.time\": {\"count\": 1, \"max\": 0.02384185791015625, \"sum\": 0.02384185791015625, \"min\": 0.02384185791015625}}, \"EndTime\": 1641045636.608338, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045636.607748}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Serializing LDA model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"LDAModel.serialize.time\": {\"count\": 1, \"max\": 0.33092498779296875, \"sum\": 0.33092498779296875, \"min\": 0.33092498779296875}}, \"EndTime\": 1641045636.608757, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045636.608389}\u001b[0m\n",
      "\u001b[34m[01/01/2022 14:00:36 INFO 139762203416384] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 958.2538604736328, \"sum\": 958.2538604736328, \"min\": 958.2538604736328}, \"setuptime\": {\"count\": 1, \"max\": 58.220863342285156, \"sum\": 58.220863342285156, \"min\": 58.220863342285156}}, \"EndTime\": 1641045636.610798, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641045636.608799}\u001b[0m\n",
      "\n",
      "2022-01-01 14:00:52 Uploading - Uploading generated training model\n",
      "2022-01-01 14:00:52 Completed - Training job completed\n",
      "Training seconds: 61\n",
      "Billable seconds: 61\n"
     ]
    }
   ],
   "source": [
    "lda.fit(inputs={'train': s3_training_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "s3_output='s3://mymltextarticles/training/output/lda-2022-01-01-13-57-20-371/output/model.tar.gz'\n",
    "role = get_execution_role()\n",
    "lda=sagemaker.LDAModel(s3_output, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "lda_predictor = lda.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.large')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip < output.tar.gz | tar -xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sea\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics=pd.read_csv('doc-topics.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms=pd.read_csv('topic-terms.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0_df=topic_terms[topic_terms['topic']==0]\n",
    "plot=sea.barplot(x=topic0_df.term,y=topic0_df.weight,order=topic0_df.sort_values(by=['weight'], ascending=False).set_index('term').index)\n",
    "plot.set_xticklabels(plot.get_xticklabels(),rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = topic_terms.sort_values(by=['topic','weight'], ascending=False)\n",
    "grid = sea.FacetGrid(final_df, col=\"topic\", col_wrap=3,hue=\"topic\",margin_titles=True, sharex=False)\n",
    "grid_map=grid.map(sea.barplot,\"term\",\"weight\",order=None);\n",
    "for ax in grid_map.axes.flatten():\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=60)\n",
    "grid_map.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = []\n",
    "for i in range(number_of_topics):\n",
    "    keywords = []\n",
    "    df_sub= topic_terms[topic_terms['topic']==i].sort_values(by=['weight'], ascending=False)\n",
    "    keywords = \", \".join(df_sub['term'])\n",
    "    i=i+1\n",
    "    topic_keywords.append(keywords)\n",
    "topic_keywords_df= pd.DataFrame(topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame()\n",
    "for filename in filenames:\n",
    "    sub_topics_df=doc_topics[doc_topics['docname']==filename].sort_values(by=['proportion'], ascending=False)\n",
    "    row=sub_topics_df.iloc[0]\n",
    "    topics_df = topics_df.append(pd.Series([row['docname'],int(row['topic']), round(row['proportion'],4)]), ignore_index=True)\n",
    "topics_df.columns = ['Document name', 'Topic', 'Contribution']\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 3 Keywords for each Topic\n",
    "\n",
    "topic_top3words = [(i,topic) for i, topics in enumerate(topic_keywords) for (j, topic) in enumerate(topics.split(',')) if j<3]\n",
    "print(topic_top3words)\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)\n",
    "df_top3words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "#fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax=sea.histplot(data=topics_df,x=topics_df.Topic)\n",
    "ax.set_xticks(range(topics_df.Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax.xaxis.set_major_formatter(tick_formatter)\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=60)\n",
    "ax.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "ax.set_ylabel('Number of Documents')\n",
    "ax.set_ylim(0, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
