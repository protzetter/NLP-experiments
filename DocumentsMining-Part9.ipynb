{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on December 28th  2021 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "## Small experiment of document mining with various techniques Part 9\n",
    "\n",
    "Let us use AWS built-in LDA algorithm for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: texthero in /opt/conda/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.local/lib/python3.7/site-packages (from texthero) (1.21.5)\n",
      "Requirement already satisfied: plotly>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (5.4.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from texthero) (1.3.2)\n",
      "Requirement already satisfied: tqdm>=4.3 in /opt/conda/lib/python3.7/site-packages (from texthero) (4.42.1)\n",
      "Requirement already satisfied: pandas>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from texthero) (1.3.5)\n",
      "Requirement already satisfied: spacy<3.0.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (2.3.7)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.1.3)\n",
      "Requirement already satisfied: gensim<4.0,>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.8.3)\n",
      "Requirement already satisfied: wordcloud>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (1.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.7/site-packages (from texthero) (0.22.1)\n",
      "Requirement already satisfied: nltk>=3.3 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.4.5)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.14.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.4.6)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.2->texthero) (2019.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /root/.local/lib/python3.7/site-packages (from scikit-learn>=0.22->texthero) (1.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (0.7.5)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.0)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (59.5.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (2.0.6)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (0.9.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (3.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (2.26.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (7.4.5)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.6)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from wordcloud>=1.5.0->texthero) (8.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (1.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.26.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (2.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install NLTK and gensim if required\n",
    "!pip3 -q install nltk gensim\n",
    "!pip3 install texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import require libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup S3 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize some parameters depending where you are running the experiment, adapt the parameters to your AWS environment\n",
    "bucket='mymltextarticles'\n",
    "subfolder=''\n",
    "region='us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sampledocs/AI-bank-of-the-future-Can-banks-meet-the-AI-challenge-1.txt\n",
      "./sampledocs/Artificial Financial Intelligence.txt\n",
      "./sampledocs/Data machine the insurers using AI to reshape the industry Financial Times.txt\n",
      "./sampledocs/Digital-disruption-in-Insurance.txt\n",
      "./sampledocs/Impact-Big-Data-AI-in-the-Insurance-Sector.txt\n",
      "./sampledocs/Innovation_Artificial-Intelligence-in-Insurance-Whitepaper-deloitte-digital.txt\n",
      "./sampledocs/Insurance-2030-The-impact-of-AI-on-the-future-of-insurance-F.txt\n",
      "./sampledocs/Issues_Paper_on_Increasing_Digitalisation_in_Insurance_and_its_Potential_Impact_on_Consumer_Outcomes.txt\n",
      "./sampledocs/Kaggle State of Machine Learning and Data Science 2020.txt\n",
      "./sampledocs/Module-1-Lecture-Slides.txt\n",
      "./sampledocs/Technology-and-innovation-in-the-insurance-sector.txt\n",
      "./sampledocs/WEF_Governance_of_Chatbots_in_Healthcare_2020.txt\n",
      "./sampledocs/ai-360-research.txt\n",
      "./sampledocs/ai-insurance.txt\n",
      "./sampledocs/ai_in_insurance_web_0.txt\n",
      "./sampledocs/fra-2020-artificial-intelligence_en.txt\n",
      "./sampledocs/merged.txt\n",
      "./sampledocs/sigma-5-2020-en.txt\n",
      "./sampledocs/sigma1_2020_en.txt\n"
     ]
    }
   ],
   "source": [
    "# let us list the files available for analysis in the S3 bucket\n",
    "import os\n",
    "s3s = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#contents = s3.list_objects(Bucket=bucket, Prefix=subfolder)#['Contents']\n",
    "mybucket = s3.Bucket(bucket)\n",
    "mybucket.objects.filter(Prefix='foo/bar')\n",
    "for file in mybucket.objects.all():\n",
    "    root,ext = os.path.splitext(file.key)\n",
    "    if ext in ['.txt']:\n",
    "        filename=os.path.basename(file.key)\n",
    "        target_filename='./sampledocs/'+filename\n",
    "        print(target_filename)\n",
    "        s3s.download_file(bucket, file.key, target_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of text files\n",
    "path='./sampledocs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "docNLP=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.txt')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    f = open(file_name,'r')\n",
    "    fileText=f.read()\n",
    "    docName.append(file_name)\n",
    "    docType.append('txt')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text:166530.15789473685\n",
      "Min length of text:9170\n",
      "Max length of text:1513210\n"
     ]
    }
   ],
   "source": [
    " print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/ai-360-research.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>ai insights next frontier business corner offi...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>application ai insurtech real estate technolog...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/Insurance-2030-The-impact-of-AI-o...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Insurance Practice\\n\\nInsurance 2030窶能\nThe imp...</td>\n",
       "      <td>insurance practice insurance -- impact ai futu...</td>\n",
       "      <td>4424</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>machine intelligence insurance insights end en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/Technology-and-innovation-in-the-...  txt   \n",
       "1                   ./sampledocs/ai-360-research.txt  txt   \n",
       "2           ./sampledocs/Module-1-Lecture-Slides.txt  txt   \n",
       "3  ./sampledocs/Insurance-2030-The-impact-of-AI-o...  txt   \n",
       "4                   ./sampledocs/sigma-5-2020-en.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Insurance Practice\\n\\nInsurance 2030窶能\nThe imp...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  technology innovation insurance sector technol...            16742   \n",
       "1  ai insights next frontier business corner offi...             5281   \n",
       "2  application ai insurtech real estate technolog...             3728   \n",
       "3  insurance practice insurance -- impact ai futu...             4424   \n",
       "4  machine intelligence insurance insights end en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               4228  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               1782  \n",
       "4               4329  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 19 entries, 0 to 18\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Name               19 non-null     object\n",
      " 1   Type               19 non-null     object\n",
      " 2   Text               19 non-null     object\n",
      " 3   cleanText          19 non-null     object\n",
      " 4   text_word_count    19 non-null     int64 \n",
      " 5   text_unique_words  19 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.0+ KB\n"
     ]
    }
   ],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    text = text.lower().split()\n",
    "    text = [w for w in text if not w in stop_words] \n",
    "    text = [wnl.lemmatize(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['cleanText'] = fullDocs['cleanText'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>[technology, innovation, insurance, sector, te...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/ai-360-research.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>[ai, insight, next, frontier, business, corner...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>[application, ai, insurtech, real, estate, tec...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/Insurance-2030-The-impact-of-AI-o...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Insurance Practice\\n\\nInsurance 2030窶能\nThe imp...</td>\n",
       "      <td>[insurance, practice, insurance, impact, ai, f...</td>\n",
       "      <td>4424</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>[machine, intelligence, insurance, insight, en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/Technology-and-innovation-in-the-...  txt   \n",
       "1                   ./sampledocs/ai-360-research.txt  txt   \n",
       "2           ./sampledocs/Module-1-Lecture-Slides.txt  txt   \n",
       "3  ./sampledocs/Insurance-2030-The-impact-of-AI-o...  txt   \n",
       "4                   ./sampledocs/sigma-5-2020-en.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Insurance Practice\\n\\nInsurance 2030窶能\nThe imp...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  [technology, innovation, insurance, sector, te...            16742   \n",
       "1  [ai, insight, next, frontier, business, corner...             5281   \n",
       "2  [application, ai, insurtech, real, estate, tec...             3728   \n",
       "3  [insurance, practice, insurance, impact, ai, f...             4424   \n",
       "4  [machine, intelligence, insurance, insight, en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               4228  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               1782  \n",
       "4               4329  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(fullDocs['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(11382 unique tokens: ['ab', 'ability', 'able', 'abundantly', 'abusive']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1024 unique tokens: ['accelerating', 'acceleration', 'acceptance', 'accepted', 'accessing']...)\n"
     ]
    }
   ],
   "source": [
    "# Let us filter only the top 1024 words\n",
    "dictionary.filter_extremes(keep_n=1024)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for index in range(0,len(dictionary)):\n",
    "        f.write(dictionary.get(index)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['tokens'] = fullDocs.apply(lambda row: dictionary.doc2bow(row['cleanText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(2, 2), (4, 1), (11, 1), (18, 1), (22, 1), (2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(7, 1), (9, 1), (13, 3), (19, 2), (26, 3), (3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(0, 2), (6, 1), (8, 4), (9, 1), (10, 1), (18,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(4, 1), (5, 1), (8, 1), (9, 3), (11, 2), (22,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens\n",
       "0  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2...\n",
       "1  [(2, 2), (4, 1), (11, 1), (18, 1), (22, 1), (2...\n",
       "2  [(7, 1), (9, 1), (13, 3), (19, 2), (26, 3), (3...\n",
       "3  [(0, 2), (6, 1), (8, 4), (9, 1), (10, 1), (18,...\n",
       "4  [(4, 1), (5, 1), (8, 1), (9, 3), (11, 2), (22,..."
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fullDocs.drop(['cleanText'], axis=1)\n",
    "data = data.drop(['Name'], axis=1)\n",
    "data = data.drop(['Type'], axis=1)\n",
    "data = data.drop(['Text'], axis=1)\n",
    "data = data.drop(['text_word_count'], axis=1)\n",
    "data = data.drop(['text_unique_words'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.amazon.common as smac\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "prefix = 'training'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protobuf_dataset(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    token_matrix = lil_matrix((num_lines, num_columns)).astype('float32')\n",
    "    line = 0\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            token_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_protbuf_dataset(buf, bucket, prefix, key):\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    buf.seek(0)\n",
    "    s3s.upload_fileobj(buf, bucket, obj)\n",
    "    path = 's3://{}/{}'.format(bucket,obj)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/training/training.protobuf\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "training_buf = build_protobuf_dataset(data, dictionary)\n",
    "s3_training_path = upload_protbuf_dataset(training_buf, bucket, prefix, 'training/training.protobuf')\n",
    "print(s3_training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/output/\n"
     ]
    }
   ],
   "source": [
    "s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "region = 'us-east-1'  \n",
    "container = retrieve('lda', region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::012086180905:role/service-role/AmazonSageMaker-ExecutionRole-20211121T093897\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)\n",
    "\n",
    "lda = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.2xlarge',\n",
    "    output_path=s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.set_hyperparameters(\n",
    "    num_topics=10, \n",
    "    feature_dim=len(dictionary), \n",
    "    mini_batch_size=1,\n",
    "    alpha0=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-05 08:29:37 Starting - Starting the training job...\n",
      "2022-01-05 08:29:46 Starting - Launching requested ML instancesProfilerReport-1641371377: InProgress\n",
      "......\n",
      "2022-01-05 08:30:56 Starting - Preparing the instances for training.........\n",
      "2022-01-05 08:32:37 Downloading - Downloading input data\n",
      "2022-01-05 08:32:37 Training - Downloading the training image...\n",
      "2022-01-05 08:33:08 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mUsing mxnet backend.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:11 INFO 139908622219072] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'alpha0': u'1.0', u'max_restarts': u'10', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'allow_svd_init': u'true', u'epochs': u'1', u'tol': u'1e-8', u'_kvstore': u'local', u'max_iterations': u'1000'}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:11 INFO 139908622219072] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'feature_dim': u'1024', u'mini_batch_size': u'1', u'num_topics': u'10', u'alpha0': u'0.1'}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:11 INFO 139908622219072] Final configuration: {u'alpha0': u'0.1', u'max_restarts': u'10', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'allow_svd_init': u'true', u'epochs': u'1', u'feature_dim': u'1024', u'num_topics': u'10', u'tol': u'1e-8', u'_kvstore': u'local', u'mini_batch_size': u'1', u'max_iterations': u'1000'}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:11 INFO 139908622219072] Using default worker.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:11 INFO 139908622219072] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:11 INFO 139908622219072] Running LDA for 13 topics (10 requested)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"LDA.initialize.time\": {\"count\": 1, \"max\": 0.4119873046875, \"sum\": 0.4119873046875, \"min\": 0.4119873046875}}, \"EndTime\": 1641371591.988556, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371591.972539}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Batches Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Total Records Seen\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}, \"Reset Count\": {\"count\": 1, \"max\": 0, \"sum\": 0.0, \"min\": 0}}, \"EndTime\": 1641371591.988712, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"init_train_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371591.988681}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Loaded all data into matrix with shape: (19, 1024)\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}, \"get_all_batches_from_iter.time\": {\"count\": 1, \"max\": 15.268087387084961, \"sum\": 15.268087387084961, \"min\": 15.268087387084961}}, \"EndTime\": 1641371592.004929, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371591.988649}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"moment1.time\": {\"count\": 1, \"max\": 1.7549991607666016, \"sum\": 1.7549991607666016, \"min\": 1.7549991607666016}}, \"EndTime\": 1641371592.011522, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.004971}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Number of iterations: 3\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Seed of random SVD : ('MT19937', array([         0,          1, 1812433255, 1900727105, 1208447044,\n",
      "       2481403966, 4042607538,  337614300, 3232553940, 1018809052,\n",
      "       3202401494, 1775180719, 3192392114,  594215549,  184016991,\n",
      "        829906058,  610491522, 3879932251, 3139825610,  297902587,\n",
      "       4075895579, 2943625357, 3530655617, 1423771745, 2135928312,\n",
      "       2891506774, 1066338622,  135451537,  933040465, 2759011858,\n",
      "       2273819758, 3545703099, 2516396728, 1272276355, 3172048492,\n",
      "       3267256201, 2332199830, 1975469449,  392443598, 1132453229,\n",
      "       2900699076, 1998300999, 3847713992,  512669506, 1227792182,\n",
      "       1629110240,  112303347, 2142631694, 3647635483, 1715036585,\n",
      "       2508091258, 1355887243, 1884998310, 3906360088,  952450269,\n",
      "       3647883368, 3962623343, 3077504981, 2023096077, 3791588343,\n",
      "       3937487744, 3455116780, 1218485897, 1374508007, 2815569918,\n",
      "       1367263917,  472908318, 2263147545, 1461547499, 4126813079,\n",
      "       2383504810,   64750479, 2963140275, 1709368606, 4143643781,\n",
      "        835933993, 1881494649,  674663333, 2076403047,  858036109,\n",
      "       1667579889, 1706666497,  607785554, 1995775149, 1941986352,\n",
      "       3448871082, 2109910019, 1474883361, 1623095288, 1831376534,\n",
      "       2612738285,   81681830, 2204289242, 1365038485,  251164610,\n",
      "       4268495337, 1805601714, 1262528768, 1442526919, 1675006593,\n",
      "        965627108,  646339161,  499795587,  840887574,  380522518,\n",
      "       3023789847, 1457635507, 1947093157, 2600365344, 2729853143,\n",
      "       1550618999, 1390905853, 3021294812,  882647559,  838872117,\n",
      "       1663880796, 4222103589, 2754172275, 3844026123, 3199260319,\n",
      "       4176064873, 3591027019, 2690294242, 2978135515, 3172796441,\n",
      "       3263669796, 1451257057, 1427035359, 4174826006, 2171992010,\n",
      "       1537002090, 3122405306, 4162452508, 3271954368, 3794310005,\n",
      "       3240514581, 1270412086, 3030475836, 2281945856, 2644171349,\n",
      "       3109139423, 4253563838, 1289926431, 1396919653,  733220100,\n",
      "       2753316645, 1196225013, 3699575255, 3569440056, 2675979228,\n",
      "       2624079148, 3463113149,  863430286,  623703199, 2113837653,\n",
      "       2656425919,  175981357, 4271478366, 4238022735, 1665483419,\n",
      "         86880610, 2963435083, 1830392943,  847801865, 3237296945,\n",
      "        332143967, 3973606945, 2671879697, 2236330279, 2360127810,\n",
      "       3283955434,  203240344, 4048139172,   13189264, 2263058814,\n",
      "        247241371, 1566765783, 3084408095, 3719371299, 1958375251,\n",
      "       1985924622, 1712739232, 1861691451, 2644502937, 2337807839,\n",
      "        784993770, 2962208780, 2190810177, 1523122731,  714888527,\n",
      "        578678761, 3698481324, 1801168075,  534650483, 3390213921,\n",
      "       3923356461, 3586009066, 2059432114,   52511333, 1969897376,\n",
      "       3630122061,  524661135, 3513619765,  563070233,  501359785,\n",
      "        477489274,  658768624,  938973567, 1548584683, 1345287459,\n",
      "       2488691004, 3441144905, 3849305094, 2430000078,  855172178,\n",
      "        614463281, 2092744749,  176381493, 1655802051, 2273888101,\n",
      "       2474494847, 3471978030, 2138918303,  575352373, 1658230985,\n",
      "       1675972553, 2946663114,  915579339,  284981499,   53939948,\n",
      "       3022598146, 1861218535, 3403620774, 4203516930, 2360471119,\n",
      "       3134536268, 1383448498, 1307602316, 3847663247, 3027225131,\n",
      "       3597251613, 3186237127,  725127595, 1928526954, 1843386923,\n",
      "       3560410503,   54688266, 1791983849, 2519860352, 4256389699,\n",
      "       2328812602,  486464275, 3578698363,  301279829, 1303654791,\n",
      "       4181868765,  971794070, 1933885487, 3996807464, 2144053754,\n",
      "       4079903755, 3775774765, 3481760044, 1212862354, 1067356423,\n",
      "       3764189132, 1609862325, 2209601551, 2565747501,  161962392,\n",
      "       4045451782, 2605574664, 2520953090, 3490240017, 1082791980,\n",
      "         44474324,  101811128, 4268650669, 4171338684,  772375154,\n",
      "       3920460306, 2319139534,  599033750, 2950874441, 3373922995,\n",
      "       1496848525, 4095253594, 1271943484, 1498723121, 3097453329,\n",
      "       3698082465,  281869581, 3148270661, 3591477288,  747441437,\n",
      "       2809508504, 3896107498,  303747862, 2368081624, 1844217645,\n",
      "        886825352,  287949781, 1444561207, 2512101757, 2062331723,\n",
      "        741720931, 1383797313, 3876746355, 2041045348, 2627599118,\n",
      "       1124169970,  200524822, 3484820454,   55883666, 1135054804,\n",
      "        669498692, 2677215504, 3097911127, 1509628615,  617580381,\n",
      "       2229022193,   85601568, 3243896546, 3715672328,  912168347,\n",
      "       2359163500, 1180347564, 4243175048, 2092067103,  880183327,\n",
      "       4000664709, 2045044777, 3500474644, 1515175520, 1862207123,\n",
      "        186628841, 3337252925,  708933575, 4015964629, 3136815297,\n",
      "       3314919747, 2891909013, 3316567785, 3944275369, 3608506218,\n",
      "       2884839110, 3054055598, 2707439927, 1381111877, 3275487281,\n",
      "       4292456216, 2639563270, 3327301876, 3576924628,  721056309,\n",
      "       2002808140,  748967365,   52380958, 2200261692,  763456477,\n",
      "       1708381337, 2038446433, 2682979402, 1526413779, 2211263302,\n",
      "       3879771969,   75966584, 3645059271, 2985763524, 4085690255,\n",
      "         82390958, 1883631385, 1647521260, 1598026998, 3038041577,\n",
      "       2501913134, 3279302868, 1738888524,  805035483,  756399074,\n",
      "       3863810982, 1097797270, 1505792529,  898904527,  583561003,\n",
      "        717152376, 3333867738, 1099456544, 1663473545, 1242141229,\n",
      "       3828627682, 1966201676, 1713552361, 3852160017, 1584965284,\n",
      "         21695908, 1013262144,  145341901, 3995441263, 3462066219,\n",
      "       2239637848, 1214086163, 2428868268, 1650037305, 1545513388,\n",
      "       1621198806, 4232947817, 1823092073,  256414624, 1745018809,\n",
      "       1357102386, 2055139770, 3280958307, 2482431613, 1664870585,\n",
      "        859130423, 4097751123, 3079768369, 2470211009, 2984880786,\n",
      "       2808568948, 2877071923, 1984903163,  302768457, 1866396789,\n",
      "        869566317, 3746415787, 4169433075, 3025005404, 3980733379,\n",
      "       3539207278, 3953071536,  876960847, 2548872156,  800507464,\n",
      "       1865466907, 1273317878, 3754712872, 1757188269, 3229950355,\n",
      "       3731640200, 2283390608, 2204990292,  411873449,  447423849,\n",
      "       1852437802,  472825525, 3044219944, 2913114194, 1859709265,\n",
      "       4053786194,  574820536, 2104496732,  865469814, 2438352724,\n",
      "       4208743605, 4215067542, 1364015250, 4139974345, 3838747005,\n",
      "       1818502786, 2914274940, 1402365828, 1751123528, 2302578077,\n",
      "       2463168652, 1968705496, 1730700144, 3023943273, 1139096844,\n",
      "       2658667767, 2063547264,  705791165, 1444775274, 2415454225,\n",
      "       1575664730,  921044163,  648101324, 1212387162, 4191962054,\n",
      "       1787702169, 1888718041, 1518218010, 3398792842, 4079359729,\n",
      "        149721439,  750400353, 2661036076, 3802767886,  520152586,\n",
      "        951852508, 2939585975, 1375969109,  385733137, 3523607459,\n",
      "       1902438415, 4250996086, 2712727066,  484493674, 3932107461,\n",
      "       1428488210, 1764242548, 3424801055, 4004904451, 2226862072,\n",
      "       2393366939, 3609584727, 3614444319,  317349896, 3826527525,\n",
      "        204023804,  981902443, 3356042039, 3051207045, 1869902661,\n",
      "        561831895, 3706675415, 1527687593, 1227610446, 2596341042,\n",
      "       3191717368, 3269246891,  557877074, 4062070629, 3052520266,\n",
      "       3772487029,  400039836, 3195205275, 4085394797, 1655557239,\n",
      "       1345770144, 2864727192,  449281238,   73189507,  528365765,\n",
      "       2727400656,  247880434, 2408277395,  777039183, 2210179398,\n",
      "       1088433648, 2124356402, 1555630141,  604790219,  195012151,\n",
      "       3312518356,  923728373, 3999251660, 3313059535, 3478133921,\n",
      "       3395026960,  383464614, 3425869222, 2446885186, 4032184426,\n",
      "        157195416, 3158909476, 1663750443, 2046427584, 1658453076,\n",
      "       1784483001, 3146546889, 1238739785, 2297306523, 3472330897,\n",
      "       2953326031, 2421672215, 1221694592, 1588568605, 2546987845,\n",
      "       3375168573, 2137961649, 3056565164,  330165219,  235900365,\n",
      "       1000384800, 2697255904,  579122283, 3050664825,   73426122,\n",
      "       1232986102, 2940571064, 3076486824, 1708182873, 2796363264,\n",
      "        292154131, 4280019913, 1102652157, 1185393592, 1494991690,\n",
      "       4270076389, 2384840717,  425785147, 2385321880,  317514772,\n",
      "       3926962743,  392176856, 3465421709, 1878853468,  122662664,\n",
      "       2958252160, 1858961315, 2244939588, 2361884409, 2860936803,\n",
      "        683833250, 3291277128, 1686857206, 1112632275, 1200680507,\n",
      "       3342928196, 2677058150,  939442136, 3407104669, 2906783932,\n",
      "       3668048733, 2030009470, 1910839172, 1234925283, 3575831445,\n",
      "        123595418, 2362440495, 3048484911, 1796872496], dtype=uint32), 624, 0, 0.0)\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Krylov method iteration: 0.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Krylov method iteration: 1.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Krylov method iteration: 2.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Krylov method iteration: 3.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Krylov method iteration: 4.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Krylov method iteration: 5.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Krylov method iteration: 6.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Covariance matrix min value: -0.000000\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Covariance matrix max value: 0.000000\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Starting SVD...\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"svd.time\": {\"count\": 1, \"max\": 8.980989456176758, \"sum\": 8.980989456176758, \"min\": 8.980989456176758}}, \"EndTime\": 1641371592.097859, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.011566}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Finished SVD.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"rand_svd.time\": {\"count\": 1, \"max\": 86.65204048156738, \"sum\": 86.65204048156738, \"min\": 86.65204048156738}}, \"EndTime\": 1641371592.098238, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.097912}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"GetMoments.time\": {\"count\": 1, \"max\": 88.70601654052734, \"sum\": 88.70601654052734, \"min\": 88.70601654052734}}, \"EndTime\": 1641371592.098468, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.098296}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e3_term1.time\": {\"count\": 1, \"max\": 4.047870635986328, \"sum\": 4.047870635986328, \"min\": 4.047870635986328}}, \"EndTime\": 1641371592.108296, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.098519}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e3_term2.time\": {\"count\": 1, \"max\": 1.1708736419677734, \"sum\": 1.1708736419677734, \"min\": 1.1708736419677734}}, \"EndTime\": 1641371592.109558, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.108357}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e3_term3.time\": {\"count\": 1, \"max\": 7.156848907470703, \"sum\": 7.156848907470703, \"min\": 7.156848907470703}}, \"EndTime\": 1641371592.117367, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.109614}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m3_contribution_e3.time\": {\"count\": 1, \"max\": 18.50295066833496, \"sum\": 18.50295066833496, \"min\": 18.50295066833496}}, \"EndTime\": 1641371592.118786, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.117425}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e2m1_term1.time\": {\"count\": 1, \"max\": 0.13184547424316406, \"sum\": 0.13184547424316406, \"min\": 0.13184547424316406}, \"from_numpy.time\": {\"count\": 3, \"max\": 0.15091896057128906, \"sum\": 0.34880638122558594, \"min\": 0.09799003601074219}}, \"EndTime\": 1641371592.120505, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.118845}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"e2m1_term2.time\": {\"count\": 1, \"max\": 0.9748935699462891, \"sum\": 0.9748935699462891, \"min\": 0.9748935699462891}}, \"EndTime\": 1641371592.12186, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.12057}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m3_contribution_e2_m1.time\": {\"count\": 1, \"max\": 3.5889148712158203, \"sum\": 3.5889148712158203, \"min\": 3.5889148712158203}}, \"EndTime\": 1641371592.122455, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.121914}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m1_3.time\": {\"count\": 1, \"max\": 0.186920166015625, \"sum\": 0.186920166015625, \"min\": 0.186920166015625}}, \"EndTime\": 1641371592.122803, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.122508}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m3_contribution_m1_3.time\": {\"count\": 1, \"max\": 0.4889965057373047, \"sum\": 0.4889965057373047, \"min\": 0.4889965057373047}}, \"EndTime\": 1641371592.123071, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.122856}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"whitened_m3.time\": {\"count\": 1, \"max\": 24.703025817871094, \"sum\": 24.703025817871094, \"min\": 24.703025817871094}}, \"EndTime\": 1641371592.123251, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.123125}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"M3Whitening.time\": {\"count\": 1, \"max\": 24.852991104125977, \"sum\": 24.852991104125977, \"min\": 24.852991104125977}}, \"EndTime\": 1641371592.123396, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.123294}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 1/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using svd initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=7, error=0.0312436670065, var=5.96167240019e-08\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 2/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=0, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using svd initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=5, error=0.0312436688691, var=5.96167240019e-08\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 5 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 3/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=8, error=0.0312436688691, var=5.96167240019e-08\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 8 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 4/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=7, error=0.0312436688691, var=5.96167240019e-08\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 5/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=7, error=0.0312436688691, var=0.0\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 6/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=8, error=0.0312436688691, var=0.0\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 8 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 7/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=8, error=0.0312436688691, var=0.0\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 8 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 8/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=7, error=0.0312436670065, var=5.96167240019e-08\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 9/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=8, error=0.0312436688691, var=0.0\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 8 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Restart 10/10\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Starting CPDecomp...\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Configuration: num_qr_iterations=5, line_search=True, relaxation_factor=scaled\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] Using random initialization.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] [CPDecomp] iter=7, error=0.0312436688691, var=0.0\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Converged to local minimum in 7 iterations.\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Best reconstruction error: 0.031243667\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"CPDecomposition.time\": {\"count\": 1, \"max\": 674.5469570159912, \"sum\": 674.5469570159912, \"min\": 674.5469570159912}}, \"EndTime\": 1641371592.798021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.123443}\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"LDA.update.time\": {\"count\": 1, \"max\": 811.1939430236816, \"sum\": 811.1939430236816, \"min\": 811.1939430236816}, \"to_numpy.time\": {\"count\": 4, \"max\": 0.06318092346191406, \"sum\": 0.1933574676513672, \"min\": 0.030040740966796875}}, \"EndTime\": 1641371592.800856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.798081}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"Max Batches Seen Between Resets\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}, \"Number of Batches Since Last Reset\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}, \"Number of Records Since Last Reset\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}, \"Total Batches Seen\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}, \"Total Records Seen\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}, \"Max Records Seen Between Resets\": {\"count\": 1, \"max\": 19, \"sum\": 19.0, \"min\": 19}, \"Reset Count\": {\"count\": 1, \"max\": 1, \"sum\": 1.0, \"min\": 1}}, \"EndTime\": 1641371592.801149, \"Dimensions\": {\"Host\": \"algo-1\", \"Meta\": \"training_data_iter\", \"Operation\": \"training\", \"Algorithm\": \"LDA\", \"epoch\": 0}, \"StartTime\": 1641371592.80111}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] #throughput_metric: host=algo-1, train throughput=23.4107473764 records/second\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 WARNING 139908622219072] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"LDA.finalize.time\": {\"count\": 1, \"max\": 0.0247955322265625, \"sum\": 0.0247955322265625, \"min\": 0.0247955322265625}}, \"EndTime\": 1641371592.801527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.800918}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Serializing LDA model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"LDAModel.serialize.time\": {\"count\": 1, \"max\": 0.3590583801269531, \"sum\": 0.3590583801269531, \"min\": 0.3590583801269531}}, \"EndTime\": 1641371592.801986, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.80159}\u001b[0m\n",
      "\u001b[34m[01/05/2022 08:33:12 INFO 139908622219072] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 919.9779033660889, \"sum\": 919.9779033660889, \"min\": 919.9779033660889}, \"setuptime\": {\"count\": 1, \"max\": 53.42888832092285, \"sum\": 53.42888832092285, \"min\": 53.42888832092285}}, \"EndTime\": 1641371592.803418, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"LDA\"}, \"StartTime\": 1641371592.802033}\u001b[0m\n",
      "\n",
      "2022-01-05 08:33:37 Uploading - Uploading generated training model\n",
      "2022-01-05 08:33:37 Completed - Training job completed\n",
      "Training seconds: 63\n",
      "Billable seconds: 63\n"
     ]
    }
   ],
   "source": [
    "lda.fit(inputs={'train': s3_training_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model\n",
    "This will only work if the training completed successfully and the model is saved under the assumed location, you can change it if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "s3_output='s3://mymltextarticles/training/output/lda-2022-01-05-08-29-37-479/output/model.tar.gz'\n",
    "role = get_execution_role()\n",
    "lda=sagemaker.LDAModel(s3_output, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "lda_predictor = lda.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prediction using training data\n",
    "This assumes that a data frame and a dictionary have been prepared using the cells above for data preparation\n",
    "It also assumes the model has been loaded and the endpoint has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    line=0\n",
    "    sample_matrix = np.zeros((num_lines, num_columns)).astype('float32')\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            sample_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "\n",
    "    return sample_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 1024)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples= prepare_samples(data, dictionary)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "response = lda_predictor.predict(samples)\n",
    "#print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "8\n",
      "7\n",
      "5\n",
      "0\n",
      "7\n",
      "3\n",
      "0\n",
      "4\n",
      "6\n",
      "0\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "9\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#response = json.loads(response)\n",
    "for r in response:\n",
    "    topic_vector=r.label['topic_mixture'].float32_tensor.values\n",
    "    top_topic= np.argmax(topic_vector)\n",
    "    print(top_topic)\n",
    "    #vectors = [r['topic_mixture'] for r in response['predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
