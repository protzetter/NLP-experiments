{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5393b59",
   "metadata": {},
   "source": [
    "Created on December 28th  2021 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "## Small experiment of document mining with various techniques Part 9\n",
    "\n",
    "Let us use AWS built-in NTM algorithm for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb0cc5",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d26678ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install NLTK and gensim if required\n",
    "#%%sh\n",
    "#pip3 -q install nltk gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75913d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import require libraries\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import pdftotext\n",
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d9e15c",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10200f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read PDF files using pdftotext\n",
    "def readPdfFile(filename):\n",
    "    text=\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pdf = pdftotext.PDF(f)\n",
    "        for page in pdf:\n",
    "            text=text+page\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b729301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of test files\n",
    "path='./sampledocs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7bc4bfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "docNLP=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.pdf')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    fileText=readPdfFile(file_name)\n",
    "    docName.append(file_name)\n",
    "    docType.append('pdf')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b32034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text:90946.61111111111\n",
      "Min length of text:9170\n",
      "Max length of text:328295\n"
     ]
    }
   ],
   "source": [
    " print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e414484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/NIST.IR.8312.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>NISTIR 8312\\n\\nFour Principles of Explainable ...</td>\n",
       "      <td>nistir four principles explainable artificial ...</td>\n",
       "      <td>16792</td>\n",
       "      <td>5026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/ai-360-research.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>ai insights next frontier business corner offi...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>application ai insurtech real estate technolog...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/AI-bank-of-the-future-Can-banks-m...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Global Banking &amp; Securities\\n\\nAI-bank of the ...</td>\n",
       "      <td>global banking securities ai bank future banks...</td>\n",
       "      <td>5774</td>\n",
       "      <td>2144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0                      ./sampledocs/NIST.IR.8312.pdf  pdf   \n",
       "1                   ./sampledocs/ai-360-research.pdf  pdf   \n",
       "2           ./sampledocs/Module-1-Lecture-Slides.pdf  pdf   \n",
       "3  ./sampledocs/Technology-and-innovation-in-the-...  pdf   \n",
       "4  ./sampledocs/AI-bank-of-the-future-Can-banks-m...  pdf   \n",
       "\n",
       "                                                Text  \\\n",
       "0  NISTIR 8312\\n\\nFour Principles of Explainable ...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "4  Global Banking & Securities\\n\\nAI-bank of the ...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  nistir four principles explainable artificial ...            16792   \n",
       "1  ai insights next frontier business corner offi...             5281   \n",
       "2  application ai insurtech real estate technolog...             3728   \n",
       "3  technology innovation insurance sector technol...            16742   \n",
       "4  global banking securities ai bank future banks...             5774   \n",
       "\n",
       "   text_unique_words  \n",
       "0               5026  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               4228  \n",
       "4               2144  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6663472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18 entries, 0 to 17\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Name               18 non-null     object\n",
      " 1   Type               18 non-null     object\n",
      " 2   Text               18 non-null     object\n",
      " 3   cleanText          18 non-null     object\n",
      " 4   text_word_count    18 non-null     int64 \n",
      " 5   text_unique_words  18 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 992.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1d430dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrickrotzetter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/patrickrotzetter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    text = text.lower().split()\n",
    "    text = [w for w in text if not w in stop_words] \n",
    "    text = [wnl.lemmatize(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28b880eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.64 s, sys: 31.6 ms, total: 1.67 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fullDocs['cleanText'] = fullDocs['cleanText'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd117e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/NIST.IR.8312.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>NISTIR 8312\\n\\nFour Principles of Explainable ...</td>\n",
       "      <td>[nistir, four, principle, explainable, artific...</td>\n",
       "      <td>16792</td>\n",
       "      <td>5026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/ai-360-research.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>[ai, insight, next, frontier, business, corner...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>[application, ai, insurtech, real, estate, tec...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>[technology, innovation, insurance, sector, te...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/AI-bank-of-the-future-Can-banks-m...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Global Banking &amp; Securities\\n\\nAI-bank of the ...</td>\n",
       "      <td>[global, banking, security, ai, bank, future, ...</td>\n",
       "      <td>5774</td>\n",
       "      <td>2144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0                      ./sampledocs/NIST.IR.8312.pdf  pdf   \n",
       "1                   ./sampledocs/ai-360-research.pdf  pdf   \n",
       "2           ./sampledocs/Module-1-Lecture-Slides.pdf  pdf   \n",
       "3  ./sampledocs/Technology-and-innovation-in-the-...  pdf   \n",
       "4  ./sampledocs/AI-bank-of-the-future-Can-banks-m...  pdf   \n",
       "\n",
       "                                                Text  \\\n",
       "0  NISTIR 8312\\n\\nFour Principles of Explainable ...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "4  Global Banking & Securities\\n\\nAI-bank of the ...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  [nistir, four, principle, explainable, artific...            16792   \n",
       "1  [ai, insight, next, frontier, business, corner...             5281   \n",
       "2  [application, ai, insurtech, real, estate, tec...             3728   \n",
       "3  [technology, innovation, insurance, sector, te...            16742   \n",
       "4  [global, banking, security, ai, bank, future, ...             5774   \n",
       "\n",
       "   text_unique_words  \n",
       "0               5026  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               4228  \n",
       "4               2144  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "257524f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 95.1 ms, sys: 3.64 ms, total: 98.7 ms\n",
      "Wall time: 96.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(fullDocs['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab6e8d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(11925 unique tokens: ['aaai', 'aad', 'ab', 'abhishek', 'abigail']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02afc95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1024 unique tokens: ['absence', 'academic', 'accept', 'acceptance', 'accessed']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(keep_n=1024)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0258b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for index in range(0,len(dictionary)):\n",
    "        f.write(dictionary.get(index)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b41ea631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.3 ms, sys: 2.13 ms, total: 60.4 ms\n",
      "Wall time: 59.1 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fullDocs['tokens'] = fullDocs.apply(lambda row: dictionary.doc2bow(row['cleanText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd1ada4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0, 2), (1, 1), (2, 1), (3, 4), (4, 4), (5, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(2, 1), (3, 2), (8, 1), (14, 1), (42, 1), (46...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(12, 1), (18, 3), (24, 2), (39, 2), (59, 4), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(3, 1), (6, 2), (10, 3), (11, 1), (14, 5), (1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(5, 1), (9, 1), (13, 3), (21, 2), (30, 9), (3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens\n",
       "0  [(0, 2), (1, 1), (2, 1), (3, 4), (4, 4), (5, 1...\n",
       "1  [(2, 1), (3, 2), (8, 1), (14, 1), (42, 1), (46...\n",
       "2  [(12, 1), (18, 3), (24, 2), (39, 2), (59, 4), ...\n",
       "3  [(3, 1), (6, 2), (10, 3), (11, 1), (14, 5), (1...\n",
       "4  [(5, 1), (9, 1), (13, 3), (21, 2), (30, 9), (3..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fullDocs.drop(['cleanText'], axis=1)\n",
    "data = data.drop(['Name'], axis=1)\n",
    "data = data.drop(['Type'], axis=1)\n",
    "data = data.drop(['Text'], axis=1)\n",
    "data = data.drop(['text_word_count'], axis=1)\n",
    "data = data.drop(['text_unique_words'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79718256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.72.1\n"
     ]
    }
   ],
   "source": [
    "import io, boto3\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = 'mynlpexperimentbucketforsagemaker'\n",
    "prefix = 'headlines-lda-ntm'\n",
    "\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id='xxxxx',\n",
    "    aws_secret_access_key='yyyyy'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "622fe2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protobuf_dataset(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    token_matrix = lil_matrix((num_lines, num_columns)).astype('float32')\n",
    "    line = 0\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            token_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e3d80d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_protbuf_dataset(buf, bucket, prefix, key):\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    buf.seek(0)\n",
    "    s3.upload_fileobj(buf, bucket, obj)\n",
    "    path = 's3://{}/{}'.format(bucket,obj)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e16bec1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mynlpexperimentbucketforsagemaker/headlines-lda-ntm/training/training.protobuf\n",
      "CPU times: user 84.8 ms, sys: 8.26 ms, total: 93 ms\n",
      "Wall time: 1.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_buf = build_protobuf_dataset(data, dictionary)\n",
    "s3_training_path = upload_protbuf_dataset(training_buf, bucket, prefix, 'training/training.protobuf')\n",
    "print(s3_training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50680964",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "29813b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mynlpexperimentbucketforsagemaker/headlines-lda-ntm/output/\n"
     ]
    }
   ],
   "source": [
    "s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "35de041c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "region = session.boto_session.region_name    \n",
    "container = retrieve('lda', region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3d18da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DUMMY_IAM_ROLE = 'arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001'\n",
    "\n",
    "lda = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=DUMMY_IAM_ROLE,\n",
    "    #    role='arn:aws:iam::783491625988:role/SageMakerFullAccess',\n",
    "    instance_count=1, \n",
    "    instance_type='local',\n",
    "    output_path=s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a7e256a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.set_hyperparameters(\n",
    "    num_topics=10, \n",
    "    feature_dim=len(dictionary), \n",
    "    mini_batch_size=1,\n",
    "    alpha0=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "47b3d9c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "Error response from daemon: pull access denied for 766337827248.dkr.ecr.us-east-1.amazonaws.com/lda, repository does not exist or may require 'docker login': denied: User: arn:aws:iam::783491625988:user/Administrator is not authorized to perform: ecr:BatchGetImage on resource: arn:aws:ecr:us-east-1:766337827248:repository/lda because no resource-based policy allows the ecr:BatchGetImage action\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['docker', 'pull', '766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:1']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/s5w544vs5qnb9dprpz5xqgnw0000gn/T/ipykernel_85580/2352995172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms3_training_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \"\"\"\n\u001b[1;32m   1470\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_training_job\u001b[0;34m(self, TrainingJobName, AlgorithmSpecification, OutputDataConfig, ResourceConfig, InputDataConfig, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhyperparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"HyperParameters\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"HyperParameters\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting training job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m         \u001b[0mtraining_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mInputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOutputDataConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingJobName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_training_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrainingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         self.model_artifacts = self.container.train(\n\u001b[0m\u001b[1;32m    221\u001b[0m             \u001b[0minput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_data_config, output_data_config, hyperparameters, job_name)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_ecr_login_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 231\u001b[0;31m             \u001b[0m_pull_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         process = subprocess.Popen(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_pull_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m   1094\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"docker command: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpull_image_command\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpull_image_command\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"image pulled: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mempty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[0m\u001b[1;32m    416\u001b[0m                **kwargs).stdout\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    517\u001b[0m                                      output=stdout, stderr=stderr)\n\u001b[1;32m    518\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mCompletedProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['docker', 'pull', '766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:1']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "lda.fit(inputs={'train': s3_training_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab2137e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first initialize some AWS S3 and role parameters to be used later\n",
    "# depending where you are running the experiment, adapt the parameters to your AWS environment\n",
    "\n",
    "bucket='mynlpexperimentbucketforsagemaker'\n",
    "input_s3_url = \"s3://mynlpexperimentbucketforsagemaker/\"\n",
    "output_s3_url = \"s3://mycomprehendoutputbucket3110\"\n",
    "data_access_role_arn = \"arn:aws:iam::783491625988:role/ComprehendAccess\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff18d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import boto and connect to S3\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "# let us list the files available for analysis in the S3 bucket\n",
    "subfolder=''\n",
    "contents = s3.list_objects(Bucket=bucket, Prefix=subfolder)['Contents']\n",
    "number_of_docs=0\n",
    "filenames=[]\n",
    "for f in contents:\n",
    "    number_of_docs=number_of_docs+1\n",
    "    print(f['Key'])\n",
    "    filenames.append(f['Key'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3568d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s3_path(s3_path):\n",
    "    path_parts=s3_path.replace(\"s3://\",\"\").split(\"/\")\n",
    "    bucket=path_parts.pop(0)\n",
    "    key=\"/\".join(path_parts)\n",
    "    return bucket, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket, key = split_s3_path(outputfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0de327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(bucket,key,'./output.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68805ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip < output.tar.gz | tar -xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sea\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86146cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics=pd.read_csv('doc-topics.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms=pd.read_csv('topic-terms.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebbbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0_df=topic_terms[topic_terms['topic']==0]\n",
    "plot=sea.barplot(x=topic0_df.term,y=topic0_df.weight,order=topic0_df.sort_values(by=['weight'], ascending=False).set_index('term').index)\n",
    "plot.set_xticklabels(plot.get_xticklabels(),rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ec865",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = topic_terms.sort_values(by=['topic','weight'], ascending=False)\n",
    "grid = sea.FacetGrid(final_df, col=\"topic\", col_wrap=3,hue=\"topic\",margin_titles=True, sharex=False)\n",
    "grid_map=grid.map(sea.barplot,\"term\",\"weight\",order=None);\n",
    "for ax in grid_map.axes.flatten():\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=60)\n",
    "grid_map.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd187abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = []\n",
    "for i in range(number_of_topics):\n",
    "    keywords = []\n",
    "    df_sub= topic_terms[topic_terms['topic']==i].sort_values(by=['weight'], ascending=False)\n",
    "    keywords = \", \".join(df_sub['term'])\n",
    "    i=i+1\n",
    "    topic_keywords.append(keywords)\n",
    "topic_keywords_df= pd.DataFrame(topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf488f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame()\n",
    "for filename in filenames:\n",
    "    sub_topics_df=doc_topics[doc_topics['docname']==filename].sort_values(by=['proportion'], ascending=False)\n",
    "    row=sub_topics_df.iloc[0]\n",
    "    topics_df = topics_df.append(pd.Series([row['docname'],int(row['topic']), round(row['proportion'],4)]), ignore_index=True)\n",
    "topics_df.columns = ['Document name', 'Topic', 'Contribution']\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 3 Keywords for each Topic\n",
    "\n",
    "topic_top3words = [(i,topic) for i, topics in enumerate(topic_keywords) for (j, topic) in enumerate(topics.split(',')) if j<3]\n",
    "print(topic_top3words)\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)\n",
    "df_top3words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "#fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax=sea.histplot(data=topics_df,x=topics_df.Topic)\n",
    "ax.set_xticks(range(topics_df.Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax.xaxis.set_major_formatter(tick_formatter)\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=60)\n",
    "ax.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "ax.set_ylabel('Number of Documents')\n",
    "ax.set_ylim(0, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d794381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us list the files available for analysis in the S3 bucket\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id='AKIA3M255OQCLQRJ5W63',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "subfolder=''\n",
    "contents = s3.list_objects(Bucket=bucket, Prefix='')['Contents']\n",
    "number_of_docs=0\n",
    "filenames=[]\n",
    "for f in contents:\n",
    "    number_of_docs=number_of_docs+1\n",
    "    print(f['Key'])\n",
    "    filenames.append(f['Key'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e30fa802",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Couldn't call 'get_role' to get Role ARN from role name arn:aws:iam::783491625988:root to get Role path.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The current AWS identity is not a role: arn:aws:iam::783491625988:root, therefore it cannot be used as a SageMaker execution role",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/s5w544vs5qnb9dprpz5xqgnw0000gn/T/ipykernel_85580/3547104066.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msagemaker_session\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mrole\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_execution_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mget_execution_role\u001b[0;34m(sagemaker_session)\u001b[0m\n\u001b[1;32m   4439\u001b[0m         \u001b[0;34m\"SageMaker execution role\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4440\u001b[0m     )\n\u001b[0;32m-> 4441\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The current AWS identity is not a role: arn:aws:iam::783491625988:root, therefore it cannot be used as a SageMaker execution role"
     ]
    }
   ],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ced23ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'438346466558.dkr.ecr.eu-west-1.amazonaws.com/linear-learner:1'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve(framework='linear-learner',region='eu-west-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9d1be3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = DUMMY_IAM_ROLE # sagemaker.get_execution_role()\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n",
    "  'HF_TASK':'question-answering' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "   py_version=\"py36\", # python version of the DLC\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1a1e61a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'ModelDataUrl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/s5w544vs5qnb9dprpz5xqgnw0000gn/T/ipykernel_85580/3852854566.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# deploy model to SageMaker Inference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m predictor = huggingface_model.deploy(\n\u001b[0m\u001b[1;32m      3\u001b[0m    \u001b[0minitial_instance_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m    \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"local\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, tags, kms_key, wait, data_capture_config, **kwargs)\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0mdata_capture_config_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_capture_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_request_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         self.sagemaker_session.endpoint_from_production_variants(\n\u001b[0m\u001b[1;32m    784\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mproduction_variants\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mproduction_variant\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mendpoint_from_production_variants\u001b[0;34m(self, name, production_variants, tags, kms_key, wait, data_capture_config_dict)\u001b[0m\n\u001b[1;32m   3568\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3570\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3571\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexpand_role\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrole\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, endpoint_name, config_name, tags, wait)\u001b[0m\n\u001b[1;32m   3069\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_append_project_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3070\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3071\u001b[0;31m         self.sagemaker_client.create_endpoint(\n\u001b[0m\u001b[1;32m   3072\u001b[0m             \u001b[0mEndpointName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3073\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_endpoint\u001b[0;34m(self, EndpointName, EndpointConfigName, Tags)\u001b[0m\n\u001b[1;32m    346\u001b[0m         \u001b[0mendpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_LocalEndpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_endpoints\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEndpointName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m         \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEndpointConfigName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mserve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    576\u001b[0m         )\n\u001b[1;32m    577\u001b[0m         self.container.serve(\n\u001b[0;32m--> 578\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ModelDataUrl\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Environment\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m         )\n\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'ModelDataUrl'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"local\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8edb67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
