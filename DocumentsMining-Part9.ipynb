{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5393b59",
   "metadata": {},
   "source": [
    "Created on December 28th  2021 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "## Small experiment of document mining with various techniques Part 9\n",
    "\n",
    "Let us use AWS built-in NTM algorithm for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80b960",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9934b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install NLTK and gensim if required\n",
    "#%%sh\n",
    "#pip3 -q install nltk gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "029159d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import require libraries\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import pandas as pd\n",
    "import pdftotext\n",
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c49cac",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fff2237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read PDF files using pdftotext\n",
    "def readPdfFile(filename):\n",
    "    text=\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pdf = pdftotext.PDF(f)\n",
    "        for page in pdf:\n",
    "            text=text+page\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba19c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of test files\n",
    "path='./sampledocs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c39b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "docNLP=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.pdf')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    fileText=readPdfFile(file_name)\n",
    "    docName.append(file_name)\n",
    "    docType.append('pdf')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8419895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text:89012.35294117648\n",
      "Min length of text:9170\n",
      "Max length of text:328295\n"
     ]
    }
   ],
   "source": [
    " print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8be7a925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/ai-360-research.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>ai insights next frontier business corner offi...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>application ai insurtech real estate technolog...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/AI-bank-of-the-future-Can-banks-m...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Global Banking &amp; Securities\\n\\nAI-bank of the ...</td>\n",
       "      <td>global banking securities ai bank future banks...</td>\n",
       "      <td>5774</td>\n",
       "      <td>2144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>machine intelligence insurance insights end en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0                   ./sampledocs/ai-360-research.pdf  pdf   \n",
       "1           ./sampledocs/Module-1-Lecture-Slides.pdf  pdf   \n",
       "2  ./sampledocs/Technology-and-innovation-in-the-...  pdf   \n",
       "3  ./sampledocs/AI-bank-of-the-future-Can-banks-m...  pdf   \n",
       "4                   ./sampledocs/sigma-5-2020-en.pdf  pdf   \n",
       "\n",
       "                                                Text  \\\n",
       "0  AI 360: insights from the\\nnext frontier of bu...   \n",
       "1  Application of AI, Insurtech and Real Estate\\n...   \n",
       "2  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "3  Global Banking & Securities\\n\\nAI-bank of the ...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  ai insights next frontier business corner offi...             5281   \n",
       "1  application ai insurtech real estate technolog...             3728   \n",
       "2  technology innovation insurance sector technol...            16742   \n",
       "3  global banking securities ai bank future banks...             5774   \n",
       "4  machine intelligence insurance insights end en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               1746  \n",
       "1               1506  \n",
       "2               4228  \n",
       "3               2144  \n",
       "4               4329  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73cfb0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17 entries, 0 to 16\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Name               17 non-null     object\n",
      " 1   Type               17 non-null     object\n",
      " 2   Text               17 non-null     object\n",
      " 3   cleanText          17 non-null     object\n",
      " 4   text_word_count    17 non-null     int64 \n",
      " 5   text_unique_words  17 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 944.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71ae1ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/patrickrotzetter/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/patrickrotzetter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    text = text.lower().split()\n",
    "    text = [w for w in text if not w in stop_words] \n",
    "    text = [wnl.lemmatize(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "660612c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.7 s, sys: 44.8 ms, total: 1.74 s\n",
      "Wall time: 1.78 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fullDocs['cleanText'] = fullDocs['cleanText'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9b13954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/ai-360-research.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>[ai, insight, next, frontier, business, corner...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>[application, ai, insurtech, real, estate, tec...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>[technology, innovation, insurance, sector, te...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/AI-bank-of-the-future-Can-banks-m...</td>\n",
       "      <td>pdf</td>\n",
       "      <td>Global Banking &amp; Securities\\n\\nAI-bank of the ...</td>\n",
       "      <td>[global, banking, security, ai, bank, future, ...</td>\n",
       "      <td>5774</td>\n",
       "      <td>2144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.pdf</td>\n",
       "      <td>pdf</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>[machine, intelligence, insurance, insight, en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0                   ./sampledocs/ai-360-research.pdf  pdf   \n",
       "1           ./sampledocs/Module-1-Lecture-Slides.pdf  pdf   \n",
       "2  ./sampledocs/Technology-and-innovation-in-the-...  pdf   \n",
       "3  ./sampledocs/AI-bank-of-the-future-Can-banks-m...  pdf   \n",
       "4                   ./sampledocs/sigma-5-2020-en.pdf  pdf   \n",
       "\n",
       "                                                Text  \\\n",
       "0  AI 360: insights from the\\nnext frontier of bu...   \n",
       "1  Application of AI, Insurtech and Real Estate\\n...   \n",
       "2  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "3  Global Banking & Securities\\n\\nAI-bank of the ...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  [ai, insight, next, frontier, business, corner...             5281   \n",
       "1  [application, ai, insurtech, real, estate, tec...             3728   \n",
       "2  [technology, innovation, insurance, sector, te...            16742   \n",
       "3  [global, banking, security, ai, bank, future, ...             5774   \n",
       "4  [machine, intelligence, insurance, insight, en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               1746  \n",
       "1               1506  \n",
       "2               4228  \n",
       "3               2144  \n",
       "4               4329  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff3e734d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 139 ms, sys: 3.04 ms, total: 142 ms\n",
      "Wall time: 142 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(fullDocs['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ebd1cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(10926 unique tokens: ['ability', 'able', 'accelerates', 'accept', 'acceptance']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b8acf94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1024 unique tokens: ['accept', 'acceptance', 'accessing', 'accounting', 'accurately']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(keep_n=1024)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f695d09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for index in range(0,len(dictionary)):\n",
    "        f.write(dictionary.get(index)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b9bcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.8 ms, sys: 1.99 ms, total: 41.8 ms\n",
      "Wall time: 40.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "fullDocs['tokens'] = fullDocs.apply(lambda row: dictionary.doc2bow(row['cleanText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7116c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0, 1), (1, 2), (2, 1), (3, 2), (4, 1), (5, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(30, 1), (32, 1), (35, 4), (39, 1), (40, 2), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(1, 1), (2, 1), (6, 1), (7, 2), (8, 5), (9, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(6, 1), (14, 1), (16, 1), (24, 1), (25, 3), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(0, 3), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens\n",
       "0  [(0, 1), (1, 2), (2, 1), (3, 2), (4, 1), (5, 1...\n",
       "1  [(30, 1), (32, 1), (35, 4), (39, 1), (40, 2), ...\n",
       "2  [(1, 1), (2, 1), (6, 1), (7, 2), (8, 5), (9, 5...\n",
       "3  [(6, 1), (14, 1), (16, 1), (24, 1), (25, 3), (...\n",
       "4  [(0, 3), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1..."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fullDocs.drop(['cleanText'], axis=1)\n",
    "data = data.drop(['Name'], axis=1)\n",
    "data = data.drop(['Type'], axis=1)\n",
    "data = data.drop(['Text'], axis=1)\n",
    "data = data.drop(['text_word_count'], axis=1)\n",
    "data = data.drop(['text_unique_words'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8db9fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.72.1\n"
     ]
    }
   ],
   "source": [
    "import io, boto3\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "print(sagemaker.__version__)\n",
    "\n",
    "session = sagemaker.Session()\n",
    "bucket = 'mynlpexperimentbucketforsagemaker'\n",
    "prefix = 'headlines-lda-ntm'\n",
    "\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5576ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protobuf_dataset(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    token_matrix = lil_matrix((num_lines, num_columns)).astype('float32')\n",
    "    line = 0\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            token_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "177b4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_protbuf_dataset(buf, bucket, prefix, key):\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    buf.seek(0)\n",
    "    s3.upload_fileobj(buf, bucket, obj)\n",
    "    path = 's3://{}/{}'.format(bucket,obj)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0ac68b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mynlpexperimentbucketforsagemaker/headlines-lda-ntm/training/training.protobuf\n",
      "CPU times: user 69.5 ms, sys: 6.32 ms, total: 75.8 ms\n",
      "Wall time: 1.18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "training_buf = build_protobuf_dataset(data, dictionary)\n",
    "s3_training_path = upload_protbuf_dataset(training_buf, bucket, prefix, 'training/training.protobuf')\n",
    "print(s3_training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2823fd8d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "58ccdf14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mynlpexperimentbucketforsagemaker/headlines-lda-ntm/output/\n"
     ]
    }
   ],
   "source": [
    "s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8dbc66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "766337827248.dkr.ecr.us-east-1.amazonaws.com/lda:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "region = 'us-east-1'  \n",
    "container = retrieve('lda', region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbad9214",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DUMMY_IAM_ROLE = 'arn:aws:iam::111111111111:role/service-role/AmazonSageMaker-ExecutionRole-20200101T000001'\n",
    "\n",
    "lda = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role='arn:aws:iam::783491625988:role/SageMakerFullAccess',\n",
    "    instance_count=1, \n",
    "    instance_type='ml.g4dn.xlarge',\n",
    "    output_path=s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8251b47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.set_hyperparameters(\n",
    "    num_topics=10, \n",
    "    feature_dim=len(dictionary), \n",
    "    mini_batch_size=1,\n",
    "    alpha0=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "529ebaa3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g4dn.xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/71/s5w544vs5qnb9dprpz5xqgnw0000gn/T/ipykernel_22954/2352995172.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms3_training_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1469\u001b[0m         \"\"\"\n\u001b[1;32m   1470\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1471\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1473\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    390\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/conda-sagemaker/lib/python3.8/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    717\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.g4dn.xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "lda.fit(inputs={'train': s3_training_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e283abbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a19d552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first initialize some AWS S3 and role parameters to be used later\n",
    "# depending where you are running the experiment, adapt the parameters to your AWS environment\n",
    "\n",
    "bucket='mynlpexperimentbucketforsagemaker'\n",
    "input_s3_url = \"s3://mynlpexperimentbucketforsagemaker/\"\n",
    "output_s3_url = \"s3://mycomprehendoutputbucket3110\"\n",
    "data_access_role_arn = \"arn:aws:iam::783491625988:role/ComprehendAccess\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff18d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import boto and connect to S3\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "# let us list the files available for analysis in the S3 bucket\n",
    "subfolder=''\n",
    "contents = s3.list_objects(Bucket=bucket, Prefix=subfolder)['Contents']\n",
    "number_of_docs=0\n",
    "filenames=[]\n",
    "for f in contents:\n",
    "    number_of_docs=number_of_docs+1\n",
    "    print(f['Key'])\n",
    "    filenames.append(f['Key'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3568d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_s3_path(s3_path):\n",
    "    path_parts=s3_path.replace(\"s3://\",\"\").split(\"/\")\n",
    "    bucket=path_parts.pop(0)\n",
    "    key=\"/\".join(path_parts)\n",
    "    return bucket, key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8e702",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket, key = split_s3_path(outputfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff95e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0de327",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.download_file(bucket,key,'./output.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68805ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip < output.tar.gz | tar -xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89ce29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bc86e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sea\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86146cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_topics=pd.read_csv('doc-topics.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d06c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_terms=pd.read_csv('topic-terms.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ebbbfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(topic_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3386cce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic0_df=topic_terms[topic_terms['topic']==0]\n",
    "plot=sea.barplot(x=topic0_df.term,y=topic0_df.weight,order=topic0_df.sort_values(by=['weight'], ascending=False).set_index('term').index)\n",
    "plot.set_xticklabels(plot.get_xticklabels(),rotation=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89ec865",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = topic_terms.sort_values(by=['topic','weight'], ascending=False)\n",
    "grid = sea.FacetGrid(final_df, col=\"topic\", col_wrap=3,hue=\"topic\",margin_titles=True, sharex=False)\n",
    "grid_map=grid.map(sea.barplot,\"term\",\"weight\",order=None);\n",
    "for ax in grid_map.axes.flatten():\n",
    "    ax.set_xticklabels(ax.get_xticklabels(),rotation=60)\n",
    "grid_map.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd187abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = []\n",
    "for i in range(number_of_topics):\n",
    "    keywords = []\n",
    "    df_sub= topic_terms[topic_terms['topic']==i].sort_values(by=['weight'], ascending=False)\n",
    "    keywords = \", \".join(df_sub['term'])\n",
    "    i=i+1\n",
    "    topic_keywords.append(keywords)\n",
    "topic_keywords_df= pd.DataFrame(topic_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c5cede",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(doc_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf488f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_df = pd.DataFrame()\n",
    "for filename in filenames:\n",
    "    sub_topics_df=doc_topics[doc_topics['docname']==filename].sort_values(by=['proportion'], ascending=False)\n",
    "    row=sub_topics_df.iloc[0]\n",
    "    topics_df = topics_df.append(pd.Series([row['docname'],int(row['topic']), round(row['proportion'],4)]), ignore_index=True)\n",
    "topics_df.columns = ['Document name', 'Topic', 'Contribution']\n",
    "topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1218a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 3 Keywords for each Topic\n",
    "\n",
    "topic_top3words = [(i,topic) for i, topics in enumerate(topic_keywords) for (j, topic) in enumerate(topics.split(',')) if j<3]\n",
    "print(topic_top3words)\n",
    "\n",
    "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
    "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
    "df_top3words.reset_index(level=0,inplace=True)\n",
    "df_top3words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9371d79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "#fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "# Topic Distribution by Dominant Topics\n",
    "ax=sea.histplot(data=topics_df,x=topics_df.Topic)\n",
    "ax.set_xticks(range(topics_df.Topic.unique().__len__()))\n",
    "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
    "ax.xaxis.set_major_formatter(tick_formatter)\n",
    "#ax.set_xticklabels(ax.get_xticklabels(),rotation=60)\n",
    "ax.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
    "ax.set_ylabel('Number of Documents')\n",
    "ax.set_ylim(0, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d794381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us list the files available for analysis in the S3 bucket\n",
    "s3 = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name='us-east-1',\n",
    "    aws_access_key_id='AKIA3M255OQCLQRJ5W63',\n",
    "    aws_secret_access_key='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'\n",
    ")\n",
    "\n",
    "\n",
    "subfolder=''\n",
    "contents = s3.list_objects(Bucket=bucket, Prefix='')['Contents']\n",
    "number_of_docs=0\n",
    "filenames=[]\n",
    "for f in contents:\n",
    "    number_of_docs=number_of_docs+1\n",
    "    print(f['Key'])\n",
    "    filenames.append(f['Key'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30fa802",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = get_execution_role()\n",
    "print(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ce73eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieve(framework='linear-learner',region='eu-west-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09210b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "import sagemaker \n",
    "\n",
    "role = DUMMY_IAM_ROLE # sagemaker.get_execution_role()\n",
    "\n",
    "# Hub Model configuration. https://huggingface.co/models\n",
    "hub = {\n",
    "  'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model_id from hf.co/models\n",
    "  'HF_TASK':'question-answering' # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# create Hugging Face Model Class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "   env=hub,\n",
    "   role=role, # iam role with permissions to create an Endpoint\n",
    "   transformers_version=\"4.6\", # transformers version used\n",
    "   pytorch_version=\"1.7\", # pytorch version used\n",
    "   py_version=\"py36\", # python version of the DLC\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ae85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# deploy model to SageMaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "   initial_instance_count=1,\n",
    "   instance_type=\"local\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabb0a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
