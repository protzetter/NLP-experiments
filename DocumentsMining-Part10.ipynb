{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on December 28th  2021 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "## Small experiment of document mining with various techniques Part 10\n",
    "\n",
    "Let us use AWS built-in NTM algorithm for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting texthero\n",
      "  Downloading texthero-1.1.0-py3-none-any.whl (24 kB)\n",
      "Collecting unidecode>=1.1.1\n",
      "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting gensim<4.0,>=3.6.0\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.2/24.2 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: matplotlib>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.1.3)\n",
      "Collecting spacy<3.0.0\n",
      "  Downloading spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.3 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.7)\n",
      "Requirement already satisfied: pandas>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from texthero) (1.3.5)\n",
      "Collecting wordcloud>=1.5.0\n",
      "  Downloading wordcloud-1.8.2.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m435.2/435.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: plotly>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (5.8.2)\n",
      "Requirement already satisfied: tqdm>=4.3 in /opt/conda/lib/python3.7/site-packages (from texthero) (4.42.1)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.7/site-packages (from texthero) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from texthero) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.14.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (6.2.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk>=3.3->texthero) (7.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk>=3.3->texthero) (2022.8.17)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk>=3.3->texthero) (0.14.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.2->texthero) (2019.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (59.3.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.8-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (2.28.1)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.7-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.6/126.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from wordcloud>=1.5.0->texthero) (9.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.12.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.0.4)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (3.8.1)\n",
      "Installing collected packages: wasabi, srsly, plac, cymem, unidecode, murmurhash, blis, preshed, gensim, catalogue, wordcloud, thinc, spacy, texthero\n",
      "  Attempting uninstall: gensim\n",
      "    Found existing installation: gensim 4.2.0\n",
      "    Uninstalling gensim-4.2.0:\n",
      "      Successfully uninstalled gensim-4.2.0\n",
      "Successfully installed blis-0.7.8 catalogue-1.0.0 cymem-2.0.6 gensim-3.8.3 murmurhash-1.0.8 plac-1.1.3 preshed-3.0.7 spacy-2.3.7 srsly-1.0.5 texthero-1.1.0 thinc-7.4.5 unidecode-1.3.6 wasabi-0.10.1 wordcloud-1.8.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# install NLTK and gensim if required\n",
    "!pip3 -q install nltk gensim\n",
    "!pip3 install texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pkg_resources/__init__.py:119: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  PkgResourcesDeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "# Import require libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup S3 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize some parameters depending where you are running the experiment, adapt the parameters to your AWS environment\n",
    "bucket='mymltextarticles'\n",
    "subfolder=''\n",
    "region='us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sampledocs/vocab.txt\n",
      "./sampledocs/AI-bank-of-the-future-Can-banks-meet-the-AI-challenge-1.txt\n",
      "./sampledocs/Artificial Financial Intelligence.txt\n",
      "./sampledocs/Data machine the insurers using AI to reshape the industry Financial Times.txt\n",
      "./sampledocs/Digital-disruption-in-Insurance.txt\n",
      "./sampledocs/Impact-Big-Data-AI-in-the-Insurance-Sector.txt\n",
      "./sampledocs/Innovation_Artificial-Intelligence-in-Insurance-Whitepaper-deloitte-digital.txt\n",
      "./sampledocs/Insurance-2030-The-impact-of-AI-on-the-future-of-insurance-F.txt\n",
      "./sampledocs/Issues_Paper_on_Increasing_Digitalisation_in_Insurance_and_its_Potential_Impact_on_Consumer_Outcomes.txt\n",
      "./sampledocs/Kaggle State of Machine Learning and Data Science 2020.txt\n",
      "./sampledocs/Module-1-Lecture-Slides.txt\n",
      "./sampledocs/Technology-and-innovation-in-the-insurance-sector.txt\n",
      "./sampledocs/WEF_Governance_of_Chatbots_in_Healthcare_2020.txt\n",
      "./sampledocs/ai-360-research.txt\n",
      "./sampledocs/ai-insurance.txt\n",
      "./sampledocs/ai_in_insurance_web_0.txt\n",
      "./sampledocs/fra-2020-artificial-intelligence_en.txt\n",
      "./sampledocs/merged.txt\n",
      "./sampledocs/sigma-5-2020-en.txt\n",
      "./sampledocs/sigma1_2020_en.txt\n"
     ]
    }
   ],
   "source": [
    "# let us list the files available for analysis in the S3 bucket\n",
    "import os\n",
    "s3s = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#contents = s3.list_objects(Bucket=bucket, Prefix=subfolder)#['Contents']\n",
    "mybucket = s3.Bucket(bucket)\n",
    "mybucket.objects.filter(Prefix='foo/bar')\n",
    "for file in mybucket.objects.all():\n",
    "    root,ext = os.path.splitext(file.key)\n",
    "    if ext in ['.txt']:\n",
    "        filename=os.path.basename(file.key)\n",
    "        target_filename='./sampledocs/'+filename\n",
    "        print(target_filename)\n",
    "        s3s.download_file(bucket, file.key, target_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of text files\n",
    "path='./sampledocs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "docNLP=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.txt')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    f = open(file_name,'r')\n",
    "    fileText=f.read()\n",
    "    docName.append(file_name)\n",
    "    docType.append('txt')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text:158638.2\n",
      "Min length of text:8691\n",
      "Max length of text:1513210\n"
     ]
    }
   ],
   "source": [
    " print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/ai-360-research.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>ai insights next frontier business corner offi...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>application ai insurtech real estate technolog...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/Insurance-2030-The-impact-of-AI-o...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Insurance Practice\\n\\nInsurance 2030—\\nThe imp...</td>\n",
       "      <td>insurance practice insurance -- impact ai futu...</td>\n",
       "      <td>4424</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>machine intelligence insurance insights end en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/Technology-and-innovation-in-the-...  txt   \n",
       "1                   ./sampledocs/ai-360-research.txt  txt   \n",
       "2           ./sampledocs/Module-1-Lecture-Slides.txt  txt   \n",
       "3  ./sampledocs/Insurance-2030-The-impact-of-AI-o...  txt   \n",
       "4                   ./sampledocs/sigma-5-2020-en.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Insurance Practice\\n\\nInsurance 2030—\\nThe imp...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  technology innovation insurance sector technol...            16742   \n",
       "1  ai insights next frontier business corner offi...             5281   \n",
       "2  application ai insurtech real estate technolog...             3728   \n",
       "3  insurance practice insurance -- impact ai futu...             4424   \n",
       "4  machine intelligence insurance insights end en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               4228  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               1782  \n",
       "4               4329  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Name               20 non-null     object\n",
      " 1   Type               20 non-null     object\n",
      " 2   Text               20 non-null     object\n",
      " 3   cleanText          20 non-null     object\n",
      " 4   text_word_count    20 non-null     int64 \n",
      " 5   text_unique_words  20 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    text = text.lower().split()\n",
    "    text = [w for w in text if not w in stop_words] \n",
    "    text = [wnl.lemmatize(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['cleanText'] = fullDocs['cleanText'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/ai-360-research.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>ai insights next frontier business corner offi...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>application ai insurtech real estate technolog...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/Insurance-2030-The-impact-of-AI-o...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Insurance Practice\\n\\nInsurance 2030—\\nThe imp...</td>\n",
       "      <td>insurance practice insurance -- impact ai futu...</td>\n",
       "      <td>4424</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>machine intelligence insurance insights end en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/Technology-and-innovation-in-the-...  txt   \n",
       "1                   ./sampledocs/ai-360-research.txt  txt   \n",
       "2           ./sampledocs/Module-1-Lecture-Slides.txt  txt   \n",
       "3  ./sampledocs/Insurance-2030-The-impact-of-AI-o...  txt   \n",
       "4                   ./sampledocs/sigma-5-2020-en.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Insurance Practice\\n\\nInsurance 2030—\\nThe imp...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  technology innovation insurance sector technol...            16742   \n",
       "1  ai insights next frontier business corner offi...             5281   \n",
       "2  application ai insurtech real estate technolog...             3728   \n",
       "3  insurance practice insurance -- impact ai futu...             4424   \n",
       "4  machine intelligence insurance insights end en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               4228  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               1782  \n",
       "4               4329  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(fullDocs['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(11382 unique tokens: ['ab', 'ability', 'able', 'abundantly', 'abusive']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1024 unique tokens: ['accelerate', 'accelerating', 'acceleration', 'acceptance', 'accepted']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(keep_n=1024)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for index in range(0,len(dictionary)):\n",
    "        f.write(dictionary.get(index)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fullDocs['tokens'] = fullDocs.apply(lambda row: dictionary.doc2bow(row['cleanText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(3, 2), (5, 1), (10, 1), (13, 1), (21, 1), (2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(8, 1), (11, 1), (15, 1), (16, 3), (22, 2), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(0, 1), (1, 2), (7, 1), (9, 4), (11, 1), (12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(0, 2), (5, 1), (6, 1), (9, 1), (10, 1), (11,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens\n",
       "0  [(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...\n",
       "1  [(3, 2), (5, 1), (10, 1), (13, 1), (21, 1), (2...\n",
       "2  [(8, 1), (11, 1), (15, 1), (16, 3), (22, 2), (...\n",
       "3  [(0, 1), (1, 2), (7, 1), (9, 4), (11, 1), (12,...\n",
       "4  [(0, 2), (5, 1), (6, 1), (9, 1), (10, 1), (11,..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fullDocs.drop(['cleanText'], axis=1)\n",
    "data = data.drop(['Name'], axis=1)\n",
    "data = data.drop(['Type'], axis=1)\n",
    "data = data.drop(['Text'], axis=1)\n",
    "data = data.drop(['text_word_count'], axis=1)\n",
    "data = data.drop(['text_unique_words'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.amazon.common as smac\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "prefix = 'training'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protobuf_dataset(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    token_matrix = lil_matrix((num_lines, num_columns)).astype('float32')\n",
    "    line = 0\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            token_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_protbuf_dataset(buf, bucket, prefix, key):\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    buf.seek(0)\n",
    "    s3s.upload_fileobj(buf, bucket, obj)\n",
    "    path = 's3://{}/{}'.format(bucket,obj)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/training/training.protobuf\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "training_buf = build_protobuf_dataset(data, dictionary)\n",
    "s3_training_path = upload_protbuf_dataset(training_buf, bucket, prefix, 'training/training.protobuf')\n",
    "s3_auxiliary_path='Inputfiles/auxiliary/'+'vocab.txt'\n",
    "s3s.upload_file('vocab.txt', bucket,'Inputfiles/auxiliary/'+'vocab.txt')\n",
    "\n",
    "print(s3_training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/output/\n"
     ]
    }
   ],
   "source": [
    "s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382416733822.dkr.ecr.us-east-1.amazonaws.com/ntm:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = 'us-east-1'  \n",
    "container = retrieve('ntm', region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::012086180905:role/service-role/AmazonSageMaker-ExecutionRole-20211121T093897\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)\n",
    "import sagemaker\n",
    "ntm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.2xlarge',\n",
    "    output_path=s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.set_hyperparameters(\n",
    "    num_topics=10, \n",
    "    feature_dim=len(dictionary), \n",
    "    mini_batch_size=256,\n",
    "    optimizer='adam',\n",
    "    num_patience_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-02 07:06:16 Starting - Starting the training job...\n",
      "2022-10-02 07:06:40 Starting - Preparing the instances for trainingProfilerReport-1664694375: InProgress\n",
      "......\n",
      "2022-10-02 07:07:44 Downloading - Downloading input data...\n",
      "2022-10-02 07:08:04 Training - Downloading the training image..............\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '1024', 'mini_batch_size': '256', 'num_patience_epochs': '10', 'num_topics': '10', 'optimizer': 'adam'}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adam', 'tolerance': '0.001', 'num_patience_epochs': '10', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '1024', 'num_topics': '10'}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] nvidia-smi: took 0.031 seconds to run.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] Using default worker.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] Initializing\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] vocab.txt\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:30 INFO 140392443905856] Loading pre-trained token embedding vectors from /opt/amazon/lib/python3.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\n",
      "2022-10-02 07:10:45 Uploading - Uploading generated training model\u001b[34m[10/02/2022 07:10:39 WARNING 140392443905856] 8 out of 1024 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Vocab embedding shape: (1024, 50)\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.2296119, \"EndTime\": 1664694639.2296386, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.229] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 8729, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.321] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 90, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 1 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5418505668640137\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 2.8756887331837788e-05\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5418217778205872\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5418505668640137\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=0.5418505668640137\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.10s, val: 0.00s, epoch: 0.10s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.2299333, \"EndTime\": 1664694639.3302202, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=199.04915123127228 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.344] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 2 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5402754545211792\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 3.469527291599661e-05\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5402407646179199\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5402754545211792\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=0.5402754545211792\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.330587, \"EndTime\": 1664694639.3497636, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Total Batches Seen\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1034.1496129000443 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.360] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 3 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5393866896629333\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 5.086118108010851e-05\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5393357872962952\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5393866896629333\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=0.5393866896629333\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.01s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.3500724, \"EndTime\": 1664694639.3675392, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Total Batches Seen\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1133.886808775226 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.378] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 4 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5383170247077942\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 6.450529326684773e-05\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5382524728775024\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5383170247077942\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=0.5383170247077942\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.3678436, \"EndTime\": 1664694639.382942, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Total Batches Seen\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1316.2523732563432 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.393] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 5 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5373157262802124\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 7.166105933720246e-05\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5372440814971924\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5373157262802124\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=0.5373157262802124\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.01s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.3831184, \"EndTime\": 1664694639.3992074, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Total Batches Seen\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1233.9273053557506 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.409] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 6 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5347923040390015\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 8.842299575917423e-05\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5347039103507996\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5347923040390015\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=0.5347923040390015\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.3994813, \"EndTime\": 1664694639.4148364, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 120.0, \"count\": 1, \"min\": 120, \"max\": 120}, \"Total Batches Seen\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1291.2702419801735 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.425] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 7 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5312498807907104\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.00012927697389386594\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5311205983161926\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5312498807907104\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=0.5312498807907104\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.01s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.4151037, \"EndTime\": 1664694639.4322832, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 140.0, \"count\": 1, \"min\": 140, \"max\": 140}, \"Total Batches Seen\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1153.5330922292048 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.443] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 8 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.531370222568512\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0002047326706815511\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5311654806137085\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.531370222568512\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=0.531370222568512\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.4326043, \"EndTime\": 1664694639.4442575, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 160.0, \"count\": 1, \"min\": 160, \"max\": 160}, \"Total Batches Seen\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1695.0107092341887 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.464] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 19, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 9 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5300067067146301\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0003272420435678214\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5296794772148132\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5300067067146301\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=0.5300067067146301\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.02s, val: 0.00s, epoch: 0.03s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.4445565, \"EndTime\": 1664694639.4699292, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 180.0, \"count\": 1, \"min\": 180, \"max\": 180}, \"Total Batches Seen\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=783.5646431340314 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.479] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 8, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 10 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5281423926353455\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0004595471837092191\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5276828408241272\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5281423926353455\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=0.5281423926353455\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.4702463, \"EndTime\": 1664694639.4837427, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 200.0, \"count\": 1, \"min\": 200, \"max\": 200}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1465.5918374478047 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.494] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 11 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5270696878433228\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0005910681211389601\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5264785885810852\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5270696878433228\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=0.5270696878433228\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5418505668640137, 0.5402754545211792, 0.5393866896629333, 0.5383170247077942, 0.5373157262802124, 0.5347923040390015, 0.5312498807907104, 0.531370222568512, 0.5300067067146301, 0.5281423926353455] min patience loss:0.5281423926353455 current loss:0.5270696878433228 absolute loss difference:0.001072704792022705\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.48404, \"EndTime\": 1664694639.499229, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Total Batches Seen\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1305.5996015626215 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.509] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 12 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5277960896492004\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0007159890374168754\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5270801186561584\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5277960896492004\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=0.5277960896492004\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5402754545211792, 0.5393866896629333, 0.5383170247077942, 0.5373157262802124, 0.5347923040390015, 0.5312498807907104, 0.531370222568512, 0.5300067067146301, 0.5281423926353455, 0.5270696878433228] min patience loss:0.5270696878433228 current loss:0.5277960896492004 absolute loss difference:0.0007264018058776855\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.499503, \"EndTime\": 1664694639.510617, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 240.0, \"count\": 1, \"min\": 240, \"max\": 240}, \"Total Batches Seen\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1781.9666489644185 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.520] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 13 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5282295942306519\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0007853853749111295\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5274442434310913\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5282295942306519\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=0.5282295942306519\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5393866896629333, 0.5383170247077942, 0.5373157262802124, 0.5347923040390015, 0.5312498807907104, 0.531370222568512, 0.5300067067146301, 0.5281423926353455, 0.5270696878433228, 0.5277960896492004] min patience loss:0.5270696878433228 current loss:0.5282295942306519 absolute loss difference:0.0011599063873291016\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.5108433, \"EndTime\": 1664694639.5218897, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 260.0, \"count\": 1, \"min\": 260, \"max\": 260}, \"Total Batches Seen\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1791.863291680017 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.532] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 14 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5255881547927856\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0007609896711073816\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5248271226882935\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5255881547927856\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=0.5255881547927856\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5383170247077942, 0.5373157262802124, 0.5347923040390015, 0.5312498807907104, 0.531370222568512, 0.5300067067146301, 0.5281423926353455, 0.5270696878433228, 0.5277960896492004, 0.5282295942306519] min patience loss:0.5270696878433228 current loss:0.5255881547927856 absolute loss difference:0.0014815330505371094\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.522121, \"EndTime\": 1664694639.5371382, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 280.0, \"count\": 1, \"min\": 280, \"max\": 280}, \"Total Batches Seen\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1320.0012588512982 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.547] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 15 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5264938473701477\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0007382534095086157\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5257555842399597\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5264938473701477\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=0.5264938473701477\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5373157262802124, 0.5347923040390015, 0.5312498807907104, 0.531370222568512, 0.5300067067146301, 0.5281423926353455, 0.5270696878433228, 0.5277960896492004, 0.5282295942306519, 0.5255881547927856] min patience loss:0.5255881547927856 current loss:0.5264938473701477 absolute loss difference:0.0009056925773620605\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.5374029, \"EndTime\": 1664694639.5480435, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 300.0, \"count\": 1, \"min\": 300, \"max\": 300}, \"Total Batches Seen\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1860.0430164748664 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.558] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 16 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5239037871360779\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0007124538533389568\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5231913328170776\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5239037871360779\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=0.5239037871360779\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5347923040390015, 0.5312498807907104, 0.531370222568512, 0.5300067067146301, 0.5281423926353455, 0.5270696878433228, 0.5277960896492004, 0.5282295942306519, 0.5255881547927856, 0.5264938473701477] min patience loss:0.5255881547927856 current loss:0.5239037871360779 absolute loss difference:0.0016843676567077637\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.5482478, \"EndTime\": 1664694639.5640924, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 320.0, \"count\": 1, \"min\": 320, \"max\": 320}, \"Total Batches Seen\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1253.7338773558117 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.574] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 17 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5238827466964722\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0006802297430112958\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5232024788856506\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5238827466964722\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=0.5238827466964722\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5312498807907104, 0.531370222568512, 0.5300067067146301, 0.5281423926353455, 0.5270696878433228, 0.5277960896492004, 0.5282295942306519, 0.5255881547927856, 0.5264938473701477, 0.5239037871360779] min patience loss:0.5239037871360779 current loss:0.5238827466964722 absolute loss difference:2.104043960571289e-05\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.5642948, \"EndTime\": 1664694639.5793319, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 340.0, \"count\": 1, \"min\": 340, \"max\": 340}, \"Total Batches Seen\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1320.7494410681109 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.588] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 8, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 18 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5246065258979797\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0006576132727786899\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5239489078521729\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5246065258979797\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=0.5246065258979797\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.531370222568512, 0.5300067067146301, 0.5281423926353455, 0.5270696878433228, 0.5277960896492004, 0.5282295942306519, 0.5255881547927856, 0.5264938473701477, 0.5239037871360779, 0.5238827466964722] min patience loss:0.5238827466964722 current loss:0.5246065258979797 absolute loss difference:0.0007237792015075684\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.5795696, \"EndTime\": 1664694639.589481, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 360.0, \"count\": 1, \"min\": 360, \"max\": 360}, \"Total Batches Seen\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1996.7171284394935 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.599] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 19 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5223674774169922\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0006418623961508274\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.521725594997406\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5223674774169922\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=0.5223674774169922\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5300067067146301, 0.5281423926353455, 0.5270696878433228, 0.5277960896492004, 0.5282295942306519, 0.5255881547927856, 0.5264938473701477, 0.5239037871360779, 0.5238827466964722, 0.5246065258979797] min patience loss:0.5238827466964722 current loss:0.5223674774169922 absolute loss difference:0.0015152692794799805\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.589696, \"EndTime\": 1664694639.6053662, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 380.0, \"count\": 1, \"min\": 380, \"max\": 380}, \"Total Batches Seen\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1268.5601947767175 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.615] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 20 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5232678055763245\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0006508385995402932\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.522616982460022\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5232678055763245\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=0.5232678055763245\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5281423926353455, 0.5270696878433228, 0.5277960896492004, 0.5282295942306519, 0.5255881547927856, 0.5264938473701477, 0.5239037871360779, 0.5238827466964722, 0.5246065258979797, 0.5223674774169922] min patience loss:0.5223674774169922 current loss:0.5232678055763245 absolute loss difference:0.0009003281593322754\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.6055605, \"EndTime\": 1664694639.6170797, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"Total Batches Seen\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1720.5636344990257 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.627] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 21 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5223882794380188\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0006888671196065843\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5216994285583496\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5223882794380188\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=0.5223882794380188\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5270696878433228, 0.5277960896492004, 0.5282295942306519, 0.5255881547927856, 0.5264938473701477, 0.5239037871360779, 0.5238827466964722, 0.5246065258979797, 0.5223674774169922, 0.5232678055763245] min patience loss:0.5223674774169922 current loss:0.5223882794380188 absolute loss difference:2.0802021026611328e-05\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.6173248, \"EndTime\": 1664694639.628538, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 420.0, \"count\": 1, \"min\": 420, \"max\": 420}, \"Total Batches Seen\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1761.4244918528473 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.638] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 22 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5230469107627869\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0007365074125118554\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5223104357719421\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5230469107627869\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=0.5230469107627869\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5277960896492004, 0.5282295942306519, 0.5255881547927856, 0.5264938473701477, 0.5239037871360779, 0.5238827466964722, 0.5246065258979797, 0.5223674774169922, 0.5232678055763245, 0.5223882794380188] min patience loss:0.5223674774169922 current loss:0.5230469107627869 absolute loss difference:0.0006794333457946777\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.6288097, \"EndTime\": 1664694639.6401124, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 440.0, \"count\": 1, \"min\": 440, \"max\": 440}, \"Total Batches Seen\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1752.3726759974932 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.650] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 23 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5239227414131165\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0007804982597008348\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5231422185897827\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5239227414131165\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=0.5239227414131165\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5282295942306519, 0.5255881547927856, 0.5264938473701477, 0.5239037871360779, 0.5238827466964722, 0.5246065258979797, 0.5223674774169922, 0.5232678055763245, 0.5223882794380188, 0.5230469107627869] min patience loss:0.5223674774169922 current loss:0.5239227414131165 absolute loss difference:0.0015552639961242676\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.6403537, \"EndTime\": 1664694639.6513982, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 460.0, \"count\": 1, \"min\": 460, \"max\": 460}, \"Total Batches Seen\": {\"sum\": 23.0, \"count\": 1, \"min\": 23, \"max\": 23}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1792.9739665711966 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.662] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 24 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.522839367389679\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0008407519198954105\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5219986438751221\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.522839367389679\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=0.522839367389679\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5255881547927856, 0.5264938473701477, 0.5239037871360779, 0.5238827466964722, 0.5246065258979797, 0.5223674774169922, 0.5232678055763245, 0.5223882794380188, 0.5230469107627869, 0.5239227414131165] min patience loss:0.5223674774169922 current loss:0.522839367389679 absolute loss difference:0.0004718899726867676\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.6516604, \"EndTime\": 1664694639.6646254, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 480.0, \"count\": 1, \"min\": 480, \"max\": 480}, \"Total Batches Seen\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1515.7213067360508 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.677] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 25 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.523128092288971\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0009094455162994564\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.522218644618988\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.523128092288971\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=0.523128092288971\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5264938473701477, 0.5239037871360779, 0.5238827466964722, 0.5246065258979797, 0.5223674774169922, 0.5232678055763245, 0.5223882794380188, 0.5230469107627869, 0.5239227414131165, 0.522839367389679] min patience loss:0.5223674774169922 current loss:0.523128092288971 absolute loss difference:0.0007606148719787598\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.6650376, \"EndTime\": 1664694639.679097, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 500.0, \"count\": 1, \"min\": 500, \"max\": 500}, \"Total Batches Seen\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1406.6585059109584 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.688] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 26 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5203132033348083\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0009897826239466667\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5193234086036682\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5203132033348083\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=0.5203132033348083\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5239037871360779, 0.5238827466964722, 0.5246065258979797, 0.5223674774169922, 0.5232678055763245, 0.5223882794380188, 0.5230469107627869, 0.5239227414131165, 0.522839367389679, 0.523128092288971] min patience loss:0.5223674774169922 current loss:0.5203132033348083 absolute loss difference:0.002054274082183838\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.679384, \"EndTime\": 1664694639.6941144, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 520.0, \"count\": 1, \"min\": 520, \"max\": 520}, \"Total Batches Seen\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1347.307828209823 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.704] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 27 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5237722396850586\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0010734755778685212\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5226988196372986\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5237722396850586\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=0.5237722396850586\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5238827466964722, 0.5246065258979797, 0.5223674774169922, 0.5232678055763245, 0.5223882794380188, 0.5230469107627869, 0.5239227414131165, 0.522839367389679, 0.523128092288971, 0.5203132033348083] min patience loss:0.5203132033348083 current loss:0.5237722396850586 absolute loss difference:0.003459036350250244\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.6943166, \"EndTime\": 1664694639.7055562, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 540.0, \"count\": 1, \"min\": 540, \"max\": 540}, \"Total Batches Seen\": {\"sum\": 27.0, \"count\": 1, \"min\": 27, \"max\": 27}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1758.6917690469202 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.715] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 28 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5222266316413879\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0011582581792026758\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5210683345794678\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5222266316413879\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=0.5222266316413879\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5246065258979797, 0.5223674774169922, 0.5232678055763245, 0.5223882794380188, 0.5230469107627869, 0.5239227414131165, 0.522839367389679, 0.523128092288971, 0.5203132033348083, 0.5237722396850586] min patience loss:0.5203132033348083 current loss:0.5222266316413879 absolute loss difference:0.0019134283065795898\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.7058444, \"EndTime\": 1664694639.717026, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 560.0, \"count\": 1, \"min\": 560, \"max\": 560}, \"Total Batches Seen\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1769.5245327595662 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.727] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 29 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.521943211555481\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0012334765633568168\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.520709753036499\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.521943211555481\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=0.521943211555481\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5223674774169922, 0.5232678055763245, 0.5223882794380188, 0.5230469107627869, 0.5239227414131165, 0.522839367389679, 0.523128092288971, 0.5203132033348083, 0.5237722396850586, 0.5222266316413879] min patience loss:0.5203132033348083 current loss:0.521943211555481 absolute loss difference:0.0016300082206726074\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.717259, \"EndTime\": 1664694639.7289355, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 580.0, \"count\": 1, \"min\": 580, \"max\": 580}, \"Total Batches Seen\": {\"sum\": 29.0, \"count\": 1, \"min\": 29, \"max\": 29}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1696.6906009182662 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.738] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 30 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5213876962661743\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0013004846405237913\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5200872421264648\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5213876962661743\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=0.5213876962661743\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5232678055763245, 0.5223882794380188, 0.5230469107627869, 0.5239227414131165, 0.522839367389679, 0.523128092288971, 0.5203132033348083, 0.5237722396850586, 0.5222266316413879, 0.521943211555481] min patience loss:0.5203132033348083 current loss:0.5213876962661743 absolute loss difference:0.0010744929313659668\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.7291584, \"EndTime\": 1664694639.7396135, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 600.0, \"count\": 1, \"min\": 600, \"max\": 600}, \"Total Batches Seen\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1892.693756909817 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.749] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 31 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5196858048439026\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0013701579300686717\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5183156728744507\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5196858048439026\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=0.5196858048439026\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5223882794380188, 0.5230469107627869, 0.5239227414131165, 0.522839367389679, 0.523128092288971, 0.5203132033348083, 0.5237722396850586, 0.5222266316413879, 0.521943211555481, 0.5213876962661743] min patience loss:0.5203132033348083 current loss:0.5196858048439026 absolute loss difference:0.0006273984909057617\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.7398357, \"EndTime\": 1664694639.7547863, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 620.0, \"count\": 1, \"min\": 620, \"max\": 620}, \"Total Batches Seen\": {\"sum\": 31.0, \"count\": 1, \"min\": 31, \"max\": 31}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1327.3323944999129 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.765] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 32 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5203210711479187\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0014168999623507261\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5189041495323181\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5203210711479187\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=0.5203210711479187\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5230469107627869, 0.5239227414131165, 0.522839367389679, 0.523128092288971, 0.5203132033348083, 0.5237722396850586, 0.5222266316413879, 0.521943211555481, 0.5213876962661743, 0.5196858048439026] min patience loss:0.5196858048439026 current loss:0.5203210711479187 absolute loss difference:0.0006352663040161133\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.755043, \"EndTime\": 1664694639.7670236, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 640.0, \"count\": 1, \"min\": 640, \"max\": 640}, \"Total Batches Seen\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1652.5043831136852 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.777] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 33 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5201999545097351\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0014434768818318844\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5187564492225647\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5201999545097351\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=0.5201999545097351\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5239227414131165, 0.522839367389679, 0.523128092288971, 0.5203132033348083, 0.5237722396850586, 0.5222266316413879, 0.521943211555481, 0.5213876962661743, 0.5196858048439026, 0.5203210711479187] min patience loss:0.5196858048439026 current loss:0.5201999545097351 absolute loss difference:0.0005141496658325195\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:7\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.7673109, \"EndTime\": 1664694639.7782226, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 660.0, \"count\": 1, \"min\": 660, \"max\": 660}, \"Total Batches Seen\": {\"sum\": 33.0, \"count\": 1, \"min\": 33, \"max\": 33}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1816.7384242214232 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.788] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 34 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5199106931686401\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0014685976784676313\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5184420347213745\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5199106931686401\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=0.5199106931686401\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.522839367389679, 0.523128092288971, 0.5203132033348083, 0.5237722396850586, 0.5222266316413879, 0.521943211555481, 0.5213876962661743, 0.5196858048439026, 0.5203210711479187, 0.5201999545097351] min patience loss:0.5196858048439026 current loss:0.5199106931686401 absolute loss difference:0.00022488832473754883\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:8\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.7784193, \"EndTime\": 1664694639.7899437, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 680.0, \"count\": 1, \"min\": 680, \"max\": 680}, \"Total Batches Seen\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1717.2527585006858 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.800] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 35 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5198279023170471\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0014640591107308865\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5183638334274292\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5198279023170471\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=0.5198279023170471\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.523128092288971, 0.5203132033348083, 0.5237722396850586, 0.5222266316413879, 0.521943211555481, 0.5213876962661743, 0.5196858048439026, 0.5203210711479187, 0.5201999545097351, 0.5199106931686401] min patience loss:0.5196858048439026 current loss:0.5198279023170471 absolute loss difference:0.00014209747314453125\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:9\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.7902226, \"EndTime\": 1664694639.8018363, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 700.0, \"count\": 1, \"min\": 700, \"max\": 700}, \"Total Batches Seen\": {\"sum\": 35.0, \"count\": 1, \"min\": 35, \"max\": 35}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1703.4090078382 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.814] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 36 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.519122838973999\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0014427053974941373\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.5176801681518555\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.519122838973999\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=0.519122838973999\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5203132033348083, 0.5237722396850586, 0.5222266316413879, 0.521943211555481, 0.5213876962661743, 0.5196858048439026, 0.5203210711479187, 0.5201999545097351, 0.5199106931686401, 0.5198279023170471] min patience loss:0.5196858048439026 current loss:0.519122838973999 absolute loss difference:0.0005629658699035645\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:10\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.8021061, \"EndTime\": 1664694639.8194618, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 720.0, \"count\": 1, \"min\": 720, \"max\": 720}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1143.9842897665285 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] \u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[34m[2022-10-02 07:10:39.829] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 9, \"num_examples\": 1, \"num_bytes\": 54768}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] # Finished training epoch 37 on 20 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) total: 0.5196706652641296\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) kld: 0.0013964491663500667\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) recons: 0.518274188041687\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Loss (name: value) logppx: 0.5196706652641296\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=0.5196706652641296\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] patience losses:[0.5237722396850586, 0.5222266316413879, 0.521943211555481, 0.5213876962661743, 0.5196858048439026, 0.5203210711479187, 0.5201999545097351, 0.5199106931686401, 0.5198279023170471, 0.519122838973999] min patience loss:0.519122838973999 current loss:0.5196706652641296 absolute loss difference:0.0005478262901306152\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epoch: loss has not improved (enough). Bad count:11\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Bad epochs exceeded patience. Stopping training early!\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Early stop condition met. Stopping training.\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #progress_metric: host=algo-1, completed 100 % epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694639.81972, \"EndTime\": 1664694639.8313072, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 740.0, \"count\": 1, \"min\": 740, \"max\": 740}, \"Total Batches Seen\": {\"sum\": 37.0, \"count\": 1, \"min\": 37, \"max\": 37}, \"Max Records Seen Between Resets\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] #throughput_metric: host=algo-1, train throughput=1707.500407099821 records/second\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 WARNING 140392443905856] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Best model based on early stopping at epoch 36. Best loss: 0.519122838973999\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Topics from epoch:final (num_topics:10) [wetc 0.25, tu 0.43]:\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.22, 0.62] q strongly say digitalisation conducted guideline robot crucial discrimination eliminate bot org chatbot carrier loan australia embedded refer authority doubt\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.27, 0.41] l n f q none robo fairness say popular employment half usd k funding al spend behaviour patient doctor programming\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.24, 0.41] say series eu carrier n european education accessed conversational q china deploy l quarter rising classification funding respondent behaviour broker\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.21, 0.47] exhibit shift say carrier near n picture engagement behaviour fairness china spot stack swiss l guideline lemonade doctor j insure\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.32, 0.47] say n l q comfortable revenue swiss education behaviour behavior talent willing facing respondent shift think cultural exhibit popular programming\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.17, 0.37] say employment chatbot respondent none programming n guideline robo digitalisation education popular carrier bot engagement administration unlocking fairness org comfortable\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.24, 0.42] q l guideline swiss say n carrier al china digitalisation behaviour november committee supervisory fairness note root shift supervisor patient\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.32, 0.55] n eu q renewal series try strongly shift quarter sign detecting think yes picture say researcher behaviour broker bought patient\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.25, 0.28] say q n l patient employment swiss education comfortable european discrimination digitalisation authority f chatbot j deploy guideline shift carrier\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] [0.26, 0.30] q l n series carrier root education behaviour k say eu f respondent none discrimination swiss popular never classification chatbot\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Saved checkpoint to \"/tmp/tmpoi5xztpw/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[10/02/2022 07:10:39 INFO 140392443905856] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1664694630.500298, \"EndTime\": 1664694639.889708, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 8724.459886550903, \"count\": 1, \"min\": 8724.459886550903, \"max\": 8724.459886550903}, \"epochs\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"early_stop.time\": {\"sum\": 75.93536376953125, \"count\": 37, \"min\": 0.023365020751953125, \"max\": 5.703926086425781}, \"update.time\": {\"sum\": 585.6361389160156, \"count\": 37, \"min\": 9.768486022949219, \"max\": 100.05617141723633}, \"finalize.time\": {\"sum\": 50.17709732055664, \"count\": 1, \"min\": 50.17709732055664, \"max\": 50.17709732055664}, \"model.serialize.time\": {\"sum\": 6.197690963745117, \"count\": 1, \"min\": 6.197690963745117, \"max\": 6.197690963745117}, \"setuptime\": {\"sum\": 42.101383209228516, \"count\": 1, \"min\": 42.101383209228516, \"max\": 42.101383209228516}, \"totaltime\": {\"sum\": 9464.860200881958, \"count\": 1, \"min\": 9464.860200881958, \"max\": 9464.860200881958}}}\u001b[0m\n",
      "\n",
      "2022-10-02 07:11:00 Completed - Training job completed\n",
      "Training seconds: 193\n",
      "Billable seconds: 193\n"
     ]
    }
   ],
   "source": [
    "ntm.fit(inputs={'train': s3_training_path,\n",
    "               'auxiliary': 's3://{}/{}'.format(bucket,s3_auxiliary_path)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/output/ntm-2022-10-02-07-06-15-895/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(ntm.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model\n",
    "This will only work if the training completed successfully and the model is saved under the assumed location, you can change it if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "s3_output='s3://mymltextarticles/training/output/ntm-2022-01-07-12-25-30-277/output/model.tar.gz'\n",
    "role = get_execution_role()\n",
    "ntm=sagemaker.NTMModel(s3_output, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prediction using training data\n",
    "This assumes that a data frame and a dictionary have been prepared using the cells above for data preparation\n",
    "It also assumes the model has been loaded and the endpoint has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    line=0\n",
    "    sample_matrix = np.zeros((num_lines, num_columns)).astype('float32')\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            sample_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "\n",
    "    return sample_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1024)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples= prepare_samples(data, dictionary)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.0936905593, 0.1389863193, 0.0947074741, 0.1033818424, 0.0892445073, 0.0907449126, 0.1050878465, 0.1209227592, 0.0732400119, 0.0899937451]}, {'topic_weights': [0.0942306966, 0.1395254582, 0.093277961, 0.1066259518, 0.0872382671, 0.0872918814, 0.1035153791, 0.127722621, 0.0706498623, 0.0899219513]}, {'topic_weights': [0.093900241, 0.1317909658, 0.0963539556, 0.1017725244, 0.0913976654, 0.0924328268, 0.1046868265, 0.1172470152, 0.0786318853, 0.091786094]}, {'topic_weights': [0.0940068364, 0.1450598687, 0.0934277996, 0.1054858267, 0.0861590505, 0.0875128135, 0.1048642397, 0.1266106665, 0.0677180365, 0.0891548917]}, {'topic_weights': [0.0937443897, 0.1378456056, 0.0950068235, 0.1030102, 0.0895280764, 0.0911481082, 0.1050277427, 0.1202597842, 0.0741740167, 0.0902552828]}, {'topic_weights': [0.0936863124, 0.1390734762, 0.0946850851, 0.1034097522, 0.0892229378, 0.090714395, 0.1050926, 0.1209723204, 0.0731692314, 0.0899739489]}, {'topic_weights': [0.0939848199, 0.1315651536, 0.0965988338, 0.1010027826, 0.0911360532, 0.0933054313, 0.1046159789, 0.1166041195, 0.0795118585, 0.0916748866]}, {'topic_weights': [0.0939243361, 0.1461472213, 0.0916947275, 0.1082606465, 0.0857118219, 0.085280247, 0.1040154845, 0.1306664497, 0.0657837242, 0.0885153636]}, {'topic_weights': [0.0940074027, 0.1440491676, 0.0936924964, 0.1050821766, 0.0865867063, 0.0880182087, 0.1048052609, 0.1257834136, 0.0686343387, 0.0893407762]}, {'topic_weights': [0.0936872959, 0.1390536875, 0.0946904197, 0.1034032032, 0.0892276838, 0.0907213986, 0.1050916836, 0.1209608465, 0.0731852576, 0.0899785683]}, {'topic_weights': [0.0937080532, 0.1386536509, 0.0947964564, 0.1032728776, 0.0893233418, 0.0908608511, 0.1050698534, 0.1207332909, 0.0735099167, 0.0900716558]}, {'topic_weights': [0.0937898234, 0.1408807635, 0.0943230018, 0.104012467, 0.0883116722, 0.0897750109, 0.1050187275, 0.1226325408, 0.0715242177, 0.0897317529]}, {'topic_weights': [0.0936867893, 0.1390818059, 0.0946834311, 0.1034125164, 0.0892187431, 0.0907101035, 0.1050922945, 0.1209798977, 0.073161602, 0.0899728313]}, {'topic_weights': [0.0939547643, 0.1335726231, 0.0959680155, 0.1018009037, 0.0906294212, 0.0925262347, 0.1047212332, 0.1179491431, 0.0776918158, 0.0911857486]}, {'topic_weights': [0.093686305, 0.1390733719, 0.0946850926, 0.103409715, 0.0892229453, 0.0907144174, 0.1050925851, 0.1209722757, 0.073169291, 0.0899739563]}, {'topic_weights': [0.0940567926, 0.1379723847, 0.0951495618, 0.1030817032, 0.0887585059, 0.0906228647, 0.1047778279, 0.1213246062, 0.0737530589, 0.0905027911]}, {'topic_weights': [0.0939998999, 0.1271010935, 0.097440064, 0.1000882611, 0.0928507075, 0.0943280607, 0.104214482, 0.1141287088, 0.0831805542, 0.0926680788]}, {'topic_weights': [0.0941229761, 0.1393713206, 0.0944880843, 0.1042495519, 0.0882353783, 0.0893992037, 0.1045266688, 0.1232367158, 0.072137773, 0.090232335]}, {'topic_weights': [0.0937474966, 0.1523562968, 0.090323709, 0.1088945493, 0.0841194317, 0.0843541995, 0.1043811962, 0.1326250881, 0.0622567274, 0.0869412646]}, {'topic_weights': [0.0936863124, 0.1390734613, 0.0946850777, 0.1034097373, 0.0892229304, 0.090714395, 0.1050925925, 0.1209723204, 0.0731692314, 0.0899739489]}]}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "#ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer\n",
    "\n",
    "\n",
    "ntm_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "response = ntm_predictor.predict(samples)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09369056 0.13898632 0.09470747 0.10338184 0.08924451 0.09074491\n",
      "  0.10508785 0.12092276 0.07324001 0.08999375]\n",
      " [0.0942307  0.13952546 0.09327796 0.10662595 0.08723827 0.08729188\n",
      "  0.10351538 0.12772262 0.07064986 0.08992195]\n",
      " [0.09390024 0.13179097 0.09635396 0.10177252 0.09139767 0.09243283\n",
      "  0.10468683 0.11724702 0.07863189 0.09178609]\n",
      " [0.09400684 0.14505987 0.0934278  0.10548583 0.08615905 0.08751281\n",
      "  0.10486424 0.12661067 0.06771804 0.08915489]\n",
      " [0.09374439 0.13784561 0.09500682 0.1030102  0.08952808 0.09114811\n",
      "  0.10502774 0.12025978 0.07417402 0.09025528]\n",
      " [0.09368631 0.13907348 0.09468509 0.10340975 0.08922294 0.0907144\n",
      "  0.1050926  0.12097232 0.07316923 0.08997395]\n",
      " [0.09398482 0.13156515 0.09659883 0.10100278 0.09113605 0.09330543\n",
      "  0.10461598 0.11660412 0.07951186 0.09167489]\n",
      " [0.09392434 0.14614722 0.09169473 0.10826065 0.08571182 0.08528025\n",
      "  0.10401548 0.13066645 0.06578372 0.08851536]\n",
      " [0.0940074  0.14404917 0.0936925  0.10508218 0.08658671 0.08801821\n",
      "  0.10480526 0.12578341 0.06863434 0.08934078]\n",
      " [0.0936873  0.13905369 0.09469042 0.1034032  0.08922768 0.0907214\n",
      "  0.10509168 0.12096085 0.07318526 0.08997857]\n",
      " [0.09370805 0.13865365 0.09479646 0.10327288 0.08932334 0.09086085\n",
      "  0.10506985 0.12073329 0.07350992 0.09007166]\n",
      " [0.09378982 0.14088076 0.094323   0.10401247 0.08831167 0.08977501\n",
      "  0.10501873 0.12263254 0.07152422 0.08973175]\n",
      " [0.09368679 0.13908181 0.09468343 0.10341252 0.08921874 0.0907101\n",
      "  0.10509229 0.1209799  0.0731616  0.08997283]\n",
      " [0.09395476 0.13357262 0.09596802 0.1018009  0.09062942 0.09252623\n",
      "  0.10472123 0.11794914 0.07769182 0.09118575]\n",
      " [0.0936863  0.13907337 0.09468509 0.10340971 0.08922295 0.09071442\n",
      "  0.10509259 0.12097228 0.07316929 0.08997396]\n",
      " [0.09405679 0.13797238 0.09514956 0.1030817  0.08875851 0.09062286\n",
      "  0.10477783 0.12132461 0.07375306 0.09050279]\n",
      " [0.0939999  0.12710109 0.09744006 0.10008826 0.09285071 0.09432806\n",
      "  0.10421448 0.11412871 0.08318055 0.09266808]\n",
      " [0.09412298 0.13937132 0.09448808 0.10424955 0.08823538 0.0893992\n",
      "  0.10452667 0.12323672 0.07213777 0.09023233]\n",
      " [0.0937475  0.1523563  0.09032371 0.10889455 0.08411943 0.0843542\n",
      "  0.1043812  0.13262509 0.06225673 0.08694126]\n",
      " [0.09368631 0.13907346 0.09468508 0.10340974 0.08922293 0.0907144\n",
      "  0.10509259 0.12097232 0.07316923 0.08997395]]\n"
     ]
    }
   ],
   "source": [
    "results = ntm_predictor.predict(samples)\n",
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-a6d6d434f659>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#response = json.loads(response)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtopic_vector\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'topic_weights'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtop_topic\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_topic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'label'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#response = json.loads(response)\n",
    "for r in response:\n",
    "    topic_vector=r.label['topic_weights'].float32_tensor.values\n",
    "    top_topic= np.argmax(topic_vector)\n",
    "    print(top_topic)\n",
    "    #vectors = [r['topic_mixture'] for r in response['predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Topic ID')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs = 12\n",
    "df=pd.DataFrame(predictions.T)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
