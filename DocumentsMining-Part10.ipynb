{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on December 28th  2021 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "## Small experiment of document mining with various techniques Part 10\n",
    "\n",
    "Let us use AWS built-in NTM algorithm for topic modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: texthero in /opt/conda/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: unidecode>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from texthero) (1.3.2)\n",
      "Requirement already satisfied: spacy<3.0.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (2.3.7)\n",
      "Requirement already satisfied: gensim<4.0,>=3.6.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.8.3)\n",
      "Requirement already satisfied: tqdm>=4.3 in /opt/conda/lib/python3.7/site-packages (from texthero) (4.42.1)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.1.3)\n",
      "Requirement already satisfied: pandas>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from texthero) (1.3.5)\n",
      "Requirement already satisfied: wordcloud>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (1.8.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/.local/lib/python3.7/site-packages (from texthero) (1.21.5)\n",
      "Requirement already satisfied: plotly>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from texthero) (5.4.0)\n",
      "Requirement already satisfied: nltk>=3.3 in /opt/conda/lib/python3.7/site-packages (from texthero) (3.4.5)\n",
      "Requirement already satisfied: scikit-learn>=0.22 in /opt/conda/lib/python3.7/site-packages (from texthero) (0.22.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (5.2.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.4.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from gensim<4.0,>=3.6.0->texthero) (1.14.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (1.1.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas>=1.0.2->texthero) (2019.3)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from plotly>=4.2.0->texthero) (8.0.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /root/.local/lib/python3.7/site-packages (from scikit-learn>=0.22->texthero) (1.1.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (0.9.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (59.5.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (2.0.6)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (7.4.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (0.7.5)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.1.3)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (1.0.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.7/site-packages (from spacy<3.0.0->texthero) (2.26.0)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (from wordcloud>=1.5.0->texthero) (8.4.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in /opt/conda/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (1.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0->texthero) (2.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0->texthero) (2.2.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# install NLTK and gensim if required\n",
    "!pip3 -q install nltk gensim\n",
    "!pip3 install texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import require libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import texthero as hero\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup S3 parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize some parameters depending where you are running the experiment, adapt the parameters to your AWS environment\n",
    "bucket='mymltextarticles'\n",
    "subfolder=''\n",
    "region='us-east-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./sampledocs/vocab.txt\n",
      "./sampledocs/AI-bank-of-the-future-Can-banks-meet-the-AI-challenge-1.txt\n",
      "./sampledocs/Artificial Financial Intelligence.txt\n",
      "./sampledocs/Data machine the insurers using AI to reshape the industry Financial Times.txt\n",
      "./sampledocs/Digital-disruption-in-Insurance.txt\n",
      "./sampledocs/Impact-Big-Data-AI-in-the-Insurance-Sector.txt\n",
      "./sampledocs/Innovation_Artificial-Intelligence-in-Insurance-Whitepaper-deloitte-digital.txt\n",
      "./sampledocs/Insurance-2030-The-impact-of-AI-on-the-future-of-insurance-F.txt\n",
      "./sampledocs/Issues_Paper_on_Increasing_Digitalisation_in_Insurance_and_its_Potential_Impact_on_Consumer_Outcomes.txt\n",
      "./sampledocs/Kaggle State of Machine Learning and Data Science 2020.txt\n",
      "./sampledocs/Module-1-Lecture-Slides.txt\n",
      "./sampledocs/Technology-and-innovation-in-the-insurance-sector.txt\n",
      "./sampledocs/WEF_Governance_of_Chatbots_in_Healthcare_2020.txt\n",
      "./sampledocs/ai-360-research.txt\n",
      "./sampledocs/ai-insurance.txt\n",
      "./sampledocs/ai_in_insurance_web_0.txt\n",
      "./sampledocs/fra-2020-artificial-intelligence_en.txt\n",
      "./sampledocs/merged.txt\n",
      "./sampledocs/sigma-5-2020-en.txt\n",
      "./sampledocs/sigma1_2020_en.txt\n"
     ]
    }
   ],
   "source": [
    "# let us list the files available for analysis in the S3 bucket\n",
    "import os\n",
    "s3s = boto3.client('s3')\n",
    "s3 = boto3.resource('s3')\n",
    "\n",
    "#contents = s3.list_objects(Bucket=bucket, Prefix=subfolder)#['Contents']\n",
    "mybucket = s3.Bucket(bucket)\n",
    "mybucket.objects.filter(Prefix='foo/bar')\n",
    "for file in mybucket.objects.all():\n",
    "    root,ext = os.path.splitext(file.key)\n",
    "    if ext in ['.txt']:\n",
    "        filename=os.path.basename(file.key)\n",
    "        target_filename='./sampledocs/'+filename\n",
    "        print(target_filename)\n",
    "        s3s.download_file(bucket, file.key, target_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of text files\n",
    "path='./sampledocs/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "docNLP=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.txt')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    f = open(file_name,'r')\n",
    "    fileText=f.read()\n",
    "    docName.append(file_name)\n",
    "    docType.append('txt')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of text:158638.2\n",
      "Min length of text:8691\n",
      "Max length of text:1513210\n"
     ]
    }
   ],
   "source": [
    " print (\"Average length of text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>technology innovation insurance sector technol...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/ai-360-research.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>ai insights next frontier business corner offi...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>application ai insurtech real estate technolog...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/Insurance-2030-The-impact-of-AI-o...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Insurance Practice\\n\\nInsurance 2030窶能\nThe imp...</td>\n",
       "      <td>insurance practice insurance -- impact ai futu...</td>\n",
       "      <td>4424</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>machine intelligence insurance insights end en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/Technology-and-innovation-in-the-...  txt   \n",
       "1                   ./sampledocs/ai-360-research.txt  txt   \n",
       "2           ./sampledocs/Module-1-Lecture-Slides.txt  txt   \n",
       "3  ./sampledocs/Insurance-2030-The-impact-of-AI-o...  txt   \n",
       "4                   ./sampledocs/sigma-5-2020-en.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Insurance Practice\\n\\nInsurance 2030窶能\nThe imp...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  technology innovation insurance sector technol...            16742   \n",
       "1  ai insights next frontier business corner offi...             5281   \n",
       "2  application ai insurtech real estate technolog...             3728   \n",
       "3  insurance practice insurance -- impact ai futu...             4424   \n",
       "4  machine intelligence insurance insights end en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               4228  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               1782  \n",
       "4               4329  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20 entries, 0 to 19\n",
      "Data columns (total 6 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   Name               20 non-null     object\n",
      " 1   Type               20 non-null     object\n",
      " 2   Text               20 non-null     object\n",
      " 3   cleanText          20 non-null     object\n",
      " 4   text_word_count    20 non-null     int64 \n",
      " 5   text_unique_words  20 non-null     int64 \n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 1.1+ KB\n"
     ]
    }
   ],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "    \n",
    "stop_words = stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def process_text(text):\n",
    "    for p in string.punctuation:\n",
    "        text = text.replace(p, '')\n",
    "    text = ''.join([c for c in text if not c.isdigit()])\n",
    "    text = text.lower().split()\n",
    "    text = [w for w in text if not w in stop_words] \n",
    "    text = [wnl.lemmatize(w) for w in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['cleanText'] = fullDocs['cleanText'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Type</th>\n",
       "      <th>Text</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>text_word_count</th>\n",
       "      <th>text_unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./sampledocs/Technology-and-innovation-in-the-...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Technology and\\ninnovation in the\\ninsurance s...</td>\n",
       "      <td>[technology, innovation, insurance, sector, te...</td>\n",
       "      <td>16742</td>\n",
       "      <td>4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./sampledocs/ai-360-research.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>AI 360: insights from the\\nnext frontier of bu...</td>\n",
       "      <td>[ai, insight, next, frontier, business, corner...</td>\n",
       "      <td>5281</td>\n",
       "      <td>1746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./sampledocs/Module-1-Lecture-Slides.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>Application of AI, Insurtech and Real Estate\\n...</td>\n",
       "      <td>[application, ai, insurtech, real, estate, tec...</td>\n",
       "      <td>3728</td>\n",
       "      <td>1506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./sampledocs/Insurance-2030-The-impact-of-AI-o...</td>\n",
       "      <td>txt</td>\n",
       "      <td>Insurance Practice\\n\\nInsurance 2030窶能\nThe imp...</td>\n",
       "      <td>[insurance, practice, insurance, impact, ai, f...</td>\n",
       "      <td>4424</td>\n",
       "      <td>1782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./sampledocs/sigma-5-2020-en.txt</td>\n",
       "      <td>txt</td>\n",
       "      <td>No 5 /2020\\n\\nMachine intelligence in\\ninsuran...</td>\n",
       "      <td>[machine, intelligence, insurance, insight, en...</td>\n",
       "      <td>14478</td>\n",
       "      <td>4329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Name Type  \\\n",
       "0  ./sampledocs/Technology-and-innovation-in-the-...  txt   \n",
       "1                   ./sampledocs/ai-360-research.txt  txt   \n",
       "2           ./sampledocs/Module-1-Lecture-Slides.txt  txt   \n",
       "3  ./sampledocs/Insurance-2030-The-impact-of-AI-o...  txt   \n",
       "4                   ./sampledocs/sigma-5-2020-en.txt  txt   \n",
       "\n",
       "                                                Text  \\\n",
       "0  Technology and\\ninnovation in the\\ninsurance s...   \n",
       "1  AI 360: insights from the\\nnext frontier of bu...   \n",
       "2  Application of AI, Insurtech and Real Estate\\n...   \n",
       "3  Insurance Practice\\n\\nInsurance 2030窶能\nThe imp...   \n",
       "4  No 5 /2020\\n\\nMachine intelligence in\\ninsuran...   \n",
       "\n",
       "                                           cleanText  text_word_count  \\\n",
       "0  [technology, innovation, insurance, sector, te...            16742   \n",
       "1  [ai, insight, next, frontier, business, corner...             5281   \n",
       "2  [application, ai, insurtech, real, estate, tec...             3728   \n",
       "3  [insurance, practice, insurance, impact, ai, f...             4424   \n",
       "4  [machine, intelligence, insurance, insight, en...            14478   \n",
       "\n",
       "   text_unique_words  \n",
       "0               4228  \n",
       "1               1746  \n",
       "2               1506  \n",
       "3               1782  \n",
       "4               4329  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(fullDocs['cleanText'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(11382 unique tokens: ['ab', 'ability', 'able', 'abundantly', 'abusive']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(1024 unique tokens: ['accelerate', 'accelerating', 'acceleration', 'acceptance', 'accepted']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary.filter_extremes(keep_n=1024)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vocab.txt', 'w') as f:\n",
    "    for index in range(0,len(dictionary)):\n",
    "        f.write(dictionary.get(index)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fullDocs['tokens'] = fullDocs.apply(lambda row: dictionary.doc2bow(row['cleanText']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[(3, 2), (5, 1), (10, 1), (13, 1), (21, 1), (2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[(8, 1), (11, 1), (15, 1), (16, 3), (22, 2), (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[(0, 1), (1, 2), (7, 1), (9, 4), (11, 1), (12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[(0, 2), (5, 1), (6, 1), (9, 1), (10, 1), (11,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              tokens\n",
       "0  [(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...\n",
       "1  [(3, 2), (5, 1), (10, 1), (13, 1), (21, 1), (2...\n",
       "2  [(8, 1), (11, 1), (15, 1), (16, 3), (22, 2), (...\n",
       "3  [(0, 1), (1, 2), (7, 1), (9, 4), (11, 1), (12,...\n",
       "4  [(0, 2), (5, 1), (6, 1), (9, 1), (10, 1), (11,..."
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = fullDocs.drop(['cleanText'], axis=1)\n",
    "data = data.drop(['Name'], axis=1)\n",
    "data = data.drop(['Type'], axis=1)\n",
    "data = data.drop(['Text'], axis=1)\n",
    "data = data.drop(['text_word_count'], axis=1)\n",
    "data = data.drop(['text_unique_words'], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker.amazon.common as smac\n",
    "from scipy.sparse import lil_matrix\n",
    "\n",
    "prefix = 'training'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_protobuf_dataset(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    token_matrix = lil_matrix((num_lines, num_columns)).astype('float32')\n",
    "    line = 0\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            token_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "        \n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, token_matrix, None)\n",
    "    return buf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_protbuf_dataset(buf, bucket, prefix, key):\n",
    "    obj = '{}/{}'.format(prefix, key)\n",
    "    buf.seek(0)\n",
    "    s3s.upload_fileobj(buf, bucket, obj)\n",
    "    path = 's3://{}/{}'.format(bucket,obj)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/training/training.protobuf\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "training_buf = build_protobuf_dataset(data, dictionary)\n",
    "s3_training_path = upload_protbuf_dataset(training_buf, bucket, prefix, 'training/training.protobuf')\n",
    "s3_auxiliary_path='Inputfiles/auxiliary/'+'vocab.txt'\n",
    "s3s.upload_file('vocab.txt', bucket,'Inputfiles/auxiliary/'+'vocab.txt')\n",
    "\n",
    "print(s3_training_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/output/\n"
     ]
    }
   ],
   "source": [
    "s3_output = 's3://{}/{}/output/'.format(bucket, prefix)\n",
    "\n",
    "print(s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382416733822.dkr.ecr.us-east-1.amazonaws.com/ntm:1\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "region = 'us-east-1'  \n",
    "container = retrieve('ntm', region)\n",
    "print(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::012086180905:role/service-role/AmazonSageMaker-ExecutionRole-20211121T093897\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "print(role)\n",
    "import sagemaker\n",
    "ntm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    role=role,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.c4.2xlarge',\n",
    "    output_path=s3_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm.set_hyperparameters(\n",
    "    num_topics=10, \n",
    "    feature_dim=len(dictionary), \n",
    "    mini_batch_size=256,\n",
    "    optimizer='adam',\n",
    "    num_patience_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-01-07 12:25:30 Starting - Starting the training job...\n",
      "2022-01-07 12:25:53 Starting - Launching requested ML instancesProfilerReport-1641558330: InProgress\n",
      "......\n",
      "2022-01-07 12:26:53 Starting - Preparing the instances for training......\n",
      "2022-01-07 12:27:56 Downloading - Downloading input data\n",
      "2022-01-07 12:27:56 Training - Downloading the training image......\n",
      "2022-01-07 12:28:56 Training - Training image download completed. Training in progress..\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/default-input.json: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adadelta', 'tolerance': '0.001', 'num_patience_epochs': '3', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu'}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '1024', 'optimizer': 'adam', 'num_topics': '10', 'num_patience_epochs': '10', 'mini_batch_size': '256'}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] Final configuration: {'encoder_layers': 'auto', 'mini_batch_size': '256', 'epochs': '50', 'encoder_layers_activation': 'sigmoid', 'optimizer': 'adam', 'tolerance': '0.001', 'num_patience_epochs': '10', 'batch_norm': 'false', 'rescale_gradient': '1.0', 'clip_gradient': 'Inf', 'weight_decay': '0.0', 'learning_rate': '0.01', 'sub_sample': '1.0', '_tuning_objective_metric': '', '_data_format': 'record', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_kvstore': 'auto_gpu', 'feature_dim': '1024', 'num_topics': '10'}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] nvidia-smi: took 0.028 seconds to run.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] Using default worker.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] Initializing\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] vocab.txt\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] Vocab file vocab.txt is expected at /opt/ml/input/data/auxiliary\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:28:59 INFO 140562606671680] Loading pre-trained token embedding vectors from /opt/amazon/lib/python3.7/site-packages/algorithm/s3_binary/glove.6B.50d.txt\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 WARNING 140562606671680] 9 out of 1024 in vocabulary do not have embeddings! Default vector used for unknown embedding!\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Vocab embedding shape: (1024, 50)\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.2144759, \"EndTime\": 1641558548.214503, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Total Batches Seen\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Records Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Max Batches Seen Between Resets\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.214] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 8782, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 1\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.269] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 53, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 1 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5152354836463928\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 5.36669249413535e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.515181839466095\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5152354836463928\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=1, train total_loss <loss>=0.5152354836463928\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.06s, val: 0.00s, epoch: 0.06s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.2147596, \"EndTime\": 1641558548.2741857, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=319.111744684259 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 2\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.284] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 5, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 2 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5135707259178162\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 3.2210176868829876e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5135384798049927\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5135707259178162\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=2, train total_loss <loss>=0.5135707259178162\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.2744215, \"EndTime\": 1641558548.2896996, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Total Batches Seen\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1234.0390845179472 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 3\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.303] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 3 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5128304362297058\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 4.452664143173024e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5127859115600586\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5128304362297058\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=3, train total_loss <loss>=0.5128304362297058\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.2899306, \"EndTime\": 1641558548.3083317, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 57.0, \"count\": 1, \"min\": 57, \"max\": 57}, \"Total Batches Seen\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1026.0963883345137 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 4\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.319] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 11, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 4 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5120370388031006\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 4.8663190682418644e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5119883418083191\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5120370388031006\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=4, train total_loss <loss>=0.5120370388031006\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.308591, \"EndTime\": 1641558548.3243003, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Total Batches Seen\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1200.3762069017457 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 5\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.338] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 5 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.510773241519928\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 4.272518344805576e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.510730504989624\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.510773241519928\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=5, train total_loss <loss>=0.510773241519928\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.3245556, \"EndTime\": 1641558548.343614, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 95.0, \"count\": 1, \"min\": 95, \"max\": 95}, \"Total Batches Seen\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=990.242876846801 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 6\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.355] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 17, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 6 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5089190602302551\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 4.015629747300409e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5088789463043213\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5089190602302551\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=6, train total_loss <loss>=0.5089190602302551\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.3438835, \"EndTime\": 1641558548.3602035, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 114.0, \"count\": 1, \"min\": 114, \"max\": 114}, \"Total Batches Seen\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1156.1429296812662 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 7\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.374] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 7 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5052835941314697\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 4.713907765108161e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5052365064620972\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5052835941314697\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=7, train total_loss <loss>=0.5052835941314697\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.360429, \"EndTime\": 1641558548.3788803, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 133.0, \"count\": 1, \"min\": 133, \"max\": 133}, \"Total Batches Seen\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1023.5922676770921 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 8\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.390] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 23, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 8 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5058403015136719\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 7.477285544155166e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5057654976844788\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5058403015136719\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=8, train total_loss <loss>=0.5058403015136719\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.3791065, \"EndTime\": 1641558548.3914108, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 152.0, \"count\": 1, \"min\": 152, \"max\": 152}, \"Total Batches Seen\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1530.326951512242 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 9\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.404] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 9 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5035326480865479\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.00013132512685842812\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5034013390541077\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5035326480865479\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=9, train total_loss <loss>=0.5035326480865479\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.3916795, \"EndTime\": 1641558548.4099538, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 171.0, \"count\": 1, \"min\": 171, \"max\": 171}, \"Total Batches Seen\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1032.7986418009098 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 10\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.423] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 29, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 10 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5019988417625427\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.00022544788953382522\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5017734169960022\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5019988417625427\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=10, train total_loss <loss>=0.5019988417625427\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.4101858, \"EndTime\": 1641558548.4280965, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 190.0, \"count\": 1, \"min\": 190, \"max\": 190}, \"Total Batches Seen\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1053.4133851502293 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 11\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.440] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 11 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5004531145095825\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.00036653145798482\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5000866055488586\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5004531145095825\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=11, train total_loss <loss>=0.5004531145095825\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5152354836463928, 0.5135707259178162, 0.5128304362297058, 0.5120370388031006, 0.510773241519928, 0.5089190602302551, 0.5052835941314697, 0.5058403015136719, 0.5035326480865479, 0.5019988417625427] min patience loss:0.5019988417625427 current loss:0.5004531145095825 absolute loss difference:0.001545727252960205\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.4283705, \"EndTime\": 1641558548.4451041, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 209.0, \"count\": 1, \"min\": 209, \"max\": 209}, \"Total Batches Seen\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1126.2900107411385 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 12\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.459] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 35, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 12 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.5017843246459961\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0005413881735876203\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.5012429356575012\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.5017843246459961\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=12, train total_loss <loss>=0.5017843246459961\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5135707259178162, 0.5128304362297058, 0.5120370388031006, 0.510773241519928, 0.5089190602302551, 0.5052835941314697, 0.5058403015136719, 0.5035326480865479, 0.5019988417625427, 0.5004531145095825] min patience loss:0.5004531145095825 current loss:0.5017843246459961 absolute loss difference:0.0013312101364135742\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.4453375, \"EndTime\": 1641558548.4602752, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 228.0, \"count\": 1, \"min\": 228, \"max\": 228}, \"Total Batches Seen\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1262.4039792798644 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 13\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.473] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 13 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49769827723503113\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0006875067483633757\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4970107674598694\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49769827723503113\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=13, train total_loss <loss>=0.49769827723503113\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5128304362297058, 0.5120370388031006, 0.510773241519928, 0.5089190602302551, 0.5052835941314697, 0.5058403015136719, 0.5035326480865479, 0.5019988417625427, 0.5004531145095825, 0.5017843246459961] min patience loss:0.5004531145095825 current loss:0.49769827723503113 absolute loss difference:0.0027548372745513916\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.4605005, \"EndTime\": 1641558548.4777844, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 247.0, \"count\": 1, \"min\": 247, \"max\": 247}, \"Total Batches Seen\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1090.7268521686763 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 14\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.490] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 41, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 14 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49847763776779175\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.000747963844332844\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49772965908050537\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49847763776779175\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=14, train total_loss <loss>=0.49847763776779175\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5120370388031006, 0.510773241519928, 0.5089190602302551, 0.5052835941314697, 0.5058403015136719, 0.5035326480865479, 0.5019988417625427, 0.5004531145095825, 0.5017843246459961, 0.49769827723503113] min patience loss:0.49769827723503113 current loss:0.49847763776779175 absolute loss difference:0.0007793605327606201\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.4780402, \"EndTime\": 1641558548.4922934, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 266.0, \"count\": 1, \"min\": 266, \"max\": 266}, \"Total Batches Seen\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1320.3839946980365 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 15\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.505] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 44, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 15 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4984701871871948\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0008009729208424687\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4976692199707031\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4984701871871948\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=15, train total_loss <loss>=0.4984701871871948\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.510773241519928, 0.5089190602302551, 0.5052835941314697, 0.5058403015136719, 0.5035326480865479, 0.5019988417625427, 0.5004531145095825, 0.5017843246459961, 0.49769827723503113, 0.49847763776779175] min patience loss:0.49769827723503113 current loss:0.4984701871871948 absolute loss difference:0.0007719099521636963\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.4925392, \"EndTime\": 1641558548.5068908, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 285.0, \"count\": 1, \"min\": 285, \"max\": 285}, \"Total Batches Seen\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1310.9356144102649 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 16\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.520] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 47, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 16 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49932625889778137\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0008036080980673432\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49852266907691956\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49932625889778137\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=16, train total_loss <loss>=0.49932625889778137\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5089190602302551, 0.5052835941314697, 0.5058403015136719, 0.5035326480865479, 0.5019988417625427, 0.5004531145095825, 0.5017843246459961, 0.49769827723503113, 0.49847763776779175, 0.4984701871871948] min patience loss:0.49769827723503113 current loss:0.49932625889778137 absolute loss difference:0.0016279816627502441\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.5071504, \"EndTime\": 1641558548.5216906, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 304.0, \"count\": 1, \"min\": 304, \"max\": 304}, \"Total Batches Seen\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1294.2018968429259 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 17\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.534] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 50, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 17 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49894240498542786\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0007553262985311449\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4981870651245117\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49894240498542786\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=17, train total_loss <loss>=0.49894240498542786\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5052835941314697, 0.5058403015136719, 0.5035326480865479, 0.5019988417625427, 0.5004531145095825, 0.5017843246459961, 0.49769827723503113, 0.49847763776779175, 0.4984701871871948, 0.49932625889778137] min patience loss:0.49769827723503113 current loss:0.49894240498542786 absolute loss difference:0.0012441277503967285\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.5219593, \"EndTime\": 1641558548.5359852, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 323.0, \"count\": 1, \"min\": 323, \"max\": 323}, \"Total Batches Seen\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1344.3735618610615 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 18\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.549] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 53, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 18 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4987736642360687\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0006938785663805902\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.498079776763916\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4987736642360687\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=18, train total_loss <loss>=0.4987736642360687\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5058403015136719, 0.5035326480865479, 0.5019988417625427, 0.5004531145095825, 0.5017843246459961, 0.49769827723503113, 0.49847763776779175, 0.4984701871871948, 0.49932625889778137, 0.49894240498542786] min patience loss:0.49769827723503113 current loss:0.4987736642360687 absolute loss difference:0.0010753870010375977\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.5361934, \"EndTime\": 1641558548.5512366, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 342.0, \"count\": 1, \"min\": 342, \"max\": 342}, \"Total Batches Seen\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1253.7447257052058 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 19\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.565] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 56, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 19 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49649450182914734\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0006071833777241409\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4958873391151428\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49649450182914734\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=19, train total_loss <loss>=0.49649450182914734\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5035326480865479, 0.5019988417625427, 0.5004531145095825, 0.5017843246459961, 0.49769827723503113, 0.49847763776779175, 0.4984701871871948, 0.49932625889778137, 0.49894240498542786, 0.4987736642360687] min patience loss:0.49769827723503113 current loss:0.49649450182914734 absolute loss difference:0.001203775405883789\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 38.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.5514958, \"EndTime\": 1641558548.5694628, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 361.0, \"count\": 1, \"min\": 361, \"max\": 361}, \"Total Batches Seen\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1049.2386770591952 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 20\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.583] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 59, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 20 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49555298686027527\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0005531806382350624\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49499979615211487\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49555298686027527\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=20, train total_loss <loss>=0.49555298686027527\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5019988417625427, 0.5004531145095825, 0.5017843246459961, 0.49769827723503113, 0.49847763776779175, 0.4984701871871948, 0.49932625889778137, 0.49894240498542786, 0.4987736642360687, 0.49649450182914734] min patience loss:0.49649450182914734 current loss:0.49555298686027527 absolute loss difference:0.0009415149688720703\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.5697405, \"EndTime\": 1641558548.589083, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 380.0, \"count\": 1, \"min\": 380, \"max\": 380}, \"Total Batches Seen\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=975.2285476528465 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 21\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.599] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 62, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 21 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4958260655403137\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0005187372444197536\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4953073263168335\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4958260655403137\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=21, train total_loss <loss>=0.4958260655403137\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5004531145095825, 0.5017843246459961, 0.49769827723503113, 0.49847763776779175, 0.4984701871871948, 0.49932625889778137, 0.49894240498542786, 0.4987736642360687, 0.49649450182914734, 0.49555298686027527] min patience loss:0.49555298686027527 current loss:0.4958260655403137 absolute loss difference:0.00027307868003845215\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 42.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.5893424, \"EndTime\": 1641558548.6010635, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 20, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 399.0, \"count\": 1, \"min\": 399, \"max\": 399}, \"Total Batches Seen\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1606.1709126088358 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 22\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.611] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 65, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 22 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4964565336704254\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0004980732337571681\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49595844745635986\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4964565336704254\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=22, train total_loss <loss>=0.4964565336704254\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.5017843246459961, 0.49769827723503113, 0.49847763776779175, 0.4984701871871948, 0.49932625889778137, 0.49894240498542786, 0.4987736642360687, 0.49649450182914734, 0.49555298686027527, 0.4958260655403137] min patience loss:0.49555298686027527 current loss:0.4964565336704254 absolute loss difference:0.0009035468101501465\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 44.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.6012862, \"EndTime\": 1641558548.612729, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 21, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 418.0, \"count\": 1, \"min\": 418, \"max\": 418}, \"Total Batches Seen\": {\"sum\": 22.0, \"count\": 1, \"min\": 22, \"max\": 22}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1644.8590476583624 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 23\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.623] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 68, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 23 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49598217010498047\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0004763200122397393\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4955058693885803\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49598217010498047\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=23, train total_loss <loss>=0.49598217010498047\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.49769827723503113, 0.49847763776779175, 0.4984701871871948, 0.49932625889778137, 0.49894240498542786, 0.4987736642360687, 0.49649450182914734, 0.49555298686027527, 0.4958260655403137, 0.4964565336704254] min patience loss:0.49555298686027527 current loss:0.49598217010498047 absolute loss difference:0.0004291832447052002\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 46.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.6129456, \"EndTime\": 1641558548.6249974, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 22, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 437.0, \"count\": 1, \"min\": 437, \"max\": 437}, \"Total Batches Seen\": {\"sum\": 23.0, \"count\": 1, \"min\": 23, \"max\": 23}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1563.472876733829 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 24\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.638] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 71, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 24 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4958648085594177\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0004699817218352109\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49539482593536377\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4958648085594177\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=24, train total_loss <loss>=0.4958648085594177\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.49847763776779175, 0.4984701871871948, 0.49932625889778137, 0.49894240498542786, 0.4987736642360687, 0.49649450182914734, 0.49555298686027527, 0.4958260655403137, 0.4964565336704254, 0.49598217010498047] min patience loss:0.49555298686027527 current loss:0.4958648085594177 absolute loss difference:0.00031182169914245605\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 48.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.6252275, \"EndTime\": 1641558548.639891, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 23, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 456.0, \"count\": 1, \"min\": 456, \"max\": 456}, \"Total Batches Seen\": {\"sum\": 24.0, \"count\": 1, \"min\": 24, \"max\": 24}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1286.5755476986164 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 25\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.653] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 74, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 25 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4955591857433319\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.00047336029820144176\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49508583545684814\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4955591857433319\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=25, train total_loss <loss>=0.4955591857433319\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4984701871871948, 0.49932625889778137, 0.49894240498542786, 0.4987736642360687, 0.49649450182914734, 0.49555298686027527, 0.4958260655403137, 0.4964565336704254, 0.49598217010498047, 0.4958648085594177] min patience loss:0.49555298686027527 current loss:0.4955591857433319 absolute loss difference:6.198883056640625e-06\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.6401403, \"EndTime\": 1641558548.6549594, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 24, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 475.0, \"count\": 1, \"min\": 475, \"max\": 475}, \"Total Batches Seen\": {\"sum\": 25.0, \"count\": 1, \"min\": 25, \"max\": 25}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1269.1794234750757 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 26\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.669] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 77, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 26 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4937986433506012\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0004889454576186836\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4933096766471863\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4937986433506012\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=26, train total_loss <loss>=0.4937986433506012\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.49932625889778137, 0.49894240498542786, 0.4987736642360687, 0.49649450182914734, 0.49555298686027527, 0.4958260655403137, 0.4964565336704254, 0.49598217010498047, 0.4958648085594177, 0.4955591857433319] min patience loss:0.49555298686027527 current loss:0.4937986433506012 absolute loss difference:0.0017543435096740723\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.01s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 52.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.6552262, \"EndTime\": 1641558548.6760314, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 25, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 494.0, \"count\": 1, \"min\": 494, \"max\": 494}, \"Total Batches Seen\": {\"sum\": 26.0, \"count\": 1, \"min\": 26, \"max\": 26}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 52.0, \"count\": 1, \"min\": 52, \"max\": 52}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=908.4893352637399 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 27\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.686] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 80, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 27 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4971923530101776\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0005249966634437442\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49666735529899597\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4971923530101776\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=27, train total_loss <loss>=0.4971923530101776\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.49894240498542786, 0.4987736642360687, 0.49649450182914734, 0.49555298686027527, 0.4958260655403137, 0.4964565336704254, 0.49598217010498047, 0.4958648085594177, 0.4955591857433319, 0.4937986433506012] min patience loss:0.4937986433506012 current loss:0.4971923530101776 absolute loss difference:0.003393709659576416\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 54.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.6762626, \"EndTime\": 1641558548.6879938, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 26, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 513.0, \"count\": 1, \"min\": 513, \"max\": 513}, \"Total Batches Seen\": {\"sum\": 27.0, \"count\": 1, \"min\": 27, \"max\": 27}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 54.0, \"count\": 1, \"min\": 54, \"max\": 54}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1603.1659458045826 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 28\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.698] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 83, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 28 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4938223659992218\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0005723508074879646\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4932500123977661\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4938223659992218\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=28, train total_loss <loss>=0.4938223659992218\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4987736642360687, 0.49649450182914734, 0.49555298686027527, 0.4958260655403137, 0.4964565336704254, 0.49598217010498047, 0.4958648085594177, 0.4955591857433319, 0.4937986433506012, 0.4971923530101776] min patience loss:0.4937986433506012 current loss:0.4938223659992218 absolute loss difference:2.372264862060547e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 56.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.688217, \"EndTime\": 1641558548.699621, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 27, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 532.0, \"count\": 1, \"min\": 532, \"max\": 532}, \"Total Batches Seen\": {\"sum\": 28.0, \"count\": 1, \"min\": 28, \"max\": 28}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 56.0, \"count\": 1, \"min\": 56, \"max\": 56}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1650.5141768324254 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 29\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.710] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 86, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 29 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4953359365463257\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0006247995188459754\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49471113085746765\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4953359365463257\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=29, train total_loss <loss>=0.4953359365463257\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.49649450182914734, 0.49555298686027527, 0.4958260655403137, 0.4964565336704254, 0.49598217010498047, 0.4958648085594177, 0.4955591857433319, 0.4937986433506012, 0.4971923530101776, 0.4938223659992218] min patience loss:0.4937986433506012 current loss:0.4953359365463257 absolute loss difference:0.0015372931957244873\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 58.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.6998339, \"EndTime\": 1641558548.7117436, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 28, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 551.0, \"count\": 1, \"min\": 551, \"max\": 551}, \"Total Batches Seen\": {\"sum\": 29.0, \"count\": 1, \"min\": 29, \"max\": 29}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 58.0, \"count\": 1, \"min\": 58, \"max\": 58}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1580.8410069230922 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 30\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.725] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 89, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 30 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4941178560256958\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0006909877411089838\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4934268891811371\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4941178560256958\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=30, train total_loss <loss>=0.4941178560256958\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.49555298686027527, 0.4958260655403137, 0.4964565336704254, 0.49598217010498047, 0.4958648085594177, 0.4955591857433319, 0.4937986433506012, 0.4971923530101776, 0.4938223659992218, 0.4953359365463257] min patience loss:0.4937986433506012 current loss:0.4941178560256958 absolute loss difference:0.0003192126750946045\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.7119606, \"EndTime\": 1641558548.7267046, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 29, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 570.0, \"count\": 1, \"min\": 570, \"max\": 570}, \"Total Batches Seen\": {\"sum\": 30.0, \"count\": 1, \"min\": 30, \"max\": 30}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 60.0, \"count\": 1, \"min\": 60, \"max\": 60}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1281.0329052066422 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 31\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.740] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 92, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 31 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4921177625656128\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0007734361570328474\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4913443326950073\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4921177625656128\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=31, train total_loss <loss>=0.4921177625656128\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4958260655403137, 0.4964565336704254, 0.49598217010498047, 0.4958648085594177, 0.4955591857433319, 0.4937986433506012, 0.4971923530101776, 0.4938223659992218, 0.4953359365463257, 0.4941178560256958] min patience loss:0.4937986433506012 current loss:0.4921177625656128 absolute loss difference:0.0016808807849884033\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 62.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.7269323, \"EndTime\": 1641558548.745988, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 30, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 589.0, \"count\": 1, \"min\": 589, \"max\": 589}, \"Total Batches Seen\": {\"sum\": 31.0, \"count\": 1, \"min\": 31, \"max\": 31}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 62.0, \"count\": 1, \"min\": 62, \"max\": 62}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=992.1290772371893 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 32\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.759] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 95, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 32 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4920191466808319\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0008635802660137415\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49115556478500366\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4920191466808319\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=32, train total_loss <loss>=0.4920191466808319\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4964565336704254, 0.49598217010498047, 0.4958648085594177, 0.4955591857433319, 0.4937986433506012, 0.4971923530101776, 0.4938223659992218, 0.4953359365463257, 0.4941178560256958, 0.4921177625656128] min patience loss:0.4921177625656128 current loss:0.4920191466808319 absolute loss difference:9.861588478088379e-05\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 64.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.7462053, \"EndTime\": 1641558548.7649958, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 31, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 608.0, \"count\": 1, \"min\": 608, \"max\": 608}, \"Total Batches Seen\": {\"sum\": 32.0, \"count\": 1, \"min\": 32, \"max\": 32}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 64.0, \"count\": 1, \"min\": 64, \"max\": 64}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1003.5862833251476 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 33\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.778] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 98, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 33 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4923599064350128\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0009671726729720831\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49139276146888733\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4923599064350128\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=33, train total_loss <loss>=0.4923599064350128\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.49598217010498047, 0.4958648085594177, 0.4955591857433319, 0.4937986433506012, 0.4971923530101776, 0.4938223659992218, 0.4953359365463257, 0.4941178560256958, 0.4921177625656128, 0.4920191466808319] min patience loss:0.4920191466808319 current loss:0.4923599064350128 absolute loss difference:0.0003407597541809082\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 66.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.7652693, \"EndTime\": 1641558548.779837, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 32, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 627.0, \"count\": 1, \"min\": 627, \"max\": 627}, \"Total Batches Seen\": {\"sum\": 33.0, \"count\": 1, \"min\": 33, \"max\": 33}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 66.0, \"count\": 1, \"min\": 66, \"max\": 66}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1295.0221167752734 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 34\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.793] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 101, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 34 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4938870072364807\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.001045204815454781\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49284180998802185\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4938870072364807\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=34, train total_loss <loss>=0.4938870072364807\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4958648085594177, 0.4955591857433319, 0.4937986433506012, 0.4971923530101776, 0.4938223659992218, 0.4953359365463257, 0.4941178560256958, 0.4921177625656128, 0.4920191466808319, 0.4923599064350128] min patience loss:0.4920191466808319 current loss:0.4938870072364807 absolute loss difference:0.0018678605556488037\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 68.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.780074, \"EndTime\": 1641558548.7947323, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 33, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 646.0, \"count\": 1, \"min\": 646, \"max\": 646}, \"Total Batches Seen\": {\"sum\": 34.0, \"count\": 1, \"min\": 34, \"max\": 34}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 68.0, \"count\": 1, \"min\": 68, \"max\": 68}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1285.0610507304802 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 35\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.808] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 104, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 35 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4928329885005951\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.001101676607504487\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49173131585121155\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4928329885005951\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=35, train total_loss <loss>=0.4928329885005951\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4955591857433319, 0.4937986433506012, 0.4971923530101776, 0.4938223659992218, 0.4953359365463257, 0.4941178560256958, 0.4921177625656128, 0.4920191466808319, 0.4923599064350128, 0.4938870072364807] min patience loss:0.4920191466808319 current loss:0.4928329885005951 absolute loss difference:0.0008138418197631836\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.7949784, \"EndTime\": 1641558548.809584, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 34, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 665.0, \"count\": 1, \"min\": 665, \"max\": 665}, \"Total Batches Seen\": {\"sum\": 35.0, \"count\": 1, \"min\": 35, \"max\": 35}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 70.0, \"count\": 1, \"min\": 70, \"max\": 70}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1292.1873135296407 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 36\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.823] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 107, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 36 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4911295473575592\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.001103272894397378\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4900262951850891\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4911295473575592\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=36, train total_loss <loss>=0.4911295473575592\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4937986433506012, 0.4971923530101776, 0.4938223659992218, 0.4953359365463257, 0.4941178560256958, 0.4921177625656128, 0.4920191466808319, 0.4923599064350128, 0.4938870072364807, 0.4928329885005951] min patience loss:0.4920191466808319 current loss:0.4911295473575592 absolute loss difference:0.0008895993232727051\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 72.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.8097866, \"EndTime\": 1641558548.828304, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 35, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 684.0, \"count\": 1, \"min\": 684, \"max\": 684}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 72.0, \"count\": 1, \"min\": 72, \"max\": 72}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1020.3159336790218 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 37\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.840] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 110, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 37 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4920920729637146\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0010885628871619701\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49100351333618164\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4920920729637146\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=37, train total_loss <loss>=0.4920920729637146\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4971923530101776, 0.4938223659992218, 0.4953359365463257, 0.4941178560256958, 0.4921177625656128, 0.4920191466808319, 0.4923599064350128, 0.4938870072364807, 0.4928329885005951, 0.4911295473575592] min patience loss:0.4911295473575592 current loss:0.4920920729637146 absolute loss difference:0.0009625256061553955\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 74.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.828507, \"EndTime\": 1641558548.8412757, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 36, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 703.0, \"count\": 1, \"min\": 703, \"max\": 703}, \"Total Batches Seen\": {\"sum\": 37.0, \"count\": 1, \"min\": 37, \"max\": 37}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 74.0, \"count\": 1, \"min\": 74, \"max\": 74}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1474.162970088237 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 38\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.853] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 113, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 38 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49184978008270264\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0010990643640980124\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4907507300376892\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49184978008270264\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=38, train total_loss <loss>=0.49184978008270264\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4938223659992218, 0.4953359365463257, 0.4941178560256958, 0.4921177625656128, 0.4920191466808319, 0.4923599064350128, 0.4938870072364807, 0.4928329885005951, 0.4911295473575592, 0.4920920729637146] min patience loss:0.4911295473575592 current loss:0.49184978008270264 absolute loss difference:0.0007202327251434326\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:7\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 76.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.8415031, \"EndTime\": 1641558548.854179, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 37, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 722.0, \"count\": 1, \"min\": 722, \"max\": 722}, \"Total Batches Seen\": {\"sum\": 38.0, \"count\": 1, \"min\": 38, \"max\": 38}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 76.0, \"count\": 1, \"min\": 76, \"max\": 76}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1485.6504539438117 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 39\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.867] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 116, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 39 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4908621907234192\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0011252537369728088\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.489736944437027\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4908621907234192\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=39, train total_loss <loss>=0.4908621907234192\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4953359365463257, 0.4941178560256958, 0.4921177625656128, 0.4920191466808319, 0.4923599064350128, 0.4938870072364807, 0.4928329885005951, 0.4911295473575592, 0.4920920729637146, 0.49184978008270264] min patience loss:0.4911295473575592 current loss:0.4908621907234192 absolute loss difference:0.00026735663414001465\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:8\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 78.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.8544059, \"EndTime\": 1641558548.8731685, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 38, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 741.0, \"count\": 1, \"min\": 741, \"max\": 741}, \"Total Batches Seen\": {\"sum\": 39.0, \"count\": 1, \"min\": 39, \"max\": 39}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 78.0, \"count\": 1, \"min\": 78, \"max\": 78}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1006.8322067946078 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 40\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.886] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 119, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 40 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4916303753852844\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0011641314486041665\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.49046623706817627\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4916303753852844\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=40, train total_loss <loss>=0.4916303753852844\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4941178560256958, 0.4921177625656128, 0.4920191466808319, 0.4923599064350128, 0.4938870072364807, 0.4928329885005951, 0.4911295473575592, 0.4920920729637146, 0.49184978008270264, 0.4908621907234192] min patience loss:0.4908621907234192 current loss:0.4916303753852844 absolute loss difference:0.0007681846618652344\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:9\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.873427, \"EndTime\": 1641558548.8877194, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 39, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 760.0, \"count\": 1, \"min\": 760, \"max\": 760}, \"Total Batches Seen\": {\"sum\": 40.0, \"count\": 1, \"min\": 40, \"max\": 40}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 80.0, \"count\": 1, \"min\": 80, \"max\": 80}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1318.461625002068 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 41\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.900] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 122, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 41 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4895123243331909\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.001205275533720851\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.48830705881118774\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4895123243331909\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=41, train total_loss <loss>=0.4895123243331909\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4921177625656128, 0.4920191466808319, 0.4923599064350128, 0.4938870072364807, 0.4928329885005951, 0.4911295473575592, 0.4920920729637146, 0.49184978008270264, 0.4908621907234192, 0.4916303753852844] min patience loss:0.4908621907234192 current loss:0.4895123243331909 absolute loss difference:0.0013498663902282715\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 82.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.887972, \"EndTime\": 1641558548.9050405, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 40, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 779.0, \"count\": 1, \"min\": 779, \"max\": 779}, \"Total Batches Seen\": {\"sum\": 41.0, \"count\": 1, \"min\": 41, \"max\": 41}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 82.0, \"count\": 1, \"min\": 82, \"max\": 82}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1105.9239789616843 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 42\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.918] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 125, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 42 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49034103751182556\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0012379428371787071\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4891031086444855\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49034103751182556\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=42, train total_loss <loss>=0.49034103751182556\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4920191466808319, 0.4923599064350128, 0.4938870072364807, 0.4928329885005951, 0.4911295473575592, 0.4920920729637146, 0.49184978008270264, 0.4908621907234192, 0.4916303753852844, 0.4895123243331909] min patience loss:0.4895123243331909 current loss:0.49034103751182556 absolute loss difference:0.0008287131786346436\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:1\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 84.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.9052932, \"EndTime\": 1641558548.9201176, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 41, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 798.0, \"count\": 1, \"min\": 798, \"max\": 798}, \"Total Batches Seen\": {\"sum\": 42.0, \"count\": 1, \"min\": 42, \"max\": 42}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 84.0, \"count\": 1, \"min\": 84, \"max\": 84}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1271.609637785224 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 43\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.934] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 128, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 43 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.49005651473999023\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0012742362450808287\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.48878228664398193\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.49005651473999023\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=43, train total_loss <loss>=0.49005651473999023\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4923599064350128, 0.4938870072364807, 0.4928329885005951, 0.4911295473575592, 0.4920920729637146, 0.49184978008270264, 0.4908621907234192, 0.4916303753852844, 0.4895123243331909, 0.49034103751182556] min patience loss:0.4895123243331909 current loss:0.49005651473999023 absolute loss difference:0.0005441904067993164\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:2\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 86.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.9203715, \"EndTime\": 1641558548.9353669, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 42, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 817.0, \"count\": 1, \"min\": 817, \"max\": 817}, \"Total Batches Seen\": {\"sum\": 43.0, \"count\": 1, \"min\": 43, \"max\": 43}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 86.0, \"count\": 1, \"min\": 86, \"max\": 86}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1256.8293090668224 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 44\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.949] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 131, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 44 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.48871102929115295\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0013113796012476087\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4873996376991272\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.48871102929115295\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=44, train total_loss <loss>=0.48871102929115295\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4938870072364807, 0.4928329885005951, 0.4911295473575592, 0.4920920729637146, 0.49184978008270264, 0.4908621907234192, 0.4916303753852844, 0.4895123243331909, 0.49034103751182556, 0.49005651473999023] min patience loss:0.4895123243331909 current loss:0.48871102929115295 absolute loss difference:0.0008012950420379639\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:3\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 88.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.935623, \"EndTime\": 1641558548.954803, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 43, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 836.0, \"count\": 1, \"min\": 836, \"max\": 836}, \"Total Batches Seen\": {\"sum\": 44.0, \"count\": 1, \"min\": 44, \"max\": 44}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 88.0, \"count\": 1, \"min\": 88, \"max\": 88}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=984.6270633587031 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 45\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.969] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 134, \"duration\": 14, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 45 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.48895934224128723\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0013442004565149546\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4876151382923126\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.48895934224128723\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=45, train total_loss <loss>=0.48895934224128723\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4928329885005951, 0.4911295473575592, 0.4920920729637146, 0.49184978008270264, 0.4908621907234192, 0.4916303753852844, 0.4895123243331909, 0.49034103751182556, 0.49005651473999023, 0.48871102929115295] min patience loss:0.48871102929115295 current loss:0.48895934224128723 absolute loss difference:0.00024831295013427734\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:4\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.02s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.955071, \"EndTime\": 1641558548.9708314, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 44, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 855.0, \"count\": 1, \"min\": 855, \"max\": 855}, \"Total Batches Seen\": {\"sum\": 45.0, \"count\": 1, \"min\": 45, \"max\": 45}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 90.0, \"count\": 1, \"min\": 90, \"max\": 90}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1195.4065251631291 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 46\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.981] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 137, \"duration\": 10, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 46 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4889758825302124\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.0013946454273536801\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4875812232494354\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4889758825302124\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=46, train total_loss <loss>=0.4889758825302124\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] patience losses:[0.4911295473575592, 0.4920920729637146, 0.49184978008270264, 0.4908621907234192, 0.4916303753852844, 0.4895123243331909, 0.49034103751182556, 0.49005651473999023, 0.48871102929115295, 0.48895934224128723] min patience loss:0.48871102929115295 current loss:0.4889758825302124 absolute loss difference:0.00026485323905944824\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:5\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #progress_metric: host=algo-1, completed 92.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.9710639, \"EndTime\": 1641558548.9828138, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 45, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 874.0, \"count\": 1, \"min\": 874, \"max\": 874}, \"Total Batches Seen\": {\"sum\": 46.0, \"count\": 1, \"min\": 46, \"max\": 46}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 92.0, \"count\": 1, \"min\": 92, \"max\": 92}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1601.104534587025 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Starting training for epoch 47\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:08.996] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 140, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] # Finished training epoch 47 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) total: 0.4879416823387146\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) kld: 0.001446408685296774\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) recons: 0.4864952564239502\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] Loss (name: value) logppx: 0.4879416823387146\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:08 INFO 140562606671680] #quality_metric: host=algo-1, epoch=47, train total_loss <loss>=0.4879416823387146\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] patience losses:[0.4920920729637146, 0.49184978008270264, 0.4908621907234192, 0.4916303753852844, 0.4895123243331909, 0.49034103751182556, 0.49005651473999023, 0.48871102929115295, 0.48895934224128723, 0.4889758825302124] min patience loss:0.48871102929115295 current loss:0.4879416823387146 absolute loss difference:0.0007693469524383545\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:6\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Timing: train: 0.01s, val: 0.01s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #progress_metric: host=algo-1, completed 94.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558548.9830387, \"EndTime\": 1641558549.0030003, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 46, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 893.0, \"count\": 1, \"min\": 893, \"max\": 893}, \"Total Batches Seen\": {\"sum\": 47.0, \"count\": 1, \"min\": 47, \"max\": 47}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 94.0, \"count\": 1, \"min\": 94, \"max\": 94}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=946.0087369420703 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] # Starting training for epoch 48\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:09.014] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 143, \"duration\": 11, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] # Finished training epoch 48 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) total: 0.48743119835853577\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) kld: 0.001480904989875853\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) recons: 0.4859502911567688\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) logppx: 0.48743119835853577\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #quality_metric: host=algo-1, epoch=48, train total_loss <loss>=0.48743119835853577\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] patience losses:[0.49184978008270264, 0.4908621907234192, 0.4916303753852844, 0.4895123243331909, 0.49034103751182556, 0.49005651473999023, 0.48871102929115295, 0.48895934224128723, 0.4889758825302124, 0.4879416823387146] min patience loss:0.4879416823387146 current loss:0.48743119835853577 absolute loss difference:0.000510483980178833\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:7\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.02s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #progress_metric: host=algo-1, completed 96.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558549.0032392, \"EndTime\": 1641558549.0194533, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 47, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 912.0, \"count\": 1, \"min\": 912, \"max\": 912}, \"Total Batches Seen\": {\"sum\": 48.0, \"count\": 1, \"min\": 48, \"max\": 48}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 96.0, \"count\": 1, \"min\": 96, \"max\": 96}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1162.7894652367404 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] # Starting training for epoch 49\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:09.033] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 146, \"duration\": 13, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] # Finished training epoch 49 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) total: 0.4876335859298706\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) kld: 0.0015081202145665884\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) recons: 0.48612546920776367\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) logppx: 0.4876335859298706\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #quality_metric: host=algo-1, epoch=49, train total_loss <loss>=0.4876335859298706\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] patience losses:[0.4908621907234192, 0.4916303753852844, 0.4895123243331909, 0.49034103751182556, 0.49005651473999023, 0.48871102929115295, 0.48895934224128723, 0.4889758825302124, 0.4879416823387146, 0.48743119835853577] min patience loss:0.48743119835853577 current loss:0.4876335859298706 absolute loss difference:0.00020238757133483887\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:8\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #progress_metric: host=algo-1, completed 98.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558549.0197177, \"EndTime\": 1641558549.0348423, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 48, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 931.0, \"count\": 1, \"min\": 931, \"max\": 931}, \"Total Batches Seen\": {\"sum\": 49.0, \"count\": 1, \"min\": 49, \"max\": 49}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 98.0, \"count\": 1, \"min\": 98, \"max\": 98}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1245.6705900742477 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] \u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] # Starting training for epoch 50\u001b[0m\n",
      "\u001b[34m[2022-01-07 12:29:09.047] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 149, \"duration\": 12, \"num_examples\": 1, \"num_bytes\": 45468}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] # Finished training epoch 50 on 19 examples from 1 batches, each of size 256.\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Metrics for Training:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) total: 0.48768603801727295\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) kld: 0.0015352867776528\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) recons: 0.4861507713794708\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Loss (name: value) logppx: 0.48768603801727295\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #quality_metric: host=algo-1, epoch=50, train total_loss <loss>=0.48768603801727295\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] patience losses:[0.4916303753852844, 0.4895123243331909, 0.49034103751182556, 0.49005651473999023, 0.48871102929115295, 0.48895934224128723, 0.4889758825302124, 0.4879416823387146, 0.48743119835853577, 0.4876335859298706] min patience loss:0.48743119835853577 current loss:0.48768603801727295 absolute loss difference:0.0002548396587371826\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Bad epoch: loss has not improved (enough). Bad count:9\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Timing: train: 0.01s, val: 0.00s, epoch: 0.01s\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558549.0350997, \"EndTime\": 1641558549.0491323, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 49, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 950.0, \"count\": 1, \"min\": 950, \"max\": 950}, \"Total Batches Seen\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"Max Records Seen Between Resets\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 100.0, \"count\": 1, \"min\": 100, \"max\": 100}, \"Number of Records Since Last Reset\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] #throughput_metric: host=algo-1, train throughput=1342.5616766063547 records/second\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 WARNING 140562606671680] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Best model based on early stopping at epoch 48. Best loss: 0.48743119835853577\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Topics from epoch:final (num_topics:10) [wetc 0.22, tu 0.51]:\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.20, 0.58] chatbot l fairness digitalisation n robo half deploy believe workshop comfortable worry spot explainability employment entry strongly australia enhancing conducted\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.26, 0.40] q l chatbot n fairness eu microsoft employment al f discrimination accountability india society administration popular guideline funding h ethic\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.19, 0.41] q swiss eu funding n guideline rising digitalisation carrier supervisor broker strongly modeling blockchain administration h committee segmentation ethic l\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.29, 0.56] n root renewal none popular spot employment monthly f insure tell correctly braking something fear decade guideline pooling l half\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.22, 0.73] n ethic strongly unlocking manufacturing bottom discovery carrier chatbot comfortable novel spend recommend employment hospital achieved usd entity developer intended\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.19, 0.39] f guideline carrier conversational l n swiss eu aviva q comfortable gdpr committee prefer broker chatbot fintech employment supervisor microsoft\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.21, 0.34] q n microsoft swiss l digitalisation fairness supervisor carrier h discrimination robo none note chart guideline f committee regression popular\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.21, 0.47] n l f q worry spot decade none renewal insure lemonade carrier microsoft researcher london swiss respondent calculate prefer root\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.18, 0.69] proposition digitalisation exhibit percent architecture swiss augmented division comfortable cycle n note bot operational discrimination supervisor cybersecurity hard exposure augment\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] [0.23, 0.52] q n respondent eu f l percent talent conventional swiss funding employment discrimination investor h algorithmic linear false architecture leveraged\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Serializing model to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Saved checkpoint to \"/tmp/tmp1ysd25mo/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[01/07/2022 12:29:09 INFO 140562606671680] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1641558539.43196, \"EndTime\": 1641558549.1033332, \"Dimensions\": {\"Algorithm\": \"AWS/NTM\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 8778.454303741455, \"count\": 1, \"min\": 8778.454303741455, \"max\": 8778.454303741455}, \"epochs\": {\"sum\": 50.0, \"count\": 1, \"min\": 50, \"max\": 50}, \"early_stop.time\": {\"sum\": 92.86308288574219, \"count\": 50, \"min\": 0.015020370483398438, \"max\": 5.742311477661133}, \"update.time\": {\"sum\": 814.5487308502197, \"count\": 50, \"min\": 11.269569396972656, \"max\": 59.26847457885742}, \"finalize.time\": {\"sum\": 49.254417419433594, \"count\": 1, \"min\": 49.254417419433594, \"max\": 49.254417419433594}, \"model.serialize.time\": {\"sum\": 3.8247108459472656, \"count\": 1, \"min\": 3.8247108459472656, \"max\": 3.8247108459472656}, \"setuptime\": {\"sum\": 38.057565689086914, \"count\": 1, \"min\": 38.057565689086914, \"max\": 38.057565689086914}, \"totaltime\": {\"sum\": 9740.923404693604, \"count\": 1, \"min\": 9740.923404693604, \"max\": 9740.923404693604}}}\u001b[0m\n",
      "\n",
      "2022-01-07 12:29:20 Uploading - Uploading generated training model\n",
      "2022-01-07 12:29:20 Completed - Training job completed\n",
      "Training seconds: 90\n",
      "Billable seconds: 90\n"
     ]
    }
   ],
   "source": [
    "ntm.fit(inputs={'train': s3_training_path,\n",
    "               'auxiliary': 's3://{}/{}'.format(bucket,s3_auxiliary_path)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://mymltextarticles/training/output/ntm-2022-01-07-12-25-30-277/output/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "print(ntm.model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load trained model\n",
    "This will only work if the training completed successfully and the model is saved under the assumed location, you can change it if required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "s3_output='s3://mymltextarticles/training/output/ntm-2022-01-07-12-25-30-277/output/model.tar.gz'\n",
    "role = get_execution_role()\n",
    "ntm=sagemaker.NTMModel(s3_output, role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------!"
     ]
    }
   ],
   "source": [
    "ntm_predictor = ntm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.large')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prediction using training data\n",
    "This assumes that a data frame and a dictionary have been prepared using the cells above for data preparation\n",
    "It also assumes the model has been loaded and the endpoint has been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_samples(data, dictionary):\n",
    "    num_lines = data.shape[0]\n",
    "    num_columns = len(dictionary)\n",
    "    line=0\n",
    "    sample_matrix = np.zeros((num_lines, num_columns)).astype('float32')\n",
    "    for _, row in data.iterrows():\n",
    "        for token_id, token_count in row['tokens']:\n",
    "            sample_matrix[line, token_id] = token_count\n",
    "        line+=1\n",
    "\n",
    "    return sample_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1024)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples= prepare_samples(data, dictionary)\n",
    "samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictions': [{'topic_weights': [0.0936905593, 0.1389863044, 0.0947074741, 0.1033818498, 0.0892445073, 0.0907449126, 0.1050878465, 0.1209227517, 0.0732400194, 0.0899937451]}, {'topic_weights': [0.094230704, 0.1395254433, 0.093277961, 0.1066259518, 0.0872382745, 0.0872918889, 0.1035153791, 0.1277225912, 0.0706498697, 0.0899219587]}, {'topic_weights': [0.093900241, 0.1317909658, 0.0963539556, 0.1017725244, 0.0913976729, 0.0924328193, 0.1046868265, 0.1172470003, 0.0786318928, 0.091786094]}, {'topic_weights': [0.094006829, 0.1450598538, 0.0934277847, 0.1054858118, 0.086159043, 0.0875128135, 0.1048642397, 0.1266106516, 0.0677180365, 0.0891548917]}, {'topic_weights': [0.0937443972, 0.1378456056, 0.095006831, 0.1030102074, 0.0895280838, 0.0911481157, 0.1050277352, 0.1202597693, 0.0741740316, 0.0902552977]}, {'topic_weights': [0.0936863124, 0.1390734464, 0.0946850777, 0.1034097373, 0.0892229304, 0.090714395, 0.1050925925, 0.120972313, 0.0731692314, 0.0899739414]}, {'topic_weights': [0.0939848274, 0.1315651536, 0.0965988487, 0.10100279, 0.091136083, 0.0933054388, 0.1046159938, 0.116604127, 0.0795118734, 0.091674909]}, {'topic_weights': [0.0939243361, 0.1461472213, 0.0916947126, 0.1082606465, 0.0857118219, 0.085280247, 0.1040154845, 0.1306664497, 0.0657837167, 0.0885153636]}, {'topic_weights': [0.0940074101, 0.1440491676, 0.0936925039, 0.1050821915, 0.0865867212, 0.0880182087, 0.1048052683, 0.1257834136, 0.0686343536, 0.0893407837]}, {'topic_weights': [0.0936872959, 0.1390536577, 0.0946904123, 0.1034031957, 0.0892276838, 0.0907213986, 0.1050916612, 0.1209608391, 0.0731852725, 0.0899785683]}, {'topic_weights': [0.0937080607, 0.1386536658, 0.0947964787, 0.1032728925, 0.0893233493, 0.0908608735, 0.1050698534, 0.1207332909, 0.0735099316, 0.0900716782]}, {'topic_weights': [0.0937898234, 0.1408807486, 0.0943230018, 0.1040124595, 0.0883116648, 0.0897750109, 0.10501872, 0.1226325259, 0.0715242252, 0.0897317454]}, {'topic_weights': [0.0936867893, 0.1390817761, 0.0946834311, 0.1034125015, 0.0892187506, 0.0907101035, 0.1050922871, 0.1209798828, 0.073161602, 0.0899728313]}, {'topic_weights': [0.0939547718, 0.1335726231, 0.0959680229, 0.1018009037, 0.0906294361, 0.0925262421, 0.1047212481, 0.1179491505, 0.0776918307, 0.091185756]}, {'topic_weights': [0.0936863199, 0.1390733719, 0.0946851075, 0.103409715, 0.0892229602, 0.0907144323, 0.1050925925, 0.1209722757, 0.0731693059, 0.0899739712]}, {'topic_weights': [0.0940567851, 0.1379723549, 0.0951495543, 0.1030816883, 0.0887584984, 0.0906228572, 0.104777813, 0.1213245764, 0.0737530589, 0.0905027911]}, {'topic_weights': [0.0939999074, 0.1271010935, 0.0974400789, 0.1000882611, 0.0928507149, 0.0943280756, 0.104214482, 0.1141287014, 0.0831805617, 0.0926680863]}, {'topic_weights': [0.0941229761, 0.1393712908, 0.0944880918, 0.1042495444, 0.0882353857, 0.0893992037, 0.1045266688, 0.1232366934, 0.0721377805, 0.090232335]}, {'topic_weights': [0.0937475041, 0.1523562968, 0.090323709, 0.1088945493, 0.0841194391, 0.0843541995, 0.1043812037, 0.1326250881, 0.0622567311, 0.0869412646]}, {'topic_weights': [0.0936863124, 0.1390734464, 0.0946850777, 0.1034097373, 0.0892229378, 0.090714395, 0.1050925925, 0.120972313, 0.0731692314, 0.0899739414]}]}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "#ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer\n",
    "\n",
    "\n",
    "ntm_predictor.serializer = sagemaker.serializers.CSVSerializer()\n",
    "response = ntm_predictor.predict(samples)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The json_deserializer has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09369056 0.1389863  0.09470747 0.10338185 0.08924451 0.09074491\n",
      "  0.10508785 0.12092275 0.07324002 0.08999375]\n",
      " [0.0942307  0.13952544 0.09327796 0.10662595 0.08723827 0.08729189\n",
      "  0.10351538 0.12772259 0.07064987 0.08992196]\n",
      " [0.09390024 0.13179097 0.09635396 0.10177252 0.09139767 0.09243282\n",
      "  0.10468683 0.117247   0.07863189 0.09178609]\n",
      " [0.09400683 0.14505985 0.09342778 0.10548581 0.08615904 0.08751281\n",
      "  0.10486424 0.12661065 0.06771804 0.08915489]\n",
      " [0.0937444  0.13784561 0.09500683 0.10301021 0.08952808 0.09114812\n",
      "  0.10502774 0.12025977 0.07417403 0.0902553 ]\n",
      " [0.09368631 0.13907345 0.09468508 0.10340974 0.08922293 0.0907144\n",
      "  0.10509259 0.12097231 0.07316923 0.08997394]\n",
      " [0.09398483 0.13156515 0.09659885 0.10100279 0.09113608 0.09330544\n",
      "  0.10461599 0.11660413 0.07951187 0.09167491]\n",
      " [0.09392434 0.14614722 0.09169471 0.10826065 0.08571182 0.08528025\n",
      "  0.10401548 0.13066645 0.06578372 0.08851536]\n",
      " [0.09400741 0.14404917 0.0936925  0.10508219 0.08658672 0.08801821\n",
      "  0.10480527 0.12578341 0.06863435 0.08934078]\n",
      " [0.0936873  0.13905366 0.09469041 0.1034032  0.08922768 0.0907214\n",
      "  0.10509166 0.12096084 0.07318527 0.08997857]\n",
      " [0.09370806 0.13865367 0.09479648 0.10327289 0.08932335 0.09086087\n",
      "  0.10506985 0.12073329 0.07350993 0.09007168]\n",
      " [0.09378982 0.14088075 0.094323   0.10401246 0.08831166 0.08977501\n",
      "  0.10501872 0.12263253 0.07152423 0.08973175]\n",
      " [0.09368679 0.13908178 0.09468343 0.1034125  0.08921875 0.0907101\n",
      "  0.10509229 0.12097988 0.0731616  0.08997283]\n",
      " [0.09395477 0.13357262 0.09596802 0.1018009  0.09062944 0.09252624\n",
      "  0.10472125 0.11794915 0.07769183 0.09118576]\n",
      " [0.09368632 0.13907337 0.09468511 0.10340971 0.08922296 0.09071443\n",
      "  0.10509259 0.12097228 0.07316931 0.08997397]\n",
      " [0.09405679 0.13797235 0.09514955 0.10308169 0.0887585  0.09062286\n",
      "  0.10477781 0.12132458 0.07375306 0.09050279]\n",
      " [0.09399991 0.12710109 0.09744008 0.10008826 0.09285071 0.09432808\n",
      "  0.10421448 0.1141287  0.08318056 0.09266809]\n",
      " [0.09412298 0.13937129 0.09448809 0.10424954 0.08823539 0.0893992\n",
      "  0.10452667 0.12323669 0.07213778 0.09023233]\n",
      " [0.0937475  0.1523563  0.09032371 0.10889455 0.08411944 0.0843542\n",
      "  0.1043812  0.13262509 0.06225673 0.08694126]\n",
      " [0.09368631 0.13907345 0.09468508 0.10340974 0.08922294 0.0907144\n",
      "  0.10509259 0.12097231 0.07316923 0.08997394]]\n"
     ]
    }
   ],
   "source": [
    "results = ntm_predictor.predict(samples)\n",
    "predictions = np.array([prediction['topic_weights'] for prediction in results['predictions']])\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "7\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "7\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "#response = json.loads(response)\n",
    "for r in response:\n",
    "    topic_vector=r.label['topic_weights'].float32_tensor.values\n",
    "    top_topic= np.argmax(topic_vector)\n",
    "    print(top_topic)\n",
    "    #vectors = [r['topic_mixture'] for r in response['predictions']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Topic ID')"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fs = 12\n",
    "df=pd.DataFrame(predictions.T)\n",
    "df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "plt.xlabel('Topic ID', fontsize=fs+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntm_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
