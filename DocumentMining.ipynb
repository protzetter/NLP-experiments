{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created on December 20th 2020 by Patrick Rotzetter\n",
    "\n",
    "https://www.linkedin.com/in/rotzetter/\n",
    "\n",
    "# Small experiment of document mining with various techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import main libraries\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import re\n",
    "from pptx import Presentation\n",
    "import pdftotext\n",
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validate spacy language models just in case\n",
    "!python -m spacy validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read PDF files using PYPdf2\n",
    "def readPdfFilePY(filename):\n",
    "    text=\"\"\n",
    "    read_pdf = PyPDF2.PdfFileReader(filename,'rb')   \n",
    "    for i in range(read_pdf.getNumPages()):\n",
    "        page = read_pdf.getPage(i)\n",
    "        txt=page.extractText()\n",
    "#        txt=txt.strip('\\n')\n",
    "#        txt=remove_special_characters(txt, remove_digits=True)\n",
    "        text=text+txt\n",
    "#    text = text.replace('\\n\\n', '\\n')\n",
    "#    text = text.replace('\\n \\n', ' ')\n",
    "#    text = text.replace('\\n', ' ')\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read PDF files using pdftotext\n",
    "def readPdfFile(filename):\n",
    "#   import subprocess\n",
    "#   try:\n",
    "#       extInfo = subprocess.check_output('pdftotext.exe '+filePath + ' -',shell=True,stderr=subprocess.STDOUT).strip()\n",
    "#   except Exception as e:\n",
    "#   print (e) \n",
    "    text=\"\"\n",
    "    with open(filename, \"rb\") as f:\n",
    "        pdf = pdftotext.PDF(f)\n",
    "        for page in pdf:\n",
    "            text=text+page\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to read PPT files\n",
    "def readPPTFile(filename):\n",
    "    text=\"\"  \n",
    "    prs = Presentation(filename)\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text=text+shape.text\n",
    "    text=remove_special_characters(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of first input file\n",
    "path='C:\\\\Users\\\\Patrick_Rotzetter\\\\OneDrive\\\\Documents\\\\samplesdocs\\\\'\n",
    "docFile = path+'Technology-and-innovation-in-the-insurance-sector.pdf' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read PDF files\n",
    "    \n",
    "textFromPdf=readPdfFile(docFile)\n",
    "len(textFromPdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textFromPdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path of second input file\n",
    "path='C:\\\\Users\\\\Patrick_Rotzetter\\\\OneDrive\\\\Documents\\\\samplesdocs\\\\'\n",
    "docFile = path+'Digital-disruption-in-Insurance.pdf' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read PDF files\n",
    "    \n",
    "textFromPdf2=readPdfFile(docFile)\n",
    "len(textFromPdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(textFromPdf2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process files with spacy and calculate their similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load spacy wtih large English model\n",
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us process the 2 files using spacy pipeline\n",
    "docpdf1=nlp(textFromPdf)\n",
    "docpdf2=nlp(textFromPdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us check the document vector shape\n",
    "docpdf1.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate document similarity with spacy function\n",
    "docpdf1.similarity(docpdf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# document similarity can also be calculated by multiplying the document vectors\n",
    "np.dot(docpdf1.vector,docpdf2.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process all directory files with spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to process documents in an apply function and return the nlp object\n",
    "def processDoc(doc):\n",
    "    return nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us scan the full directory, read PDF and PPT documents, clean them and process them with spacy\n",
    "\n",
    "docName=[]\n",
    "docType=[]\n",
    "docText=[]\n",
    "docNLP=[]\n",
    "import glob\n",
    "list_of_files = glob.glob(path+'*.pdf')           # create the list of file\n",
    "fileNames=[]\n",
    "for file_name in list_of_files:\n",
    "    fileText=readPdfFile(file_name)\n",
    "    docName.append(file_name)\n",
    "    docType.append('pdf')\n",
    "    docText.append(fileText)\n",
    "list_of_files = glob.glob(path+'*.pptx')           # create the list of file\n",
    "for file_name in list_of_files:\n",
    "    fileText=readPPTFile(file_name)\n",
    "    docName.append(file_name)\n",
    "    docType.append('ppt')\n",
    "    docText.append(fileText)\n",
    "fullDocs = pd.DataFrame({'Name':docName,'Type':docType,'Text':docText})\n",
    "fullDocs['cleanText']=hero.clean(fullDocs['Text'])\n",
    "fullDocs['NLP']=fullDocs['cleanText'].apply(processDoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " print (\"Average length of section text:\" + str((np.mean(fullDocs['Text'].str.len()))))\n",
    " print (\"Min length of section text:\" + str((np.min(fullDocs['Text'].str.len()))))\n",
    " print (\"Max length of section text:\" + str((np.max(fullDocs['Text'].str.len()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['text_word_count'] = fullDocs['Text'].apply(lambda x: len(x.strip().split()))  # word count in abstract\n",
    "fullDocs['text_unique_words']=fullDocs['Text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\n",
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word count and most common words and nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us look most used words for each part of speech\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "posCounts = defaultdict(Counter)\n",
    "\n",
    "for doc in fullDocs.NLP:\n",
    "    for token in doc:\n",
    "        posCounts[token.pos][token.orth] += 1\n",
    "\n",
    "for pos_id, counts in sorted(posCounts.items()):\n",
    "    pos = doc.vocab.strings[pos_id]\n",
    "    for orth_id, count in counts.most_common(1):\n",
    "        print(pos, count, doc.vocab.strings[orth_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add some common words to stop wordds that may not add much context\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "customStopWords = [\n",
    "    'insurance','insurer','customer','technology']\n",
    "\n",
    "for w in customStopWords:\n",
    "    if w not in STOP_WORDS:\n",
    "        STOP_WORDS.add(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most common nouns\n",
    "from collections import defaultdict, Counter\n",
    "from spacy.symbols import nsubj, VERB, dobj, NOUN, root, xcomp, PROPN, NUM,SYM\n",
    "# all tokens that arent stop words or punctuations\n",
    "words=[]\n",
    "for doc in fullDocs.NLP:\n",
    "#    words += [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True \\\n",
    "#              and token.is_space != True and token.pos != NUM and token.pos != SYM \\\n",
    "#              and token.text != '©' and (not token.text in STOP_WORDS)and (not token.lemma_ in STOP_WORDS)\\\n",
    "#              and token.text not in punctuations]\n",
    "    words += [token.lemma_ for token in doc if token.pos==NOUN and (not token.lemma_ in STOP_WORDS)] \n",
    "\n",
    "\n",
    "# five most common tokens\n",
    "wordFreq = Counter(words)\n",
    "commonWords = wordFreq.most_common(20)\n",
    "print(commonWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in commonWords[:20]:\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most common proper nouns\n",
    "from collections import defaultdict, Counter\n",
    "from spacy.symbols import nsubj, VERB, dobj, NOUN, root, xcomp, PROPN, NUM,SYM\n",
    "# all tokens that arent stop words or punctuations\n",
    "words=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    \n",
    "    words += [token.lemma_ for token in doc if token.pos==PROPN and (not token.lemma_ in STOP_WORDS)\\\n",
    "             and (len(token)>1)] \n",
    "\n",
    "\n",
    "# five most common tokens\n",
    "wordFreq = Counter(words)\n",
    "commonWords = wordFreq.most_common(20)\n",
    "print(commonWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in commonWords[:20]:\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequest ORG entities\n",
    "entities=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    # all entities\n",
    "    for ent in doc.ents:\n",
    "        #print(ent.text)\n",
    "        if ent.label_ == 'ORG':\n",
    "            if ent.text not in STOP_WORDS:\n",
    "                entities += [ent.text]\n",
    "entityFreq = Counter(entities)\n",
    "commonEntities = entityFreq.most_common(40)\n",
    "print(commonEntities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in commonEntities[:20]:\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequest PERSON entities\n",
    "entities=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    # all entities\n",
    "    for ent in doc.ents:\n",
    "        #print(ent.text)\n",
    "        if ent.label_ == 'PERSON':\n",
    "            if ent.text not in STOP_WORDS:\n",
    "                entities += [ent.text]\n",
    "entityFreq = Counter(entities)\n",
    "commonEntities = entityFreq.most_common(40)\n",
    "print(commonEntities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in commonEntities[:20]:\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let us test TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import string\n",
    "vectorizer = TfidfVectorizer(max_features=4096)\n",
    "ents=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    # all entities\n",
    "    words=[]\n",
    "    words += [token.lemma_ for token in doc if token.is_stop != True and token.is_punct != True \\\n",
    "              and token.is_space != True and token.pos != NUM and token.pos != SYM \\\n",
    "              and token.text != '©' and (not token.text in STOP_WORDS)and (not token.lemma_ in STOP_WORDS)]\n",
    "    ents.append(' '.join(words))\n",
    "entitiesdf=pd.DataFrame(ents)\n",
    "entitiesdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return them with their corresponding feature names.'''\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_features_in_document(mat, features, row_id, top_n=25):\n",
    "    row = np.squeeze(mat[row_id].toarray())\n",
    "    return top_tfidf_feats(row, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_mean_feats(Xtr, features, grp_ids=None, min_tfidf=0.2, top_n=25):\n",
    "    ''' Return the top n features that on average are most important amongst documents in rows\n",
    "        indentified by indices in grp_ids. '''\n",
    "    if grp_ids:\n",
    "        D = Xtr[grp_ids].toarray()\n",
    "    else:\n",
    "        D = Xtr.toarray()\n",
    "\n",
    "    D[D < min_tfidf] = 0\n",
    "    tfidf_means = np.mean(D, axis=0)\n",
    "    return top_tfidf_feats(tfidf_means, features, top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the vectorizer with new settings and check the new vocabulary length\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "cvec = CountVectorizer(stop_words='english', min_df=.0025, max_df=.1, ngram_range=(1,1))\n",
    "cvec_counts=cvec.fit_transform(entitiesdf[entitiesdf.columns[0]])\n",
    "len(cvec.vocabulary_)\n",
    "print ('sparse matrix shape:', cvec_counts.shape)\n",
    "print ('nonzero count:', cvec_counts.nnz)\n",
    "print (' sparsity: %.2f%%' % (100.0 * cvec_counts.nnz / (cvec_counts.shape[0] * cvec_counts.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ = np.asarray(cvec_counts.sum(axis=0)).ravel().tolist()\n",
    "counts_df = pd.DataFrame({'term': cvec.get_feature_names(), 'occurrences': occ})\n",
    "counts_df.sort_values(by='occurrences', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = TfidfTransformer()\n",
    "transformed_weights = transformer.fit_transform(cvec_counts)\n",
    "transformed_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.asarray(transformed_weights.mean(axis=0)).ravel().tolist()\n",
    "weights_df = pd.DataFrame({'term': cvec.get_feature_names(), 'weight': weights})\n",
    "weights_df.sort_values(by='weight', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_weights.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entitiesdf.head()\n",
    "dfv = vectorizer.fit_transform(entitiesdf[entitiesdf.columns[0]]) \n",
    "scores = (dfv.toarray()) \n",
    "print(\"\\n\\nScores : \\n\", scores) \n",
    "dfv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#warray = transformed_weights.toarray()\n",
    "warray = dfv.toarray()\n",
    "warray = warray-np.mean(warray,axis=0)\n",
    "evecs,evalues,V0 =np.linalg.svd(transformed_weights.toarray())\n",
    "evecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=5, random_state=42)\n",
    "dfvRed= pca.fit_transform(warray)\n",
    "dfvRed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from scipy.spatial.distance import cdist\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# run kmeans with many different k\n",
    "distortions = []\n",
    "K = range(2, 10)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k, random_state=42).fit(dfvRed)\n",
    "    #kmeanModel.fit(dfvRed)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "    print('Processed {} clusters'.format(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
    "pred_y = kmeans.fit_predict(dfvRed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter3D(dfvRed[:,0], dfvRed[:,1],dfvRed[:,2], c=pred_y);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(dfvRed[:,0], dfvRed[:,1],c=pred_y)\n",
    "plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs[pred_y==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us compare documents similarities using spacy similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityList=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    docSimilarity=[]\n",
    "    for compDoc in fullDocs.NLP:\n",
    "        docSimilarity.append(doc.similarity(compDoc))\n",
    "    similarityList.append(docSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityDF= pd.DataFrame(data=similarityList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pcolor(similarityDF)\n",
    "plt.yticks(np.arange(0.5, len(similarityDF.index), 1), similarityDF.index)\n",
    "plt.xticks(np.arange(0.5, len(similarityDF.columns), 1), similarityDF.columns)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "ax = sns.heatmap(similarityDF,linewidths=.5,center=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us use vector norm to calculate their distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityList=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    docSimilarity=[]\n",
    "    for compDoc in fullDocs.NLP:\n",
    "        #docSimilarity.append(np.dot(doc.vector,compDoc.vector))\n",
    "        docSimilarity.append(np.linalg.norm(doc.vector-compDoc.vector))\n",
    "    similarityList.append(docSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityDFVec= pd.DataFrame(data=similarityList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityDFVec.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "\n",
    "ax = sns.heatmap(similarityDFVec,linewidths=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us use entity vector average  to calculate similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# most frequestentities\n",
    "entities=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    # all entities\n",
    "    for ent in doc.ents:\n",
    "#        for token in ent:\n",
    "            if ent.label_!= 'MONEY' and ent.label_!= 'CARDINAL' and ent.label_!= 'QUANTITY' \\\n",
    "            and ent.label_!= 'PERCENT' and ent.label_!= 'TIME' and ent.label_!= 'ORDINAL' \\\n",
    "            and ent.label_!= 'DATE' and ent.label_!= 'PERSON' and ent.label_!= 'GPE':\n",
    "                #print(token.ent_type_)\n",
    "                if ent.text not in STOP_WORDS:\n",
    "                    entities += [ent.text]\n",
    "entityFreq = Counter(entities)\n",
    "commonEntities = entityFreq.most_common(40)\n",
    "print(commonEntities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageEntityVector(doc):\n",
    "    vectorAvg = 0\n",
    "    i=0\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_!= 'MONEY' and ent.label_!= 'CARDINAL' and ent.label_!= 'QUANTITY' \\\n",
    "        and ent.label_!= 'PERCENT' and ent.label_!= 'TIME' and ent.label_!= 'ORDINAL' \\\n",
    "        and ent.label_!= 'DATE' and ent.label_!= 'PERSON' and ent.label_!= 'GPE':\n",
    "            if ent.text not in STOP_WORDS:\n",
    "                vectorAvg += ent.vector\n",
    "                i+=1\n",
    "    if i>0:\n",
    "        vectorAvg=vectorAvg/i\n",
    "    else:\n",
    "        vectotAvg=float('Inf')\n",
    "    return vectorAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average entity vector for documents\n",
    "entitiesVec=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    # all entities\n",
    "    vectorAvg=getAverageEntityVector(doc)\n",
    "    entitiesVec.append(vectorAvg)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all documents using entityt similarity vectors\n",
    "similarityList=[]\n",
    "for ents in entitiesVec:\n",
    "    docSimilarity=[]\n",
    "    for entComp in entitiesVec:\n",
    "        #docSimilarity.append(np.dot(doc.vector,compDoc.vector))\n",
    "        docSimilarity.append(np.linalg.norm(ents-entComp))\n",
    "    similarityList.append(docSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityDFVecAvg= pd.DataFrame(data=similarityList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "mask = np.zeros_like(similarityDF, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "ax = sns.heatmap(similarityDFVecAvg,linewidths=.5, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchClosestDocumentDocLevel(text):\n",
    "    searchQueryNLP=nlp(text)\n",
    "    simQuery=[]\n",
    "    for i,doc in fullDocs.iterrows():\n",
    "        simQuery.append((np.linalg.norm(doc.NLP.vector-searchQueryNLP.vector),i))\n",
    "    simQuery.sort()\n",
    "    idMin=simQuery[0][1]\n",
    "    return idMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchClosestDocumentEntLevel(text):\n",
    "    searchQueryNLP=nlp(text)\n",
    "    simQuery=[]\n",
    "    for i,doc in fullDocs.iterrows():\n",
    "        simQuery.append((np.linalg.norm(getAverageEntityVector(doc.NLP)-getAverageEntityVector(searchQueryNLP)),i))\n",
    "    simQuery.sort()\n",
    "    idMin=simQuery[0][1]\n",
    "    return idMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='LOOKING FOR INNOVATION IN CLAIMS PROCESSING'\n",
    "id= searchClosestDocumentDocLevel(text)\n",
    "fullDocs.iloc[id].Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='we need to submit a proposal for a master data management system'\n",
    "id= searchClosestDocumentDocLevel(text)\n",
    "fullDocs.iloc[id].Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='we need to submit a proposal for claim notification'\n",
    "id= searchClosestDocumentEntLevel(text)\n",
    "fullDocs.iloc[id].Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='we need to submit a proposal  for broker risk placement'\n",
    "id= searchClosestDocumentEntLevel(text)\n",
    "fullDocs.iloc[id].Name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us use sentences vector  to calculate similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAverageSentenceVector(doc):\n",
    "    vectorAvg = 0\n",
    "    i=0\n",
    "    for sent in doc.sents:\n",
    "        vectorAvg += sent.vector\n",
    "        i+=1\n",
    "    if i>0:\n",
    "        vectorAvg=vectorAvg/i\n",
    "    else:\n",
    "        vectorAvg=float('Inf')\n",
    "    return vectorAvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate average entity vector for documents\n",
    "entitiesVec=[]\n",
    "for doc in fullDocs.NLP:\n",
    "    # all entities\n",
    "    vectorAvg=getAverageSentenceVector(doc)\n",
    "    entitiesVec.append(vectorAvg)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare all documents using sentence similarity vectors\n",
    "similarityList=[]\n",
    "for ents in entitiesVec:\n",
    "    docSimilarity=[]\n",
    "    for entComp in entitiesVec:\n",
    "        #docSimilarity.append(np.dot(doc.vector,compDoc.vector))\n",
    "        docSimilarity.append(np.linalg.norm(ents-entComp))\n",
    "    similarityList.append(docSimilarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarityDFVecAvg= pd.DataFrame(data=similarityList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "mask = np.zeros_like(similarityDF, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "ax = sns.heatmap(similarityDFVecAvg,linewidths=.5, mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchClosestDocumentSentLevel(text):\n",
    "    searchQueryNLP=nlp(text)\n",
    "    simQuery=[]\n",
    "    for i,doc in fullDocs.iterrows():\n",
    "        simQuery.append((np.linalg.norm(getAverageSentenceVector(doc.NLP)-getAverageSentenceVector(searchQueryNLP)),i))\n",
    "    simQuery.sort()\n",
    "    idMin=simQuery[0][1]\n",
    "    return idMin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='What are innovations in underwriting ?'\n",
    "id= searchClosestDocumentSentLevel(text)\n",
    "fullDocs.iloc[id].Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='What about claim notification.'\n",
    "id= searchClosestDocumentSentLevel(text)\n",
    "fullDocs.iloc[id].Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in nlp(text).sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us try texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import texthero as hero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['cleantext']=hero.clean(fullDocs.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['tfidf_cleantext'] = hero.tfidf(fullDocs['cleantext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['pca_tfidf_clean_text'] = hero.pca(fullDocs['tfidf_cleantext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hero.scatterplot(fullDocs, col='pca_tfidf_clean_text',title=\"Document\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hero.kmeans(fullDocs['pca_tfidf_clean_text'] , n_clusters=5, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=- 1, algorithm='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What about NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_to_sentences(data):\n",
    "    \"\"\"\n",
    "    Split data by linebreak \"\\n\"\n",
    "    \n",
    "    Args:\n",
    "        data: str\n",
    "    \n",
    "    Returns:\n",
    "        A list of sentences\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    sentences = data.split(\"\\n\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Additional clearning (This part is already implemented)\n",
    "    # - Remove leading and trailing spaces from each sentence\n",
    "    # - Drop sentences if they are empty strings.\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    sentences = [s for s in sentences if len(s) > 0]\n",
    "    \n",
    "    return sentences    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(sentences):\n",
    "    \"\"\"\n",
    "    Tokenize sentences into tokens (words)\n",
    "    \n",
    "    Args:\n",
    "        sentences: List of strings\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the list of lists of tokenized sentences\n",
    "    tokenized_sentences = []\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "    \n",
    "    # Go through each sentence\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Convert to lowercase letters\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Convert into a list of words\n",
    "        tokenized = nltk.word_tokenize(sentence)\n",
    "        \n",
    "        # append the list of words to the list of lists\n",
    "        tokenized_sentences.append(tokenized)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs[\"nltkText\"]=fullDocs.Text.apply(split_to_sentences).apply(tokenize_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
    "    \"\"\"\n",
    "    Find the words that appear N times or more\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of sentences\n",
    "        count_threshold: minimum number of occurrences for a word to be in the closed vocabulary.\n",
    "    \n",
    "    Returns:\n",
    "        List of words that appear N times or more\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to contain the words that\n",
    "    # appear at least 'minimum_freq' times.\n",
    "    closed_vocab = []\n",
    "    \n",
    "    # Get the word couts of the tokenized sentences\n",
    "    # Use the function that you defined earlier to count the words\n",
    "    word_counts = count_words(tokenized_sentences)\n",
    "    \n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # for each word and its count\n",
    "    for word, cnt in word_counts.items(): # complete this line\n",
    "        \n",
    "        # check that the word's count\n",
    "        # is at least as great as the minimum count\n",
    "        if cnt>= count_threshold:\n",
    "            \n",
    "            # append the word to the list\n",
    "            closed_vocab.append(word)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return closed_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
    "    \"\"\"\n",
    "    Replace words not in the given vocabulary with '<unk>' token.\n",
    "    \n",
    "    Args:\n",
    "        tokenized_sentences: List of lists of strings\n",
    "        vocabulary: List of strings that we will use\n",
    "        unknown_token: A string representing unknown (out-of-vocabulary) words\n",
    "    \n",
    "    Returns:\n",
    "        List of lists of strings, with words not in the vocabulary replaced\n",
    "    \"\"\"\n",
    "    \n",
    "    # Place vocabulary into a set for faster search\n",
    "    vocabulary = set(vocabulary)\n",
    "    \n",
    "    # Initialize a list that will hold the sentences\n",
    "    # after less frequent words are replaced by the unknown token\n",
    "    replaced_tokenized_sentences = []\n",
    "    \n",
    "    # Go through each sentence\n",
    "    for sentence in tokenized_sentences:\n",
    "        \n",
    "        # Initialize the list that will contain\n",
    "        # a single sentence with \"unknown_token\" replacements\n",
    "        replaced_sentence = []\n",
    "        ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "        # for each token in the sentence\n",
    "        for token in sentence: # complete this line\n",
    "            \n",
    "            # Check if the token is in the closed vocabulary\n",
    "            if token in vocabulary: # complete this line\n",
    "                # If so, append the word to the replaced_sentence\n",
    "                replaced_sentence.append(token)\n",
    "            else:\n",
    "                # otherwise, append the unknown token instead\n",
    "                replaced_sentence.append(unknown_token)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Append the list of tokens to the list of lists\n",
    "        replaced_tokenized_sentences.append(replaced_sentence)\n",
    "        \n",
    "    return replaced_tokenized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data, test_data, count_threshold):\n",
    "    \"\"\"\n",
    "    Preprocess data, i.e.,\n",
    "        - Find tokens that appear at least N times in the training data.\n",
    "        - Replace tokens that appear less than N times by \"<unk>\" both for training and test data.        \n",
    "    Args:\n",
    "        train_data, test_data: List of lists of strings.\n",
    "        count_threshold: Words whose count is less than this are \n",
    "                      treated as unknown.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of\n",
    "        - training data with low frequent words replaced by \"<unk>\"\n",
    "        - test data with low frequent words replaced by \"<unk>\"\n",
    "        - vocabulary of words that appear n times or more in the training data\n",
    "    \"\"\"\n",
    "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
    "\n",
    "    # Get the closed vocabulary using the train data\n",
    "    vocabulary = get_words_with_nplus_frequency(train_data,count_threshold)\n",
    "    \n",
    "    # For the train data, replace less common words with \"<unk>\"\n",
    "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
    "    \n",
    "    # For the test data, replace less common words with \"<unk>\"\n",
    "    test_data_replaced = replace_oov_words_by_unk(test_data,vocabulary)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return train_data_replaced, test_data_replaced, vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let us try Azure Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#just a quick check that everything is working correctly\n",
    "\n",
    "credential = AzureKeyCredential(\"acdb3322a2fc49edb80a228889abefb9\")\n",
    "endpoint=\"https://textanalyticspr.cognitiveservices.azure.com/\"\n",
    "\n",
    "text_analytics_client = TextAnalyticsClient(endpoint, credential)\n",
    "\n",
    "documents = [\n",
    "    \"I did not like the restaurant. The food was too spicy.\",\n",
    "    \"The restaurant was decorated beautifully. The atmosphere was unlike any other restaurant I've been to.\",\n",
    "    \"The food was yummy. :)\"\n",
    "]\n",
    "\n",
    "response = text_analytics_client.analyze_sentiment(documents, language=\"en\")\n",
    "result = [doc for doc in response if not doc.is_error]\n",
    "\n",
    "for doc in result:\n",
    "    print(\"Overall sentiment: {}\".format(doc.sentiment))\n",
    "    print(\"Scores: positive={}; neutral={}; negative={} \\n\".format(\n",
    "        doc.confidence_scores.positive,\n",
    "        doc.confidence_scores.neutral,\n",
    "        doc.confidence_scores.negative,\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us create the entity recognition endpoint\n",
    "text_analytics_client = TextAnalyticsClient(endpoint, credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fullDocs['cleanText'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let us get all entities looping through all sentences\n",
    "response=[]\n",
    "for sent in sentences:\n",
    "    response.append(text_analytics_client.recognize_entities([sent], language=\"en\",logging_enable=True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all entity texts\n",
    "entities=[]\n",
    "for res in response:\n",
    "    result = [doc for doc in res if not doc.is_error]\n",
    "    for doc in result:\n",
    "        for entity in doc.entities:\n",
    "            entities.append(entity.text)\n",
    "#            print(\"Entity: \\t\", entity.text, \"\\tCategory: \\t\", entity.category,\n",
    "#                  \"\\tConfidence Score: \\t\", entity.confidence_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityFreq = Counter(entities)\n",
    "commonEntities = entityFreq.most_common(40)\n",
    "print(commonEntities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in commonEntities[:20]:\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect all entity categories\n",
    "entities=[]\n",
    "for res in response:\n",
    "    result = [doc for doc in res if not doc.is_error]\n",
    "    for doc in result:\n",
    "        for entity in doc.entities:\n",
    "            if entity.category=='Skill':\n",
    "                entities.append(entity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entityFreq = Counter(entities)\n",
    "commonEntities = entityFreq.most_common(40)\n",
    "print(commonEntities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "x, y= [], []\n",
    "for word,count in commonEntities[:20]:\n",
    "        x.append(word)\n",
    "        y.append(count)\n",
    "        \n",
    "sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A few more experiments with wordcloud and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "import wordcloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = ' '.join(papers['title_processed'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wc = wordcloud.WordCloud()\n",
    "\n",
    "# Generate a word cloud\n",
    "wc.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wc.to_image()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the library with the CountVectorizer method\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Helper function\n",
    "def plot_10_most_common_words(count_data, count_vectorizer):\n",
    "    import matplotlib.pyplot as plt\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    total_counts = np.zeros(len(words))\n",
    "    for t in count_data:\n",
    "        total_counts+=t.toarray()[0]\n",
    "    \n",
    "    count_dict = (zip(words, total_counts))\n",
    "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
    "    words = [w[0] for w in count_dict]\n",
    "    counts = [w[1] for w in count_dict]\n",
    "    x_pos = np.arange(len(words)) \n",
    "\n",
    "    plt.bar(x_pos, counts,align='center')\n",
    "    plt.xticks(x_pos, words, rotation=90) \n",
    "    plt.xlabel('words')\n",
    "    plt.ylabel('counts')\n",
    "    plt.title('10 most common words')\n",
    "    plt.show()\n",
    "\n",
    "# Initialise the count vectorizer with the English stop words\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the processed titles\n",
    "count_data = count_vectorizer.fit_transform(papers['title_processed'])\n",
    "\n",
    "# Visualise the 10 most common words\n",
    "plot_10_most_common_words(count_data, count_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "# Load the LDA model from sk-learn\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    " \n",
    "# Helper function\n",
    "def print_topics(model, count_vectorizer, n_top_words):\n",
    "    words = count_vectorizer.get_feature_names()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"\\nTopic #%d:\" % topic_idx)\n",
    "        print(\" \".join([words[i]\n",
    "                        for i in topic.argsort()[:-n_top_words - 1:-1]]))\n",
    "        \n",
    "# Tweak the two parameters below (use int values below 15)\n",
    "number_topics = 10\n",
    "number_words = 10\n",
    "\n",
    "# Create and fit the LDA model\n",
    "lda = LDA(n_components=number_topics)\n",
    "lda.fit(count_data)\n",
    "\n",
    "# Print the topics found by the LDA model\n",
    "print(\"Topics found via LDA:\")\n",
    "print_topics(lda, count_vectorizer, number_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
